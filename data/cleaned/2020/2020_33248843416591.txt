multi task learning based pre trained language model for code completion fang liu key lab of high confidence software technology moe peking university beijing china liufang816 pku.edu.cnge li key lab of high confidence software technology moe peking university beijing china lige pku.edu.cn yunfei zhao key lab of high confidence software technology moe peking university beijing china zhaoyunfei pku.edu.cnzhi jin key lab of high confidence software technology moe peking university beijing china zhijin pku.edu.cn abstract code completion is one of the most useful features in the integrated development environments ides which can accelerate software development by suggesting the next probable token based on the contextual code in real time.
recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large scale software repositories.
however these models suffer from two major drawbacks a existing research uses static embeddings which map a word to the same vector regardless of its context.
the differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation b existing language model based code completion models perform poor on completing identifiers and the type information of the identifiers is ignored in most of these models.
to address these challenges in this paper we develop a multi task learning based pre trained language model for code understanding and code generation with a transformer based neural architecture.
we pre train it with hybrid objective functions that incorporate both code understanding and code generation tasks.
then we fine tune the pre trained model on code completion.
during the completion our model does not directly predict the next token.
instead we adopt multi task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction.
experiments results on two real world datasets demonstrate the effectiveness of our model when compared with state of the art methods.
corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn .
.
.
.
concepts computing methodologies artificial intelligence software and its engineering software maintenance tools .
keywords code completion multi task learning pre trained language model transformer networks acm reference format fang liu ge li yunfei zhao and zhi jin.
.
multi task learning based pre trained language model for code completion.
in 35th ieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
introduction as the complexity and scale of the software development continue to grow large corpora of open source software projects present an opportunity for modeling source code on machine learning .
most of these approaches are based on the observation of source code s naturalness that is source code is written by humans and for humans to read it displays some of the statistical properties as natural language.
thus statistical language models have been used for source code modeling benefiting many software engineering tasks including code summarization code clone detection program repair especially in code completion .
code completion is an essential feature of integrated development environments ides .
it speeds up the process of software development by suggesting the next probable token based on existing code.
in recent years as the success of deep learning recurrent neural network rnn based language models have been applied to source code modeling .
in these models a piece of source code is represented as a source code token sequence or an abstract syntactic tree ast node sequence.
given a partial code sequence the model computes the probability of the next token or ast node and recommends the one with the highest probability.
furthermore these language models can also learn useful word embeddings which can be used for other downstream tasks in the same way as word2vec style embeddings .
however source code has some special properties which have not been exploited in 35th ieee acm international conference on automated software engineering ase ase september virtual event australia liu et al.
1p u b l i c long getmaximumtime ioeventtype type i f !
timermanager .
c o n t a i n s k e y type throw new i l l e g a l a r g u m e n t e x c e p t i o n p l e a s e add t h i s e v ent f i r s t .
r e t u r n timermanager .
g e t type .
getmaximum code a java method example.
figure java ide completion example.
existing statistical language models.
we discuss two critical issues in detail below.
the contextual information is not well considered in the existing code completion models.
writing clean and readable code that conforms to the specification has been paid more attention in software development which helps the developers reuse and maintain the code.
when programming developers tend to use meaningful and conventional identifier names and natural language documentation .
as a result information contained in the source code can be exploited by machine learning algorithms.
most of these models are based on learned representations called embeddings which transform words into a continuous vector space .
however existing research uses static embeddings which map a word to the same vector regardless of its context.
for example in java method overloading the same function name can have different meanings based on the number and type of the parameters.
however the static embedding will map it to the same vector.
the differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation.
the surrounding tokens of the program entities usually contain certain information that reflects the roles of the entities.
for instance for a method name the surrounding tokens might include the variables fields methods that are used accessed invoked to implement the method.
taking the java method in code as an example the function name getmaximumtime can be inferred from the variables names and method calls in the body e.g.
getmaximum timermanager.
these tokens provide information about possible values the function could take and so should affect its representation.
identifier completions are challenging and existing statistical language model lm based code completion models perform poorly on completing identifiers.
these approaches consider every token in the source code file as targets for completion.
morethan two thirds of the completions do not refer to identifiers.
instead the majority concern punctuation like tokens e.g.
operators braces which are much easier to complete than identifiers but these completions are not that beneficial to developers .
besides the type information of the identifiers is ignored in most of the models.
modern ides for most languages heavily rely on types to make helpful suggestions for completing partial code.
for example when accessing the field of an object in a java ide code completion suggests suitable field names based on the object s type .
taking the code completion example of a java ide itellij idea in figure as an example the ide suggests scanner as the next token based on its type i.e.
java.util and not just predict the frequent token in the corpus.
for those dynamic languages such as python and javascript ides often fail to make accurate suggestions because the types of code elements are unknown which further demonstrates the importance of the type information.
however most of the existing lm based source code modeling techniques and code completion studies do not take the type information into consideration.
in response to the observations and concerns raised above we have developed a code understanding and generation pre trained language model cuglm for source code modeling.
recent work on pre trained language models has found that the contextual embeddings produced by these models can lead to better performance for many natural language processing nlp tasks .
in these models the representation for each word is learned using the language models where the vector of the word is computed based on the context it is used.
thus the vector of the same word under different contexts can be different.
in particular bert proposes a bidirectional transformer encoder with two new pre training objective masked language model and next sentence prediction where masked language model randomly masks some of the tokens from the input and the objective is to predict the masked word based only on its context and next sentence prediction predicts whether two sentences follow each other in a natural discourse.
by using these two objectives bert can produce powerful bidirectional contextual representations and advances the state of the art for many nlp tasks.
inspired by the success of pre trained language models in nlp we propose a multi task learning based pre trained language model to produce general and contextual representations for programs that can broadly support code understanding and generation tasks and then apply it to code completion.
during the pre training period we adopt the multi task learning framework to learn the following three training objectives jointly masked bidirectional language modeling the identifiers are more informative for understanding the program and correctly suggesting the identifiers is challenging in existing code completion research .
thus producing contextual and general representations for tokens especially for identifiers would be helpful for source code modeling and code completion.
for these reasons we mask the identifiers from the programs and the objective is to predict the masked tokens based on their bidirectional context.
next code segment predicting we argue that understanding relationships between code segments can help in source code modeling.
in order to achieve this we pre train a binarized next code segment prediction task that is predicting whether two segments of code tokens follow each other in a piece of code snippet.
474multi task learning based pre trained language model for code completion ase september virtual event australia unidirectional language modeling a left to right language modeling task where the representation of each token encodes only the leftward context tokens and itself.
this training objective is added because for the generation tasks e.g.
code completion only leftward contextual tokens are allowed.
after the model has been pre trained we fine tune it directly apply the pre trained model and adapt the model on downstream tasks by fine tuning the pre trained parameters on the code completion task.
during the code completion our model does not directly predict next token instead we adopt a multi task learning framework to predict the token and its type.
we first predict the type of the token and then use predicted type to assist the token prediction.
we create two massive corpora of java and typescript programs collected from github to pre train and fine tune the model.
we compare our model with two state of the art code completion approaches byte pair encoding based neural language model bpe nlm and pointer mixture network .
for completing all types of tokens our model achieves the accuracy of and on java and typescript datasets respectively which improves pointer mixture network by and and improves bpe nlm by and in terms of relative improvements.
for identifier completion our model achieves the accuracy of and respectively which improves pointer mixture network by and and improves bpe nlm by and in terms of relative improvements.
the main contributions of this paper are summarized as follows we present the first attempt at pre training a language model with a transformer based architecture for code completion.
we take advantage of the type information to help our model make better suggestions on identifiers.
we compare our model with state of the art code completion models and evaluate the performance of these models on two real world datasets.
experimental results demonstrate that our model achieves the best performance compared with the baseline models.
background .
statistical language model statistical language models capture the statistical patterns in languages by assigning occurrence probabilities to a sequence of words in a particular sequence which will score an utterance high if it sounds natural to a native speaker and score low the unnatural or wrong sentences.
programming languages are kind of languages that contain predictable statistical properties which can be modeled by statistical language models.
given a token sequence s s1 s2 ... st the probability of the sequence is computed as p s p s1 p s2 s1 p s3 s1s2 ... p st s1s2 ... st the probabilities are hard to estimate when the number of the context tokens s1 s2 ... st 1is tremendous.
the n gram model based on the markov assumption is proposed to address this challenge where the probability of a token is dependent only on the n 1most recent tokens.
n gram based models have been generally applied to code completion .
these models have been proved to capture the repetitive regularities in the source code effectively.
in recent years deep recurrent neural networks including long short term memory lstm and gate recurrent unit gru have shown great performance on modeling programming languages .
by using recurrent connections and gate mechanisms information can cycle inside these networks for a long time which loosens the fixed context size and can capture longer dependencies than the n gram model.
however the introduction of the gating mechanism in lstms and grus might not be sufficient to address the gradient vanishing and explosion issue fully.
to ease this issue attention mechanisms which add direct connections between long distance word pairs are proposed.
for example the transformer is an architecture based solely on attention mechanism.
it uses a multiheaded self attention mechanism to replace the recurrent layers to reduce sequential computation and capture longer range dependency.
later transformer xl is proposed by introducing the notion of recurrence into the deep self attention network.
thus it enables the transformer networks to capture the very long term dependency during language modeling.
.
multi task learning multi task learning is an approach for knowledge transfer across related tasks.
it improves generalization by leveraging the domainspecific information contained in the training signals of related tasks .
through sharing hidden layers among tasks the model can capture the common features among all the tasks.
furthermore by preferring the representation that all tasks prefer the risk of over fitting is reduced and the model can be more general to new tasks in the future.
multi task learning has been successfully used in many fields including natural language processing speech recognition and computer vision .
.
pre trained language models language model pre training has shown to be effective for nlp and has achieved the state of the art results across many nlp tasks .
the advantages of the pre trained model can be summarized as follows by pre training on the huge corpus the model can learn universal representations and help with the target tasks the pre trained model can provide a better model initialization which leads to a better generalization performance on the downstream tasks.
pre training can be regarded as a kind of regularization to avoid over fitting on small data.
to apply the pre trained language representations to downstream tasks the feature based approaches use the pre trained representations as additional features and the fine tuning approaches directly adapt the model on the downstream tasks by simply fine tuning the pre trained parameters .
generative pre trained transformer gpt and bidirectional encoder representations from transformers bert are the widely used fine tuning approach where bert has significantly improved the performance of a wide range of natural language understanding tasks.
however the bidirectionality nature of bert makes it difficult to be applied to natural language generation tasks.
to overcome this limitation unified pre trained language model unilm that can be applied to both natural language understanding nlu and natural language generation nlg tasks was proposed.
inspired by these models we build a pre trained language model for code understanding and generation and then fine tune it on code completion.
475ase september virtual event australia liu et al.
x2 x3 xn ... token embeddings segment embeddings position embeddings transformer layer transformer layer transformer layer l ... attend to all tokens attend to left context ncp self attention masks code understanding and generation pre trained lm with shared parameters xn x5 h h2 h3 hn ... h hn h h5 h h2 h3 hn ... h hn h h5 y4 yn transformer transformer a masked bidirectional lm mlm h h2 h3 hn ... h hn h h5 y transformer transformer b next code segment prediction ncp transformer transformer h h2 h3 hn ... h hn h h5 y4 yn y2 y3 y5 yn yn c unidirectional lm ulm prevent from attending figure overview of cuglm pre training.
the model parameters are shared across the pre training objectives i.e.
mlm ncp and ulm .
we use different self attention masks to control the access to context for each token.
cuglm we describe the details about our proposed codeunderstanding and generation pre trained language model cuglm in this section.
.
model architecture given an input program token sequences x x1 x2 ... xn cuglm obtains a contextualized vector representation for each token.
the model architecture is shown in figure .
we adopt an l layer transformer as the language model to encode the input vectors x x1 x2 ... xninto contextual representations at different levels hl hl hl ... hln wherehl transformer l hl l .
in figure and later sections we omit the superscript lfor the hidden vectors of the final layer hl ito make the illustration less cluttered.
for each transformer layer block multi attention heads are used to aggregate the output of the previous layer and the output of a self attention head alis computed as q hl 1wq l k hl 1wk l v hl 1wv l mij allow to attend prevent from attending al softmax qkt p dk m v wherehi r x dhdenotes the i th layer s output.
the queries q keysk and values vare computed by linearly projecting the previous layer s output hl 1using parameter matrices wq l wk l wv l. m r x x is the mask matrix that determines whether a pair of tokens can be attended to each other.
for different pre training objectives we use different mask matrices mto control how many contextual tokens can a token attend to when computing its contextualized representations as illustrated in figure .
for bidirectional lm the elements of the mask matrix are all 0s which means thatall the tokens have access to each other.
for unidirectional lm the upper triangular part of the mask is set to indicating that each token can only access the leftward context tokens and itself.
the output of cuglm includes contextual vector representation of each input token and the representation of which is short for classification and works as the aggregated sequence representation and can be used for classification tasks.
during the pre training period the model s parameters are shared and optimized with several objectives namely masked bidirectional lm next code segment prediction and unidirectional lm.
after the model is pre trained we can then fine tune it for downstream tasks.
in this paper we fine tune cuglm on code completion.
.
input representation the inputxis a token sequence which is a pair of segments packed together.
as shown in figure for a given token its vector representation is computed by summing the corresponding token segment and position embeddings.
for token embeddings the embedding matrix is randomly initialized and then is adjusted as part of the training process.
two special tokens are defined where which is short for classification always appears at the beginning of the input.
the final hidden state corresponding to can be used as the aggregate sequence representation for classification tasks for example in next sentence prediction task.
which is short for seperation is used to separate the sentence pairs.
the segment embeddings i.e.
eaandebare also used to differentiate the code segment pairs.
for each token of the first code segment a learned embedding eais added and a learned embedding ebis added to each token of the second code segment.
the embedding matrix for the segment embeddings is also randomly initialized 476multi task learning based pre trained language model for code completion ase september virtual event australia thread new thread ct thread .
start input tokenstoken embeddingssegment embeddingsposition embeddings e ethread e enew ethread e ect e e e ethread e. e tart e e e ea ea ea ea ea ea ea ea ea ea eb eb eb eb eb eb e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11 e12 e13 e14 e15 figure input representation.
the input embeddings is the sum of the token embeddings the segment embeddings and the position embeddings.
to make use of the order of the sequence we use learned positional embeddings with sequence lengths up to tokens.
.
pre training procedure to pre train cuglm we adopt multi task learning to learn three tasks jointly as shown in figure including masked bidirectional language modeling mlm next code segment predicting ncp and unidirectional language modeling ulm .
for the first two objectives the transformer network is under the bidirectional settings and for the last objective the transformer network is unidirectional.
a masked bidirectional language modeling in order to train deep bidirectional representations for the program we adopt a similar objective with bert that is masking some percentage of the input tokens and then predicting only those masked tokens.
different from bert we only mask the identifiers with type information where the type information can be extracted by static analysis or be annotated by developers considering that these identifiers are more informative for understanding the program.
then the objective is to predict the masked identifiers based on their bidirectional contextual tokens where all tokens can attend to each other in prediction.
it encodes contextual information from both directions and can generate better contextual representations of the masked identifiers as well as the other tokens than its unidirectional counterpart.
the final hidden vectors corresponding to the mask tokens are fed into the output softmax layer to produce the probability distribution of the outputs.
b next code segment predicting understanding the relationship between two sentences is quite important for many nlp tasks for example question answering qa and natural language inference nli which can help to understand the input text in more depth.
we argue that understanding relationships between code segments also help in source code modeling.
in order to achieve this we pre train a binarized next code segment prediction task that is predicting whether two segments of code tokens follow each other in a piece of code snippet.
specifically when choosing the code segments a and b for each pre training example of the time b is the actual next code segment that follows a and of the time it is a random code segment from the corpus.
for example input public void settextdirection int textdirection this .
mtextdirection textdirection label 1input public void settextdirection int textdirection this .
request request label the final hidden vector corresponding to which works as the aggregated sequence representation is fed into the output softmax layer to produce the probability distribution of classification results.
c unidirectional language modeling for language generation tasks for example code completion the context of the predicted token should only consist of the token on its left.
thus we create the left to right language modeling task as another pre training objective namely predicting the next token xt 1given the preceding context tokens x1 x2 ... xt.
the representation of each token encodes only the leftward context tokens and itself.
this can be done using a triangular matrix for self attention mask m where the upper triangular part of the self attention mask is set to and others to .
at each time step t the final hidden vector corresponding toxtis fed into the softmax layer to produce the probability distribution of the predicted token yt.
the pre training procedure follows the existing language model pre training approaches.
the parameters of cuglm are learned to minimize the sum of the cross entropy losses of the three pretraining tasks and are shared among all the tasks.
the final loss function is given below min lmlm lncp lulm .
fine tuning procedure when the model is pre trained we fine tune it on code completion task.
in code completion the context of the predicted token should only consist of all the token on its left.
thus the representation of each token can encode only the leftward context tokens and itself.
during the fine tuning procedure the following two objectives are optimized a unidirectional masked language modeling umlm different from the mlm objective in pre training the umlm objective in fine tuning is to predict the masked token based only on its leftward context where all tokens can only attend to the tokens on its left in prediction.
the transformer network is set to unidirectional using a triangular matrix for the self attention mask.
all the identifiers that have type information are masked in each sequence.
477ase september virtual event australia liu et al.
besides our model not directly predicts the masked token.
instead we adopt the multi task learning framework to predict the token and its type.
we first predict the type of the token and then the predicted type is used to assist the token prediction as shown in figure .
the reason for formulating the code completion task as a two step prediction instead of predicting the type and token jointly lies in that by predicting the type firstly and then use the predicted results as extra input for the token prediction can constraint our model to make more accurate prediction on the type and further enhance the token prediction performance.
h1 h2 h3 hn ... h hn h h5 token type 4token prediction type prediction token n type n transformer transformer figure model architecture for umlm.
type prediction the final hidden vector i.e.
the output of the transformer corresponding to the mask token h is used to compute the output vector for the token s type otype.
we use thesoftmax function to produce the probability distribution of the outputsytype otype tanh woh ytype softmax wyotype by wherewo rh htype wy rvtype htype by rvtypeare trainable parameters.
vtype is the vocabulary size of the token s type h is the hidden size of the transformer network htype is the embedding size of type vector.
token prediction after predicting the token s type we use the predicted type to assist the token prediction.
the vector of the predicted type etype and the hidden vector of the mask token h are concatenated to compute the output vector for the tokenotoken .
then the output vector is fed into the output softmax layer to compute the output vector for the token ytoken otoken tanh wo h etype ytoken softmax wyotoken by whereetype is the embedding of the predicted type wo rhtoken h wy rvtoken htoken by rvtoken are trainable parameters.
vtoken is the vocabulary size of the token and denotes the concatenation operation.
b unidirectional language modeling ulm this objective is a left to right language modeling task that is the same as the pretraining procedure.
given the preceding context tokens x1 x2 ... xt the model predicts the next token xt where the representation of each token encodes only the leftward context tokens and itself.table statistics of the datasets.
java typescript projects files lines .
.
of tokens .
.
of types .
.
masked id proportion .
.
during the fine tuning procedure the parameters of cuglm are learned to minimize the sum of the cross entropy losses of the two fine tuning tasks and are shared among all the tasks.
the final loss function is given below min lumlm lulm through learning these two objectives jointly we hope the model can make better predictions on both the identifiers and the other tokens.
experiments and analysis .
data preparation we pre train and fine tune our model across two programming languages java and typescript.
the programs in the corpus are collected from publicly available open source github repositories by removing duplicate files and project forks.
each program is tokenized into token sequence.
the detailed information is shown in table .
we use of the projects for pre training and of the projects for fine tuning on code completion task.
during the fine tuning we split the projects into train validation test sets in the proportion .
for the other baselines all the programs used in pre training and the training programs used in fine tuning are used as the training set and the validation and test sets are the same as in our fine tuning procedure.
we also randomly sample program files from both java and typescript test sets as the small test sets for byte pair encoding based neural language model bpe nlm evaluation since when performing completion testing in their model they use a variation of the beam search algorithm to combine the sub units to complete tokens which is very timeconsuming.
it takes several minutes to complete a single program file and will take tens of days to perform completion on the large test sets e.g.
the java test set contains files .
thus we create small test sets.
for java programs we extract the identifiers type information through static analysis.
for typescript programs we apply the approach in hellendoorn et al .
to extract type annotations of the identifiers.
we filter the programs to make sure at least of type annotations are user defined types in each typescript file.
figure shows the examples for java and typescript code where the identifiers that have type are marked with underlines and the green tokens next to the identifiers are the corresponding types.
to generate each training input sequence for pre training we sample two spans of tokens from the corpus which we refer to as segments s1ands2.
each segment contains several lines of source code tokens.
for the first segment s1 we sample the first n lines from one program file where nis randomly sampled from 478multi task learning based pre trained language model for code completion ase september virtual event australia table performance of baseline models and our approach.
modeljava typescript large test small test large test small test all tokens identifiers all tokens identifiers all tokens identifiers all tokens identifiers vanilla lstm .
.
.
.
.
.
.
.
pointer mixture network .
.
.
.
.
.
.
.
bpe nlm .
.
.
.
transformer xl .
.
.
.
.
.
.
.
cuglm .
.
.
.
.
.
.
.
type namespacename s s s s s s s interface signature name string email string when date interface commit author signature committer signature sha string message string interface namespaceinfo count number namespace namespacename data intro string name string package com .labo .kaji.swipeawaydialog import android .app.application import android .test.applicationtestcase public class applicationtest extends applicationtestcase application public applicationtest super application .class applicationtestcase android .test.applicationtestcase application android .app.application applicationtest com .labo .kaji.swipeawaydialog .applicationtest application android .app.applicationtypescript java figure code examples for type annotations.
to the length of the code lines of the program file.
of the time the second segment s2is the rest of the lines from the same program file that follows s1 and of the time it is a random code segment sampled from other program files of the corpus which is done for the next code segment prediction ncp task.
they are sampled such that the combined length is tokens.
for the masked bidirectional language modeling mlm task we only mask those identifiers that have type information.
for example the underlined tokens in figure .
.
experimental setup parameter configuration.
we use transformer with layers dimensional hidden states and attention heads.
the inner hidden size of the feed forward layer is .
we pre train our model with batch size of sequences for steps.
we use adamwith learning rate of 5e 1 .
2 .
l2 weight decay of .
learning rate warmup over the first steps and linear decay of the learning rate.
we use a dropout probability of .
on all layers.
we use a gelu activation following openai gpt.
the training loss is the sum of the cross entropy losses of the pretraining objectives or fine tuning objectives.
training of cuglm was performed on geforce gtx ti gpus with 12gb memory.
for each dataset the model is pre trained for steps and takes days to complete and is fine tuned for steps and takes days to complete.
metric.
we use accuracy to evaluate the performance of code completion.
our model provides an ordered list of suggestions for each token in the source code file given the context.
we compute the top accuracy i.e.
the fraction of times the correct suggestion appears in the first of the predicted list.
vocabulary.
as shown in table in the datasets the number of unique tokens and types is too large to build neural models to learn directly.
we choose k most frequent tokens in each training set to build the token vocabulary which is the same as li et al .
s study.
for those tokens outside the vocabulary we use unk unknow values to represent them.
the size of type vocabulary is also set to .
in both the training and test process the predictions of the unk targets are treated as wrong predictions.
the token unk rates for java and typescript test sets are and the type unk rates are respectively.
.
research questions and results to evaluate our proposed approach in this section we conduct experiments to investigate the following research questions rq1 how does our proposed approach perform in code completion when compared with state of the art models?
to answer this research question we compare our model with the following baseline models vanilla lstm a vanilla lstm neural network based language model.
pointer mixture network an attention and pointergenerator network based code completion model.
byte pair encoding based neural language model bpe nlm a large scale open vocabulary nlm for code completion which leverage bpe algorithm to keep vocabulary size low and successfully predict oov out ofvocabulary tokens.
transformer xl a self attentional neural network based language model for code completion.
479ase september virtual event australia liu et al.
table effects of each pre training task fine tuning task and the type prediction in our proposed model.
modeljava typescript large test small test large test small test all tokens identifiers all tokens identifiers all tokens identifiers all tokens identifiers full model .
.
.
.
.
.
.
.
pre training tasks ulm .
.
.
.
.
.
.
.
mlm .
.
.
.
.
.
.
.
ncp .
.
.
.
.
.
.
.
fine tuning tasks umlm .
.
.
.
.
.
.
.
ulm .
.
.
.
type prediction .
.
.
.
.
.
.
.
comparison with lstm based closed vocabulary models the first two baselines to compare with pointer mixture network we downloaded their publicly available source code1.
in their model the programs in the datasets are parsed into asts and they build the model to perform code completion on ast node sequences.
although the asts can provide more information representing the programs as ast node sequences is not the natural order of typing and the precision does not directly reflect the productivity gain of the code completion tool.
more importantly in practice the code is incomplete so the software project might not be compilable code is not parsable into asts or parsed asts miss a lot of information .
thus representing programs as token sequences and performing code completion on the token level might be more practical.
in this paper we focus on token level code completion.
in our corpus the programs are tokenized into token sequences.
to compare with them we train their model within our tokenized programs using the command line arguments given in the artifact s readme file2.
their base model is a single layer lstm network with an unrolling length of and hidden unit size of .
the initial learning rate is .
and is decayed by multiplying .
after every epoch.
the gradients norm is clipped to .
the size of the attention window is .
since the pointer mixture network is based on lstm language model we also list the results of the vanilla lstm where the parameter configuration of the vanilla lstm network is set the same as the pointer mixture network.
as shown from the results our model outperforms the two lstm based models on both java and typescript datasets by a large margin especially in identifier completion.
on the java large test set our model achieves the accuracy of .
and .
on token s completion and identifier s completion respectively which outperforms pointer mixture network by .
and .
in terms of relative improvement.
on the typescript large test set our model achieves the accuracy of .
and .
on token s completion and identifier s completion respectively which outperforms pointer mixture network by .
and .
.
the results on small test sets are similar to the large test set.
we can find 2since the pointer mixture network also makes use of the additional information derived from asts the results of using token sequence as input might understate the accuracy of the plain pointer mixture network.that the improvements on the typescript dataset are smaller than java especially in identifier completion.
the reason lies in that the masked identifier proportion in typescript .
is smaller than java .
because the type information in typescript is annotated by developers and only a part of the identifiers are annotated.
in the mlm pre training task these identifiers are masked and are predicted based on their contextual tokens aiming at generating better contextual and informative representations for these identifiers as well as other tokens.
during fine tuning the type information of these identifiers is used to assist the identifiers prediction.
due to the lower masked proportion the pre training and fine tuning procedure can offer less information than java thus resulting in smaller improvements.
comparison with open vocabulary model bpe nlm to compare with bpe nlm we downloaded their publicly available source code3and train their model on our datasets.
they use a single layer gru nlm with an unrolling length of built upon sub word units learned from bpe.
the embedding size and the hidden unit size are both set to in their model.
to keep the number of parameters comparable with our model and other baselines we increase the hidden unit size and the embedding size of their model to .
there are three scenarios static dynamic and maintenance where the dynamic and maintenance settings update model s parameters during testing.
since our model and other baselines do not update parameters during the test process we present the results of the static scenario to make the comparison fair and realize that evaluating dynamically may improve accuracy.
as shown from the results bpe nlm performs best on completing identifiers among all the baseline models on both datasets which proves the power of the open vocabulary lm for predicting the identifiers.
even though our model still outperforms the bpe nlm on completing identifiers.
when evaluating on completing all kinds of tokens the performance of bpe nlm is not as well as the identifier completion.
our model outperforms bpe nlm on completing all kinds of tokens by a large margin.
comparison with transformer network based model transformerxl to find out if cuglm s promising results derive more from using a transformer based model for code completion or from the 480multi task learning based pre trained language model for code completion ase september virtual event australia multi task learning based pre training and fine tuning we compare our results to a transformer based model trained from scratch i.e.
without the benefit of a pre trained embedding.
transformer xl is a transformer network based language model which introduces the notion of recurrence to model the long term dependency of the input sequence.
we use a layer transformer xl network with parallel heads.
the dimension of each head is set to .
we set the segment length to be and the length of the cached segments to .
the dimensionality of the model hidden unit and the embedding size is set to .
the dimension of the feed forward layer is set to .
as seen from table transformer xl model outperforms the other baseline models that are based on the recurrent neural networks on both datasets which demonstrates that the transformer based network is more powerful than recurrent neural networks on this task.
the performance of our model is substantially higher than the transformer xl model trained from scratch.
we therefore conclude that pre training and fine tuning are crucial to cuglm s success.
rq2 what are the contributions of the pre training tasks?
we perform an ablation study to examine the effects of the three pre trained tasks ulm mlm and ncp.
we conduct experiments on pre training the model without each task and the fine tuning procedure remains unchanged.
the results are shown in table .
the first row shows the results of our full model.
the second to the fourth rows present the results of removing ulm mlm and ncp from the full model during pre training respectively.
ulm removing the ulm task during pre training.
the loss function of the pre training procedure consists of lmlm andlncp and both these tasks are based on the bidirectional transformer.
as seen from the results removing this task hurts the model s performance.
during fine tuning the objectives are based on the unidirectional transformer.
thus adding the ulm task during pre training makes the learned text representations more general because they are optimized for both bidirectional and unidirectional language modeling objectives jointly mitigating over fitting to bidirectional language modeling task.
removing the ulm task would make the parameters hard to optimized when fine tuned on the unidirectional objectives.
thus the accuracy drops.
ncp removing the next code segment prediction task during the pre training.
the loss function consists of lulm andlmlm .
the ncp tasks are added to help the model understand the relationships between the code segments.
the model removing ncp performs worse than the full model but performs better than removing ulm which demonstrates that the ncp task is necessary to improve the performance but contributes less than the ulm task.
mlm pre training the model without the masked bidirectional language modeling objective and the loss function consists of lulm andlncp.
as shown from the results removing the mlm hurts the performance more than the other two tasks especially on identifier completion.
mlm task can help the model generate better contextual representations of the tokens especially the identifiers thus can improve the model s performance significantly.
the above results demonstrate that all of the pre training tasks are necessary to improve the performance and mlm contributes most to the improvements.
rq3 what are the contributions of the fine tuning tasks?
to figure out the effectiveness of the fine tuning procedure we alsoconduct experiments by removing each of the fine tuning task.
the results are shown in fifth and sixth rows of table .
umlm removing the unidirectional masked language modeling task during fine tuning procedure.
only the left to right language modeling task is performed and the loss function becomes lulm .
as seen from the results removing this task hurts the model s performance on both two datasets especially for the identifier prediction.
umlm task can help the model generate better contextual representations for the tokens.
besides it can also utilize the type information of the identifiers during the fine tuning.
thus this fine tuning task is necessary for improving the performance of the code completion.
ulm removing the unidirectional language modeling task during fine tuning procedure.
under this setting the model can only produce the results of the masked identifier prediction.
the loss function becomeslumlm .
as seen from the results when removing ulm task the performance of the identifier prediction drops a lot which demonstrates that the language modeling task can offer much help for the identifier prediction.
through optimizing the model on this task jointly the model can capture the semantic of the input code segment better which serves as the basis of the improvement on identifier prediction.
rq4 could the predicted type help the model on token prediction?
when fine tuning our model on code completion task we utilize multi task learning to predict the token and its type jointly.
we first predict the type and then use the type to assist the token s prediction.
to confirm whether our model can correctly predict the identifier s type we present the accuracy of the type prediction.
our model achieves the accuracy of .
and .
on java and typescript large test sets respectively.
the results demonstrate that our model can correctly predict the identifiers type in most cases.
to find out whether the type prediction really helps we conduct experiment by removing the type prediction.
the results are shown in the last row of table .
as shown from the results when removing the type prediction the model performs worse than the full model on completing both identifiers and all tokens which demonstrates that the predicted type information can help the model achieve better performance on code completion.
discussion .
the type of completions except for identifiers we also give a detailed breakdown of the accuracies for completing different types of tokens on both our model and bpe nlm and also present these tokens proportion.
the results are shown in table .
punctuations make up the majority of the completions and the accuracies of both our model and bpe nlm on predicting punctuations are high where bpe nlm performs better than cuglm.
the punctuation tokens are much easier to complete than identifiers but these completions are not that useful for developers .
for keyword completions our model outperforms bpe lm by a large margin.
the keywords are predefined reserved words used in programming that have special meanings to the compiler which contain the syntactic information or the attribute information of the objects.
the great performance of cuglm on completing keywords further demonstrates that through multi task learning based pre training and 481ase september virtual event australia liu et al.
table performance of completing different types of tokens.
java typescript type proportion cuglm bpe nlm proportion cuglm bpe nlm identifiers .
.
.
.
.
.
keyword .
.
.
.
.
.
punctuation .
.
.
.
.
.
numerals .
.
.
.
.
.
operator .
.
.
.
.
.
fine tuning the representations generated by our model can capture syntactic and semantic information better.
for numeral and operator completions which are more related to the semantic of the programs our model also outperforms bpe nlm substantially.
.
model complexity comparison to analyze the complexity of our model and the baseline models we present the number of trainable parameters for all the models shown in table .
the number of trainable parameters of our model is less than all the baselines.
although we adopt multi task learning for both pre training and fine tuning the number of trainable parameters does not increase much as all of the tasks share one multi layer transformer network.
to improve training efficiency and avoid over fitting we do not use large parameter settings.
even though our model still outperforms the other baselines by a large margin thanks to the pre training and fine tuning.
table parameters of the baseline models and our model.
model of parameters vanilla lstm 168m pointer mixture network 177m bpe nlm 145m transformer xl 173m cuglm 104m .
effect of applying bpe algorithm to further improve the performance of our model we also conduct experiments on applying byte pair encoding bpe algorithm to build up the vocabulary of sub words as in where the rare tokens will be segmented into more common sub word units and no word is oov.
however the performance on java corpus is comparable with the origin model and the accuracy decreases slightly on typescript corpus.
we analyze the possible reasons are as follows.
during pre training we mask the identifiers with type information.
when we apply bpe algorithm most of these masked identifiers will be split into sub word units.
thus all of these units will be masked which leads to the high mask proportion and increased the difficulty of learning the semantics of embeddings.
besides during fine tuning our model utilizes the predicted type information to assist the token s prediction.
after splitting the tokens into subword units all of the units from one token correspond to the same type resulting in the semantic inconsistencies between the type information and the sub word units.
for example the same unit from different tokens might correspond to different types.
thus applying bpe does not improve the performance of our model.
.
threats to validity threats to external validity relate to the quality of the datasets we used and the generalizability of our results.
we create two massive datasets java and typescript to pre train and fine tune our model.
all of the programs in the datasets are collected from github repositories.
the reasons for using these two languages are as follows.
these two languages are commonly used for software development and we can get the identifiers type through static analysis or through the developers annotations.
however further studies are needed to validate and generalize our findings to other programming languages.
threats to internal validity include the influence of the hyperparameters used in our model.
the performance of our model would be affected by different hyper parameter settings which are tuned empirically in our experiments.
thus there is little threat to the hyper parameter choosing and there might be room for further improvement.
however current settings have achieved a considerable performance increase.
another threat to internal validity relates to the implementation of the baseline methods.
for li et al .
s model we apply their model to the token level code completion which is originally used for ast level code completion.
in their model the additional information derived from asts is utilized to improve the performance.
the results of using token sequence as input might understate the accuracy of the plain pointer mixture network.
however in practice the code is incomplete so the code is not parsable into asts or parsed asts miss a lot of information.
thus representing programs as token sequences and performing code completion on the token level is more practical.
under this setting we have tried our best to make fair comparison with li et al.
by only changing the format of the input and keeping the model unchanged.
for bpe nlm we compare our model with the static setting of their model considering the fairness of the comparison.
we realize that evaluating dynamically may improve accuracy.
the dynamic and maintenance scenarios are not implemented and compared in this work which will be considered as our future work.
threats to construct validity relate to the suitability of our evaluation measure.
we use accuracy as the metric which evaluates the proportion of correctly predicted next token.
it is a classical evaluation measure for code completion and is used in almost all the previous code completion work .
related work statistical code completion code completion is a hot research topic in the field of software engineering.
early work in code completion mainly bases on heuristic rules and static type information 482multi task learning based pre trained language model for code completion ase september virtual event australia to make suggestions .
since hindle et al .
found that source code contained predictable statistical properties statistical language models began to be used for modeling source code where n gram is the most widely used model.
observed that source code has a unique property of localness which could not be captured by the traditional n gram model.
they improved n gram by adding a cache mechanism to exploit localness and achieved better performance than other n gram based models.
hellendoorn and devanbu introduced an improved n gram model that considered the unlimited vocabulary nested scope locality and dynamism in source code.
in recent years deep recurrent neural network based language models have been applied to learning source code and have made great progress .
liu et al .
proposed a code completion model based on a vanilla lstm network.
li et al .
proposed a pointer mixture network to address the oov issue.
liu et al.
propose a multi task learning and transformer based language model for ast level code completion.
they built model to predict the ast node s type and value jointly and also utilized the hierarchical structural information in the program s representation which achieves state of the art results on ast level code completion.
kim et al .
presented a transformer model for code prediction and incorporated syntactic structure into the transformer to further improve the model s performance.
svyatkovskiy et al .
proposed a code completion system based on lstm for recommending python method calls.
their system is deployed as part of the intellicode extension in visual studio code ide.
karampatsis et al.
proposed a large scale open vocabulary neural language model for source code which leverages the bpe algorithm beam search algorithm and cache mechanism to both keep vocabulary size low and successfully predict oov tokens.
the experimental results demonstrate that their open vocabulary model outperforms both n gram models and closed vocabulary neural language models and achieve state of the art performance on token level code completion.
most recently svyatkovskoy et al .
implemented and evaluated a number of neural code completion models which offer varying trade offs in terms of memory speed and accuracy.
they provided a well engineered approach to deep learning based code completion which is important to the software engineering community.
pre trained language models language model pre training has shown to be effective for nlp and has achieved the state of theart results across many nlp tasks .
pre trained language models can learn token contextualized representations by predicting tokens based on their context by training on large amounts of data and then can be adapted to downstream tasks.
bidirectional encoder representations from transformers bert is the widely used approach in nlp which learns to predict the masked words of a randomly masked word sequence given surrounding contexts.
bert has significantly improved the performance of a wide range of natural language understanding tasks.
kanade et al .
extended this idea to programming language understanding tasks.
they derived contextual embedding of source code by training a bert model on source code.
they evaluate their model on a benchmark of five classification tasks in programs.
results show that their model outperforms the baseline lstm models supported by word2vec embeddings and transformers trainedfrom scratch.
the bidirectionality nature of bert makes it difficult to be applied to natural language generation tasks.
to overcome this limitation dong et al .
proposed a unified pre trained language model unilm that can be applied to both natural language understanding and natural language generation tasks.
unilm can be configured using different self attention masks to aggregate context for different types of language models and thus can be used for both language understanding and generation tasks.
in the above work the models are learned from the input of a single modal for example only from natural languages or source code.
in recent years multi modal pre trained models that can learn implicit alignment between inputs of different modalities are proposed.
these models are learned from bi modal data such as pairs of language image language video or language code .
feng et al.
proposed codebert a bimodal pre trained model for natural language and programming language aiming at capturing the semantic connection between natural language nl and programming language pl .
they trained codebert with masked language modeling task and replaced token detection task and evaluated it on two downstream nl pl tasks including natural language code search and code documentation generation.
inspired by the above models we propose a code understanding and generation pre trained language model with a transformerbased architecture and tailored it for code completion which is the first attempt at pre training a language model for code completion.
conclusions and future work in this paper we propose a multi task learning based code understanding and generation pre trained language model for source code modeling with a transformer based neural architecture.
we pre train our model on two massive datasets and with three objective functions and then fine tune it on code completion.
experimental results demonstrate that the proposed model achieves better results than previous state of the art models on completing tokens especially on completing identifiers.
to the best of our knowledge we are the first to apply the pre trained language model to code completion.
we believe this work represents a significant advance in source code modeling which will be beneficial as a building block for many other applications in this area.
in the future we plan to apply our model to other programming languages and fine tune our model to adapt to other tasks.