aligning the objective of llm based program repair junjielong xu ying fu shin hwei tan pinjia he the chinese university of hong kong shenzhen china chongqing university china concordia university canada junjielongxu link.cuhk.edu.cn fuying cqu.edu.cn shinhwei.tan concordia.ca hepinjia cuhk.edu.cn abstract large language models llms have achieved decent results on automated program repair apr .
however the next token prediction training objective of decoder only llms e.g.
gpt is misaligned with the masked span prediction objective of current infilling style methods which impedes llms from fully leveraging pre trained knowledge for program repair.
in addition while some llms can locate and repair bugs in certain functions using the related artifacts e.g.
test cases existing methods still depend on statement level fault localization methods to provide a list of buggy hunks for repair.
this restriction hinders llms from exploring potential patches beyond the given locations.
in this paper we investigate a new approach to adapt llms to program repair.
our core insight is that llm s apr capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements.
based on this insight we designed d4c a straightforward prompting framework for apr.
d4c can repair bugs correctly in defects4j with each patch being sampled only times.
this surpasses the sota apr methods with perfect fault localization by and reduces the patch sampling number by .
our findings reveal that objective alignment is crucial for fully exploiting llm s pre trained capability and replacing the traditional localizebuggy hunks then repair workflow with direct debugging is more effective for llm based apr methods.
thus we believe this paper introduces a new mindset for harnessing llms in apr.
index terms automated program repair large language model objective alignment i. i ntroduction program repair is a critical part of the software cycle.
to fix bugs in software systems developers often need to dedicate substantial time exceeding of regular development time for manual program repair .
to reduce such human effort researchers have started exploring automatic program repair apr methods .
based on how the patches are generated apr methods can be categorized into heuristic based constraint based template based and learning based methods .
traditional methods often rely on pre defined patterns for solving specific bug types e.g.
nullpointerexception .
learning based methods typically use large scale high quality bug fix pair data to train neural machine translation nmt models which transform a buggy program to a fixed version .
recently as large language models llm have shown strong code understanding abilities many researchers have begun exploring pinjia he is the corresponding author.llm based apr methods .
with broad pre training across diverse general corpus llms can achieve impressive repair performance through prompt engineering or minimal data fine tuning .
however current application of llms is not well adapted for program repair due to two major reasons.
first the inference objective of current approaches is misaligned with llm s training objective.
llms can be classified into encoder only encoder decoder and decoder only .
the first two are typically trained to predict the masked tokens i.e.
infilling or denoising while the third is trained to predict the next tokens i.e.
completion .
existing llmbased methods generally adopt the infilling style method to predict the fixed code at the masked buggy code location during inference.
the early work achieved decent results as the models used are also trained on infilling task .
recently larger decoder only llms like gpt4 have shown stronger capability on code tasks and people attempt to directly employ them for infillingstyle apr .
however while the model parameters being scaled up by hundreds of times the fixed bugs did not even doubled which contrasts with the clear scalability in other tasks .
we hypothesize that this is due to decoder only llms are trained for completion rather than infilling resulting in an objective misalignment between training and inference.
specifically due to the sparsity of parallel masked denoised code corpus in training these llms are less proficient in patch generation on masked buggy code.
as noted in recent studies objective misalignment can yield significant sub optimal performance.
second the current workflow limits llm from fully exploiting its pre trained capability.
existing approaches still adhere to the workflow of first using statement level fault localization statement level fl tools to obtain a ranked list of potential buggy hunks i.e.
contiguous statements to be modified in patches and then using apr methods to generate patches by modifying the buggy program at these hunks .
however llms already have the capability to fix trivial syntax bugs in their generated code independently by referring to different artifacts e.g.
error report from compilers and virtual machines illustrating the capability of identifying bugs without given buggy hunks.
considering llm s superior code comprehension ability and several studies show that current fl tools may fail to providearxiv .08877v5 feb 2025objective misalignment between training and apr inferencetraining objectiveinference objectivelossfunction completion losshunk infilling training case write a bubblesort function void bubblesort int a for int i i a.length i for int j i j a.length j if arr arr int temp arr arr arr arr temp repair case fill the buggy line masked by infill void bubblesort int a for int i i a.length i infill ... for int j i j a.length j aligning training objective and apr inference objectivetraining objectiveinference objectivelossfunction completion lossfunction completion training case repair case write a refined function void bubblesort int a for int i i a.length i infill ... void bubblesort int a for int i i a.length i for int j i j a.length j ...encoder only llmdecoder only llmencoder decoder llmi x thosecuteflowersilovethosecuteflowers ilovethosecuteflowers i x flowers x lovethose write a bubblesort function void bubblesort int a for int i i a.length i for int j i j a.length j if arr arr int temp arr arr arr arr temp cutefig.
.
first row llm sturctures and their training objectives.
second row the training and inference objective is misaligned when using decoder only llms for infilling style apr.
third row an intuitive way to align the gap using llms for entire program completion rather than masked span prediction.
precise buggy hunks asking llms to generate patches at provided several hunks might contrarily hinder them from exploring broader potential patch space beyond the provided ones.
a recent study also revealed that llms fail to make good use of the given buggy lines and tend to over rely on them whereas another study suggested using a more flexible fault localization to enhance apr .
as observed in these studies the traditional localize buggy hunks then repair workflow may lead to ineffective llm based apr.
in this work we explore a novel way to use llm for program repair.
we propose that aligning the output from infilling discrete hunks to completing entire functions can better attain the training objective and allowing llm to locate and repair buggy hunks with artifacts in a humanlike manner can further improve its apr performance.
based on these insights we implemented d4c i.e.
direct debug drives decent code a straightforward apr approach without complex prompting or extra training.
we conducted experiments on a total of bugs associated with individual functions i.e.
single function bugs from defects4j and debugbench .
d4c repaired out of the singlefunction bugs in defects4j outperforming the state of the art sota apr methods with perfect statement level fl by .
meanwhile each patch in d4c was sampled only 10times accounting for used by sota methods which use samples .
our results show that by fully aligning the output format and augmenting the input prompt d4c can achieve the best apr performance without given buggy hunks additional fine tuning or multi round dialogue.
this paper is not aimed at proposing d4c as a new apr technique but rather to introduce a new mindset or paradigm for better harnessing llms in apr in the future.
in summary this paper makes the following contributions.
problem reformulation we reformulate infiling style repair problem as a program refinement problem.
our experiments show that asking llms to generate an entire refined function can align with the pre training objective of decoder only llms leading to sota apr performance.
new apr workflow instead of relying on statement level fl tools to obtain the list of buggy hunks for repair as in traditional apr workflow we show that allowing llms to locate and repair buggy hunks by using diverse types of artifacts e.g.
failed tests error messages and code comments in a human like manner can further improve its apr performance.
implementation and evaluation we implemented d4c based on these insights and the evaluation results on two widely used benchmarks i.e.
defects4j and debugbench have demonstrated its effectiveness.ii.
b ackground and motivation a. llm architectures and training objectives background a large language model llm is a language model consisting of a neural network with many parameters typically billions of weights or more trained on large quantities of unlabelled corpus using self supervised learning .
the llms usually adopt the transformer architecture or one of its sub structures i.e.
encoder ordecoder .
the encoder usually consists of feed forward networks with selfattention while the decoder usually consists of feedforward networks with cross attention .
thus llms can be categorized into three types encoder only decoder only andencoder decoder llms.
encoder only llms such as bert and its variants like codebert have a bidirectional transformer encoder structure.
they are typically trained on the masked language modeling objective i.e.
mlm aiming to denoise and reconstruct the masked tokens via understanding the surrounding context .
as shown in eq.
the loss of mlm training objective can be explicitly represented as the log sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known.
lmlm mx i mlogp ti ti tk mask where mis the total number of masked tokens.
decoder only llms including gpt series and llama series have an autoregressive transformer decoder structure.
they are mainly trained on the causal language modeling objective i.e.
clm aiming to predict and complete next tokens via following the prefix context .
as shown in eq.
the loss of clm training objective can be explicitly represented as the log sum of the conditional probabilities of generating the next token when all the preceeding tokens are known.
lclm nx i nlogp ti ti t1 t2 ... t i where n is the total number of the input tokens.
encoder decoder llms such as t5 and its variants like codet5 have a complete transformer structure.
they use an encoder to embed the input text into vector representations and a decoder to causally generate new tokens after the input prompt.
specifically t5 series llms are often pre trained on the denoising objective which aims to generate the reconstructed spans i.e.
a sequence of adjacent tokens from the masked spans for the masked span in the prompt .
as shown in eq.
the loss of denoising training objective can be explicitly represented as the log sum of the conditional probabilities of generating the masked token when all input tokens are known.
ldenoising mx i mlogp ti ti tk input where m is the total number of the masked tokens.
while this process is similar to the mlm objective of encoder only linescoreif value !
null value.length .98percentage float currentnum totalnum .94for int i i max number i .
......logger logger mylogger logconfig .12fault localization patchscoreif value !
null value.isempty .99if stringutils.isnotempty value .97if value !
null value.length .
......if value !
null .08teststatuscom.demo.myclasstest.testvaluenotnull passcom.demo.myclasstest.testvalueisempty passcom.demo.myclasstest.testvaluetrim fail......com.demo.myclasstest.testvaluewithnum passpatch generation patch validationfig.
.
an example of automated program repair workflow.
llms it only generates the reconstructed tokens from the masked position ignoring other parts e.g.
i and flowers in encoder decoder llm in fig.
.
similar to decoder only llms it also has an unfixed length of text generation.
motivation as introduced both the encoder only and encoder decoder especially t5 series llms involve the corruption and denoising process during pre training which substitutes some tokens in the input text for mask tokens and reconstruct these tokens from the mask tokens.
to make effective use of them for apr xia et al.
proposed the infilling style apr.
it aims to replace the entire input buggy hunk with a mask token e.g.
infill and prompts the model for direct inference restoring the fixed hunk from the mask.
the infilling style apr can be implemented differently across various models.
for instance in codebert it recovers the correct code from masked hunks while leaving the rest of the input tokens the same .
in codet5 it only outputs the correct code restored from the masked hunks .
recently several apr approaches use decoder only llms as they have exhibited a stronger coding capacity.
influenced by the prior success of infilling it has been generally assumed that such methods would be useful for decoder only llms.
thus they use decoder only llms to generate fixed hunks for the given buggy hunks as illustrated in fig.
.
however except for very few models e.g.
incoder decoderonly llms barely incorporate denoising objective during pretraining.
as the masked denoised parallel corpus is rarely available it becomes challenging for the models to learn the generation of the fixed hunk from the masked tokens without further adaptation after the next token prediction pre training.
such an objective misalignment might hamper llm s performance on specific tasks extensively such as the text classification task and the qa task whose inference objectives are misaligned with the training objective of decoder only llms.
thus we have the following insight.void bubblesort int a for int i i a.length i for int j i j a.length j ... void bubblesort int a for int i i a.length i for int j i j a.length j ...buggy program void bubblesort int a for int i i a.length i for int j i j a.length j ... fill the buggy line masked by infill void bubblesort int a infill for int j i j a.length j ... test errors assertionerror error refine this buggy function void bubblesort int a for int i i a.length i for int j i j a.length j ...fixed programbuggy programstatement level fl toolstest suitellmgenerated patchesgenerated promptsmodified hunk 1modified hunk ...modified hunk n modified function 1modified function ...modified function nwrong buggy hunk!
locate and repair hunks subsequentlylocate and repair hunks simultaneously all the patches are at the wrong hunks patches can be employed at any hunks no buggy hunk providedfig.
.
an example of two different apr paradigms.
first row locate and repair buggy hunks subsequently may cause many invalid attempts for patch generation.
second row locate and repair buggy hunks simultaneously can mitigate the cost of patching at specific hunks.
the wavy line is a buggy hunk insight modifying decoder only llms output from fixed hunks to the entire refined program can better align the inference objective to the training objective thus significantly enhancing apr performance.
this insight was inspired by previous approaches focusing on optimizing model performance on specific tasks by aligning the pre training objectives of the model .
we will verify our hypothesis in sec.
iv c via comparing completion perplexity of two output formats i.e.
fixed hunks or complete function on white box llms i.e.
mixtral moe .
b. apr techniques and workflow background given a buggy program and an artifact e.g.
usually a test suite with at least one failed test automated program repair apr approaches generate a fixed program that fulfills a correctness criteria e.g.
passing all tests .
as illustrated in fig.
an apr workflow typically encompasses three steps fault localization patch generation and patch validation.
particularly apr research mainly focuses on the patch generation step and obtains the fault locations using existing fl techniques e.g.
statistical fault localization or uses perfect fl results in evaluation.
based on their patch generation strategies apr methods can be categorized into heuristic based constraint based template based and learning based .
the non learningbased approaches are usually restricted by a limited set of program transformations causing the generation of a large number of invalid patches.
to identify location to apply these transformations the current strategy is to first generate a ranked list of suspicious buggy hunks using statement level fl tools.
the apr tool then sequentially generates patches for each provided hunks.
each of the generated patches is then validated using the given artifacts e.g.
test cases .
with the advent of deep learning dl learning based apr tools based on neural machine translation nmt have emerged.
these tools outperform the traditional methods via learning code semantics on a large bug fix parallel training corpus without specially designed patch templates nor search heuristics.
moreover since nmt models generally have a limitedparameter size their training costs and inference efficiency are manageable.
therefore they are well suited for the traditional apr workflow.
motivation in recent years there is a growing interest in using llms e.g.
gpt for apr .
researchers usually follow the current apr workflow positioning llm as a new patch generator to replace former nmt models.
however taking llm as a simple substitute in the patch generation step in the current workflow is an under utilization of its pre trained knowledge since these models exhibit the capability of locating and fixing bugs in buggy functions independently.
for example selfdebugging has been proposed with the concept of allowing llms to progressively refine their generated buggy functions to produce bug free functions by engaging in multiple self dialogue rounds with artifacts like execution traces without using statement level fl to identify buggy hunks.
as shown in fig.
since statement level fl tools may not always provide perfect predictions of buggy hunks restricting llm to fixing the provided hunks may lead to time waste in validating patches at incorrect locations.
moreover as llm s inference overhead and time cost is much higher than traditional apr tools using llm to generate patches for all given hunks may result in huge resource waste.
thus we have the insight below insight prompting llms with buggy programs and corresponding artifacts can enable them to locate and repair buggy hunks simultaneously without statementlevel fl thus further improving apr performance.
this insight arises from the apr methodology of programmers who typically identify buggy hunks through tests and documents and subsequently fix bugs based on these test results.
moreover existing research revealed that models can produce decent fixes on basic compile bugs when supplemented with compiler errors and their patch generation capability can be further improved with the assistant of the given failed test information .
thus we can use a more flexible fl method to identify the buggy segments of the program at a coarse grained level e.g.
using methodlevel fl to find the buggy function and allow the llmas an debugger you should refine the buggy program for bug report.
instruction pre defined example this is a fixed example of bug report and its refine function.
compute a linear combination accurately.
param a factors.
param b factors.
return i ai bi throws dimensionmismatchexception if arrays dimensions don t match public static double linearcombination final double a final double b throws dimensionmismatchexception return result public static double linearcombination final double a final double b throws dimensionmismatchexception if len return a b return result buggy codefixed code java.lang.arrayindexoutofboundsexception final double a .
final double b .
assert.assertequals a b linearcombination a b artifact extractiondocumentfailed testtest infoconstruct bug reportconstruct entire promptbug report program document buggy document failed test failed test test info test info buggy code buggy code instruction and examplellmmodified function 1modified function ...modified function nautomated testingdeveloperverifypatches the prompt instruction and example are pre defined and fixed fig.
.
the workflow of d4c.
it uses the buggy code and its corresponding documents failed tests and test info e.g.
error message to construct the prompt for one shot prompting based program repair without a specific buggy hunk usually provided by statement level fl tools .
to simultaneously locate and fix the buggy hunks within those segments as locating the segments containing the buggy hunks is easier and more practical than using statement level fl to directly find the correct buggy hunks for repair.
we will verify our hypothesis in sec.
iv c via comparing the number of correct patches of two input formats i.e.
w or w o artifacts .
iii.
d4c direct debug drives decent code based on these two insights we developed an apr framework d4c i.e.
direct debug drives decent code.
as shown in fig.
when presented with a new buggy program1 d4c uses the related artifacts including the documents and failed test information to construct a bug report.
then d4c will use this bug report to instruct the model to generate a refined version of the buggy program.
to align llm s inference objective with its pre trained completion objective we adopt a one shot prompting strategy.
this involves prefixing a fixed example of buggy program and its refined version before the target buggy program in the prompt enabling llm to infer their input output relations and finally generate a refined version of target program in expected format.
d4c does not require any additional fine tuning.
it only changes the output format to align with the pre training objectives and uses test information and documents e.g.
javadoc comments in the input prompt to enable repair without the prior knowledge of the correct buggy hunks.
we implemented our approach on gpt series models and mixtral moe models.
next we will introduce our problem definition sec.
iii a and the design details of d4c including model selection sec.
iii b artifact extraction sec.
iii c prompt construction sec.
iii d patch generation sec.
iii e and validation sec.
iii f .
a. problem definition we model the apr task as a completion task that aims at generating a complete refined program based on a buggy 1in our implementation due to the llm s token length limitation a buggy program refer to a buggy function that can be provided by method level fl.program and associated artifacts.
this is different from previous llm based apr approaches which treated the apr task as an infilling orcloze task .
notably the artifacts are essential for d4c to identify the location to modify while infilling apr methods rely on statement level fault localization to pinpoint buggy hunks.
although the artifacts are also used in some approaches to enhance repair they are not necessarily required by all infilling methods.
specifically the optimization objective of d4c can be formally written as arg max p tfixed program tbuggy program tartifacts and the objective of existing infilling style apr is arg max p tfixed hunk tbuggy program tartifacts where tmeans the tokens is the trainable parameters of llms.
the reason for the difference between d4c and the previous methods in task modeling is that d4c is designed to exploit the pre training capability of decoder only llms trained on clm objective whereas infilling style apr is designed to exploit the pre training capability of encoder only llms trained on mlm objective or encoder decoder llm trained on denoising objective.
b. model selection the llm itself is the most vital part of llm based applications.
since the goal of proposing d4c is to illustrate an adaptive way to use current advanced decoder only llms for apr the backbone of d4c should be a decoder only llm which has been pre trained on a substantial code corpus via next token prediction objective.
moreover d4c does not aim at teaching extra knowledge to llms for apr.
instead its apr ability primarily comes from mining the pre trained knowledge via aligning the model s response with its training corpus e.g.
a complete function.
thus to enable d4c exhibit the state of the art apr performance we choose the most advanced llms gpt to serve as the backbone in our implementation.
however since gpt is a black box model whose inference loss of clm objective i.e.
perplexity isunavailable we have to choose another state of the art whitebox model mixtral moe as alternative backbone to validate whether generating a whole refined function can better align the training objective.
we select mixtral moe because at the time of submission the strongest model on the humaneval leaderboard with the longest context window was mixtralmoe.
we will not dive into their architecture or pre training details in this paper but it is worth noting that the backbone for d4c can be changed to other llms with next token prediction clm training objective.
to best ensure our replicability we employed fixed remote api checkpoints or fixed local model versions.
for reference we also provide comparative experiments about using different backbones in sec.
iv c. c. artifact extraction before patch generation d4c needs to extract related artifacts for subsequent prompting.
as shown in fig.
once the buggy function is identified d4c will automated extract documents or comments that describe the general purpose of the function and its input output data types the inputs and expected outputs of the failed test cases and error messages from executing those failed test cases.
our underlying motivation for providing these artifacts to the llm is that llm has gradually exhibited human like analysis and reasoning abilities recently .
instead of treating llm as a tool as in the traditional mindset of training nmt models from scratch we should treat it as an intelligent agent.
from this perspective we contemplate how humans debug and present this procedure to llm for execution.
specifically in the human program repair process we need to first intuitively understand the function s purpose so we refer to related code documents or functionlevel comments.
then we wonder what input can reproduce the bug.
thus we will check the relevant failed test cases and error messages.
finally in the absence of communication or confirmation from other developers i.e.
in a rubber duck debugging cases we can only use these artifacts to simultaneously locate and repair the bug.
therefore we extract artifacts and construct bug report for llm to repair.
notably the implementations of the artifacts extraction procedure may differ due to various data characteristics.
for instance although we default to using function level comments as documents if certain functions do not have this artifact d4c can alternatively exploit related readme documents.
moreover if some artifacts are unavailable in some scenarios e.g.
the test cases of the online judgment are unknown d4c will replace these in the table with a placeholder statement like this program does not possess any known test cases.
d. prompt construction after the extraction of artifacts d4c assembles these artifacts into a comprehensive bug report.
as shown in fig.
the report template is concise with only the most basic notations of the different components in the bug report e.g.
program and comment without providing any complex instructions or requirements.
this allows llm to focus more on the bug report thus accomplishing our goal of inducing and exploiting prompt expected response java public int find int arr extracted responseas an debugger you should refine the buggy program for bug report.
program document java public string findranks string score program document example bug report example refined code target bug reportsystem instruction public int find int arr fig.
.
the prompt structure of d4c.
the details of the code are omitted.
the example pair is fixed which is used to constrain llm s response format.
llm s pre trained knowledge and capabilities for program repair rather than teaching the model to repair.
following this d4c integrates the bug report into a complete prompt which is carefully designed to include a concise role play system instruction e.g.
you are an ai debugger ... and a fixed example consisting of a handcrafted bug report and its refined program.
our underlying goal is to stimulate llm s in context learning icl ability i.e.
analogizing the report refine pair in the prompt to further understand the instruction.
this one shot icl also allows us to restrict the output format of the llm to a refined function rather than other common output formats related to apr e.g.
unit diff patch .
moreover such restricted output format assists us in automatically extracting the corrected functions from llm response for automated patch verification as illustrated in fig.
.
we only use one fixed example to make llms follow the output format since our goal is using d4c to show the effectiveness of our insight rather than developing a powerful method with complex prompting strategy like rag .
notably the implementation of prompts may vary across different models.
specifically in the local text completion llms the prompt can be treated as a single text input.
in the remote chat models that support multi turn dialogues each module of the prompt represents an individual dialogue message in the conversation.
for instance in openai models instructions are encapsulated within the structure body of system messages while the input and output of examples are encapsulated within the structure body of user messages and assistant messages respectively.
in open sourced mixtral instructions are marked via special tokens and while the remaining content is split into different messages using s as separators.
e. patch generation unlike traditional apr tools that generate patches specifically for locations that need modification the patch generation step of d4c refines the entire program in the inference stage.
this is primarily because the core goal of d4c is to explore the extent of apr enhancement by aligning the inference objectiveto decoder only llm s pre training objective.
furthermore we do not introduce an additional patch ranking method which is widely adopted by current apr approaches.
this is because d4c only needs to generate at most 10patches during decoding i.e.
beam search which is significantly less than the sampling number of existing methods i.e.
over hundreds of sampling .
thus d4c can subsequently verify all patches without involving significant waste of effort on incorrect patches within minutes rather than suffering from a time consuming process of verifying hundreds of patches up to5 hours .
additionally although d4c can rank the patches by comparing their naturalness quantified by perplexity or entropy a prior study indicates that it is inaccurate to identify the correctness of the program using naturalness .
therefore we do not use any re ranking strategy to save the computational cost incurred by the perplexity calculation.
following previous llm based work that focus on method level generation we assume that the buggy function is provided where d4c can select lines within the function to modify.
in a real world scenario the buggy function can be provided by existing method level fl tools via analyzing the execution results of failed tests.
f .
patch validation after generating the candidate patches d4c extracts the refined program and uses them to replace the buggy program in the original source code.
then d4c runs the corresponding test suite to find patches that compile successfully and pass all tests.
however due to the potential incomplete test coverage problem of benchmarks the apr methods may often produce patches that pass all tests without exactly meeting the expected functionality of the developer thus do not truly fix the bug.
these patches are known as plausible patches .
ideally developer validation is helpful but the bugs in defects4j are historically fixed and outdated making developer feedback impractical.
to ensure rigor experiment we follow existing apr papers by adding a manual validation for each plausible patch after automated evaluation to identify the correct patches that are the same or semantically equivalent to the human written patch.
iv.
e valuation this section aims to answer the research questions below rq1 how does d4c compare against llm based apr methods?
we compared d4c to state of the art apr methods with and without perfect fl.
we aim to investigate the improvement from aligning output and enhancing input as mentioned in sec.
i. rq2 how effective are the two insights that drive the design of d4c?
we aim to show that generating a refined function better aligns llm s training objective and using the artifacts to locate and repair bugs without given buggy hunks further exploits llm s apr ability.
rq3 how do different components and parameter affects d4c?
we conduct an ablation study and a sensitivity analysis on d4c.
we aim to quantify thecontribution of each prompt component and characterize the performance of d4c in different parameter settings.
a. experiment setup environment and implementation we use a blackbox llm i.e.
gpt and a white box llm i.e.
mixtral 8x7b instruct v0.
via openai apis and 8xa100 nvidia gpu server for program repair.
we use python .
to implement the inference and evaluation scripts in a local machine with ubuntu .
.
lts.
to enhance llm s instruction following ability we manually crafted a fixed bug fix example.
this example is employed in each experiment to construct shot prompting for all bugs.
to adapt to different experiments we modified its input output format according to the corresponding experiment requirement.
to control a moderated randomness of text generation we follow the previous work and set the randomness hyper parameter temperature as1.
.
this is also the default value of openai .
to facilitate patch validation we set a timeout threshold of minutes for each patch.
furthermore we adopt a much smaller sampling number of as the inference budgets fee time etc.
for llms are much larger than traditional methods.
unless otherwise specified all experiments adhered to this setting.
table i statistics of the benchmarks the number of single funciton bugs and the threat of data leakage benchmark bug number data leakage language defects4j yes java debugbench no c java python3 datasets our experiments are conducted on logic single function bugs from debugbench and bugs from defects4j .
specifically debugbench is a new debug benchmark designed to counter data leakage by implanting bugs into source data with gpt which contains a total number of bugs from java c and python3 from leetcode.
in general fixing logic bug is more difficult than fixing other types of bugs.
for example syntax bugs can be easily detected by the compiler or interpreter without testing and their fixing methods are usually provided in the raised exceptions.
thus our paper aligns with the practice of existing apr methods that only focus on fixing logic bugs.
since debugbench is not exposed to the threat of data leakage we use it for our insight validation rq2 .
however due to the complexity to reproduce current baselines on this new benchmark we do not use it for the comparison between d4c and baselines.
defects4j is a widely used apr benchmark with real world bugs from open source repositories.
following previous work we separate the single function bugs of defects4j to v1.
bugs and v2.
bugs .
since it contains data from previous versions of open source projects where most code models have been trained on it faces the threat of data leakage.
however this threat is believed to be less severe in apr compared to otherdomains due to the sparsity of bug fix parallel corpus in the training data meaning that model can only learn the individual buggy or fixed version of the program not their pairs.
despite this the potential threat of data leakage could still undermine the validity of hypothesis verification.
hence defects4j is mainly used as a reference for comparing d4c with existing baselines.
the statistic of the benchmarks are shown in table i. metrics following the previous work we use the number of correct patch to evaluate the apr effectiveness.
specifically if a patch can pass all unit tests then it will considered as a plausible patch.
if this plausible patch truly resolves the bug rather than overfit to pass the unit test only e.g.
via referring the provided failed tests in the prompt then it will be confirmed as a correct patch.
in our evaluation we adhere to prior studies and manually identify whether a plausible patch truly resolves the bug.
notably the bugs from debugbench are collected from leetcode and the leetcode programs that can pass all the unseen test suites from the website are deemed as correct programs.
in our experiment we can only access the provided test examples of each leetcode problem in our bug report rather than the failed unseen test from leetcode oj.
thus we also determine the patches that can pass all the leetcode tests as correct.
to distinguish from manually checked correct patches we refer to patches can pass the leetcode validation as verified patches .
baselines in rq1 we selected several state of the art llm based apr methods as baselines including alpharepair fitrepair chatrepair rap gen and repilot .
we did not compare d4c with learningbased apr methods trained from scratch since our goal is to illustrate the effectiveness of d4c among all the llmbased approaches.
all these baselines employ the infilling objective.
in rq2 we chose different input and output format for comparison to show the superiority of aligning output format and providing artifacts in the input.
we also chose prompts that add or delete different information e.g.
function comments as baselines to investigate the contributions of different information to apr.
b. rq1 comparison against llm based apr we compare d4c with five state of the art llm based apr approaches on defects4j the most commonly used benchmark for apr.
since some of the baselines are not open sourced we cannot re run them to reproduce the result.
thus we reuse their defects4j results reported in the original paper.
although using defects4j may have the the risk of exposing its data to llm s training corpus we still use it for evaluation because existing work has shown that data leakage is less of a concern for apr compared to other code related tasks as the training corpus often contains at most the individual versions of the program i.e.
learning buggy or fixed version only and llms can hardly learn the apr tasks without bug fix pair corpus during training and the baselines have also been evaluated on defects4j hence they are subject to the same data leakage risk .
following prior work we retain thissetting to ensure fairness of the comparison.
the comparison result is shown in table ii and table iii.
table ii the number of correct patches generated by different approaches on defects 4jv1.
v2.
.
method model v1.
v2.
sum sampled alpharepair codebert repilot incoder rap gen codet5 fitrepair codet5 chatrepair gpt d4c gpt table iii the number of correct patches generated in perfect fl and statistical fl ondefects 4jv1.
.
method model perf.
stat.
drop sampled alpharepair codebert .
rap gen codet5 .
d4c gpt .
in table ii we calculated the number of correct patches on defects4j v1.
v2.
and their sum and patches sampled per bug notated as sampled in the perfect localization settings.
specifically for baselines the perfect buggy hunks within the buggy function is provided while for d4c only the buggy function is provided.
notably d4c largely outperforms all the existing state of the art methods provided with perfect buggy hunks for infilling style apr.
d4c gpt can generate almost more correct patches than chatrepair the latest apr methods which uses gpt for multi run dialogue with provided buggy hunks.
moreover d4c only needs to sample times for each bug in patch generation which is fewer than that required by the most efficient baseline sampling at least patches .
the fewer generated patches also illustrate the better efficiency of d4c and less waste of cost in patch validation.
however using only perfect fl settings does not demonstrate the flexibility and practicality of d4c which does not rely on given buggy hunks.
to illustrate its effectiveness in real world scenarios we further conducted experiments of d4c and baselines using existing statistical fl tools.
in table iii we report the number of correct patches on defects4j under perfect fl perf.
and statistical fl stat.
settings.
the baselines use statement level fl tools e.g.
tarantula to identify buggy hunks whereas d4c employs a method level fl tool i.e.
fluccs to locate buggy functions.
we compare d4c only with alpharepair and rapgen on defects4j v1.
for two reasons these are the only methods that reported the evaluation results with nonperfect fl setting on defects4j v1.
in the paper and fluccs s code supports only defects4j v1.
.
in our experiments d4c generates only patches for the most suspicious faulty function top method given by fluccs.
notably d4c repairs only fewer bugs compared to the perfect fl buggy code void bubblesort int a for int i i a.length i for int j i j a.length j ...void bubblesort int a for int i i a.length i infill ...report mask void bubblesort int a for int i i a.length i for int j i j a.length j ...func for int j i j a.length j hunkoutput formatinput formatfig.
.
examples of input and output format used in rq2.
settings with a minimal drop of .
in repair success.
this is significantly lower than the infilling style apr baselines which experienced at least a drop.
these results suggest that d4c is less affected by inaccuracies in fl since locating buggy functions is easier than locating buggy hunks.
overall the results in table ii and table iii suggest that d4c is effective and efficient.
however due to the potential of data leakage we cannot directly verify our insight of d4c via comparing current approaches.
thus we conduct more indepth experiments on debugbench in sec.
iv c and sec.
iv d. c. rq2 effectiveness of the two insights rq2 aims to validate the effectiveness of the two insights introduced in sec.
ii and how the two insights contribute to the overall performance improvement in d4c.
we design two additional questions for the insights iq1 does allowing llm to generate a complete refined function better align its training objectives and result in better performance?
iq2 does providing the artifacts enable llm to locate and repair buggy hunks itself and achieves a better performance?
specifically we evaluate the two insights on debugbench since there is no threat of data leakage.
thus we use verified patches i.e.
patches verified by leetcode unseen tests in debugbench for the evaluation.
validation of insight to answer the iq1 question we calculate the average perplexity the inference loss of clm objective eq.
for various output format on the white box model i.e.
mixtral moe .
measuring perplexity is the most straightforward approach to evaluate whether the generated content aligns with the distribution of training corpus as llms are trained to minimize the clm objective .
thus a lower perplexity value indicate the model is more confident at predicting a given sequence.
specifically we adopt two output formats including the fixed hunk patches that is used by infilling style apr denoted as hunk and the full function output used by d4c func .
fig.
shows the examples of these output format.
in our implementation we also carefully modified the system instructions to adapt to each output setting.
to control variables we also calculate the perplexity of generated output o and the entire input output io .
thus we can compare the perplexity among different output format to validate our insight ignoring the difference between each input format.table iv comparison of different settings of d4c ondebug bench .
w e measure perplexity and verified patches to validate insights .
formatperplexity mixtral verified gpt verified o io c java python c java python mask hunk .
.
mask func .
.
report hunk .
.
report func .
.
table iv shows the results.
we observe that when the output is an entire function the output perplexity is much lower than using the fixed hunks as the output regardless of the input.
this directly validates that generating a complete function is the better way to align the training objective.
to further validate that it is the objective alignment that helps d4c achieve a better apr performance we evaluate the verified patches generated in each case.
the result illustrates that using complete function as output can result in generation of more verified patches with lower perplexity than using discrete hunks.
moreover when only generating the fixed hunks as the output the output perplexity is much higher than its perplexity calculated with the whole input and output sequence.
this indicates that completing discrete hunks only is misaligned with llm s training objective.
overall these findings illustrate the effectiveness of our first insight.
validation of insight to answer the iq2 question we evaluate the patch generation ability on debugbench using two different input formats i.e.
the bug report notated as report and the buggy program where the buggy hunks are masked mask .
to control variables we also conducted experiments using the two previously established output formats as shown in fig.
.
for the generated patches by report hunk as we do not provide a specific location to modify and cannot precisely determine where the generated patch should be applied we instruct mixtral moe to help us to automatically apply the generated patches to the original buggy program before patch validation.
table iv shows the number of generated verified patches on both mixtral moe and gpt .
the results indicate that report func is the most effective among all evaluated groups including mask hunk which is the input output format of infilling style apr.
this phenomenon aligns with prior evaluations on defects4j rq1 .
it is worthwhile to note in the experiment in sec.
iv b the effectiveness of d4c stems not only from the bug report design but also the objective alignment of the output.
however even when we use the same output format in this experiment the performance of using bug report as input still outperforms that of using program where the buggy hunks are masked.
this suggests that bug report helps not only in locating the buggy hunks but also give more information for patch generation.
thus this phenomenon validates our second insight.table v the number of patches generated for our ablation study and sensitive analysis .
w e measure plausible patches for defects 4j and patches verified via unseen tests for debug bench .
benchmarkdefects4j plausible debugbench verified v1.
v2.
sum c java python sum w o document w o test w o message mask pure samp samp samp default temp .
temp .
default d. rq3 ablation study and sensitive analysis in this section we aim to quantify the contribution of each component and investigate the sensitivity of d4c to the sampling number and temperature.
we do not additionally validate the sensitivity on different prompts since we have carefully investigated the impact of different prompt formats in rq2.
we use gpt as the backbone set the default temperature to .
and use the sampling number of by default.
as the manual validation of patch correctness on defects4j is time consuming we use the plausible patches in this section.
we also use verified patches to evaluate the repair performance on debugbench.
we design the baselines below to check the effectiveness of each component w o document d4c without including the program documents or function comments in the prompt.
w o test d4c without including inputs and expected outputs of the failed test cases in the prompt.
w o message d4c without including error message from the failed tests in the prompt.
mask d4c using masks to identify bugs without including any artifacts in the prompt mask func .
pure d4c without including any guidance like mask or artifacts to identify bugs in the prompt i.e.
an aligned llm.
table v shows the results.
overall the results show that each component is important in guiding d4c in generating plausible verified patches across the two benchmarks.
we also observe that the mask baseline are the least effective among all baselines with bug location guidance because d4c cannot use any information from the artifact to fix the defect.
meanwhile among the first three baselines where we remove one information from the prompt we notice that program documents w o document are the least effective as d4c is able to generate the greatest number of plausible verified patches without using the document.
this is expected as the error message and test are more closely related to the defect than the comments that merely describe general code features.
without guidance on buggy locations pure gpt can still fix bugs by following the aligned one shot example pair to generate a refined function.
while its effectiveness ondefects4j might be due to potential data leakage during pretraining debugbench is not affected by this issue.
despite this gpt with the completion objective still fixes more bugs vs on debugbench than gpt with the infilling objective and masked hunk mask hunk in table iv .
this suggests llms have learned to write bug free code during pretraining.
by aligning the inference task with the pre trained code completion task we can effectively leverage their pretrained knowledge to produce high quality code.
in terms of sample size we notice that a sample size of is the optimal setting to allow d4c to find the correct patches.
we found that increasing the sampling number from to significantly increased the number of plausible and verified patches but further increasing the sampling number to resulted in a smaller improvement.
since d4c can already achieve satisfactory results when setting the sampling number to we use it as the default sample number for d4c.
we also observed that setting the temperature to results in fewer plausible and verified patches.
thus we adopt a multisampling for d4c rather than greedy decoding temp .
v. d iscussion a. threats to validity external threats.
one external threat comes from the potential data leakage problem i.e.
using defects4j in rq1 which may have been incorporated in llm pre training .
we have discussed this threat in rq1 i.e.
data leakage is not significant on apr and the baselines have also been evaluated on defects4j .
to eliminate this threat in our insight validation we selected debugbench a latest leakage free benchmark for evaluation.
this not only keeps a fair comparison in rq1 but also ensures the credibility of our conclusions in rq2 and rq3.
as the effectiveness of the model may vary in different settings our results may not generalize beyond the studied settings and other programming languages beyond the supported ones in defects4j and debugbench.
we mitigate this threat by reusing configurations in prior work and widely used benchmarks.
internal threats.
an internal threat lies in the incomplete test coverage problem of defects4j which do not guarantee the correctness of test passing patches.
to mitigate this threat we follow the previous work to manually check the correctness of plausible patches.
to mitigate the potential bias in the manual analysis two authors independently confirm the patch correctness.
any patches with disagreement were presented to the third author for review.
however since we could not submit the patch to the developers of the corresponding repository for confirmation this threat could not be completely eliminated.
therefore we decide to release our experimental results for public verification.
b. qualitative analysis of plausible patches table v shows that d4c generates a total of plausible patches but are incorrect with respect to the human patches.
fig.
shows one example of the incorrect patch.
since the failed test contains the src tag as input llm can naturallydeveloper patch d4c patchprivate boolean testvalidprotocol element el attribute attr set protocol protocols string value el.absurl attr.getkey if value.
length value attr.getvalue if attr.getkey .equals src attr.getvalue .startswith cid attr.getvalue .startswith data value attr.getvalue if !preserverelativelinks attr.
setvalue value string html img src cid img src data gzzt string obj whitelist.
basicwithimages .addprotocols img src cid data string preserved jsoup.clean html obj assertequals img src cid n img src data gzzt preserved failed testfig.
.
a plausible patch generated at developer patch locations jsoup developer patch d4c patchprivate boolean compute object left object right double ld infosetutil.
doublevalue left if double.
isnan ld return false double rd infosetutil.
doublevalue right if double.
isnan rd return false if double.
isnan ld double.
isnan rd return false return evaluatecompare ld rd ?
ld rd ?
fig.
.
a correct patch generated at non developer patch locations jxpath generate patches that only pass the test containing src without considering patch robustness.
this scenario is similar to the overfitting problem in apr .
to check for the overfitting problem two of the authors independently evaluate the patch correctness followed by a third person to review and confirm their disagreement to ensure the accuracy of the validation.
in our manual validation we also found that when d4c can generate five or more plausible patches for a buggy program in samplings most of these patches are correct.
this observation aligns with previous work i.e.
responses generated multiple times tend to be more reliable .
debugbench does not suffer from the overfitting problem because the given test examples are not used in its unseen test suite when checking for patch correctness.
as we allow d4c to debug without given buggy hunks we also observe that some correct patches have been applied at different locations compared to the developer patches.
fig.
shows an example where the developer patch modifies two if conditions so it is a hunk bug in the dataset.
however d4c only needs to insert one line which combines the two conditions and results in a correct patch that is semantically equivalent to the developer patch.
furthermore if we ask gpt4 to generate patches at these two hunks gpt can only write twoexception statements and fails to pass the unit tests.
this example shows that allowing llms to generate patches without restricting the location may provide more flexibility allowing generation of correct patches.
c. token cost and price of patch generation the cost of patch generation is a vital criteria to evaluate d4c s practicality.
for defects4j the average input promptlength is .
tokens overall with .
for resolved bugs and .
for unresolved bugs.
the average output completion length is .
tokens with .
for resolved bugs and .
for unresolved bugs.
given openai s pricing of .
per input tokens and .
per output tokens the average cost per patch is .
.
for resolved bugs and .
for unresolved bugs .
generating patches per bug results in a total cost of .
for the entire dataset.
notably d4c fixed bugs in defects4j with the correct patch being the .29th out of patches generated per bug on average std.
.
.
thus if we stop patch generation once a correct patch is found the average cost per bug will be reduced to .
.
d. wasted effort of patch validation the wasted effort of patch validation is another criteria to evaluate d4c s practicality.
due to weak test oracles not all test passing plausible patches are correct and may have anti patterns .
thus there is a labor cost of manually validating the correctness of plausible patches beyond the token cost of patch generation.
specifically d4c generates an average of .
plausible patches per bug std.
.
with every .06th plausible patch being correct std.
.
.
compared to infilling apr methods which often require users to manually verify over plausible patches before finding a correct patch with patch ranking and over without ranking d4c is significantly more user friendly.
even in the theoretical worst case users only need to validate up to patches per bug which is significantly fewer than with existing methods.
as discussed in sec.
iii e this efficiency explains why d4c does not require the additional re ranking mechanisms to facilitate patch validation.
moreover there is also a time cost for running unit tests during automated patch validation.
however d4c requires running tests only up to minutes per bug with a timeout of 1min per patch.
this is much fewer than a search time limit of up to hours used in previous work .
this illustrates d4c s efficiency.
vi.
c onclusion this paper presents a new approach to adapt llms to automated program repair.
our key insight is that the effectiveness of llm based apr can be enhanced by aligning the output to their training objective and allowing them to refine the whole program without given buggy hunks.
based on this insight we designed d4c an llm based prompting framework for apr.
our evaluation shows that d4c outperforms the sota apr methods with perfect fl by .
our findings call for adopting a new paradigm and debugging workflow for future llm based apr approaches.
vii.
a cknowledgment this paper was supported by the guangdong basic and applied basic research foundation no.
2024a1515010145 and the shenzhen science and technology program no.
zdsys20230626091302006 1data availability our source code and experimental results are publicly available at d. h. o dell understanding the psychology of learning strategies leads to effective problem solving skills cfm?id .
m. monperrus automatic software repair a