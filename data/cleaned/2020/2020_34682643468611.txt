empirical studyof transformers for sourcecode nadezhda chirkova hse university moscow russia nchirkova hse.rusergeytroshin hse university moscow russia stroshin hse.ru abstract initiallydevelopedfornaturallanguageprocessing nlp transformers are now widely used for source code processing due to the format similarity betweensource code and text.
in contrast to naturallanguage sourcecodeisstrictlystructured i.e.
itfollows the syntax of the programming language.
several recent works developtransformermodificationsforcapturingsyntacticinformationinsourcecode.thedrawbackoftheseworksisthatthey donotcomparetoeachotherandconsiderdifferenttasks.inthis work we conduct a thorough empirical study of the capabilities of transformers to utilize syntactic information in different tasks.
we consider three tasks code completion function naming and bug fixing andre implement different syntax capturingmodifications in a unified framework.
we show that transformers are able to makemeaningfulpredictionsbasedpurelyonsyntacticinformation andunderlinethebestpracticesoftakingthesyntacticinformation intoaccount forimprovingthe performance ofthe model.
ccsconcepts computing methodologies neural networks .
keywords neural networks transformer variable misuse detection function naming code completion acm referenceformat nadezhda chirkova and sergey troshin.
.
empirical study of transformers for source code.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of softwareengineering esec fse august 23 28 athens greece.
acm newyork ny usa 13pages.
introduction transformer iscurrentlyastate of the artarchitectureinalot of source code processing tasks including code completion code translation and bug fixing .
particularly transformers were shown to outperform classic deep learning architectures e.g.
recurrent rnns recursive and convolutional neural networks in the mentioned tasks.
these architectures focus on localconnections between input elements while transformer processesallinputelementsinparallelandfocusesoncapturing global esec fse august23 28 athens greece copyright held bytheowner author s .
acm isbn978 .
in data producing more meaningful code representations .thisparallelismalso speedsuptrainingandprediction.
transformer is often applied to source code directly treating code as a sequence of language keywords punctuation marks and identifiers.inthiscase aneuralnetworkmostlyreliesonidentifiers e.g.variable names to make predictions .
high quality variablenamescanbearichsourceofinformationaboutthesemantics ofthecode however thisisonlyanindirect secondarysourceof information.theprimary source ofinformationof whatthe code implements is its syntactic structure.
transformer architecture relies on the self attention mechanism that is not aware of the order or structure of input elements and treatstheinputasanunordered bagofelements.toaccountforthe particularstructureoftheinput additionalmechanismsareusually used e.g.positionalencodingforprocessingsequentialstructure.
in recent years a line of research has developed mechanisms for utilizingtreestructureofcodeintransformer .however the mosteffective wayofutilizing syntacticinformationin transformerisstillunclearforthreereasons.first themechanismswere developedconcurrently sotheywerenotcomparedtoeachotherby their authors.
moreover different workstest the proposed mechanismsondifferentcodeprocessingtasks makingithardtoalignthe empiricalresultsreportedinthepapers.
secondly the mentioned worksusedstandardtransformerwithpositionalencodingsasa baseline while modern practice uses more advanced modifications of transformer e.g.
equipping it with relative attention .
as a result it is unclearwhether usingsophisticated mechanisms for utilizing syntactic information is needed at all.
thirdly most of the worksfocusonutilizingtreestructureintransformeranddonot investigatethe effectof processingothersyntax components e. g. the syntactic units ofthe programminglanguage.
inthiswork weconductanempiricalstudyofusingtransformer for processing source code.
firstly we would like to answer the question what is the best way of utilizing syntactic information in transformer and to provide practical recommendations for the use oftransformersinsoftwareengineering tasks.secondly we aim at understanding whether transformer is generally suitable for capturing code syntax to ground the future development of transformers forcode.ourcontributions are as follows were implementseveralapproachesforcapturingsyntactic structure in transformerand investigate their effectiveness inthreecodeprocessingtasksontwodatasets.weunderline the importance of evaluating code processing models on severaldifferenttasks andbelieve that our workwill help to establish standardbenchmarks in neuralcode processing.
weintroduceananonymizedsettinginwhichalluser defined identifiers are replaced with placeholders and show that transformer is capable of making meaningful predictions based purely on syntactic information in all three tasks.
we this work is licensed under a creative commons attributionnoderivatives .
internationallicense.
esec fse august athens greece n.chirkova ands.troshin elem lst latexit sha1 base64 lhnyvmnst74ibxifjnqjliqemv0 aaab 3icbvbns8naen34wetxvpdizbeinkrsi16eohepfewhpkfsnpn26wytdjdisxvwr3jxoihx 4y3 43bngdtftdweg gmxlbypnsjvntrayurw9slrbk2zu7e v2wwfljzmk0kqjt2qniao4e9duthpopbjihhbob8obqd9 aklyiu71kau jn3bikajnllppu4anxgdhxhfya60x8jhf9yzk07vmqeve7cgfvsg0bo umfcsxieppwo5bloqv2csm0oh0m5mylicr2spnigchkd8vpz rn8zpqqr4k0jtseqb8nchirnyod0xktpvcl3lt8z myhv36ornppkhq aio41gnebogdpkeqvnieeilm7dioicsug0ik5sq3mwxl0mrvnwdqntxq9svizhk6asdonpkogtur7eogzqiojf6rq ozxqyxqx362peumivm0fod6zph9h3lfg latexit latexit sha1 base64 lhnyvmnst74ibxifjnqjliqemv0 aaab 3icbvbns8naen34wetxvpdizbeinkrsi16eohepfewhpkfsnpn26wytdjdisxvwr3jxoihx 4y3 43bngdtftdweg gmxlbypnsjvntrayurw9slrbk2zu7e v2wwfljzmk0kqjt2qniao4e9duthpopbjihhbob8obqd9 aklyiu71kau jn3bikajnllppu4anxgdhxhfya60x8jhf9yzk07vmqeve7cgfvsg0bo umfcsxieppwo5bloqv2csm0oh0m5mylicr2spnigchkd8vpz rn8zpqqr4k0jtseqb8nchirnyod0xktpvcl3lt8z myhv36ornppkhq aio41gnebogdpkeqvnieeilm7dioicsug0ik5sq3mwxl0mrvnwdqntxq9svizhk6asdonpkogtur7eogzqiojf6rq ozxqyxqx362peumivm0fod6zph9h3lfg latexit latexit sha1 base64 lhnyvmnst74ibxifjnqjliqemv0 aaab 3icbvbns8naen34wetxvpdizbeinkrsi16eohepfewhpkfsnpn26wytdjdisxvwr3jxoihx 4y3 43bngdtftdweg gmxlbypnsjvntrayurw9slrbk2zu7e v2wwfljzmk0kqjt2qniao4e9duthpopbjihhbob8obqd9 aklyiu71kau jn3bikajnllppu4anxgdhxhfya60x8jhf9yzk07vmqeve7cgfvsg0bo umfcsxieppwo5bloqv2csm0oh0m5mylicr2spnigchkd8vpz rn8zpqqr4k0jtseqb8nchirnyod0xktpvcl3lt8z myhv36ornppkhq aio41gnebogdpkeqvnieeilm7dioicsug0ik5sq3mwxl0mrvnwdqntxq9svizhk6asdonpkogtur7eogzqiojf6rq ozxqyxqx362peumivm0fod6zph9h3lfg latexit latexit sha1 base64 lhnyvmnst74ibxifjnqjliqemv0 aaab 3icbvbns8naen34wetxvpdizbeinkrsi16eohepfewhpkfsnpn26wytdjdisxvwr3jxoihx 4y3 43bngdtftdweg gmxlbypnsjvntrayurw9slrbk2zu7e v2wwfljzmk0kqjt2qniao4e9duthpopbjihhbob8obqd9 aklyiu71kau jn3bikajnllppu4anxgdhxhfya60x8jhf9yzk07vmqeve7cgfvsg0bo umfcsxieppwo5bloqv2csm0oh0m5mylicr2spnigchkd8vpz rn8zpqqr4k0jtseqb8nchirnyod0xktpvcl3lt8z myhv36ornppkhq aio41gnebogdpkeqvnieeilm7dioicsug0ik5sq3mwxl0mrvnwdqntxq9svizhk6asdonpkogtur7eogzqiojf6rq ozxqyxqx362peumivm0fod6zph9h3lfg latexit assign namestore elemsubscriptload nameload lstindex nameload idx1 6u d d4 latexit sha1 base64 qqjnmssqddqo32tpxszjqif3oiw aaacbhicbvhjtsmwehxchrayheavkkvfxalnynpecoeiegwkpqocd9paoe5ko0hv6ik 5myncoebcnoulzaxnvvm5o3hhgcxz0q77qdlz80vlc4trzira sbm4wt7ucvjzjcnuy8ks8bucczglpmmsnzligeayen4oumyz 9glqseg 6h0mzjf3boowsbuktwrsfqjejvjmg4uqo0jc6sqby72uno9jdzvwzode4ntgzome pxzujv nrnxacmv52mt8z0lug2j 9mwvsm7fhrqejv5osii3u1bhw29hnalbamqjug3pjxuzjvizymhg imcmnax0owgoykeojrpcfgdfgqibdyjpihqebidrehjqfq diwyjlqnpnnz8l9ci9gdy2bkrjxoehtuqjnwrcoctr63mqsqed8qqiuzd8w0rysh2vypy4bgtt95ljzwkp5b8e5rpavrfbzlqigo0thy0aw6qrfodturrv wprvn7vvf9q5dta9gutvka3bqh7plp8s8r7g latexit latexit sha1 base64 qqjnmssqddqo32tpxszjqif3oiw aaacbhicbvhjtsmwehxchrayheavkkvfxalnynpecoeiegwkpqocd9paoe5ko0hv6ik 5myncoebcnoulzaxnvvm5o3hhgcxz0q77qdlz80vlc4trzira sbm4wt7ucvjzjcnuy8ks8bucczglpmmsnzligeayen4oumyz 9glqseg 6h0mzjf3boowsbuktwrsfqjejvjmg4uqo0jc6sqby72uno9jdzvwzode4ntgzome pxzujv nrnxacmv52mt8z0lug2j 9mwvsm7fhrqejv5osii3u1bhw29hnalbamqjug3pjxuzjvizymhg imcmnax0owgoykeojrpcfgdfgqibdyjpihqebidrehjqfq diwyjlqnpnnz8l9ci9gdy2bkrjxoehtuqjnwrcoctr63mqsqed8qqiuzd8w0rysh2vypy4bgtt95ljzwkp5b8e5rpavrfbzlqigo0thy0aw6qrfodturrv wprvn7vvf9q5dta9gutvka3bqh7plp8s8r7g latexit latexit sha1 base64 qqjnmssqddqo32tpxszjqif3oiw aaacbhicbvhjtsmwehxchrayheavkkvfxalnynpecoeiegwkpqocd9paoe5ko0hv6ik 5myncoebcnoulzaxnvvm5o3hhgcxz0q77qdlz80vlc4trzira sbm4wt7ucvjzjcnuy8ks8bucczglpmmsnzligeayen4oumyz 9glqseg 6h0mzjf3boowsbuktwrsfqjejvjmg4uqo0jc6sqby72uno9jdzvwzode4ntgzome pxzujv nrnxacmv52mt8z0lug2j 9mwvsm7fhrqejv5osii3u1bhw29hnalbamqjug3pjxuzjvizymhg imcmnax0owgoykeojrpcfgdfgqibdyjpihqebidrehjqfq diwyjlqnpnnz8l9ci9gdy2bkrjxoehtuqjnwrcoctr63mqsqed8qqiuzd8w0rysh2vypy4bgtt95ljzwkp5b8e5rpavrfbzlqigo0thy0aw6qrfodturrv wprvn7vvf9q5dta9gutvka3bqh7plp8s8r7g latexit latexit sha1 base64 qqjnmssqddqo32tpxszjqif3oiw aaacbhicbvhjtsmwehxchrayheavkkvfxalnynpecoeiegwkpqocd9paoe5ko0hv6ik 5myncoebcnoulzaxnvvm5o3hhgcxz0q77qdlz80vlc4trzira sbm4wt7ucvjzjcnuy8ks8bucczglpmmsnzligeayen4oumyz 9glqseg 6h0mzjf3boowsbuktwrsfqjejvjmg4uqo0jc6sqby72uno9jdzvwzode4ntgzome pxzujv nrnxacmv52mt8z0lug2j 9mwvsm7fhrqejv5osii3u1bhw29hnalbamqjug3pjxuzjvizymhg imcmnax0owgoykeojrpcfgdfgqibdyjpihqebidrehjqfq diwyjlqnpnnz8l9ci9gdy2bkrjxoehtuqjnwrcoctr63mqsqed8qqiuzd8w0rysh2vypy4bgtt95ljzwkp5b8e5rpavrfbzlqigo0thy0aw6qrfodturrv wprvn7vvf9q5dta9gutvka3bqh7plp8s8r7g latexit assign namestore subscriptload ... hemptyielem hemptyi... latexit sha1 base64 lkpxnop1jfhiuekugx0o76isz9e aaaclxichvflb9qwehbcq4txug4cufhsqzyipbd6qwolqhwqfjvtk61xq4kzm7xq2je9qaxc hg hhv bm 6qtaimzblz9 8z4pgk09z9jokr12 cfpw1u3kzt179x mhm6fens6irnptxvnbxjuyucefgk8axxcxwg8lc5fr wnx9b5zc1nwju4q6eyaqekukdmo iweqzjqbonbi k9 g0 pkwhtvgf6cf4aaj8k6dpi4lbx0qqh3fsrwt9nucc6w6 zjjtbgko2cc8kv1ghd0krnwg3stjbhjxv4 m8y4viqobfoyt fzufjlm0g4vdbvgfjtpgj eihkk1sazqknxg zbogzh04uljjn4jwywpyhcqcbmhco37wdvpt bpalhxhxbig md 6dfb7f2qlojldbt0l3vr8l 6auulvvmntnmsgnmranfqtpavv8rl5vcsxguaydahvi6x4ebswgqshpbfbvkqonln8yznp 2o9w8349hit9ht9oll7cxbz yezswgw1he9fbdbg jl fb k3f6zxtpf5xp6s omvtlbgcw latexit latexit sha1 base64 lkpxnop1jfhiuekugx0o76isz9e aaaclxichvflb9qwehbcq4txug4cufhsqzyipbd6qwolqhwqfjvtk61xq4kzm7xq2je9qaxc hg hhv bm 6qtaimzblz9 8z4pgk09z9jokr12 cfpw1u3kzt179x mhm6fens6irnptxvnbxjuyucefgk8axxcxwg8lc5fr wnx9b5zc1nwju4q6eyaqekukdmo iweqzjqbonbi k9 g0 pkwhtvgf6cf4aaj8k6dpi4lbx0qqh3fsrwt9nucc6w6 zjjtbgko2cc8kv1ghd0krnwg3stjbhjxv4 m8y4viqobfoyt fzufjlm0g4vdbvgfjtpgj eihkk1sazqknxg zbogzh04uljjn4jwywpyhcqcbmhco37wdvpt bpalhxhxbig md 6dfb7f2qlojldbt0l3vr8l 6auulvvmntnmsgnmranfqtpavv8rl5vcsxguaydahvi6x4ebswgqshpbfbvkqonln8yznp 2o9w8349hit9ht9oll7cxbz yezswgw1he9fbdbg jl fb k3f6zxtpf5xp6s omvtlbgcw latexit latexit sha1 base64 lkpxnop1jfhiuekugx0o76isz9e aaaclxichvflb9qwehbcq4txug4cufhsqzyipbd6qwolqhwqfjvtk61xq4kzm7xq2je9qaxc hg hhv bm 6qtaimzblz9 8z4pgk09z9jokr12 cfpw1u3kzt179x mhm6fens6irnptxvnbxjuyucefgk8axxcxwg8lc5fr wnx9b5zc1nwju4q6eyaqekukdmo iweqzjqbonbi k9 g0 pkwhtvgf6cf4aaj8k6dpi4lbx0qqh3fsrwt9nucc6w6 zjjtbgko2cc8kv1ghd0krnwg3stjbhjxv4 m8y4viqobfoyt fzufjlm0g4vdbvgfjtpgj eihkk1sazqknxg zbogzh04uljjn4jwywpyhcqcbmhco37wdvpt bpalhxhxbig md 6dfb7f2qlojldbt0l3vr8l 6auulvvmntnmsgnmranfqtpavv8rl5vcsxguaydahvi6x4ebswgqshpbfbvkqonln8yznp 2o9w8349hit9ht9oll7cxbz yezswgw1he9fbdbg jl fb k3f6zxtpf5xp6s omvtlbgcw latexit latexit sha1 base64 lkpxnop1jfhiuekugx0o76isz9e aaaclxichvflb9qwehbcq4txug4cufhsqzyipbd6qwolqhwqfjvtk61xq4kzm7xq2je9qaxc hg hhv bm 6qtaimzblz9 8z4pgk09z9jokr12 cfpw1u3kzt179x mhm6fens6irnptxvnbxjuyucefgk8axxcxwg8lc5fr wnx9b5zc1nwju4q6eyaqekukdmo iweqzjqbonbi k9 g0 pkwhtvgf6cf4aaj8k6dpi4lbx0qqh3fsrwt9nucc6w6 zjjtbgko2cc8kv1ghd0krnwg3stjbhjxv4 m8y4viqobfoyt fzufjlm0g4vdbvgfjtpgj eihkk1sazqknxg zbogzh04uljjn4jwywpyhcqcbmhco37wdvpt bpalhxhxbig md 6dfb7f2qlojldbt0l3vr8l 6auulvvmntnmsgnmranfqtpavv8rl5vcsxguaydahvi6x4ebswgqshpbfbvkqonln8yznp 2o9w8349hit9ht9oll7cxbz yezswgw1he9fbdbg jl fb k3f6zxtpf5xp6s omvtlbgcw latexit ...nameload index nameload ... lst hemptyiidx latexit sha1 base64 e0vvauzxo4rrexdh3y 3kueu0ai aaacc3icbvfnb9qwehxcvxs uodepqes7ok4ecuicy5vuybuoskxbax1ajvxznetok5kt9cuqv5afx43 gux7jjzfsoty9l6evoezjytvvo5spkfqxjr9p2793z2o sphj7agzx curk2kocy1kx9jwdh1ozhjmijeevrsgyjwfzxycuf ynrvol urrcqcfliyakwnkqdngums4ukyhygontm3k9 60pirjml kn6ha4xjydz zhfdxksg4whavi74ra0f hqknzqgrc0g4ogalitytf7znr16h8lvnjgsa g p2wcyxekf czit2ditneyg wqesnrag1jdc5n0qsiaqowlntyrqj2wig8gavopds btdt pm1 ivncj4vrb gem9edtrqolcumq8sgjbueq4j 5eb1dr p22uqwpcizef5rxmvpjuatxxfixptqcgrfk9crkec5l8mii hpt6l2 c0zdxmstpl7fdw6ptohbypjtgr1jk3rfd9pgdsdgt7ffwlhge8ob3ub8ehkonnay2nqfsnwhf wh0frkg latexit latexit sha1 base64 e0vvauzxo4rrexdh3y 3kueu0ai aaacc3icbvfnb9qwehxcvxs uodepqes7ok4ecuicy5vuybuoskxbax1ajvxznetok5kt9cuqv5afx43 gux7jjzfsoty9l6evoezjytvvo5spkfqxjr9p2793z2o sphj7agzx curk2kocy1kx9jwdh1ozhjmijeevrsgyjwfzxycuf ynrvol urrcqcfliyakwnkqdngums4ukyhygontm3k9 60pirjml kn6ha4xjydz zhfdxksg4whavi74ra0f hqknzqgrc0g4ogalitytf7znr16h8lvnjgsa g p2wcyxekf czit2ditneyg wqesnrag1jdc5n0qsiaqowlntyrqj2wig8gavopds btdt pm1 ivncj4vrb gem9edtrqolcumq8sgjbueq4j 5eb1dr p22uqwpcizef5rxmvpjuatxxfixptqcgrfk9crkec5l8mii hpt6l2 c0zdxmstpl7fdw6ptohbypjtgr1jk3rfd9pgdsdgt7ffwlhge8ob3ub8ehkonnay2nqfsnwhf wh0frkg latexit latexit sha1 base64 e0vvauzxo4rrexdh3y 3kueu0ai aaacc3icbvfnb9qwehxcvxs uodepqes7ok4ecuicy5vuybuoskxbax1ajvxznetok5kt9cuqv5afx43 gux7jjzfsoty9l6evoezjytvvo5spkfqxjr9p2793z2o sphj7agzx curk2kocy1kx9jwdh1ozhjmijeevrsgyjwfzxycuf ynrvol urrcqcfliyakwnkqdngums4ukyhygontm3k9 60pirjml kn6ha4xjydz zhfdxksg4whavi74ra0f hqknzqgrc0g4ogalitytf7znr16h8lvnjgsa g p2wcyxekf czit2ditneyg wqesnrag1jdc5n0qsiaqowlntyrqj2wig8gavopds btdt pm1 ivncj4vrb gem9edtrqolcumq8sgjbueq4j 5eb1dr p22uqwpcizef5rxmvpjuatxxfixptqcgrfk9crkec5l8mii hpt6l2 c0zdxmstpl7fdw6ptohbypjtgr1jk3rfd9pgdsdgt7ffwlhge8ob3ub8ehkonnay2nqfsnwhf wh0frkg latexit latexit sha1 base64 e0vvauzxo4rrexdh3y 3kueu0ai aaacc3icbvfnb9qwehxcvxs uodepqes7ok4ecuicy5vuybuoskxbax1ajvxznetok5kt9cuqv5afx43 gux7jjzfsoty9l6evoezjytvvo5spkfqxjr9p2793z2o sphj7agzx curk2kocy1kx9jwdh1ozhjmijeevrsgyjwfzxycuf ynrvol urrcqcfliyakwnkqdngums4ukyhygontm3k9 60pirjml kn6ha4xjydz zhfdxksg4whavi74ra0f hqknzqgrc0g4ogalitytf7znr16h8lvnjgsa g p2wcyxekf czit2ditneyg wqesnrag1jdc5n0qsiaqowlntyrqj2wig8gavopds btdt pm1 ivncj4vrb gem9edtrqolcumq8sgjbueq4j 5eb1dr p22uqwpcizef5rxmvpjuatxxfixptqcgrfk9crkec5l8mii hpt6l2 c0zdxmstpl7fdw6ptohbypjtgr1jk3rfd9pgdsdgt7ffwlhge8ob3ub8ehkonnay2nqfsnwhf wh0frkg latexit 1iddddddddd 2u iududdudduddd 3uud idddd 4uuuud uiududd 5uuuud uud id 6uuuuuud uuuud u i latexit sha1 base64 jmc2zujayrkvdevqpyvniekocg aaadonicbzi9b9sweiyppultt0mczuxc1ejryzdy1y5b6yhdhcbkaligqdfnmzbfcsrvwfd9u7rkv3tlkcvdiqjrfkcoj9ayharv4xd38bv1yhbzprtj3fr22ov1jzebrxqv32xt7zr3316qkjeupbrxsf4hrafnajznnifrwaijaw5xwfrr1r 6dlkxsfzowqz9kiwfgzfktckndq2uh8cyivstiofeztmfdomzn xjzt7ah7brdgb0ahrkdgx0gn0fl0jw 2bu adk6fs4zmmrwa th6vyzq9lnudq5gsqo42xy4vf3xxbhq sfesfd5ijbviqdfsf64my h gonly2k6 8grilkkllas7ap7yhxfnqhcacqjuz3vi3u j1ixympnpfmsetskyeiyvjatvt nfp8f7pjleo0gacy3zanvhskklzmfgyj sha1 base64 jmc2zujayrkvdevqpyvniekocg aaadonicbzi9b9sweiyppultt0mczuxc1ejryzdy1y5b6yhdhcbkaligqdfnmzbfcsrvwfd9u7rkv3tlkcvdiqjrfkcoj9ayharv4xd38bv1yhbzprtj3fr22ov1jzebrxqv32xt7zr3316qkjeupbrxsf4hrafnajznnifrwaijaw5xwfrr1r 6dlkxsfzowqz9kiwfgzfktckndq2uh8cyivstiofeztmfdomzn xjzt7ah7brdgb0ahrkdgx0gn0fl0jw 2bu adk6fs4zmmrwa th6vyzq9lnudq5gsqo42xy4vf3xxbhq sfesfd5ijbviqdfsf64my h gonly2k6 8grilkkllas7ap7yhxfnqhcacqjuz3vi3u j1ixympnpfmsetskyeiyvjatvt nfp8f7pjleo0gacy3zanvhskklzmfgyj sha1 base64 jmc2zujayrkvdevqpyvniekocg aaadonicbzi9b9sweiyppultt0mczuxc1ejryzdy1y5b6yhdhcbkaligqdfnmzbfcsrvwfd9u7rkv3tlkcvdiqjrfkcoj9ayharv4xd38bv1yhbzprtj3fr22ov1jzebrxqv32xt7zr3316qkjeupbrxsf4hrafnajznnifrwaijaw5xwfrr1r 6dlkxsfzowqz9kiwfgzfktckndq2uh8cyivstiofeztmfdomzn xjzt7ah7brdgb0ahrkdgx0gn0fl0jw 2bu adk6fs4zmmrwa th6vyzq9lnudq5gsqo42xy4vf3xxbhq sfesfd5ijbviqdfsf64my h gonly2k6 8grilkkllas7ap7yhxfnqhcacqjuz3vi3u j1ixympnpfmsetskyeiyvjatvt nfp8f7pjleo0gacy3zanvhskklzmfgyj sha1 base64 jmc2zujayrkvdevqpyvniekocg aaadonicbzi9b9sweiyppultt0mczuxc1ejryzdy1y5b6yhdhcbkaligqdfnmzbfcsrvwfd9u7rkv3tlkcvdiqjrfkcoj9ayharv4xd38bv1yhbzprtj3fr22ov1jzebrxqv32xt7zr3316qkjeupbrxsf4hrafnajznnifrwaijaw5xwfrr1r 6dlkxsfzowqz9kiwfgzfktckndq2uh8cyivstiofeztmfdomzn xjzt7ah7brdgb0ahrkdgx0gn0fl0jw 2bu adk6fs4zmmrwa th6vyzq9lnudq5gsqo42xy4vf3xxbhq sfesfd5ijbviqdfsf64my h gonly2k6 8grilkkllas7ap7yhxfnqhcacqjuz3vi3u j1ixympnpfmsetskyeiyvjatvt nfp8f7pjleo0gacy3zanvhskklzmfgyj code b abstract syntax tree ast c ast depth first traversal d tree positional encodings e tree relative attention f ggnn sandwich types of edges parent p child c left l right r p lp 2c r l 3cr p lp c r l cr p l c r latexit sha1 base64 vzu5ft0fe9mvzrkgnrvwuk74f9k aaadfxicbvjnb9naef27fjtwlckxl1ujebjvzje0ckzipyce0oq0leiowm8myarrtbw7ropm kqv su9caahrkjc dfmjlzmesb75oezn8 r8cazfmygwr p37p3 8hd7ue1x0 epnte33lxydjcc jyvkb6kmygpfdqtcjkumo0scswcblft1398jnoi1l1yu4z6cdsrmricgyxndjx3kyxjiuqlitzyfss mjxrlktmjjzgn1nq8qb4h2ihthehneooqxe1xf19umpuy2e3kokrr29t88xhdolpfi48zbifnoserwwjwtmleswkqy5vzqjaosrzpuiajvczmzqbwtnyb50k4qlazayoop672iy8jwbzblkxvtcilp9gmkruaqcbw4gy yajaghvleetl y 9uzfywzir2lgqesnwf 7shyysw0ivgzmdsx6zwx f tl9vrh34hvjzbuhzxoleuqu2poyj0kdrwk6digncc90r5hgnglr6kgg4hxp ktxjx0aydznjwahx lmextxbjhnldqvkehjmt0ifdwr0b78775n33b 2v g 50lqe2xps7is q d0zw q latexit latexit sha1 base64 vzu5ft0fe9mvzrkgnrvwuk74f9k aaadfxicbvjnb9naef27fjtwlckxl1ujebjvzje0ckzipyce0oq0leiowm8myarrtbw7ropm kqv su9caahrkjc dfmjlzmesb75oezn8 r8cazfmygwr p37p3 8hd7ue1x0 epnte33lxydjcc jyvkb6kmygpfdqtcjkumo0scswcblft1398jnoi1l1yu4z6cdsrmricgyxndjx3kyxjiuqlitzyfss mjxrlktmjjzgn1nq8qb4h2ihthehneooqxe1xf19umpuy2e3kokrr29t88xhdolpfi48zbifnoserwwjwtmleswkqy5vzqjaosrzpuiajvczmzqbwtnyb50k4qlazayoop672iy8jwbzblkxvtcilp9gmkruaqcbw4gy yajaghvleetl y 9uzfywzir2lgqesnwf 7shyysw0ivgzmdsx6zwx f tl9vrh34hvjzbuhzxoleuqu2poyj0kdrwk6digncc90r5hgnglr6kgg4hxp ktxjx0aydznjwahx lmextxbjhnldqvkehjmt0ifdwr0b78775n33b 2v g 50lqe2xps7is q d0zw q latexit latexit sha1 base64 vzu5ft0fe9mvzrkgnrvwuk74f9k aaadfxicbvjnb9naef27fjtwlckxl1ujebjvzje0ckzipyce0oq0leiowm8myarrtbw7ropm kqv su9caahrkjc dfmjlzmesb75oezn8 r8cazfmygwr p37p3 8hd7ue1x0 epnte33lxydjcc jyvkb6kmygpfdqtcjkumo0scswcblft1398jnoi1l1yu4z6cdsrmricgyxndjx3kyxjiuqlitzyfss mjxrlktmjjzgn1nq8qb4h2ihthehneooqxe1xf19umpuy2e3kokrr29t88xhdolpfi48zbifnoserwwjwtmleswkqy5vzqjaosrzpuiajvczmzqbwtnyb50k4qlazayoop672iy8jwbzblkxvtcilp9gmkruaqcbw4gy yajaghvleetl y 9uzfywzir2lgqesnwf 7shyysw0ivgzmdsx6zwx f tl9vrh34hvjzbuhzxoleuqu2poyj0kdrwk6digncc90r5hgnglr6kgg4hxp ktxjx0aydznjwahx lmextxbjhnldqvkehjmt0ifdwr0b78775n33b 2v g 50lqe2xps7is q d0zw q latexit latexit sha1 base64 vzu5ft0fe9mvzrkgnrvwuk74f9k aaadfxicbvjnb9naef27fjtwlckxl1ujebjvzje0ckzipyce0oq0leiowm8myarrtbw7ropm kqv su9caahrkjc dfmjlzmesb75oezn8 r8cazfmygwr p37p3 8hd7ue1x0 epnte33lxydjcc jyvkb6kmygpfdqtcjkumo0scswcblft1398jnoi1l1yu4z6cdsrmricgyxndjx3kyxjiuqlitzyfss mjxrlktmjjzgn1nq8qb4h2ihthehneooqxe1xf19umpuy2e3kokrr29t88xhdolpfi48zbifnoserwwjwtmleswkqy5vzqjaosrzpuiajvczmzqbwtnyb50k4qlazayoop672iy8jwbzblkxvtcilp9gmkruaqcbw4gy yajaghvleetl y 9uzfywzir2lgqesnwf 7shyysw0ivgzmdsx6zwx f tl9vrh34hvjzbuhzxoleuqu2poyj0kdrwk6digncc90r5hgnglr6kgg4hxp ktxjx0aydznjwahx lmextxbjhnldqvkehjmt0ifdwr0b78775n33b 2v g 50lqe2xps7is q d0zw q latexit transggnn ggnn trans inputoutputstack like enc.
s s s s s s self s figure illustrationofmechanisms forprocessing aststructure intransformer.
also show that using the proposed anonymization can improve the quality ofthemodel either a singletransformer oran ensemble oftransformers.
we conduct an ablation study of different syntax capturing components in transformer underlining which ones are essentialforachievinghighqualityandanalysingtheresults obtainedfor the anonymizedsetting.
our source code is available at code transformers .
the rest of the work is organized as follows.
in section 2we review theexistingapproachesforutilizingsyntacticinformation intransformer.insection 3wedescribe ourmethodologyforthe empiricalevaluationoftransformercapabilitiestoutilizesyntactic information.thefollowingsections 6 8describeourempiricalfindings.
in section we give thereview of the literatureconnected to our research.
finally section 11discusses threats to validity and section12concludes the work.
review oftransformers forsource code abstract syntax tree.
a syntactic structure of code is usually representedintheformofanabstractsyntaxtree ast .eachnode ofthetreecontainsatype whichrepresentsthesyntacticunitofthe programming language e.g.
assign index nameload some nodes also contain a value e.g.
idx elem .
values store userdefined variable names reserved language names integers strings etc.
an example ast for acode snippetisshowninfigure b .
.
transformer architectureand self attentionmechanism we describe transformer architecture for a task of mapping input sequence c1 ...cl ci ... m mis a vocabulary size to a sequenceof dmodel dimensionalrepresentations y1 ... ylthatcan beusedformakingtask specificpredictionsinvarioustasks.beforebeingpassedtothetransformerblocks theinputsequenceisfirstly mappedintoasequenceof embeddings x1 ... xl xi rdmodel.
a key ingredient of a transformer block is a self attention layer thatmapsinputsequence x1 ...xl xi rdmodeltoasequenceof the same length z1 ... zl zi rdz.self attention firstcomputes key query andvalue vectorsfromeachinputvector xk j xjwk xq j xjwqandxv j xjwv.
each output ziis computed as a weightedcombination ofinputs zi summationdisplay.
j ijxv j ij exp aij summationtext.
jexp aij aij xq ixk jt dz attention weights ij summationtext.1l j 1 ij are computed based onquery keysimilarities.severalattentionlayers heads areappliedinparallelwithdifferentprojectionmatrices wv h wq h wk h h ... h. the outputs are concatenated and projected to obtain xi z1 i ... zh i wo wo rhdz dmodel.
a transformer blockincludesthedescribedmulti headattention aresidualconnection a layernormalization and a position wise fully connected layer.
the overall transformer architecture is composed by the consequentstackingofthedescribedblocks.whenapplyingtransformertogenerationtasks futureelements i j aremaskedin self attention transformer decoder .
without this masking the stackofthelayersiscalledtransformer encoder.inthesequenceto sequence task when both encoder and decoder are used the attention from decoder to encoder is also incorporated into the model.
.
passingasts to transformers forourstudy weselecttwocommonlyusednlpapproachesforutilizing sequential structure and three approaches developed specifically for utilizing sourcecode structure intransformer.
sequential positional encodings and embeddings.
transformers were initially developed for nlp and therefore were augmented 704empirical studyof transformersforsource code esec fse august athens greece with sequence capturing mechanisms to account for sequential input structure.
as a result the simplest way of applying transformers to ast is to traverseastin someorder e.g.
indepth first order see figure c and use standard sequence capturing mechanisms.
toaccountforthesequentialnatureoftheinput standardtransformerisaugmentedwithpositionalencodingsorpositionalembeddings.
namely the input embeddings xi rdmodelare summed up with positional representations pi rdmodel xi xi pi.
for example positionalembeddingsimplylearningtheembeddingvector of each position i ...l pi ei ei rdmodel.
positional encoding implies computing pibased on sine and cosine functions.
we include positional embeddings in our comparisons and add the prefix sequential to the title of this mechanism.
this approach wasusedas abaselineinseveral ofworks .
sequentialrelativeattention.
shawetal .
proposedrelativeattentionforcapturingtheorderoftheinputelements.theyaugment self attentionwithrelative embeddings zi summationdisplay.
j ij xv j ev i j ij exp aij summationtext.
jexp aij aij xq i xk j ek i j t dz whereev i j ek i j rdzare learned embeddings for each relative positioni j e.g.one token is located two tokens to the left from another token.
this mechanism that we call sequential relative attention wasshowntosubstantiallyoutperformsequentialpositionalembeddingsandencodingsinsequence basedtextprocessing tasks.
ahmad et al .
reach the same conclusion evaluating sequential relative attention in a task of code summarization i.e.
generatingnaturallanguagesummariesfor code snippets.
treepositionalencodings.
inspiredbypreviouslydiscussedworks severalauthorsdevelopedmechanismsforprocessingtrees.shiv and quirk develop positional encodings for tree structured data assumingthatthemaximumnumber nwofnodechildrenand the maximum depth ndof the tree are relatively small.
example encodingsaregiveninfigure d .thepositionofeachnodeina tree is defined by its path from the root and each child number inthepathisencodedusing nw sizedone hotvector.theoverall representationofanodeisobtainedbyconcatenatingtheseone hot vectorsinreverseorderandpaddingshortpathswithzerosfrom theright.theauthorsalsointroducethelearnableparametersof theencoding theirnumberequals dmodel nw nd .pathslonger thanndare clipped the root node is clipped first .
the authors binarize asts to achieve nw .
to avoid this binarization we replaceallchildnumbersgreaterthan nwwithnw andselectthe besthyperparameters nwandndusinggridsearch seedetailsin section4.
shiv and quirk tested the approach on the task of code translation code to code and semantic parsing text to code .
the transformerwithtreepositionalencodingsoutperformedstandard transformerwithsequentialpositionalencodingsandtreelstm .
tree relative attention.
an extension of sequential relative attention for trees was proposed by kim et al .
.
in a sequence the distancebetweentwoinputpositionsisdefinedasthenumberofpositions between them.
similarly in a tree the distance between two nodes can be defined as the shortest path between nodes consisting of nu stepsupandnd stepsdown see example in figure e .
now similarly to sequential relative attention we can learn embeddings for the described distances and plug them into self attention.
learning multidimensional embeddings for the tree input requires muchmore memory than for sequentialinput sincedistancesinthetreeareobject specific whiledistancesinthe sequencearethesameforallobjectsinamini batch.asaresult the authors use scalar embedding rij rfor the distance between nodesiandjandplug itintotheattentionmechanismasfollows otherformulas staythe same ij exp aij rij summationtext.
jexp aij rij .
ourpreliminaryexperimentssuggestedthatusingsummation ij rijinsteadofmultiplicationleadstoahigherfinalscore.theauthors tested the approach on the task of code completion i.e.
predicting thenexttoken andshowedthatusingmodifiedattentionimproves qualitywhenapplyingtransformertoasttraversalandtocode as text.
ggnn sandwich.
due to the graph nature of ast source codes areoftenprocessedusinggraphgatedneuralnetworks ggnn .
toaddmoreinductivebias astisaugmentedwithedgesofseveraladditionaltypes e.g.
reflectingdata andcontrol flowinthe program.
such a model captures localdependencies in data well but lacks a globalview of the input program that is the transformer sforte.inspiredbythisreasoning hellendoornetal .
proposealternatingtransformerandggnnlayersasillustratedin figure1 f tocombine thestrengths ofboth models.
ggnnlayer reliesonpassingmessagesthroughedgesforafixednumberofiterations number of passes .
the model is called ggnn sandwich by the authors and the details can be found in .
ggnn sandwich was shown to be effective in the variable misuse detection task i.e.
predicting the location of a bug and the location used to fix thebug copyvariable .ggnnsandwichoutperformedstandard transformer withsequentialpositional encodings.
our work focuses of processing syntactic information in transformer thuswedonotusedata andcontrol flowedges.data or control flowedgesarehardtoincorporateinothermechanismsexcept ggnn sandwich.
in our ggnn sandwich we use ast edges edgesconnectingtheneighbouringnodesintheastdepth firsttraversal andedges connecting nodes to themselves see illustration infig.
f .
hellendoornetal .
alsoproposeamodelcalledgreatthatis inspired by relative attention and incorporates dimensional edge embeddingsintotheattentionmechanism.thismodelisconceptually similarto the tree relative attention thus we donot include great inour comparison.
limitationsofexisting approaches and methodologyofthe work asshowninsection 2severalapproachesforprocessingastsin transformershavebeenproposed.however itisstillunclearwhich approaches perform better than others and what mechanisms to useinpractice.first alltheworksdiscussedinsection 2conduct 705esec fse august athens greece n.chirkova ands.troshin experimentswithdifferenttasksmakingithardtoaligntheresults.
moreover almostallthelistedworkscomparetheirapproacheswith thevanillatransformer i.e.
transformerwithsequentialpositional encodings or embeddings while modern practices use advanced mechanisms like sequential relative attention by default.
even worksthatproposetree processingapproachesinspiredbysequentialrelativeattentiondonotincludethismechanismasabaseline.
thatis itisunclearwhetherusingadvancedtree processingmechanisms isbeneficialatall.
secondly theexisting approachesfocus oncapturingtree structure anddonotinvestigatetheinfluenceof othercomponentsofast i.e.
types andvalues.
inthiswork weconductathoroughempiricalstudyonutilizing astintransformers.weconsiderthreecodeprocessingtasks variablemisuse vm detection functionnaming fn andcodecompletion cc andtwosourcecodedatasets python150k and javascript150k .weselectedtasksthatareoftenusedasbenchmarksintheliteratureandonwhichthecomparedapproacheswere tested by their authors.
our selection also covers various transformerconfigurations i.e.
encoderonly vm decoderonly cc and encoder decoder fn .
we selected the python150k dataset because it is often used in the literature and javascript150k because itisdistributedbythe same authorsandhas the same format.
we re implement all mechanisms described in section 2in a unified framework and investigate the most effective approach for processing asts in transformer in different tasks.
we answer the following researchquestions what is the most effective approach for utilizing ast structureintransformer?
is transformer generally capable of utilizing syntactic information representedviaast?
whatcomponents of ast structure node types and values does transformer use indifferenttasks?
thereisnocommonpracticeofpreprocessingasts particularly processing values.
each node in ast is associated with a type but notallnodeshaveassociatedvalues.kimetal .
andshivand quirk attach values as separate child nodes so that each node storesonlyoneitem typeorvalue whilehellendoornetal .
proposeomittingtypes.theformerapproachincreasesinputlength and thus makes code processing significantly slower while the latterapproachlosestypeinformation.wechooseanin between strategyinspiredbytheapproachoflietal .
usedforrnns we associatethe empty valuewithnodesthatdonotehavevalues so thateachnode iinasthasbothtype tiandvalue vi seefigure c .
thissetuppreservestheinitialaststructureandallowsustoeasily ablate types or values leaving the other item in each node present.
some works show that splitting values based on snake case or camelcase or using splitting techniques such as byte pairencoding may improve the quality .
we do not use splitting into subtokensfortworeasons.firstly splittingmakessequencesmuch longer resultingin a substantial slow down of training procedure becauseofquadratictransformercomplexityw.r.t.theinputlength.
secondly splitting breaks the one to one correspondence between ast nodes and values i.e.
severalvalues belong to one ast node.
therearedifferentwaysofadaptingast basedtransformersto thedescribedproblem oneoptionistoaverageembeddingsover subtokens another option is to assign a chain of subtokensas a child of a node and then directly apply tree processing mechanisms.
a third option is to modify tree processing mechanisms e.g.
duplicatepathsforallsubtokensintreepositionalencodingor duplicatetree relationsfor allpairsof subtokensof twotokensin treerelativeattention.asaresult thequestionofhowsplittinginto subtokensaffectssyntax capturingmechanismsrequiresaseparate study whichwe leave for the future work.
an important part of our methodology is conducting experimentsintwosettings namely anonymized andfull data.thefulldata setting corresponds to the conventional training of transformeronastsparsedfromcode.inthiscase transformerhastwo sourcesofinformationaboutinputcodesnippets syntacticinformation and user defined identifiers stored in node values .
identifiers usuallygivemuch additional informationaboutthesemanticsof the code however their presence is not necessary for correct code execution renaming all user defined identifiers with placeholders var1 var2 var3etc.willleadtothesameresultofcodeexecution and will not change the semantics of the algorithm the code implements.
here we mean that all occurrences of an identifier are replacedwiththesameplaceholder thus importantinformation aboutidentifierrepetitionissaved.wecallthisrenamingidentifiers withplaceholdersas anonymization .intheanonymizedsetting the inputcodeisrepresentedpurelywithsyntaxstructureandtheonly way transformer can make meaningful predictions is to capture informationfromast.inthisway usingtheanonymizedsetting allows a better understanding of the capabilities of transformer to utilizesyntacticinformation.moredetailsontheanonymization procedure are given insupplementary materials.
another important part of our methodology is a thoughtful splittingofthedatasetintotrainingandtestingsets whichincludes splitting by repository and removing code duplicates.
alon et al .
leclair et al .
notice that code files inside one repository usually share variable names and code patterns thus splitting files from one repository between training and testing sets simplifies predictions for the testing set and leads to a data leak.
to avoid this oneshouldputallfilesfromonerepositoryintooneset either trainingortesting thisstrategyiscalledsplittingbyrepository .
evenusingthisstrategy duplicatecodecanstilloccurinthetesting set since developers often copy code from other projects or fork other repositories.
allamanis underline that in commonly used datasetsupto20 oftestingobjectscanberepeatedinthetraining set biasing evaluation results.
as a result the deduplication step is neededafter data splitting.
experimentalsetup data.inalltasks weusethepython150k py dataset redistributableversion andjavascript150k js dataset downloaded fromtheofficialrepositoryat .bothdatasets consistofcodefilesdownloadedfromgithubandarecommonly usedto evaluate code processing models.
most research use the train test split provided by the authors of the dataset however this split does not follow best practices described in section 3and produce biased results so we release a new split of the dataset.
we remove duplicate files from both datasets using the list of duplicates provided by allamanis .
we also filter out absolutely identical code files and when selecting 706empirical studyof transformersforsource code esec fse august athens greece functions from code files we additionally filter out absolutely identical functions.
we split data into training validation testing sets in proportion .
.
based on github usernames each repository isassignedto one username .
preprocessingdetailsforeachtaskaregivenbelow.werelease ourdatasplitandoursourcecodeincludingscriptsfordownloading data deterministic code for data preprocessing models training etc.
we largely rely on the implementations of other research and compare the quality of our baseline models to the results reported inotherpapers when possible see details insection .
variable misuse task vm .
for the variable misuse task we use the setup and evaluation strategy of hellendoorn et al .
.
given thecodeofafunction thetaskistoidentifytwopositions usingtwo pointers oneinwhichpositionawrongvariableisused andonein whichpositionacorrectvariablecanbecopiedfrom anysuchposition is accepted .
if a snippet is non buggy the first pointer should select a special no bug position.
we obtain two pointers by applying two position wise fully connected layers and softmax over positionsontopoftransformeroutputs.forexample thefirstpointer selects position as argmax1 i lsoftmax yi rdmodel u rdmodel b r bis a learnable scalar corresponding totheno bugposition denotestheconcatenationoftheelements into a vector of scalars.
the second pointer is computed in a similar way but without b. the model is trained using the cross entropy loss.
toprocess theoriginaldataset for thevariable misusetask we select all top level functions including functions inside classes from all filtered 150k files and filter out functions longer than 250nodes toavoidverylongfunctions andfunctionswithless than three positions containing user defined variables or less than three distinct user defined variables to avoid trivial bug fixes .
we select a function with a root node type functiondef for py and functiondeclaration orfunctionexpression forjs.theresultingtraining validation testingsetconsists of417k 48k 231k functions for py and 202k 29k 108k for js.
one function may occur in the dataset up to times times with a synthetically generatedbugand3timeswithoutabug.following weusethis strategytoavoidbiasingtowardslongfunctionswithalotofdifferentvariables.thebuggyexamplesaregeneratedsyntheticallyby choosingrandombugandfixpositionsfrompositionscontaining user definedvariables.
weusethejointlocalizationandrepairaccuracymetricof to assessthequalityofthemodel.thismetricestimatestheportionof buggy samples for which the model correctly localizes and repairs thebug.wealsomeasuredlocalizationaccuracyandrepairaccuracy independently and foundthat all three metricscorrelate well with eachother.
function naming task fn .
in this task given the code of a function thetaskistopredictthenameofthefunction.tosolvethistask we use the classic sequence to sequence transformer architecture that outputs the function name word by word.
a particular implementationisborrowedfrom thepaperusedanotherdataset .
firstly wepassfunctioncodetothetransformerencodertoobtain code representations y1 ... yl.
then the transformer decoder generatesthemethodnamewordbyword andduringeachwordgeneration decoderattendsto y1 ... yl usingencoder decoder attention and to previously generated tokens using masked decoderattention .toaccountforthesequentialorderofthefunction name weusesequentialpositionalembeddingsinthetransformer decoder.intheencoder weconsiderdifferentstructure capturing mechanisms.weusegreedydecoding.wetrainthewholeencoderdecoder modelend to end optimizing the cross entropy loss.
toobtaintheprocesseddatasetforthefunctionnametask we select all top level functions including functions inside classes from all filtered 150k files and filter out functions longer than ast nodes to avoid very long functions functions for which thenamecouldnotbeextracted alotof functionexpression sin js are anonymous and functions with names consisting of only underscore characters and names containing rare words less than 3occurrencesinthetrainingsetforpy js .toextractfunctions weusethesamerootnodetypesasinthevmtask.theresulting dataset consists of 523k 56k 264k training validation testing functions for py and 186k 23k 93k for js.
we replace function name in the ast with a special fun name token.
to extract target function names we remove extra underscores and spliteachfunctionnamebasedon camelcase orsnake case e.g.
name get feature names becomes .
a mean stdlengthoffunctionnameis2.
.46wordsforpyand .
.23for js.
we assess the quality of the generated function namesusing the f1 metric.
if gtnis a set of words in ground truth function name andpnis a set of words in predicted function name the f1 metric is computed as pr p r wherep gtn pn pn r gtn pn gtn denotes the number of elements.
f1 is averagedoverfunctions.wechoosethef1metricfollowingalon et al.
who solved a similar task with another dataset and model.
codecompletiontask cc .
forthetaskofcodecompletion we use the setup metrics and transformer implementation of kim et al.
.
the task is to predict the next node ti vi in the depthfirsttraversalofast .wepredicttype ti and value viusing two fully connected layers with softmax on top of the prefix representation yi p ti softmax wtyi wt r types dmodel p vi softmax wvyi wv r values dmodel.
toobtainthedatasetforthecodecompletiontask weusefull astsfrom filtered 150kfiles removingsequenceswithlengthless than2.ifthenumberofastnodesislargerthan nctx wesplit astintooverlappingchunksoflength nctxwithashift1 2nctx.the overlapprovidesacontextforthemodel.forexample ifthelength ofastis800 weselectthefollowingsamples ast ast ast .
we do not calculate loss or metrics over the intersection twice.
for the previous example the quality of predictionsismeasuredonlyon ast .
theoverlappingsplittingprocedureisborrowedfrom andis neededsinceprocessingextremelylongsequencesistooslowin transformer because of its quadratic complexity w.r.t.
input length.
theresulting datasetconsistsof186k 20k 100ktraining validation testingchunksfor pyand270k 32k 220k for js.
wemostlyfocusonvalueprediction sinceitisthemorecomplex task andpresentresultsfortypepredictionwheretheysignificantly differfromothertasks.weoptimizethesumofcross entropylosses for types andvalues.
707esec fse august athens greece n.chirkova ands.troshin table selected hyperparameters for different structurecapturing mechanisms tasks and datasets.
the details on selecting hyperparameters are given in supplementary materials.
model hypers.
lang.
vm fn cc seq.rel.
attn.
py max.
dist.
js tree pos.enc.
py8 max width depth js4 tree rel.
attn.
py100 rel.
vocab.size js600 ggnnsandwich py12 n y n a num.
layers js12 n y n a num.
edge types isggnnfirst?
weusemeanreciprocalrank mrr tomeasurethemodelquality sinceitreflectsthepracticalapplicationofcodecompletion mrr n summationtext.1n i ranki whererankiis a position of the i th true token inthemodelranking nisthetotalnumberoftargettokensina dataset excluding padding and empty tokens.asin we assign zero score if the true token is out of top predicted tokens.
hyperparameters.
welistgeneralhyperparametersforthe variablemisuse functionnaming codecompletiontasksusingslashes.
ourtransformermodelshave6layers 8heads dmodel .
we limit vocabulary sizes for values up to 50k 50k 100k tokens and preserve all types.
as discussed in section we do not split values into subtokens.
we train all transformers using adam with a starting learning rate of .
.
.
and a batch size of32for25 20epochsforpyand40 20epochsforjs the number of functions in the js dataset is smaller than in py dataset thus more epochs are needed .
in the code completion task we use the cosine learning rate schedule with warm up steps and a zero minimal learning rate and a gradient clipping of .
.
in the variable misuse task and in the function naming task for js we use a constant learning rate.
in the function naming task for py we decay the learning rate by .
after each epoch.
in the function naming task we also use a gradient clipping of .
we use residual embeddingandattentiondropoutwith p .
.
.
.all modelsweretrainedthreetimes toestimatethestandarddeviation ofthequality excepthyperparametertuning .inallexperiments wereportthequalityonthetestset excepthyperparametertuningwhere we report the qualityonthevalidation set.we trainall models onone gpu nvidia tesla p40 orv100 .
thehyperparametersfordifferentstructure capturingmechanisms were tuned using grid search based on the quality on the thevalidationset foreachdataset taskcombinationindividually.
for sequential relative attention we tune the maximum relative distance between the elements of the sequence.
for tree positional encoding wetunethemaximumpathwidthandthemaximumpath depth.fortreerelativeattention wetunethesizeoftherelationvocabulary.
for the ggnn sandwich model we consider layer and layerconfigurationsofalternatingtransformer t andggnn g layers we also consider placing both types of layers first i.e.
or andsimilarlyfor12layers .
ggnn layers include message passes.
we also consider omitting edgesof types leftandright.
sequential positionalembeddings do not have hyperparameters.
the number of parameters in all ast based modifications of transformer are approximately the same exceptggnnsandwiches layersandwichincorporates slightlymoreparametersthanvanillatransformer while6 layer incorporates slightly fewer parameters.
the details and the tables onhyperparametersearcharegiveninsupplementarymaterials the resultinghyperparameters are listedintable .
comparison ofapproachesfor utilizing syntacticstructurein transformer we begin with investigating which of the mechanisms for utilizing ast structure in transformer is the most effective one.
we obtain thetreesstoringa type value pairineachnodeusingtheapproach describedinsection 3andpassthesetreestotransformer equipped withoneofthemechanismsdescribedinsection .ggnnsandwichisnotapplicabletocodecompletionbecausemessage passing involves allnodes andprohibitsusing maskinginthe decoder.
the resultsare presented infigure .
infunctionnaming most structure capturingmechanismsperformsimilarly.insection we show that in this task quality is not affected much even if we completely ablate structure information i.e.
do not use any structurecapturing mechanism and treat input as a setof type value pairs.
thatis transformerhardlyutilizessyntacticstructurewhenpredictingfunctionnames.however inothertasks thisisnotthecase and there is more variability in different mechanisms performance.
utilizingstructureinformationintheinputembeddingsisnot effective sequential positional embeddings and tree positional encodingsdonotachievehighestscoreinanytask exceptfunction namingwheretreepositionalencodingsperformonparwithother mechanisms.
utilizingstructureintheself attentionmechanismismuchmore effective in all tasks at least one of sequential relative attention andtreerelativeattentionisthebestperformingmodel.sequential relative attention achieves the highest score in variable misuse and value prediction tasks while tree relative attention outperforms others by a high margin in type prediction task this model was developedforcodecompletiontask .thelastresultisinterpretable since tree relative attention helps to find relatives in ast tree e.g.
parent and siblings which is important in type prediction.
the advantage of sequential relative attention is that it can use the multidimensional embeddings of relations the sequential relations are shared acrossobjects leadingto affordable3 dimensionalembeddingtensorsofshape length length embeddingdimension .
in contrast tree relative attention can only afford one dimensional embeddingsofrelations becausetree basedrelationsarenotshared between objects in a mini batch and extracting them for a minibatch would already lead to a dimensional tensor batch length length .
ggnn sandwich achieves high results in the variable misuse task for which this model was developed.
the reason is that in variable misuse detection the goal is to choosetwo variables and localmessagepassinginformseachvariableofitsroleinaprogram 708empirical studyof transformersforsource code esec fse august athens greece python variablemisuse function naming code completion values code completion types joint accuracy seq.
pos.
emb.
seq.
rel.
attn.
tree pos.
enc.
tree rel.
attn.
ggnn sandwich f1 javascript variablemisuse function naming code completion values code completion types joint accuracy seq.
pos.
emb.
seq.
rel.
attn.
tree pos.
enc.
tree rel.
attn.
ggnn sandwich f1 figure a comparison of different mechanisms for processing ast structure in transformer in the full data setting.
the numeric data forbarplotsisgiven insupplementary materials.
table time and storage consumption of different structure capturing mechanisms for the variable misuse task on thepython dataset.
train time preprocess add.
train model h epoch time ms func.
data gb seq.pos.emb.
.
seq.rel.
att.
.
tree pos.enc.
.
.
.
tree rel.
attn.
.
.
ggnnsandwich .
.
.
and makes variable representations more meaningful.
the original work on ggnn sandwiches also uses additional types of edges whichwouldimprovetheperformanceofthismodelfurther.using these types of edges is out of scope of this work since we focus onutilizing syntactic information thusweonlyusesyntax based edges.
insupplementarymaterials wevisualizetheprogressoftestmetricsduringtraining fordifferentstructure capturingmechanisms inthefull datasetting.supplementarymaterialsalsopresentthe comparison of structure capturing mechanisms in the anonymized settingthatisdescribedinsection 3andimpliesreplacingvalues in asts with unique placeholders.
the leading mechanisms are the same in all tasks as in the full data setting considered above.
an interested reader may also find examples of attention maps for differentmechanismsinsupplementary materials.
in table2 we list training time and the size of auxiliary data needed for different structure capturing mechanisms.
ggnn sandwich model requires twice the time for training and prediction compared to other models because of the time consuming message passing mechanism.
tree relative attention requires dozens ofgigabytesforstoringpairwiserelationmatricesforalltraining objects that could be replaced with slow on the fly relation matrix generation.treepositionalencodingsandggnnsandwichmodelstable comparison of combinations of sequential relative attention sra with other structure capturing approaches.
all numbers in percent standard deviations vm .
fn .
cc .
.
bold emphasizes combinations that significantly outperform sra.
in the vm task sra ggnn sandwich significantly outperforms sra during the first half of epochs but loses superiority at the last epochs for both datasets.
on the python dataset sra ggnn sandwich outperforms sraby onestandard deviationat the lastepoch.
model vm fn cc val.
py sra .
.
.
sra seq.pos.emb.
.
.
.
pysra tree pos.enc.
.
.
.
sra tree rel.
attn.
.
.
.
sra ggnnsand.
.
.
n a js sra .
.
.
sra seq.pos.emb.
.
.
.
sra tree pos.enc.
.
.
.
jssra tree rel.
attn.
.
.
.
sra ggnnsand.
.
.
n a alsorequireadditionaldiskspaceforstoringpreprocessedgraph representations but thesizes ofthesefilesarerelativelysmall.sequential positional embeddings and relative attention are the most efficient models inboth time anddisk consumptionaspects.
to sum up we emphasise sequential relative attention as the most effective and efficient approach for capturing ast structure in transformer.
combining structure capturing mechanisms.
in table3 we show thatusing sophisticated structure capturing mechanisms may be useful for further improving sequential relative attention if we combine two mechanisms .
we find that tree relative attention for both datasets and tree positional encoding for js improve the score in 709esec fse august athens greece n.chirkova ands.troshin table4 illustrationofdifferentkindsofmodelsusedinthe experiments.thecodesnippetanditsastusedintheillustrationmay be foundinfigure a b .
model inputrepresentation syntax text assign empty namestore elem ... ... index empty nameload idx syntax assign empty namestore var1 ... ... index empty nameload var3 text constant predicts the mostfrequent target for any input the value prediction task while ggnn sandwich may improve the score inthe variable misuse task especiallyat earlierepochs.
capabilityoftransformer to utilizesyntacticinformation when developing approaches for utilizing syntactic information in transformer the majority of works mostly focus on the tree structure.we discussed the utilizationofstructure intheprevious section but we also wouldliketo investigate the influence of other ast components namely typesandvalues.
in the next section we conductanablationstudyofthementionedcomponents andinthis section weinvestigatewhethertransformerisgenerallycapable ofutilizingthesyntactic information insourcecode i.e.does processing ast components improve performance.
this conceptual experimentteststheoverallsuitabilityoftransformerforsource codeprocessing.weformalizethespecifiedquestioninfull data andanonymizedsettings as follows syntax text vs.text first we testwhether using syntactic information in addition to the textual information is beneficial comparedtousingpuretextualinformation.todoso we compare thequality of transformer trained on fullast data syntax text withthequalityoftransformertrained on a sequence of non empty values text see table 4for illustrations.
the textmodel relies only on textual informationstoredinvalues anddoes not have access toanyother kindofinformation.
syntaxvs.constant secondly we test whether transformer is able to make meaningful predictions given onlysyntactic information without textual information.
to do this wetestwhetherthequalityoftransformertrainedonthe anonymized astdata syntax isbetterthanthequalityofa simpleconstantbaseline constant .anonymizationremoves identifiers i.e.
textualinformation butpreservesinformation about identifiers repetition which may be essential for understandingtheprogram.the constant modeloutputsthe most frequent target e.g.
no bug in vm and name initfor py andexportsfor js in fn.
since anonymized ast data contains only syntactic information and does not contain textual information the only way the transformer can outperformthe constant baselineonthisdataistocapturesome information from ast.all the described models are trained with sequential relative attention.deduplicatingthedatasetisveryimportantinthisexperiment to avoid overestimating the syntaxbaseline.
the results for three tasks are presented in figure .
in all cases syntax text outperforms text andsyntaxoutperforms constant.in figure4 we present example predictions for code completion and function naming tasks for python language with all four models.
incodecompletion syntax basedmodelscapturefrequentcoding patterns well for instance in example b syntaxmodel correctly chooses anonymized value argmnents and notargvorparse becauseargmnents goes before assignment.
in example a syntax text correctlypredicts create bucket becauseitgoesinside the if statement checking whether the bucket exists while text modeloutputs frequent tokens associatedwithbuckets.
in the function naming task the textmodel captures code semantics based on variable names and sometimes selects wrong anchor variables e.g.
splitinexample c whilethe syntax text modelutilizesastinformationandoutputscorrectword dict.the syntaxmodeldoesnothaveaccesstovariablenamesasitprocesses datawithplaceholders andoutputsoverlygeneralpredictions e.g.
get all in example c .
nevertheless the syntaxmodel is capable ofdistinguishinggeneralcodepurpose e.g.
modelusesword getin example c and word readin example d .
to sum up transformer is indeed capable ofutilizingsyntacticinformation .
interestingly incodecompletion valueprediction py and variablemisusedetectiontasks js the syntaxmodel trainedonthe anonymizeddataoutperformsthe syntax text modeltrainedon full data though the latter uses more data than the former.
the reasonisthatthevaluesvocabularyonfulldataislimited soapprox.
of values are replaced with the unktoken and cannot be predicted correctly.
on the other hand this is not a complication for the syntaxmodel which anonymizes both frequent and rare identifiers in the same way.
for example in figure b the syntaxmodelcorrectlypredictsthemisspelledtoken argmnents whileforthe syntax text model thistokenisout of vocabulary and so model outputs frequently used strings for printing.
one moreadvantageofthe syntaxmodelinthevaluepredictiontask is that it is twice faster in training because of the small output softmax dimension.
in the function naming task the syntaxmodel performssubstantiallyworsethan syntax text becausevariable namesprovidemuch naturallanguage informationneededtomake natural language predictions.
to sum up anonymizationmay lead to ahigherqualitythanusing full data .
ablation studyofmechanisms for utilizing syntacticinformation in transformer inthissection weinvestigatetheeffectofablatingdifferentast components on the performance in three tasks.
this ablation study is important for both providing practical recommendations and understanding what mechanisms are essential for making reasonable predictions in the anonymized setting discussed in the previous section.weconsiderthreeastcomponents commentsinitems regard the usual scenario without ablation types the types of nodes are passed as one of the transformer inputs values the anonymized valuesarepassedasoneofthetransformerinputs 710empirical studyof transformersforsource code esec fse august athens greece variablemisuse python function naming python code completion values python epoch 80joint accuracy syntax text text syntax constant epoch 35f1 syntax text text syntax constant .
.
.
.
.
.
.
.
epoch 60mrr syntax text text syntax dummy variablemisuse javascript function naming javascript code completion values javascript epoch 80joint accuracy syntax text text syntax constant epoch 25f1 syntax text text syntax constant .
.
.
.
.
.
.
.
epoch 60mrr syntax text text syntax dummy figure comparisonofsyntax based transformer models with text onlyand constant baselines.
defget or create bucket s3 connection bucket s3 connection .get bucket settings .s3 bucket name ifbucketisnone bucket s3 connection .importsys fromutils.parsing importparse if name main argmnents parse sys.argv print def fun name seqs dt forseqinseqs forwordinseq.split ifnotwordindt dt else dt returndtdef fun name filename with open filename asfin return len fin.read .split n a s t b s t c s t get dict d s t read file s s s get all s read t t t get split t read file dummy dummy dummy init dummy init gold createbucket gold argmnents gold get dictionary gold count lines figure example predictionsincodecompletion a b top predictions andfunction naming c d tasks.
s syntax t text.
structure aststructureisprocessedusingoneofthemechanisms discussedinsection .
as in section we consider both anonymized and full data settings.weablateastcomponentsonebyoneinbothmodels syntax andsyntax text and check whether the quality drops.
ablating typesforsyntax text was in fact performed in section but we repeat the results in this experiment s table and also report this ablationfortheanonymizedsetting.ablating structuremeansturning offallsyntax capturingmechanismssothattheinputofthe syntax text syntaxmodelwillbeviewedasan unorderedset of type value type anonymized value pairs.
ablating valuesmeans usingonlytypesastheinputtothemodel thenumbersarethesame forbothfull dataandanonymizedsettings.weskipthisablationin thecodecompletiontask since inthiscase anonymized values are the target ofthe model.
the results are presented in table .in variable misuse and code completion allastcomponentsareessentialforachievinghighqualityresults .particularly invariablemisuse allablationsresultina largequalitydrop inbothsettings andincodecompletion ablating types results in a large quality drop and ablating structure in substantialdrop.interestingly anonymizationplaysanimportantrole in achieving high quality in the variable misuse task with absent valuesfrom the data.
however the observations differ for function naming.
in this task ablating typesresults in a substantial quality drop in both settings ablating structure results in a small but significant quality drop in both settings ablating valuesin the full data settingresultsinthelargequalitydrop and ablating anonymized valuesdoes not affecttheperformancein theanonymizedsetting.
the first and the third observations underline the importance of using both types and values in practice.
the second and the fourth observations show that transformer is now far from utilizing all the information stored in ast when predicting function names .
particularly in the anonymized setting transformer predicts function namesmostlybasedon types.ithardlyusessyntactic structure and does not use information about value repetition which is stored in anonymizedvalues andisessentialforunderstandingthealgorithm thatthecodeimplements.overcomingthisissueisaninteresting direction for future research.
711esec fse august athens greece n.chirkova ands.troshin table ablation study of processing different ast components in transformer.
bold emphasises best models and ablations that do not hurt the performance.
ast w o struct.
transformer treats input as a bag without structure ast w o types only values or anonymized values are passed to transformer ast w o an.val.
only types are passed to transformer.
n a not applicable.
fulldata anonymizeddata var.misuse fun.
naming comp.
val.
var.misuse fun.
naming comp.
val.
fullast .
.
.
.
.
.
.
.
.
.
.
.
python ast w ostruct.
.
.
.
.
.
.
.
.
.
.
.
.
ast w otypes .
.
.
.
.
.
.
.
.
.
.
.
ast w oan.val.
.
.
.
.
n a .
.
.
.
n a fullast .
.
.
.
.
.
.
.
.
.
.
.
javascript ast w ostruct.
.
.
.
.
.
.
.
.
.
.
.
.
ast w otypes .
.
.
.
.
.
.
.
.
.
.
.
ast w oan.val.
.
.
.
.
n a .
.
.
.
n a table comparison of ensembles.
notation st syntax text s syntax denotes ensembling.
all models are trained with sequential relative attention.
all numbers in percent standard deviations vm .
fn .
cc .
.
models vm fn cc types cc values st81.
.
.
.
st st .
.
.
.
pys81.
.
.
.
s s .
.
.
.
st s .
.
.
.
st76.
.
.
.
st st .
.
.
.
jss78.
.
.
.
s s .
.
.
.
st s .
.
.
.
ensemblingofsyntax basedmodels as was shown in figure syntax text andsyntaxmodels capture dependencies of different nature and are orthogonal in a sense of handlingmissingvaluesand first time seen tokens.
this allows hypothesizingthat ensembling twomentionedmodelscanboostthe performance of thetransformer.
we usethe standard ensembling approach that implies training networks from different random initializationsandaveragingtheirpredictionsaftersoftmax .we use sequentialrelative attention inthis experiment.
in table6 we compare an ensemble of syntax text andsyntax modelswithensemblesoftwo syntax text andoftwo syntaxmodels.
we observe that in variable misuse and value prediction tasks ensemblingmodelsthatviewinputdataintwocompletelydifferent formats is much more effective than ensembling two similar models.thisisthewayhowusinganonymizeddatamayboostthe transformer s performance.
validating ourimplementations and comparingto other works weensurethevalidityofourresultsintwoways byrelyingonthe codeofalreadypublishedworks andbycomparingournumbersachievedforthecommonlyuseddatasplittothenumbersinthe correspondingpapers.particularly weusethemodel loss metrics overlapping chunks code of kim et al .
as the baseline for the cc task we rewrite line by line the main parts of the model loss metrics code of hellendoorn et al .
in pytorch as the baselineforthevmtask andweusethemodel loss metricscode ofahmadetal.
as the baselinefor the fn task.
forvm thevanillatransformerofhellendoornetal .
achieve .
joint accuracy andwe achieve .
theresults are close to each other.
here the performance is given for our model closest to the model of hellendoorn et al .
thetextmodel of similar size andwithsimilarnumberoftrainingupdates usingour syntax text model achieves higher quality.
the performance of ggnn sandwich is high on vm as in .
for cc with tree relative attention we achieve .
.
mrr values types while kim et al .
achieved .
.
their travtrans variant and for standard transformer nostructureinformation weachieve59.
.
mrrwhilekimetal .
achieved58.
.
their travtrans respectively againtheresultsareclose.forfn weusedthecode ofahmadetal .
withourcustomdataandtargets sotheresults arenotcomparable butwecheckedthattheircodeproducesthe same numbers ontheirdata as inthe paper.
the results given in our paper are for our custom data split and thusarenotdirectlycomparabletothenumbersinotherworks.we arguethatdataresplittingiscrucialforachievingcorrectresults see details in section .atthe same time the remaining experimental setup e.g.architecture metrics isthe same as inrecent works.
related work variable misuse.
the field of automated program repair includes alotofdifferenttasks see forareview wefocusonaparticular variablemisusedetectiontask.thistaskwasintroducedbyallamanis et al .
who proposed using ggnn with different types of edges to predict the true variable name for each name placeholder.
vasicetal .
enhancesthevmtaskbylearningtojointlyclassify localizebugandrepairthecodesnippet.theyuseanrnnequipped with two pointers that locate and fix the bug.
hellendoorn et al .
improvedtheperformanceonvmtask usingtransformers great model andggnnsandwich model.
712empirical studyof transformersforsource code esec fse august athens greece code summarization.
the task of code summarization is formalized in literature in different ways given a code snippet predict the docstring the function name or the accompanying comment .
allamanis et al .
propose using convolutional neural networks for generating human readable function names while iyer et al .
proposed using lstm with attention togeneratenaturallanguagesummaries.alonetal .
proposed sampling random ast paths and encoding them with bidirectional lstmtoproducenaturalmethodnamesandsummariesofthecode.
fernandesetal .
proposedcombiningrnns transformerswith ggnn.ahmadetal .
empiricallyinvestigatetransformersfor codesummarization showingthattransformerwithsequentialrelative attention outperforms transformer equipped with positional encodings as well as awide range ofothermodels e g.rnns.
code completion.
early works on code generation built probabilistic models over the grammar rules.
maddison and tarlow learned markov decision process over free contextgrammars utilizingast.raychevetal .
learneddecisiontreespredictingast nodes.
sun et al .
generated code by expanding ast nodes usingnaturallanguagecommentsasadditionalsourceofinformation.
li et al.
used lstm with a pointer this model either generates thenexttokenfromvocabularyorcopiesthetokenfromapreviouslyseenposition.kimetal .
proposedusingtransformers for code generation enhanced them with tree relative attention and showed that the resulting model significantly outperforms rnn basedmodels as well as othermodels .
recent advances in neural source code processing.
the recent line of work is dedicated to learning contextual embeddings for code onthebasisofbidirectionalencoderrepresentationsfromtransformers bert .
such models are firstly pretrained on large datasets providing high quality embeddings of code and then finetunedonsmalldatasetsfordownstreamtasks .allthese transformer based models treat code as text and can potentially benefit from the further utilization of the syntactic information.
anotherline of researchregards makingtransformersmore timeandmemory efficient .investigatingtheapplicabilityofsuch methods to syntax based transformersis an interesting direction for future research.
investigatingneuralnetworksforcodewithomittedvariablenames.
a few of previous works considered training neural networks with omittedvariablenames guptaetal .
xuetal.
trainedrnns onthedatawithanonymizedvariables ahmedetal .
replaced variables with their types.
leclair et al .
investigated the effect of replacing all values in the ast traversal with unk value in the code summarization task and concluded that the quality of anrnntrainedonsuchdataisextremelylow.theirresultaligns with ours while we consider a more general procedure of value anonymization that saves information about value repetition and investigatetheeffectofusinganonymizationinawidersetoftasks for atransformer architecture.
threats to validity we did our best to make out comparison of different ast processing mechanisms as fair as possible.
however the following factors could potentially affect the validity of our results using thesame training hyperparameters for all models not using subtokenization and not using data and control flow edges in ggnn sandwich.thedecisionnottousesubtokenizationwasexplained in section .
moreover we underline that sequential relative attention our best performing mechanism allows for easy combination with any subtokenization technique which will result in further qualityimprovement.onthecontrary thisisnotthecaseformore complexconsideredast processingmechanisms.thedecisionnot use control and data flow edges was explained in section .
we notethat adding data andcontrol flow edges to theggnnsandwich equipped with sequential relative attention would increase thequalityofthiscombinedmodelevenfurther.asforthetraining hyperparameters tuning them for each model individuallywould beveryexpensivegivenourlimitedcomputationalresources.however we note that our models differ only in the ast processing mechanism that is a relatively small change to the architecture.
thus we assume that using the same training hyperparameters for differentmodels ispermissibleinour work.
conclusion in this work we investigated the capabilities of transformer to utilizesyntacticinformation insourcecodeprocessing.
ourstudy underlinedthe following practical conclusions sequential relative attention is a simple fast and not considered as the baseline in previous works mechanism that performs best in out of tasks in some cases similarly to otherslower mechanisms combining sequential relative attention with ggnn sandwichinthevariablemisusetask andwithtreerelativeattention or tree positional encoding in the code completion task mayfurther improve quality omittingtypes valuesoredges inastshurts performance ensemblingtransformertrainedonthefull datawithtransformer trained on the anonymized data outperforms the ensemble of transformers trained on the same kind of data.
further our study highlighted two conceptual insights.
on the one hand transformers are generally capable of utilizing syntactic informationinsourcecode despitetheywereinitiallydevelopedfor nlp i.e.processingsequences.ontheotherhand transformers utilizesyntacticinformationfullynotinalltasks invariablemisuse and code completion transformer uses all ast components while infunctionnaming transformermostlyreliesonasetoftypesand valuesusedinthe program hardlyutilizing syntactic structure.