understanding and detecting performance bugs in markdown compilers penghui li yinxi liu and wei meng chinese university of hong kong phli yxliu wei cse.cuhk.edu.hk abstract markdown compilers are widely used for translating plain markdown text into formatted text yet they suffer from performance bugs that cause performance degradation and resource exhaustion.
currently there is little knowledge and understanding about these performance bugs in the wild.
inthis work we first conduct a comprehensive study of known performance bugs in markdown compilers.
we identify that the ways markdown compilers handle the language s context sensitive features are the dominant root cause of performance bugs.
to detect unknown performance bugs we develop mdperf fuzz a fuzzing framework with a syntax tree based mutation strategy to efficiently generate test cases to manifest such bugs.
it equips an execution trace similarity algorithm to de duplicate the bug reports.
with mdperf fuzz we successfully identified new performance bugs in real world markdown compilers and applications.
our work demonstrates that the performance bugs are a common severe yet previously overlooked security problem.
i. i ntroduction markdown is an easy to use domain specific markup language.
markdown compilers analyze input text to generate formatted text with decorated styles according to the markdown language syntaxes.
because of the flexibility markdown offers markdown compilers are commonly included in many application scenarios such as code hosting software content management systems cmss online markdown editors etc.
for example two leading code hosting providers githuband gitlab support markdown document compilation and rendering at both the server end and the client end popular cmss like wordpress and drupal also provide support for markdown content rendering in the posts with their server side markdown compilers among the alexa top million websites are many online markdown editors such as stackedit .
due to the wide application of markdown compilers the bugs in them can potentially impact many services and their users.
in particular markdown compilers could have performance bugs which have recently become an emerging attack vector for launching denial of service dos attacks .
such bugs could cause excessive resource consumption and negatively affect user experiences.
by specially crafting inputs to exploit aperformance bug in a markdown compiler running on a server attackers can exhaust the server s computing resources e.g.
memory and cpu and significantly impair the application s availability for legitimate users.
some performance bugs require only low bandwidth traffic to exploit and can be leveragedto easily overwhelm a target system .
detectingperformance bugs in markdown compilers could help mitigate or even prevent such dos attacks to safeguard the normal operation of many popular and critical services on the internet.
prior works have studied and revealed the performance bugs in different types of software such as regular expression engines desktop software and android applications .
to the best of our knowledge performance bugs in markdown compilers have not been well investigated and understood yet.
to this end we conduct a comprehensive study of the performance bugs in markdown compilers to answer the following research questions what are the main causes of performance bugs in markdown compilers?
how widespread are such bugs in the wild?
how severe are such bugs?
we empirically analyze known performance bugs in mainstream markdown compilers and thoroughly summarize their characteristics.
we observe that there has been a continuous growth in the number of reported performance bugs inthe past years.
we further identify that the dominant rootcauses of these performance bugs are the specific ways that markdown compilers handle the language s context sensitive features.
in particular markdown compiler developers often choose to implement these language features by backtracking.
the backtracking strategies can be abused or exploited with specially crafted yet syntactically legitimate inputs for causing worst case performance issues.
we also find that the developers usually mitigate such exploitation by enforcing a hard limit on the maximum number of backtracking steps which however limits the intended functionality of the markdown compilers.
we then seek to detect unknown performance bugs in markdown compilers.
we focus on cpu exhaustion performance bugs which are the dominant type of performance bugs.
in particular we consider leveraging fuzz testing the go to approach to detect performance bugs.
fuzzing is free from some limitations of static analysis techniques e.g.
high false positives .
it has been extensively applied to detect thousands of vulnerabilities in various real world software .
existing fuzzers for performance bugs like slowfuzz perffuzz and saffron are not designed for domain specific languages such as markdown hence cannot efficiently generate inputs to thoroughly exercise the compilers.
to efficiently fuzz markdown compilers a grammar aware approach would be needed.
therefore we study ui .
oufsobujpobm pogfsfodf po vupnbufe 4pguxbsf ohjoffsjoh 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
the commonmark specification and model the markdown grammar.
we then extend existing fuzzers with a syntax tree based mutation strategy specifically for markdown.
such a mutation strategy could preserve useful markdown syntaxes during the input mutation process and help efficiently generate high quality inputs to fuzz markdown compilers.
to detect cpu exhaustion performance bugs in markdown compilers we monitor the program execution under the generated inputs.
we employ a statistical model using chebyshev inequality to label abnormal cases as performance bugs.
like in prior works our approach can potentially lead to duplicate bug reports because different yet highly similar inputs could actually trigger the same performance bug.
it is time consuming and impractical to manually de duplicate them.
y et it is non trivial to automate this process in the scenario of performance bugs.
existing bug de duplicating methods in fuzzing use coverage profile and call stack snapshots which are not applicable to performance bugs.
for example it is hard to obtain an accurate and deterministic call stack snapshot that represents the run time program state when a performance bug is triggered.
following prior trace analysis works we propose an execution trace similarity algorithm to de duplicate the reports.
specifically we represent the execution trace per report into a vector compute the cosine similarity between vector pairs and classify highly similar vectors bug reports as the same bug.
we integrate the above mentioned techniques into mdperf fuzz a fuzzer specialized in detecting performance bugs in markdown compilers implemented in c c .
with mdperf fuzz we successfully detected new performance bugs in standalone markdown compilers.
it also outperformed the state of the art works with more detected unknown performance bugs higher performance slowdown and higher code coverage.
to detect performance bugs in markdown compilers and plugins that are not written in c c we further summarized the exploits generatedby mdperf fuzz into attack patterns.
by applying these patterns we found new performance bugs whichcould potentially affect millions of websites and their users.our evaluation results demonstrate that such performancebugs were widespread and they existed in the latest stable versions of almost all markdown compilers we tested.
we further showed that such bugs can be easily exploited using only low rate traffic to launch dos attacks against a server demonstrating the severity of this emerging threat.
we also revealed that vulnerable markdown compilers implemented in different programming languages could be exploited by the same inputs because different compiler developers implemented the compilers in exactly the same buggy ways.
we are in the process of responsibly disclosing our findings to the affected parties.
at the time of writing bugs have been acknowledged and one new cve id has been assigned.
to facilitate future research we release mdperf fuzz as an opensource software at .
researchers can easily build on mdperf fuzz their tools to study performance issues in the compilers of other domain specific languages such as latex and wikitext.
in summary we make the following contributions.
we presented the first empirical study of the performance bugs in markdown compilers.
we developed mdperf fuzz to generate high quality test inputs to detect performance bugs in markdown compilers.
we proposed an execution trace similarity algorithm to effectively de duplicate performance bugs.
we detected new performance bugs in markdown compilers and new performance bugs in popular realworld applications.
ii.
b ackground a. markdown created by swartz and john gruber in markdown has been a prevalent markup language for creating formatted text from plain text.
to date commonmark has become a well recognized markdown specification in the community.
we introduce some of its important features below.
v alid markdown documents.
any sequence of characters is a valid markdown document where a character is usually a unicode code point.
text stylization.
markdown provides diverse syntax supports for stylizing the text.
specifically it treats asterisks and underscores as indicators of emphasis of the enclosed text.
a pair of asterisks or underscores e.g.
text emphasis represents text emphasis.
a pair of double asterisks or underscores e.g.
text bold denotes text bold strong .
links.
markdown allows inserting links with the format of url .
the is the link label and is the formatted preview text.
the url is the link target that directs to either internal or external resources specified by the url url .
html blocks.
markdown documents use html blocks to insert raw html contents.
the html blocks are enclosed with start and end conditions such as script script !
?
?
and !a .
b. markdown compilers markdown compilers take valid markdown strings as inputs and output formatted text e.g.
html documents.
the markdown syntax tokens in the input strings are interpreted into the ones in the target format.
markdown compilers generally take three steps to analyze the input strings markdown compilers first scan the input strings and group the characters into tokens in the lexical analysis they then analyze the tokens for their syntactical and semantic meanings in the syntax and semantic analysis based on the results of the previous step they further produce the final code e.g.
html in the code generation step.
unlike other compilers e.g.
gcc markdown compilers usually do not have the intermediate code optimization step since markdown is a markup language.
markdown has some context sensitive features .
for the same token different behaviors can be exhibited depending onthe analysis context i.e.
the other tokens in the same input .
to handle such features markdown compilers have to record the relevant compilation information as the context to determine how to process a token.
sometimes due to the nature of the markdown language design several possible choices options are applicable for interpreting a token in a given context.
for example the token of double asterisks can possibly be the open delimiter of text bold syntax or just two asterisks in plain text.
to support such features modern compilers search in multiple possible options with a default order.
a previously chosen option could become invalid when more information is collected along with the analysis on the input strings.
compiler developers usually use the strategy of backtracking to explore a different choice.
the compilers continue this strategy till a correct option is ultimately determined.
for example when analyzing the input string text bold markdown compilers usually first prioritize the choice of open delimiter of text bold syntax for the token .
when no corresponding close delimiter for the token can be found the compilers backtrack and try other options for the token e.g.
plain text.
since any sequence of characters in markdown is valid plain text is the default last option in the compilers for the tokens.
c. performance bugs in markdown compilers performance bugs could degrade a program s performance and waste computational resources.
usually people define performance bugs as software defects where relatively simple source code changes can significantly optimize the execution of the software while preserving the functionality .
there can be several different performance issues regarding different categories of resources e.g.
cpu memory .
in the past several publicized operation incidents were caused by performance bugs consequently many software projects were abandoned .
performance bugs in markdown compilers could particularly impact end user experiences.
exploitation of client side performance bugs can lead to excessive resource consumption and lower responsiveness.
what is worse server side performance bugs can be exploited by attackers for dos attacks which can impair the availability of critical services .
for example a performance bug mdperf fuzz detected had existed in all versions of gitlab community edition and enterprise edition before .
.
.
this bug allows an attacker to cause unrestricted server resource consumption with a speciallycrafted issue or merge request.
it could potentially lead to dosand impact many other users concurrently accessing the gitlab server.
iii.
u nderst anding performance bugs in markdown compilers in this section we present an empirical study on several known performance bugs in mainstream markdown compilers to understand their characteristics.
a. data collection we investigate performance bugs in the commonmark specification and representative markdown compilers chosentable i the existing performance bugs included in our study.
lang.
means the underlying programming language for implementing the markdown compiler.
software lang.
bugs time periods commonmark spec n a cmark c 2020md4c c commonmark.js js 2019markdown it js from the recommended implementations of the specification.
in particular cmark1and md4c2are two high performance markdown compilers written in c. commonmark.js3and markdownit4are two node.js packages used in both client side and server side applications.
we do not consider the software that uses markdown compilers as one of its sub components plugins e.g.
gitlab because such software usually does not modify the internal workflow of the included compilers.
we limit our manual analysis to only representative markdown compilers because the analysis is quite time consuming.
furthermore we find that our current software set already allows us to characterize the bugs and extract some general features which we will present later in this section.
in these markdown compilers we manually collected distinct performance bugs from their public bug disclosure channels and their github repository issues.
the bug distribution is presented in table i. we found all these performance bugs were abusing the cpu resources instead of other resources like memory.
this suggests cpu exhaustion performance bugs are the dominant type of performance bugs.
we next characterize the performance bugs and present our findings.
b. bug disclosure methods detecting and disclosing performance bugs is an important security and software engineering task.
we investigate how performance bugs in markdown compilers are usually detected and exposed by analyzing bug reports and relevant online discussions.
we observe that manual analysis has been the dominant approach to hunting performance bugs in markdowncompilers.
some bug reports suggested that the security analysts found the bugs by crafting special test inputs according to the commonmark specification e.g.
.
specifically out of the performance bugs were identified and reported through manual analysis or testing.
only performance bugs were detected by automated tools like oss fuzz .
this motivates us to develop better automated techniques to detect such bugs.
c. disclosure and patch time to understand the trend of performance bugs we analyze the disclosure time of the bugs.
we depict the numberof performance bugs along the time they were disclosed in fig.
the number of performance bug reports over time from september to december .
figure .
we observe that few bugs were reported before early and the number of reported bugs had been gradually growing from early till late .
in particular .
out of the performance bugs were disclosed between april and december inclusive months .
bugs were reported before .
it reveals that such bugs had been gradually drawing the attention from the compiler developers and security analysts.
we further analyze the time it took to release a patch since a bug was initially reported.
we were able to successfullydetermine the time for performance bugs.
for the restbugs either we did not find the explicit bug patch time e.g.
or they have not been patched yet e.g.
.
we find that the average duration for patching performance bugs is days.
further .
bugs were patched within days.
we particularly investigated those bugs that took much longer patch time e.g.
more than months.
we observed that they were usually related to the ambiguity of the commonmark specification.
thus their patches usually require some work from both the compiler developers and the specification maintainers.
some of the bugs and patches have led to the modifications of the language specification.
d. root causes identifying the common root causes of real world performance bugs can benefit potential future research and software developments.
we manually analyzed performance bugs and successfully figured out the root causes for bugs.
we classify the root causes into three categories.
a bug is assigned to multiple categories if it has multiple major causes.
r1 worst cases in super linear algorithms.
some normal algorithms implemented in markdown compilers have superlinear worst case complexity .
attackers can craft inputs to trigger the worst case behaviors and lead to performance issues.
in particular supporting some context sensitive features in markdown compilers is prone to introducing performance bugs.
the majority out of of bugs were related to such worst case behaviors.
some markdown syntaxes e.g.
links emphasis and strong emphasis html blocks are related to the language s context sensitive features.
as discussed in ii b supporting contextsensitive features in markdown requires the compilers to backtrack which could take more than linear time.
the backtracking strategies can easily be abused with crafted inputs hence lead to performance issues.
for instance links were the primary vulnerable syntax in markdown compilers where of the known performance bugs could be exploited with special inputs with links.
similarly of the bugs were caused by the buggy emphasis and strong emphasis handlers.
our study reveals that the implementation of the context sensitive features in the markdown compilers is prone to introducing performance bugs.
one typical input pattern that exploits the context sensitive syntax handler to trigger performance bugs is many open tokens.
this pattern can lead the compilers to repeatedly search a close token towards the end of the input string for each such open token.
to correct the wrong options the compilers have selected the compilers would normally backtrack to explore other options.
for example deeply nested cda t a block open delimiters can result in an excessive compilation time.
when fed with n nested cda t a block open delimiters e.g.
!
!cdata !
cdata !
cdata ... that are not closed with the corresponding close delimiters i.e.
or are closed in the end of the input string the compilers need to compare with all tokens in theinput string to determine if an open delimiter can be closed or not.
once the compilers find an open delimiter cannot be closed they switch to other possible options for that delimiter next for instance the open delimiter !in !a which cannot be closed either.
thus the time for handling such input stringsis at least in polynomial time complexity.
by providing a long input with many such open tokens it is simple to cost the compiler several second or even more execution time.
r2 unoptimized code.
some unoptimized code in the markdown compilers could also lead to performance issues.
for instance some functions do not coordinate well for certain functionalities.
we find that performance bugs were caused by such unoptimized code.
unlike the algorithms in r1 such performance issues could be addressed by code optimization.
however each problem needs to be separately analyzed and fixed which could be time consuming.
we next discuss an example of such unoptimized code.
minor performance issues in individual problematic functions could accumulate when the given inputs can repeatedly trigger the execution of such functions.
for example in one bug cmark calls s find first nonspace to find the first non space character from the current offset in a line.
the function in a second call would still search from the initial position even if in a previous call it has already recognized the location of the first non space character.
this means function calls tos find first nonspace sometimes were unnecessary.
crafted inputs with lots of complicated and nested indents could result in repeated invocations of this function and cause performance bugs.
the problem however can be solved by using betterstrategies like cashing the positions of the previously found non space characters.
seeds markdown grammar input mutation corpus refinement corpus poolbug de duplication fitness function fig.
the architecture of m dperf fuzz .
r3 problematic implementations.
other causes of the bugs are specific to the compiler implementations or designs.
some compilers overlooked part of the commonmark specification for example unicode support.
this can lead to infinite loops when unexpected inputs are provided to the compilers.
some other bugs in this category were caused by wrong data structures.
performance bugs fall into this category.
e. patches of performance bugs we investigate the patches of the performance bugs in the markdown compilers to understand how they were addressed.
we manage to identify the bug fix patterns for performance bugs.
we present our findings below.
p1 enforcing limits.
the most common patch pattern is to add limits for certain conditions such as the maximum depth of the nested structure although the commonmark specification does not explicitly specify any such limits.
when such limits are reached the compilers directly regard the rest unanalyzed inputs as plain text.
enforcing limits can prevent excessive cpu usages caused by the worst case exploitation with too large test cases.
however the intended functionality might be violated.
it is also difficult to set a correct limit to prevent all attacks while not breaking some unusual yet legitimate inputs.
such a strategy has been applied to patch out of the bugs we investigate.
p2 logic changes.
logic changes sometimes are necessary as the bugs are caused by incorrect coordination among multiple program components and functions.
some inefficient code snippets need to be further optimized to eliminate the underlying performance issues.
for some other performance bugs caused by incorrect regular expressions compiler developers mainly review and rewrite the regular expressions.
iv .
m dperf fuzz though we have characterized known performance bugs it is unclear if there exist many unknown performance bugsin markdown compilers and related applications.
therefore we try to detect unknown performance bugs in real world markdown compilers.
we focus on cpu resource exhaustion performance bugs in this work because they are the dominant type of performance bugs.
to avoid the high false positive rates in static analyses we propose to use dynamic fuzz testing to detect and url link demo url link label link target fig.
ast of the statement url .
exploit performance bugs.
to do so we face two technicalchallenges.
first generating markdown documents to testmarkdown compilers and exploit the performance bugs if any is naturally difficult because of the huge document search space.
prior fuzzers are not very efficient in generating the specially formatted inputs to trigger theperformance bugs in markdown compilers which we willdiscuss in v c .
second since many distinct inputs can trigger the same performance bug it is naturally challenging toaccurately de duplicate the bug reports.
prior performance bug fuzzers do not try to de duplicate performance bugs.
other fuzzers for detecting memory corruptions de duplicate bugs using the unique memory footprints e.g.
coverage profiles and call stacks when the bugs are triggered whereas one performance bug can potentially exhibit different memory footprints.
we overcome these challenges with mdperf fuzz .
the overall methodology is depicted in figure .
mdperf fuzz follows the general fuzzing workflow and is built on topof afl .
inside the main fuzzing loop we particularly design a grammar aware syntax tree based mutation strategy toefficiently generate high quality inputs iv a .
we first model markdown grammar from the commonmark specification to parse the test cases into abstract syntax trees asts .
themutation strategy then mutates the asts while preservingthe markdown syntaxes to well exercise the diverse syntax components of markdown compilers.
to guide the fuzzer to detect cpu exhaustion performance bugs we use a fitness function to measure if an input should be favored or not iv b .
the fitness function considers both code coverage and resource usage.
to report only unique bugs we compute the cosine similarity between each pair of the vector representations of the execution traces in bug reports and group highly similar reports as duplicate ones iv c .
we then present the implementation details iv d .
a. syntax tree based mutation strategy above the default mutation strategies of afl e.g.
bit flipping mdperf fuzz introduces a syntax tree based mutation strategy to better preserve the markdown syntaxes.
due to the complexity of markdown language no prior work has attempted to formalize its grammar which is non trivial.
we spent a considerable amount of efforts on analyzing the commonmark specification and modelling the language grammar.
given the grammar our syntax tree based mutation strategy parses a test case into an ast traverses the ast and randomly replaces several subtrees treesrc with candidate subtrees treedst .
we construct the simplest asts each representing a markdown syntax and include them as candidates of treedst.w ed o not consider the combination of multiple markdown syntaxes when constructing one candidate of treedst because it can be achieved via replacing multiple treesrc.
in this way compared to mutation strategies that randomly flipping bits our strategy preserves and extends the syntax of the original test case.
thus it can efficiently construct syntactically correct new test cases from the modified asts for later testing.
for example given a test case of url we first parse it into the ast shown in figure .
we identify the basic subtrees i.e.
and randomly replace each of them to generate new test cases.
for instance we can replace the whole link with an inline code span and produce a test case of asciigraverandom asciigrave .
the newly generated test case remains the text bold syntax but also exercises new syntax features.
it is worth noting that the default mutation strategies of afl can also be applied when the syntax tree based one fails to parse a test case.
b. fitness function and performance bug detection mdperf fuzz uses a fitness function to decide whether to favor a test case or not.
we include both the coverage and the control flow graph cfg edge hit counts into the fitness function.
as in other works the coverage feedback drives mdperf fuzz to explore more newly discovered code.
only it however is not sufficient for our purpose as it does not consider loop iterations which are crucial for detecting highcomplexity performance bugs .
the cfg edge hit counts standing for the times a cfg edge is visited under a test case enables mdperf fuzz to explore computationally expensive paths.
as stated in prior work many programs e.g.
php hash functions do have non convex performance space.
we thus do not use the number of executed instructions to guide mdperf fuzz because it might fail to find the performance issues caused by local maxima.
therefore as in the state ofthe art work perffuzz we design mdperf fuzz to favor those test cases that maximize certain cfg edge hit counts to better detect performance bugs.
in this way mdperf fuzz tends to select test cases to either trigger new code or exhaust certain cfg edges.
note that we do not use run time cpu usage or concrete execution time as the metric because they show large variations affected by many uncontrollable factors such as the fuzzer s concurrent features and the characteristics of the applications being tested.
prior works rely on analysts to label performance bugs which is time consuming and does not scale.
we thus design a statistical model to accurately identify performance bugs.
our statistical model first obtains the normal program execution behaviors which help label abnormal ones asperformance bugs.
in particular as in we compute the total execution path length the sum of the cfg edge hitcounts under a test case as the metric.
we first prepareabundant random normal test cases we feed each test case to the testing program and obtain the corresponding execution path length.
we calculate the mean l and the standard deviation l of the execution path lengths li .
we label a case as a performance bug if its execution path exceeds the normal level to a certain extent.
according to chebyshev inequality as shown in equation the probability of the random variable lithat is k standard deviations away from the mean l i s normally no more than k2.
since only in rare cases would the execution path length significantly deviate from the normal situations we label a performance bug if its execution path length ltis more than kl away from the l see equation .
p li l k l k2 lt l kl c. bug de duplication though the execution path lengths under different test cases could all meet equation they could actually trigger the same performance bug.
de duplicating the bug reports is necessary for a more precise result whereas prior works do not apply automated methods to de duplicate the reports.
existing fuzzing works identify unique bugs using the call stack formemory corruptions e.g.
crashes .
however it does not fit well our purposes for performance bug de duplication.
though we can possibly collect the call stack as well e.g.
by forcibly terminating the program at some point the call stacks might not be accurate enough to represent unique bugs.
this is because the exactly critical call stack for a performance bug can hardly be accurately exposed.
unlike memory corruption bugs that have deterministic call stacks when the bugs are triggered performance bugs might exhibit diverse call stacks depending on when to obtain them.
therefore a better bug de duplication method is needed.
we propose a bug de duplicating approach by merging reports with similar execution traces similar to prior trace clustering methods .
the high level idea is that different exploiting inputs of the same performance bug should exhibit similar execution traces i.e.
most cfg edges are visited in similar frequencies.
in particular we apply the test cases in the reports to the instrumented target software and obtain the cfg edge hit count i.e.
number of times a cfg edge is visited in a test for each edge.
we summarize the unique cfg edges that are visited in all reports during fuzz testing into an n dimensional vector space where nis the total number of unique cfg edges being visited and each dimension in the vector space corresponds to a cfg edge.
in other words we construct an edge hit count vector e.g.
v c1 c2 ... cn for each report.
each dimension ci represents the hit count of the ith cfg edge in that report.
to consider if two reports point to the same bug we compute the cosine similarity between their edge hit count vectors e.g.
v v prime as shown inequation .
cosine similarity is based on the inner product of the two vectors and thus naturally assigns higher weights to the dimensions with larger values i.e.
edges visited most .
table ii bug detection results of the markdown compilers in c. rep. is the reports from the fuzzer.
u rep. is the unique reports after de duplication.
con.
is the manually confirmed bugs.
software lang.
rep. u rep. con.
cmark .
.
c md4c .
.
c 0cmark gfm .
.
c therefore we calculate the cosine similarity between every two reports and merge reports as the same bug if the cosine similarity between their corresponding edge hit count vectors exceeds a threshold.
cosine v v prime v v prime v v prime summationtextn i 1cic prime i radicalbig summationtextn i 1c2 i radicalbig summationtextn i 1c prime2 i d. implementation we implemented the fuzzing part of mdperf fuzz on top of an afl based fuzzer perffuzz .
specifically we compiled a simplified markdown grammar via antlr4 into a markdown parser then we introduced the syntax tree based mutation strategy as an extension that can be flexibly plugged in we enhanced a c c compiler to instrument the testing software and modified afl s showmap functionality to trace the execution on the instrumented applications to obtain the cfg edge hit counts for bug de duplication.
v. d etecting performance bugs via mdperf fuzz in this section we investigate the prevalence of performance bugs in the wild.
we apply mdperf fuzz to detect performance bugs in several mainstream markdown compilers.
a. experimental setup dataset.
since mdperf fuzz employs an afl based fuzzer it is only capable to analyze markdown compilers implemented in c c .
therefore we select all the markdown compilers in c in the recommended implementation list of commonmark specification and list them in the first column of table ii.
experiments.
each markdown compiler is first instrumented using our enhanced c compiler.
we then apply mdperf fuzz to detect performance bugs on the instrumented markdown compilers.
we apply the pocs collected in iii as the initial seeds and configure mdperf fuzz to use a single process a timeout of hours and an input size of bytes.
after our preliminary study we empirically set kto and the cosine similarity threshold to .
for all testing software.
all experiments described in this section are conducted on a server running debian gnu linux with an intel xeon cpu and 96gb ram.
b. results we present the performance bug detection results in table ii.
duplicate performance bug reports are naturally common during fuzzing.
the fuzzing part of mdperf fuzz reported cases in total and our de duplicating algorithm merged them into distinct reports.
we observe that all the casesdid successfully slow down the markdown compilers by from .
to .
compared to normal performance cases.
we further manually check the reports to validate the performance bugs.
since mdperf fuzz limits the input size like in other works due to the concerns of large search space our manual analysis attempts to identifythe severity of the performance slowdown in more realisticscenarios e.g.
larger input sizes of thousands of characters.
to this end we first identify the exploit input patterns in the reports that exhaust the run time resources.
with the patterns we further construct larger test cases to verify the performance issues in practice.
finally cases in markdown compilers were confirmed as performance bugs including new bugs after our manual analysis.
we are in the process of reporting the new bugs to the concerned vendors.
at the time of writing bug has been well acknowledged.
we found no bug in md4c.
the developers of md4c explicitly mention that they seriously considered performance as one of their main focuses during the development.
therefore the performance bugs could be avoided with domain knowledge and special care which are often difficult for most developers.
we have investigated those false positive cases to understand the reasons.
we find that the unique buggy cases reportedby mdperf fuzz indeed triggered performance issues i.e.
those test cases led to longer execution paths.
however the larger attack inputs we constructed manually did not manifest such performance issues.
this is because in our experiments we let mdperf fuzz explore only small size test cases e.g.
hundreds of bytes to limit the search space.
as we discussed in iii e developers might choose to patch the performance bugs by enforcing certain limits e.g.
p1 .
such a strategy guarantees that there is no performance issue in large size testcases small size test cases however can still trigger worst case behaviors.
we find all the false positives were caused because of this.
c. comparison we compare mdperf fuzz with two state of the art works slowfuzz and perffuzz .mdperf fuzz and perffuzz are implemented above afl whereas slowfuzz is built on top of libfuzzer .
slowfuzz libfuzzer uses in process fuzzing which is much faster as it has no overhead for process start up however it is also more fragile and more restrictive because it traps and stops at crashes .
nevertheless we evaluate all the tools with the same dataset in table ii and run them for the same amount of time hours and the same input size bytes for a fair comparison.
we failed to run slowfuzz on md4c because of some unexpected crashes after several minutes of the execution.
to the best of our knowledge there isno way to suppress such crashes.
mdperf fuzz and perffuzz afl based fuzzers do not suffer from this problem.
the results show that mdperf fuzz outperformed perffuzz and slowfuzz by detecting and more performance bugs respectively.
in particular perffuzz reported cases in cmark md4c cmark gfm respectively slowfuzz reported cases in cmark cmark gfm respectively.
these results table iii the performance slowdown and code coverage of mdperf fuzz perffuzz and slowfuzz .
the best slowdown across all tools is normalized over the baseline of the same random normal performance case.
line cov.
and func.
cov.
denote line coverage and function coverage respectively.
tool software best slowdown line cov.
func.
cov.
mdperf fuzz cmark .
.
.
md4c .
.
.
cmark gfm .
.
.
perffuzzcmark .
.
.
md4c .
.
.
cmark gfm .
.
.
slowfuzzcmark .
.
.
cmark gfm .
.
.
also demonstrate the need of a bug de duplication method.
we applied our bug de duplication algorithm to identify unique bugs and then manually confirmed the reports.
finally perffuzz detected real performance bugs in cmark md4c cmarkgfm respectively.
slowfuzz detected real performance bugs in cmark cmark gfm respectively.
this demonstrates that our syntax tree based mutation strategy can improve fuzzing efficiency by generating better inputs within the same resource budget.
performance slowdown table iii shows the performance slowdown caused by the inputs generated by mdperf fuzz perffuzz and slowfuzz.
we use the maximum execution path length as the performance metric and normalize theperformance slowdown using a baseline obtained from the random normal performance cases.
we notice that though all tools caused performance slowdown on the testing applications mdperf fuzz achieved a .
higher average best performance slowdown over perffuzz and .
over slowfuzz.
furthermore we observe that mdperf fuzz could generate inputs that slow down the compilers much faster than the other tools.
for example to reach a .
performance slowdown on cmark mdperf fuzz took .
hours whereas perffuzz and slowfuzz used .
hours and .
hours respectively.
this demonstrates the high efficacy of mdperf fuzz in detecting performance bugs.
code coverage we also evaluate the code coverage each tool achieves to measure the efficacy of our syntax tree basedmutation strategy.
we collect the test cases generated by eachtool and run on afl cov which detects the code coverage using the overall execution traces covered by the test cases.
though slowfuzz is not based on afl we believe using its test cases on afl cov can accurately reflect the code coverage under a fair metric.
we present the results of line coverage and function coverage in table iii.
the syntax tree based mutation strategy of mdperf fuzz was effective in reaching high code coverage.
it enabled mdperf fuzz to visit .
of lines of code and .
of functions on average.
mdperf fuzz achieved higher code coverage than perffuzz and slowfuzz in all testing software.
in the markdown compilers mdperf fuzztable iv evaluation results on other markdown compilers and applications.
denotes the security mode of the compiler.
software lang.
bugsother markdown compilerscommonmark.js .
.
js markdown it .
.
js marked .
.
js snarkdown .
.
js commonmark java .
.
java flexmark java .
.
java commonmark.py .
.
python php commonmark .
.
php php commonmark .
.
php parsedown .
.
php parsedown .
.
php php markdown .
.
php markdown go go comrak .
.
rust stackedit js dillinge js 7appsgitlab .
.
ruby bitbucket .
.
java hugo .
.
go hexo .
js outperformed perffuzz by .
more lines of code and .
more functions mdperf fuzz outperformed slowfuzz by .
more lines of code and .
more functions.
with the mutation strategy mdperf fuzz successfully fuzzed .
of lines of code and .
of functions that were not ever visited by other tools.
as a result new performance bugs were identified within this proportion of code.
summary.
mdperf fuzz outperformed the state of the art works by detecting more performance bugs achieving better performance slowdown and covering more code.
vi.
s tudying performance bugs in more markdown compilers many markdown compilers are implemented in languages other than c c and they are not supported by mdperf fuzz and other afl based fuzzers.
to understand if and how these markdown compilers suffer from performance bugs we construct an extensive dataset and utilize the exploits generated by m dperf fuzz in v to detect potential bugs in them.
a. methodology dataset.
we construct a comprehensive dataset in table iv including a set of other markdown compilers written not in c c and another set of relevant real world applications.
we try to include popular markdown compilers implemented in diverse programming languages to understand the effects of programming languages on performance bugs if any .
in particular our dataset covers markdown compilers writtenin java javascript php python go and rust.
we alsoinclude the first two google search results stackedit anddillinge in january into our dataset.
stackedit is also in the suggested application list for opening markdown documents in google drive.
we notice that some compilers php commonmark and parsedown provide options to enable additional security mode to mitigate certain bugs.
we are interested in the effects of such security options thus we present them separately with an asterisk suffix .
regarding real world applications we try to cover two main uses of markdown compilers markdown document rendering in code hosting software e.g.
gitlab and bitbucket and static web page generation frameworks e.g.
hugo and hexo .
we downloaded the latest stable version of each markdown compiler from its official website or github repository in january .
we denote their actual software versions in the parenthesis if applicable.
we install and configure them with the default settings.
experiments.
since mdperf fuzz is not capable to detect performance bugs in the dataset in table iv we first collect the exploits generated from mdperf fuzz in v and summarize them into unique attack patterns.
each pattern exploits one markdown syntax feature.
we then apply them to evaluate the software in a black box manner.
such an approach is practical and scalable and enables us to analyze a diverse set of compilers.
we use the environment as in v a to test standalone markdown compilers.
for the software in the application category we empirically identify the entry points for triggering the markdown compiler components e.g.
command line api or ui operations .
for those that work in the server client modeland allow self hosting i.e.
gitlab and bitbucket we deploy them on a computer running debian gnu linux .
with a core intel xeon cpu and 16gb ram.
we use another computer in the same local area network as the client to send requests and measure the network response time client side cpu time and server side cpu time after the client issues a request.
we use such results to detect performance bugs and further understand whether the performance bugs appear in the server or the client.
it is hard to instrument the markdown compilers implemented in diverse programming languages and the markdown components in complicated software.
hence we currently are unable to de duplicate the reports for the software in the dataset.
nevertheless as our test cases especially exploit distinct markdown syntax features we believe they are most likely to trigger different performance bugs.
we will further discuss it in viii.
b. results we present the bug detection results in the last column of table iv.
the performance bugs in markdown compilers are prevalent and might have been overlooked by the compiler developers.
in particular we successfully identified performance bugs in the category of other markdown compilers .
we can observe that the number of detected performance bugs varies significantly among markdown compilers.
some markdown compilers were particularly vulnerable to performance bugs whereas some did not have any performanceissues.
for instance we detected performance bugs incommonmark.py but only bugs in markdown it.
we do not observe a distinguishable difference among programming languages in terms of the number of bugs.
the performance bugs could substantially impact end user experiences.
for example the performance bugs in stackedit and dillinge could lead to data loss once the browser tab was unresponsive or was forcibly killed.
the markdown compilers in popular applications are also vulnerable to performance bugs.
we successfully detected performance bugs on the client side and on the serverside.
in particular gitlab and bitbucket suffered from serverside performance bugs which could be exploited to significantly degrade the server performance.
they can be exploited for launching dos attacks see vii for more details .
responsibly disclosing the bugs can greatly benefit the software users and the whole community.
we are in the processof contacting the maintainers of the buggy markdown compilers and reporting the newly detected performance bugs.
at the time of writing performance bugs have been acknowledged.
one performance bug in gitlab has been recognized in cve2021 .
though we mainly focused on performance bugs we also detected memory corruptions in more than markdown compilersin our research.
in particular the markdown compilers exhibited crashes when we fed them the test cases.
the crashes happened in the markdown compilers implemented in javascript java and python.
for example we particularly analyzed one crash in snarkdown and found the maximum call stack size was exceeded when using recursive function calls to process one type of our testing inputs.
there were other memory errors e.g.
segmentation faults when our test cases triggered some illegal memory read or write.
we also detected several unexpected errors in the application category.
we observed that gitlab and bitbucket could return http internal server errors when the test cases contained special unicode characters possibly because their markdown compilers currently did not support compiling special unicodecharacters.
though such errors usually affect only the user who sends documents containing such characters they still lead to bad user experiences and shall be fixed.
c. effects of security mode php commonmark and parsedown introduce a security mode to mitigate certain bugs.
our results show that their security modes have different effects.
as shown in table iv php commonmark in its security mode shown as php commonmark was not vulnerable to any performancebugs whereas the security mode of parsedown shown as parsedown did not mitigate any performance bugs.
by reading the relevant documents and the source code we find that the security mode of parsedown mainly mitigates cross site scripting xss vulnerabilities but does not consider performance related issues.
the security mode of phpcommonmark on the other hand applies several strategies to mitigate performance bugs.
for instance it sets a threshold to limit the depth of nested structures escapes html blocks in fig.
compilation time of markdown compilers under attack inputs of the size of characters and a second maximum threshold.
the middle lines in the boxes represent the corresponding median values.
markdown inputs and disallows unsafe links.
these strategies together could successfully mitigate all performance bugs identified in its default mode.
we will further discuss the countermeasures against performance bugs in viii.
vii.
i mp act on performance to better understand the performance degradation caused by performance bugs we depict in figure the compilation time when the performance bugs are triggered by attack inputs of50 characters.
we use such a size as it can roughly represent the normal uses of markdown compilers.
the results show that performance bugs can cause significant performancedegradation.
in general our attack inputs successfully exploited the performance bugs by causing over second compilation time.
different performance bugs could result in different levels of performance degradation in a compiler.
for example thecompilation time of commonmark.js ranged from secondsup to seconds the maximum time threshold due to the performance bugs.
if a threshold was not set the exploits would even cause the compilers to run for several hours.
the performance bugs can potentially affect many users if they reside in server side applications.
specifically we present two case studies about gitlab a code hosting software and parsedown a popular markdown compiler module in php .
we deploy the latest version of gitlab with its default nginx web server we develop a server side php application that calls parsedown to compile user provided markdown documents.
we randomly choose a common attack input that can trigger performance issues in both gitlab and parsedown.
we then test the applications with different numbers of concurrent attacks requests .
we try at most concurrent requests because our server has only logical cpu cores.
we depict the server cpu usage under the attacks in figure .
we clearly observe the increase of cpu usage when more concurrent attack requests were issued.
in particular when requests were sent the server cpu usage promptly reachedfig.
server side cpu usage over time under attacks on gitlab and parsedown.
almost .
therefore an attacker can send only a fewattack requests at a very low rate to significantly degrade the performance of a vulnerable server application making it unable to responsively serve other legitimate user requests.
viii.
d iscussion and future work mitigating performance bugs.
practical mitigation and defense techniques against performance bugs are necessary for protecting vulnerable markdown compilers and applications.
we have shown that the context sensitive feature handlers in markdown compilers could be abused by attackers in iii d .
several security strategies used in the security mode of phpcommonmark e.g.
enforcing the limits escaping the html blocks and disallowing unsafe links are shown to be effectivein mitigating performance bugs.
however they can break some functionalities especially those related to the context sensitive features.
for example some html blocks cannot be compiledas expected because of the security strategies.
longer legitimate markdown documents might also be blocked because of the limits.
a trade off has to be made to balance functionality and security.
in the future we hope to port such mechanisms tomitigate attacks exploiting performance bugs in markdown compilers.
bug de duplication.
trace based analysis is effective in triaging bugs .mdperf fuzz thus adopts an execution trace similarity approach to de duplicate performance bugs.
besides the cosine similarity mdperf fuzz employs other algorithms e.g.
those measuring the euclidean distance can potentially be applied as well.
theoretically the method can be applied to software implemented in diverseprogramming languages once the run time cfg edge hit information is available.
however to the best of our knowledge not all programming languages have available instrumentation tools exactly for such a purpose.
it is also time consuming and even infeasible to develop our own instrumentation tools within this work.
we thus choose to not apply the method for the evaluation in vi.
in the future we plan to further investigate the feasibility of a language agnostic method forde duplicating performance bugs.
for example we want toexplore transforming the software into certain intermediaterepresentations irs and obtain the necessary information from irs to de duplicate performance bugs.
lessons.
we have tested markdown compilers implemented in languages other than c c in vi using the attack patterns generated by mdperf fuzz .
each pattern corresponding to one markdown syntax feature could exploit a group of bugs in different markdown compilers.
for example one attack pattern was able to trigger similar performance bugs in different markdown compilers implemented in multiple programming languages.
based on our further analysis and the feedback from the developers we learned that compiler developers often borrow implementation ideas from other similar or relevantprojects.
such a design reuse is risky and has resulted in similar performance bugs across different implementations.
we conjecture that similarly those compilers might commonly be subject to other types of bugs.
prior works have studied performance bugs in the compilers of general purpose languages e.g.
gcc and domainspecific languages e.g.
node.js regex engine .
however performance bugs in the compilers of many otherdomain specific languages such as latex and wikitext have not been systematically studied yet.
we believe the design and implementation of mdperf fuzz can shed some light on the following research on other domain specific languages.
to facilitate future research we release the source code of mdperf fuzz to help the development of fuzzers for those languages.
for example researchers can replaceour language model with their own ones in mdperf fuzz .
further mdperf fuzz could also be easily extended to detect other types of bugs e.g.
memory corruptions in markdown compilers.
ix.
r ela ted work understanding performance bugs.
understanding the characteristics of performance bugs can help design techniques to detect and fix performance bugs.
existing studies focus on the performance bugs in programs on the desktop platform mobile platform and the web server end etc.
for instance zaman et al.
studied performance bugs in firefox and chrome and provided suggestions to fix the bugs and to validate the patches .
davis et al.
and staicu et al.
analyzed redos problem in npm and pypi ecosystems .
for compilers sub et al.
studied gcc and ll vm compilers but did not focus on the performance issues .
however there is little understanding of performance bugs in markdown compilers.
we further characterize the bug patterns and reveal the close relationship between the performance bugs and the context sensitive markdown syntaxes.
detecting performance bugs.
the detection of performance issues has drawn significant attention from researchers overthe past years.
prior studies focus on application layer dos vulnerabilities algorithmic complexity dos vulnerabilities and other general performance issues .
static methods analyze the source code of the applications and diagnose vulnerable bug patterns for example repeated loops .
dynamic methods are also applied to identify performance bugs.
slowfuzz perffuzz and hotfuzz pro posed fuzzing solutions to detect the worst case algorithmic complexity vulnerabilities.
saffron also used grammaraware fuzzing to find worst case complexity vulnerabilities in java programs.
toddler detected performance bugs by identifying loops with similar memory access patterns.
hybrid approaches combining static analysis and fuzzing were proposed to detect redos in java programs.
our work is tailored for markdown compilers and is equipped with a bug de duplication method.
grammar aware fuzzing.
to find security bugs in compilers a number of grammar aware fuzzing frameworks have been proposed.
guided by the grammar fuzzers can generate syntactically correct inputs to test the compilers.
in particular langfuzz modified existing test cases by randomly combining javascript code fragments to generate new test cases.
superion extended afl to support additional grammar aware mutation strategies via pluggable language parsers.
several learning based fuzzers transformed inputs into asts and performed subtree replacement with neural network models.
inspired by these works mdperf fuzz also employs grammar aware fuzzing to detect performance bugs in markdown compilers.
x. c onclusion performance bugs in markdown compilers were previously understudied.
this paper conducted a systematic study to understand the characteristics of such bugs.
we developed a fuzzing framework mdperf fuzz to detect performance bugs in the wild.
mdperf fuzz significantly outperformed the state of the art works and successfully identified many new performance bugs in markdown compilers and applications.
we demonstrate the performance bugs in markdown compilers are not only prevalent but also severe.
we hope that our studycould attract more attention to the compilers of domain specific languages.
acknowledgment the work described in this paper was partially supported by a grant from the research grants council of the hong kong special administrative region china project no.
cuhk .