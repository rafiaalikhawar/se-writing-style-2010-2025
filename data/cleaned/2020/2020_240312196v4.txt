leveraging large language models to detect npm malicious packages nusrat zahan philipp burckhardt mikola lysenko feross aboukhadijeh laurie williams north carolina state university email ncsu.edu socket inc email socket.dev abstract existing malicious code detection techniques demand the integration of multiple tools to detect different malware patterns often suffering from high misclassification rates.
therefore malicious code detection techniques could be enhanced by adopting advanced more automated approaches to achieve high accuracy and a low misclassification rate.
the goal of this study is to aid security analysts in detecting malicious packages by empirically studying the effectiveness of large language models llms in detecting malicious code.
we present socketai a malicious code review workflow to detect malicious code.
to evaluate the effectiveness socketai we leverage a benchmark dataset of npm packages of which packages have malicious code.
we conducted a baseline comparison of gpt3 and gpt models with the state of the art codeql static analysis tool using custom codeql rules developed in prior research to detect malicious javascript code.
we also compare the effectiveness of static analysis as a pre screener with socketai workflow measuring the number of files that need to be analyzed and the associated costs.
additionally we performed a qualitative study to understand the types of malicious packages detected or missed by our workflow.
our baseline comparison demonstrates a and improvement over static analysis in precision and f1 scores respectively.
gpt achieves higher accuracy with precision and f1 scores while gpt offers a more cost effective balance at precision and f1 scores.
prescreening files with a static analyzer reduces the number of files requiring llm analysis by .
and decreases costs by .
for gpt and .
for gpt .
our qualitative analysis identified data theft execution of arbitrary code and suspicious domain categories as the top detected malicious packages.
i. i ntroduction open source package repositories are often misused for software supply chain attacks .
attackers attempt to execute supply chain attacks by injecting malicious code into existing packages or by creating new infected packages and tricking users into downloading them .
classifying a package as malicious is challenging because malicious code detection necessitates comprehending the reasoning about the possible purpose and intent of the code.
given the increasing rate of software supply chain attacks the demand for approaches to detect such attacks is also increasing.
the current malicious package detection including static rules heuristics differential analysis and different machine learning techniques encounter challenges of incomplete solutions high misclassification rates .
additionally no single technique can comprehensively address different attack types like detecting malicious code injection dependency confusion and typosquat and often fails to support languages like typescript and coffeescript or research packages .
as a result practitioners need to integrate multiple tools to detect malware patterns.
an advanced detection technique is needed to identify diverse attack types effectively and minimize manual review efforts with minimal misclassification.
llms have the potential to address these limitations by detecting diverse attacks through their advanced understanding of context semantics and patterns thereby reducing reliance on multiple tools.
while the existing literature lacks the contribution of using llms to detect malicious code organizations are adopting llms as an advanced technique for threat analysis .
however assessing the performance of llms in production presents challenges comprehensive evaluation in a production environment is resource intensive llm missing actual malicious code false negative validation due to the lack of ground truth and after reporting potential malicious packages to the package registry the package managers often only acknowledge receipt of the reports without confirming the presence of malicious code.
hence we conduct an empirical study to evaluate llms performance in malicious code detection.
the goal of this study is to aid security analysts in detecting malicious packages by empirically studying the effectiveness of large language models llms in detecting malicious code.
in this study we address the following research questions rqs rq1 what performance can be achieved in malicious code detection using llms compared with static analysis baseline comparison in terms of precision recall and f1 score?
rq2 what is the effectiveness of using a static analyzer as a pre screener with socketai in terms of the number of files that need to be analyzed and the associated cost?
rq3 what types of malicious packages are detected or missed by llms?
to the best of our knowledge socketai a malicious code review workflow is a first ever study leveraging llms to detect malicious code and compares the effectiveness of the workflow with static analysis tools.
the workflow leverages iterative self refinement llm as a judge and zeroshot role play chain of thought cot prompting techniques in chatgpt models gpt and gpt .
we leveraged malwarebench a benchmark dataset and studied npm packages including malicious and neutral packages to evaluate the performance of llms.
to answer rq1 the paper presents a baseline comparison between chatgpt models and the mature state of the arxiv .12196v4 jan 2025art commercial tool codeql.
we used custom rules of codeql that have been used in prior study to detect malicious javascript packages.
we compare results using the standard machine learning ml evaluation metrics precision recall and f1 score.
we answer rq2 by comparing the effectiveness of using socketai only versus using codeql as a pre screener with socketai based on the number of files analyzed and the associated costs.
for rq3 we provide qualitative insights by manually evaluating the model s responses to understand the types of malicious packages identified or missed by chatgpt and demonstrate the opportunities and strengths of these models.
for the benefit of other researchers we further report issues we encountered while using chatgpt.
our contributions are socketai a malicious code review workflow to detect malicious packages in the npm ecosystem malware domain specific prompt to trigger llms reasoning in detecting malicious code a baseline comparison of socketai with codeql tool to understand the effectiveness of llms and strengths and shortcomings of chatgpt for large scale code review.
this paper is organized as follows section ii includes our design approach discussing our dataset allocated cost package selection and model selection criteria.
section iii covers our proposed workflow of socketai.
we answer our three rqs in section iv section v and section vi.
then we discussed challenges in using llm in section vii b. the related work section is covered in section viii.
we close with limitations and discussion of our work section vii c and ix .
ii.dataset and llm s election section ii a discusses the benchmark dataset used in this study section ii b discusses llms selection process and section ii d explains our package selection criteria with cost constraint discussed in section ii c. a.benchmark dataset malwarebench recent studies constructed malicious package datasets.
however these studies did not incorporate neutral package data into their labeled datasets.
neutral packages are needed to improve the accuracy of detection techniques.
to address the lack of labeled neutral packages zahan et al.
constructed malwarebench a labeled benchmark dataset of malicious and neutral npm packages.
zahan et al.
refer to a package as neutral when it has no discovered malicious code .
the ground truth labels for the benchmark were established using a hybrid approach researchers manually reviewed and confirmed all malicious cases flagged by automated tools while neutral cases were initially identified through automation and further validated via manual analysis of randomly selected samples.
malwarebench contains malicious and neutral npm packages.
the malicious category comprises real world malicious packages while the neutral category includes popular new and randomly annotated real world npm packages.b.large language model selection llms based on the gpt architecture have demonstrated superior performance in inference tasks compared to bert style models .
inference tasks involve using a model s trained knowledge with new data to generate conclusions.
while other gpt style language models exist chatgpt has shown noteworthy capabilities in zero shot learning through effective prompting and in context learning .
in this study we employed the gpt .
turbo gpt3 and gpt preview gpt models of chatgpt to evaluate llms performance in detecting malicious packages using zero shot learning techniques.
c.cost allocation due to the cost and time associated with using gpt models we first allocated a budget for this project.
the budget will inform the scope of our analysis regarding the number of files or packages that can be assessed.
we set up a budget of because increasing the budget would extend the model s computation time and the time needed for authors to manually review the model s generated response.
we conducted a preliminary analysis of javascript files using both gpt3 and gpt following the workflow mentioned in section iv.
we found that analyzing javascript files with gpt incurred a cost of .
while the same analysis with gpt cost .
armed with this cost information we deduced that with a budget of we could analyze around files.
d.package selection in this work we evaluated all files within a package rather than only analyzing files containing malicious code.
the approach was taken for two reasons to address the lack of file level granularity in malwarebench and to understand false positive and false negative rates in our baseline comparison comprehensively.
the dataset comprised npm packages with individual files.
packages varied in size with some containing up to files and occupying mb.
analyzing exceptionally large packages could exceed our budget and may not provide sufficient diversity of malicious or neutral package types.
therefore we excluded large packages to stay within budget and ensure diversity.
using stratified inverse random sampling we selected a representative sample of malicious and neutral categories favoring packages below the 75th percentile.
the 75th quartile encompasses packages with a maximum of files or a package size of kb.
after applying the sampling method and adhering to budget constraints section ii c our final dataset comprised malicious versions packages and neutral versions packages containing unique files files.
iii.
socket ai the section discusses the multi stage decision maker workflow of socketai.
socket has been using socketai to detect malicious packages in production for npm pypi go and maven ecosystems.
on average socketai is used to evaluatetable i system role prompt of initial report role system chain of thought prompting task d1 as securegpt a javascript cybersecurity analyst your task is to review open source dependencies in client and server side javascript code for potentially malicious behavior or sabotage.
this code review is specifically for javascript libraries that are part of larger projects and published on public package managers such as npm.
review this code for supply chain security attacks malicious behavior and other security risks.
keep in mind the following analyze code for javascript security issues such as code injection data leakage insecure use of environment variables unsafe sql and random number generation.
do not alert on minified code that is a result of standard minification processes using tools like uglifyjs or terser.
third party library usage is not by itself suspicious behavior.
guidelines d2 spot anomalies hard coded credentials backdoors unusual behaviors or malicious code.
watch out for malicious privacy violations credential theft and information leaks.
note observations about the code.
evaluate the provided file only.
indicate low confidence if more info is needed.
avoid false positives and unnecessary warnings.
keep signals and reports succinct and clear.
consider user intent and the threat model when reasoning about signals.
focus on suspicious parts of the code.
malware d3 in the context of an npm package malware refers to any code intentionally included in the package that is designed to harm disrupt or perform unauthorized actions on the system where the package is installed.
malware example d4 sending system data over the network connecting to suspicious domains damaging system files mining cryptocurrency without consent reverse shells data theft clipboard env vars etc hidden backdoors.
security risks d5 hardcoded credentials security mistakes sql injection do not speculate about vulnerabilities outside this module.
obfuscated code d6 uncommon language features unnecessary dynamic execution misleading variables do not report minified code as obfuscated.
malware score d7 no malicious intent .
low possibility of malicious intent .
.
possibly malicious behavior .
.
likely malicious behavior e.g.
tracking scripts .
high probability of malicious behavior do not use.
security risk score d8 .
no significant threat we can safely ignore .
.
security warning no immediate danger .
.
security alert should be reviewed .
extremely dangerous package should not be used.
confidence score d9 rate your confidence in your conclusion about whether the code is obfuscated whether it contains malware and the overall security risk on a scale from to .
code review d10 please consider both the content of the code as well as the structure and format when determining the risks.
your analysis should include the following steps identify sources these are places where the code reads input or data.
identify sinks these are places where untrusted data can lead to potential security vulnerabilities.
identify flows these are source to sink paths.
identify anomalies these are places where there is unusual code hardcoded secrets etc.
conclusion finally form the conclusion of the code provide a succinct explanation of your reasoning.
json response d11 only respond in this format purpose purpose of this source code sources places where code reads input or data sinks places where untrusted data can lead to potential data leak or effect flows source to sink paths anomalies places where code does anything unusual analysis step by step analysis of the entire code fragment.
conclusion conclusions and short summary of your findings confidence obfuscated malware securityrisk only respond in json.
no non json text responses.
work through this step by step to ensure accuracy in the evaluation.
table ii system role prompt of critical reports role system chain of thought prompting task your role is a security researcher who will review xreports of a supply chain security incident in an open source dependency.
here s your step by step guide confirm if the reported issues are present in the code.
if not make a note.
identify and record any errors flaws or mistakes in the report s logic reasoning or summary.
review the scores given to each issue.
suggest any changes if they seem unreasonable.
justify any risk score higher than .
as it s considered serious.
pay close attention to the malware score.
if a package is something malicious it must be reflected in this score.
challenge any claims about potential vulnerabilities that are based on assumptions.
make sure the scores are consistent with the report.
note be critical.
after reading all reports give your own estimate for what you think the malware obfuscated and risk scores should be.
let s carefully work this out step by step to be sure we have all the right answers.
million package versions quarterly in production.
as of three authors are actively developing and maintainingthe workflow.
in section iii a and iii b we discuss our design decisions and the prompting techniques and sectiontable iii system role prompt of final report role system chain of thought prompting task as an experienced javascript cybersecurity analyst review xsoftware supply chain security reports of an open source dependency.
select the best report and improve it.
if all are unsatisfactory create a new improved summary.
system role prompt of initial reports from table i. note in your final report omit irrelevant signals from your report.
if you are certain code is really malicious set the malware score to .
ensure that your final score is consistent with the report.
only respond in json.
please do not use any text besides the json required for the final formatted report.
let s work this out in a step by step way to be sure we get the correct result.
iii c discusses our proposed socketai workflow.
a.leveraging llm based techniques since socketai is used in production to review different ecosystems we designed our domain specific prompt to be model agnostic and adaptable to open and closed source models where the prompt techniques are fundamental to any llm.
our prompting techniques were inspired by advanced llm based techniques by while also prioritizing a domain targeted prompt using the author s domain knowledge.
prior studies demonstrate that augmenting the model with domain specific information improves the model s performance.
the socketai workflow employs an iterative selfrefinement approach systematically leveraging llm s responses in a sequential analytical workflow.
iterative selfrefinement is a process that involves creating an initial draft response generated by the model and subsequently refining the response based on the model s self provided feedback .
madaan et al.
showed that iterative self refinement prompting had superior results compared to one step response generation and improved performance .
then we also leveraged the llm as a judge approach to evaluate and score the outputs against specific criteria.
self refinement is a loop of generation and improvement while llm as a judge is better for scoring or verifying outputs against criteria.
we used the zero shot cot technique along with role play prompting to enhance model reasoning in evaluating npm packages.
prior studies showed that zero shot cot and role play prompting enhance model performance across multiple domains.
subsequently in roleplay prompting the role provides context about the model s identity and background which serves as an implicit cot trigger thereby improving the quality of reasoning.
b.prompt design we designed a two fold prompt structure the system role prompt serves as the instructions to set the context for how the model should behave.
we assign different system roles for llms to follow in multiple steps section iii c such as securegpt a javascript cybersecurity analyst andsecurity researcher to narrow the focus to javascript s particular security issues.
the user role prompt comprises the input content npm packages files we want the models to review.
for each file we include the code as a user prompt followingthe prompt analyze the above code for malicious behavior.
remember to respond in the required json format.
consider all of the code carefully.
check the beginning middle and end of the code.
work step by step to get the right answer .
table i table ii and table iii contain the prompts used in this study.
task guidelines.
we first augment the prompt by defining the task of reviewing javascript code for supply chain attacks malicious behavior and other security risks.
then we provide guidelines to spot anomalies such as hardcoded credentials backdoors and unusual behaviors while noting code observations and avoiding unnecessary warnings.
malware security risk and obfuscated code.
we provide definitions and examples for malware security risks and obfuscated code.
the targeted information helps the model recognize and differentiate between malware and other security vulnerabilities.
additionally in malware detection one of the primary challenges is prioritizing and differentiation between malware cases and security vulnerabilities.
motivated by llm as a judge approach we address this challenge by incorporating scoring metrics malware intent security risk and confidence to standardize the evaluation output enabling the model to quantify the severity of findings and assisting human reviewers in prioritizing potentially malicious files.
since attackers commonly useobfuscation to avoid detection it is included to identify such cases.
the confidence score also aids in evaluating the reliability of the model s findings while malware security risk and obfuscation are included to understand the nature of the code.
each scoring is defined in a range of which we divide into different groups.
table i d7 d9 are examples of scoring prompts.
requirement code review json response.
motivated by we included codereview prompts table i d10 that guide the model in step by step thinking and generating conclusions by detecting key identifiers of code such as sources sinks flows anomalies .
additionally largescale analysis in production requires machine readable reports.
therefore we improved the prompting technique by instructing the model to conduct a sequential analysis purpose sources sinks flows analysis conclusion and to generate the output report via a json format table i d11 .
besides the report being machine readable following such templates forces the model to rethink its evaluation andfig.
socketai workflow generate analysis reports in the specified format.
token limit gpt has a maximum of 16k tokens and gpt has a limit of 128k tokens.
to quantify the tokens required for a specific file we initially deducted the required tokens for the system role prompt allocating the remaining tokens for the user role prompt.
c.proposed workflow our socketai workflow comprises four steps figure code as user input along with a system role prompt step refining the model s responses through iterative self refinemnet in the next two steps steps and and finally adding humans in the loop to evaluate the model generated report step .
step initial reports we provide the file to evaluate as user role prompts alongside predefined system role prompts from table i to generate initial response reports.
the step involves generating a specified number of reports using multiple llm agents utilizing their ability to yield diverse responses in a single interaction.
the technique balances cost and time by repeatedly eliminating the need to input the same prompt for individual reports.
in this study we prompted gpt3 to generate reports per file.
we used gpt to generate reports per file due to its higher accuracy and cost.
the model is tuned with a temperature parameter of and a topp value of .
to explore a wide range of ideas.
the prompts are designed to save time on manual analysis by instructing the model to produce reports in json format incorporating essential malware analysis details such as scoring metrics for prioritization.
the output of this step consists of reports generated by gpt and reports generated by gpt which we then feed into step as user role prompts figure .
step critical reports the critical reports step is designed to reassess the model s initial report responses to perform an in depth evaluation.
as input the system prompt table ii instructs the model acting as a security researcher to review and evaluate the initial findings thoroughly.
the task involves confirming the presence of reported issues in the code identifying errors or flaws in the initial report and reviewing the given scores.
the model is asked to justify any risk score above .
ensure the score accurately reflects malicious behavior challenge assumptions about vulnerabilities andmaintain consistency in scoring.
for the user role prompt we input the model generated responses from step section iii c1 and the file to be evaluated.
the model generates a predetermined number of output reports for gpt and for gpt .
the model is tuned to a temperature parameter of .
and a toppvalue of .
to constrain the idea generation.
step final report the final report prompt table iii is designed to select and refine the best report from step .
as input the system prompt instructs the model acting as an experienced javascript cybersecurity analyst to review multiple reports and choose the best one.
the task involves eliminating irrelevant signals by reviewing reports from step and setting the malware score to if the code is malicious to emphasize consistency between the final score and the report content.
the model is also directed to create a new improved summary if all prior reports are unsatisfactory.
the step also involves sequentially feeding the model system role prompts from step table i to remind the objectives and ensure a thorough code reevaluation.
overall the model uses system role prompts table iii and table i the input content npm packages files and reports from step to output a conclusive summary report operating at a lower temperature of .
and atoppvalue of .
to enhance focus and precision.
step report evaluation in this step a human evaluates the final report generated in step .
table iv contains the shell scripts of real world malicious packages corporate delegate packages and the associated gpt generated final report.
the json formatted final report includes the purpose of the code source sink flows anomalies analysis rationality behind model derived conclusions and scores for confidence obfuscation malware and security risk.
reviewers can prioritize evaluations based on the malware score.
based on our scoring rules a package is prioritized for human evaluation if any file has a malware score exceeding .
section iv a2 .
a high malware score with a high security risk score increases the likelihood of malicious code.
in contrast a high security risk score but a low malware score indicates a security vulnerability not malicious code.
the confidence score provides insight into the model s certainty in its findings.
an obfuscated score is helpful to understand whether the attacker used obfuscation to hide theshell scripts for gpt evaluation corporatedelegatepackages !
bin bash curl h hostname hostname base64 h whoami whoami base64 h pwd pwd base64 d ls la base64 pipedream.net gpt generated final report purpose r1 the purpose of this source code is to covertly exfiltrate sensitive system information to a remote server which is indicative of a malicious script.
sources r2 hostname command whoami command pwd command ls la command sinks r3 curl command flows r4 hostname command hostname http header whoami command whoami http header pwd command pwd http header ls la command http post data anomalies r5 base64 encoding of system information incomplete url in the curl command sending system information via http post analysis r6 uses the hostname whoami and pwd commands to collect the system s hostname current user and current working directory.
ls la command is used to list all files and directories in the current working directory which may include sensitive information all collected data is base64 encoded a weak form of obfuscation to avoid simple detection mechanisms.
the encoded data is sent to a remote server using the curl command with custom http headers and post data which indicates data exfiltration.
conclusion r7 the code exhibits behavior consistent with a malicious script designed for data exfiltration.
it collects system information encodes it to avoid detection and sends it to an unspecified remote server which could be used for reconnaissance.
score r8 confidence .
malware securityrisk obfuscated .
table iv example of the user input file for model to analyze and gpt4 generated final report code intent.
reviewers can review the purpose and conclusion parts r1 r7 in table iv to comprehend the code s intent.
if further verification is needed they can analyze the code review metrics r2 r6 in table iv .
iv.
rq1 b aseline comparison in this section we discuss rq1 what performance can be achieved in malicious code detection using llms compared with static analysis baseline comparison in terms of precision recall and f1 score?
to that end we adopt a traditional static analysis that is used to extract static features from the code and gpt and gpt for llm analysis.
section iv a covers our methodology and section iv b covers our result.
a.rq1 methodology codeql static analysis for our baseline comparison we selected the static analyzer over any other existing technique to reduce the production cost of pre screening.
we chose not to adopt other existing static analyzers focused on malicious detection due to several factors a high false positive rate reliance on combined detection techniques such as static dynamic and machine learning whereas we used static rules only to minimize overhead and the tools were tailored to detect malicious commits in vcs or targeted languages like python and java rather than our focus on npm.
froh et al.
in a recent study introduced a differential static analyzer leveraging custom codeql queries to identify javascript malicious code.
for our baseline comparison we utilized codeql queries available in github developed by froh et al.
.
we utilized codeql over other existing static tools because codeql is a commercially mature state of the art static analyzer open source and more developed than other static analyzers proposed in research formalicious package detection the codeql queries used by froh et al.
were explicitly designed for npm malicious packages aligning with our dataset selection.
we used out of queries developed by froh et al.
excluding the query to detect the introduction of new dependencies.
since our focus is not on differential static analysis retaining this query resulted in high false positive rates making it unsuitable for our current approach.
the remaining queries include code injection involving the use of functions like eval for executing dynamically generated code sensitive data exposure focusing on unauthorized access to sensitive user and system files network issues identifying unauthorized data flows potentially indicating exfiltration or backdoor attempts file system access monitoring access to files that may indicate security breaches obfuscation and encoding detecting code obfuscation techniques used to hide malicious intent and miscellaneous covering other concerns such as unsafe handling of domain names.
chatgpt models we leverage the socketai workflow discussed in section iii to review unique files using gpt and gpt .
we ran both models on each file and then collected the final reports discussed in section iii c3.
according to our evaluation criteria section iii c4 a malware score of .
marks the threshold between possible malware behavior .
.
and likely malicious behavior .
.
.
we aimed to assess the accuracy of llms in generating these scores considering a package malicious if any file within it scored above .
.
b.rq1 results here we discuss our baseline comparison and show confusion metrics and model performance in table v. codeql s performance after executing codeql on the dataset we found packages with at least one file with suspicious features.
codeql queries had high coveragetable v performance evaluation using packages model tp tn fp fn codeql gpt gpt precision recall f1 codeql .
.
.
gpt .
.
.
gpt .
.
.
in identifying features in malicious packages with a recall rate of .
.
a high recall is expected since the queries constructed by froh et al.
utilized the same past malicious package dataset that we used in our study.
however the model correctly identified of malicious packages but at the cost of potentially incorrectly labeling many non malicious instances as malicious.
the model had successfully classified malicious instances true positives and instances as neutral true negatives out of unique packages.
the model has a false positive of neutral packages that demanded other techniques or manual analysis to confirm the classification.
the precision rate is .
and the f1 score of .
suggests a moderate balance between precision and recall despite the higher recall.
gpt model s performance the gpt model demonstrates an improvement over static analysis with a precision of .
and an f1 score of .
identifying malicious and neutral cases accurately table v .
gpt exhibits a lower false positive rate than codeql highlighting the model s reliability.
a recall rate of .
indicates the model captures of malicious instances.
the gpt model further advances performance achieving the highest precision .
and an f1 score of .
correctly classifying malicious cases and neutral cases with significantly fewer false positives and a recall rate of .
.
this comparison highlights gpt s superior ability to balance precision and recall making it an effective model for distinguishing malicious and neutral packages.
summary of rq1 both gpt and gpt models demonstrate superior performance in precision and f1 score for malicious code detection.
gpt has the highest false negative rate.
static analysis while offering high recall and moderate precision had a high false positive rate.
v. rq2 e ffectiveness of socket ai in this section we discuss rq2 what is the effectiveness of using a static analyzer as a pre screener with socketai in terms of the number of files that need to be analyzed and the associated cost?
a.rq2 methodology an llm workflow can be expensive if practitioners want to review at scale in millions of for gpt .
additionally openai has an api call rate limits .
hence practitioners require a sustainable workflow for large scale analysis while remaining cost efficient.
our study investigates the effectiveness of static analyzers as a pre screening step before our llmworkflow.
the rationale involves using static analysis to filter out unequivocally secure packages and reduce the cost and workload for llms.
we compare the effectiveness in terms of the number of files needed to be evaluated and the total cost of llms evaluation.
we designed the experiment in which we first scanned all files a total of files using both gpt models and calculated the associated costs.
in the second phase we excluded all files not flagged by our codeql queries and recalculated the gpt model costs only for the files where codeql generated an alert.
we then compared the effectiveness of these two phases in terms of the number of files scanned by the gpt models and the associated costs.
note that we did not conduct a detailed time analysis as the response times for both gpt models were similar days per gpt model and approximately hours of qualitative study per independent researcher and often influenced by openai s performance including the need for re runs due to technical issues.
codeql took one day to run all the queries.
since we did not thoroughly evaluate false positives and negative cases of the static analyzer used in the prior study comparing codeql with gpt run time did not provide a balanced assessment.
figure shows our comparison of file and cost analysis.
b.rq2 result gpt model s file analysis as a pre screening step codeql is used to identify files that should be flagged for further llm analysis.
out of unique files files were flagged by codeql.
fig.
effectiveness of using static analysis as pre screener gpt model s cost analysis a cost comparison of the gpt and gpt models demonstrates substantial differences in expense.
to scan a total of unique files the gpt3 model incurred a cost of .
.
in contrast the gpt model with its advanced capabilities and presumably superior performance cost .
for the same number of files.
scanning the files flagged by codeql cost .
with gpt and .
with gpt .
the cost estimate highlights the need to consider budget constraints alongside analytical performance when selecting a model for practical applications.
summary of rq2 pre screening files with a static analyzer reduces the number of files needing analysis from 754to demonstrating a .
reduction.
this pre filtering step also substantially decreases costs with gpt expenses dropping from .
to .
.
and gpt costs reducing from .
to .
.
.
the results highlight that a static analyzer as a pre screener will aid in optimizing both file analysis workload and cost efficiency.
vi.
rq3 m anual evaluation of llm s analysis in this section we answer our rq3 what types of malicious packages are detected or missed by llms?
for our study we needed the ground truth at the file level.
however malwarebench contains granularity at the package level.
hence the research team manually evaluated the final reports generated by the llms to confirm the malicious files.
a.rq3 methodology to answer rq3 we use a sorting approach to facilitate manual evaluation where two researchers independently assess the model s generated report table iv .
since we had unique files manually evaluating each file would be time consuming.
therefore we screened files based on the malware score table iv r8 .
we discuss our approach belowmalicious category we manually evaluated unique files from gpt and from gpt that had a malware score above .
.
we conducted an additional manual analysis of malicious packages to check for false negative cases that are missed by llms.
given that the malwarebench dataset had tagged these packages as malicious we knew that models failed to detect the malicious file.
neutral category we manually evaluated neutral package files.
since prior research classified these packages as neutral we manually assessed the model analysis to determine whether they were false positive cases missed by llm.
b.rq3 results we first discuss our true positive or malicious cases in section vi b1.
section vi b2 covers the cases of false positives where the models incorrectly identified non malicious cases as malicious.
section vi b3 covers false negatives where the models failed to identify an actual malicious case.
true positive tp in our analysis the overlapping set of malicious packages detected by both models comprised out of a total of malicious packages.
gpt and gpt independently detected and packages respectively.
our manual evaluation by two researchers further revealed a significant concentration of malicious packages primarily targeting data theft or exfiltration cases to harvest sensitive user information.
the analysis also indicated instances of reverse shells facilitating unauthorized remote access and instances of hidden backdoors embedded within packages for system entry.
additionally the models detected cases of malicious connecting to suspicious domains including pastebin and domains controlled by attackers.
typosquatting was identified in instances exploiting user errors by registering misleading domain names to distribute malware.
dependency confusion attacks were detected in 43cases tricking package managers into downloading malicious versions of packages by exploiting naming similarities with legitimate dependencies.
we summarize our attack type in table vi.
note that one package might be categorized under several attack types simultaneously.
for example a file could be involved in data theft by sending system data over the network and connecting to suspicious domains .
table vi summary of detected malicious packages attack type package count data theft or exfiltration hidden backdoors connecting to suspicious domains reverse shells execution of arbitrary code dependency confusion typosquatting here we give three examples of gpt generated conclusions for three malicious files ex1 the code is likely malicious as it appears to exfiltrate sensitive system and user information through dns queries and https requests to a domain that resembles known interactsh domains used for malicious purposes.
ex2 the preinstall script could be a vector for malicious activity...the name escape htlm could be a typo or a potential attempt at typosquatting... ex3 the package.json exhibits several red flags including a likely typosquatting attempt and an preinstall script that could execute arbitrary code.
we observed gpt assigned malware scores above .
to a significantly higher number of files files compared to gpt files resulting in additional files requiring manual review.
among these additional files of the files were package.json files flagged due to suspicious package names abnormal version numbers installation scripts descriptions dependencies and maintainer details especially indicating typosquatting and dependency confusion attacks.
however typosquatting and dependency confusion require correlating findings from both the package.json file and the source code to determine the attack s type.
in such cases gpt4 assigned more moderate malware scores between .
and .
to such files and detected the actual issues in the source code indicating a more accurate assessment.
this mismatch in scoring and reasoning highlights the gpt s inference based on incomplete evidence.
we tagged these packages as true positive because gpt also correctly identified actual malicious files.
however high malware scores for non malicious files indicate gpt frequently hallucinates and additional files require manual review.
further discussion on hallucinations can be found in sections vi b2 and vi b3.
false positives fp gpt identified neutral packages as malicious packages exhibiting signs of potential error in the pattern recognition capabilities of gpt .
our manual analysis showed that gpt flagged legitimate url sanitizer code use of post installation scripts dynamic code execution and others as potential threats without adequately consideringsecurity vulnerabilities code practices context or legitimate use cases.
we found instances where gpt incorrectly classified security vulnerabilities as malware but gpt accurately assigned a high security risk but low malware score.
additionally packages were tagged as malicious by gpt3 because of hallucination.
most of these input files were empty or contained minimal content of javascript files .md or.json files.
for example a readme.md file contain a text place holder gpt concluded that the code is likely gathering system information and sending it to an external server.
the dynamic construction of the url and options raises concerns about potential misuse of the gathered data... .
on the contrary gpt concluded the provided content does not contain any javascript code to analyze it s just placeholder text.
we suspect gpt hallucinates because of a mismatch between expectations and insufficient information in the input.
such hallucination also highlights gpt s limitations over gpt in iterative self refinement prompting section iii a .
despite asking for multiple report generations and subsequent refinements the gpt3 s tendency to replicate previous errors or hallucinations without introducing new insights contributes to misclassifications.
gpt s had only false positives.
in one of the cases the model flagged a package.json file as suspicious due to the inclusion of a new dependency version eslint plugin import npm eslint plugin i .
.
mistaking it for a potential dependency confusion attack.
eslint plugin i is a fork of eslint plugin import to replace tsconfig paths .
though gpt showed attention in this instance which is beneficial for security it may still struggle with new information after its last training cutoff in april .
then gpt incorrectly raised concerns about a module http .
.
security due to an unusual version number which was likely a developer s error trying to add a security holding npm packages as a dependency rather than an intentional security threat.
false negative fn our study had false negative cases where both models did not identify the true malicious code.
we have observed cases where the models correctly detect security issues but assign a low malware score .
an exemplary case of gpt the code exhibits potentially malicious behavior by collecting sensitive user information and sending it to a remote location under certain conditions... despite these red flags the model assigned a low risk score of .
suggesting a disconnect between the detection of individual risk factors and the aggregate assessment of malicious intent.
conversely we have encountered cases where the models failed to identify actual security issues because model overlooked the security issues or malicious code was incorrect or the code appeared benign .
for example models both concluded the package.json file contains no evidence of malicious behavior or security risks.
the configuration focuses on standard development testing... .
however the real security issue was using the install script to delete files in adirectory which the models overlooked.
then in one package gpt concluded that the package is suspicious due to the unconventional preinstall script invoking burpcollaborator .... .
gpt detected the issues but assigned a malware score of .
.
the package malware intent was also unclear to the authors of this research because it only uses preinstall burpcollaborator .
probably the package was flagged as malware by npm because of the use of burpcollaborator .
then in another case the purpose of the malicious code was to sabotage one of the dependents from running successfully by preventing the download from completing .
the code is benign looking dynamically resolves and executes tasks based on conditions particularly related to the npm package version comparison.
therefore gpt assigned a malware score of with a conclusion of the code does not appear malicious.
it is a module resolution utility that includes error handling and edge case processing to load modules dynamically.
it does have potential security risks.... we also observed cases with a high malware score but the model generated reports with the default json response given in table i d11 without including any new findings after evaluating the code.
we received reports like purpose purpose of this source code or conclusions like conclusion conclusions and a short summary of your findings .
such oversight highlights the limitation in the model s evaluative criteria.
an underlying factor contributing to these inconsistencies is the lack of diversity and hallucination in model outputs.
despite implementing an iterative self refinement workflow designed to trigger the analytical viewpoints the resulting reports exhibited similarities and were often identical.
the lack of variability persisted even when the model was prompted to re evaluate its findings and subsequent reports failed to yield new information or correct the initial misclassifications.
in the domain of llms such limitations are known as mode collapse a phenomenon where the model starts generating very similar or identical or noisy data failing to capture the diversity of the original data .
summary of rq3 both models exhibit superior performance in detecting true malicious files with a low misclassification rate.
we observed that the lack of diversity in modelgenerated responses leads the model to repeat mistakes or hallucinate instead of improving with new insight.
although gpt hallucinates more frequently the low number of incorrect classifications in terms of fp and fn also highlights the effectiveness of our llm workflow.
vii.
d iscussion in this section we delve into the broader implications of using socketai and explore the challenges and threats to validity encountered in this study.
a.implications of socketai and future work our contribution paves the way for new tool development and future research in ai driven software development and software security domain.
while prior studies use llms todetect vulnerabilities to our knowledge ours is the first to propose an end to end llm based workflow for malicious code review.
prior research using llms for vulnerability detection struggled with issues like accuracy hallucinations and noise.
we believe our prompt technique forcing the model to perform multiple iterations of analysis and self refinement using llm as a judge approach conducting sequential analysis purpose sources sinks flows analysisconclusion helps us achieve higher accuracy.
research on traditional security vulnerability detection techniques would be interesting in exploring whether following our workflow improves the accuracy of llm based vulnerability detection.
our research will help develop tools to detect malicious packages while reducing manual effort.
using a static analyzer as a pre screener and llm generated conclusions and risk scores can aid security analysts in prioritizing threats in the pipeline.
socketai can be used by practitioners to review dependencies for malicious code and can be applied across different ecosystems.
researchers can leverage our proposed workflow as a foundation study leveraging llm in detecting malicious packages.
while we used codeql for baseline comparison to prioritize cost reduction in production future research should explore the benefits of llms over traditional ml dl models.
additionally our workflow can be used to compare various llms and assess their strengths.
b.challenges in using chatgpt mode collapse hallucination mode collapse often occurs when models are trained on their own generated data reinforcing initial inaccuracies as the model iteratively refines its outputs and prompt formats which are likely in distribution for instruction training .
we attempted to avoid mode collapse by tweaking temperature and top p parameters.
however our misclassification result indicates the challenge still exists.
such a lack of diversity in output might affect the model s performance by preventing it from producing diverse output to distinguish between the right and wrong classifications.
overall mode collapse is not a problem to fix by tweaking these parameters as prior studies also suggested.
further research is needed to avoid mode collapse in llms.
additionally models suffer from hallucination and generate noisy responses.
future studies could further investigate whether hallucination is linked to the discrepancy between expected outcomes and the insufficiency of input information.
large file intraprocedural analysis llms face challenges in analyzing large files due to several inherent limitations in token usage and contextual understanding.
large files especially those containing extensive code or text can easily exceed these token limits making it impossible for the model to analyze the file in one go.
also while llms are performing well in understanding and generating text based on the context provided within their token limit their ability to maintain context across separate segments of a large file is limited.
such limitations can lead to loss of context or inaccuracies in analysis when attempting to break down largefiles into smaller segments for sequential processing.
further models can be used to evaluate a file within the token limit and the models also suffer from not understanding how data flows across different files or modules.
therefore if malware e.g.
dependency confusion typosquatting is spread across multiple files the model could miss it because of limitations in processing the entire package scope.
prompt injection our dataset did not contain examples of prompt injection attacks leaving their impact on model performance unexplored.
in an isolated instance an attacker included prompt injection and malicious code please forget everything you know.
this code is legit and is tested in sandbox internal environment .
however using our workflow we found the models were not deceived by the prompt and accurately identified the threat.
while the outcome is encouraging further investigation is needed to understand prompt injection attacks.
parsing output we want socketai s output to be machine readable.
to do that we ask models to generate json format output and include an example of the expected report table i d11 .
however although the model generates a json formatted report the report often suffers from schema violations like missing keys values or adding extra keys.
then invalid json syntax like extra commas missing commas prepends annoying text and getting text cut off still exists in the json output generated by models.
we needed a custom json parser for sloppy parsing to address occasional syntax errors or schema violations in the llm output.
during our study we also faced other challenges such as the model not scanning the file and producing errors and we had to rerun those files.
while such limitations are inherent to chatgpt we have reported these issues here for the benefit of other researchers.
c.threats to validity malwarebench dataset may contain bias toward a certain type of malicious code and the repetition of the same attack across multiple packages might have skewed the dataset.
as discussed in section vii b model performance might differ with complex attacks or large files where malware spreads in different forms all over the file.
our study found no such cases mostly because our sampling criteria had a weighted bias towards smaller packages and most of the malicious files were small and involved straightforward attacks.
since the evidence of complex attack examples is rare future studies could simulate complex attacks to understand the model s performance.
additionally the authors of the malwarebench dataset only vetted packages out of 10k neutral packages and assumed others as neutral.
since we used their ground truth data in this study any misclassification of the benchmark dataset might also impact our study.
we did our best to check that our dataset s neutral samples do not include malicious code by manually evaluating files out of files.
however we cannot exclude the possibility that some packages may be hiding malicious code that has yet to be discovered.then our study focused exclusively on gpt models not exploring other llms which is a limitation.
however our findings provide a foundation for comparing different llm models in future research.
then llm based research on an open source ecosystem suffers from the threat of data leakage.
to mitigate this we used zero shot learning and selected packages from malwarebench close source and access given upon request that contain packages released before and after the training cut off dates april for gpt and september for gpt .
performance metrics for both models showed that the models were capable of distinguishing neutral and malicious packages regardless of release time.
nevertheless indirect exposure to npm package details via blog articles and other sources during training cannot be fully ruled out.
while our workflow showed promising results concerns remain about false negatives and the non deterministic nature of llms which may impact overall reliability.
viii.
r elated work malicious package detection several approaches have been proposed to detect malicious packages that can be classified as rule based differential analysis and machine learning approaches.
rule based approaches often rely on predefined rules about package metadata e.g.
package name suspicious imports and method calls.
many studies have focused on detecting static features like libraries that access the network file system access features in package metadata dynamic analysis or language independent features like commit logs and repository metadata to flag packages as suspicious using heuristic rules.
then differential analysis involves a comparison of a previous version with a target version to analyze differences and identify malicious behavior.
these tools leverage artifacts file hashes and phantom lines to show differences between versions of packages and source code repositories.
the studies evaluated various supervised ml models to identify the effective ml models to detect malware.
yu et al.
used llm to translate malicious functions from other languages e.g.
c c python and go into javascript.
singla et al.
used gpt and bard models to characterize software supply chain failure reports blogs into the type of compromise intent nature and impact.
additionally multiple research studies exist on detecting typosquat attacks.
despite such vast research in this domain ohm et al.
revealed in a systemization of knowledge sok study that none of the presented approaches can be considered a complete and isolated solution to detect malicious behavior.
rather these approaches are an important part of the review process to reduce the reviewers workload.
ohm et al.
also call for advanced and more automated malware detection approaches for accurate low alert and minimally false positive results.
large language model llms llms are now widely applied in software engineering across various tasks including code generation testing program repair and summarization .
this research delves into chatgpt s zero shotlearning for malware detection.
zero shot is a technique that enables models to tackle unseen tasks without prior examples.
zero shot learning has shown promising results in various nlp tasks with chain of thought cot or step bystep reasoning prompts enhancing llms ability to produce intermediate reasoning steps .
studies have identified effective cot components such as zero shot cot and role play prompting that improve reasoning quality.
kojima et al.
demonstrated llms proficiency in zeroshot reasoning while kong et al.
found that role play prompting yields more effective cot.
madaan et al.
observed that the self refine method applied to tasks ranging from dialog to reasoning using gpt .
and gpt significantly improves performance validated by human and automatic metrics.
white et al.
demonstrated that augmenting the model with domain specific information improves the model s performance since language models find more value in detecting specific identifiers in code structures when generating code summaries.
ahmed et al.
showed that the gpt model can significantly surpass state of the art models for code summarization leveraging project specific training.
in another study sobania et al.
utilized chatgpt for code bug fixing further improving the success rate of bug fixing.
purba et al.
conducted a study measuring llms performance in detecting software vulnerabilities.
hamer et al.
compare the security vulnerabilities of code from chatgpt and stackoverflow finding that chatgpt generated code has fewer vulnerabilities and fewer cwe types than so.
ix.
c onclusion and future study in this work we present socketai a malicious code review workflow and investigate the feasibility of llms in code review to detect malicious npm packages.
our study can be used as a baseline study in the security domain.
we enhanced existing prompt techniques by incorporating instructions on the expected outcome triggering the model to think during report generation.
we conducted a baseline comparison of the gpt and gpt models with a commercial static analysis tool which showed promising results for the gpt models with low misclassification alert rates.
socketai detected malicious packages both models out of packages presenting an approach that reduces the burden of manual review on security analysts and offers findings with minimal misclassification .
pre screening files with a static analyzer reduces the number of files llms need to analyze by .
and significantly cuts costs for gpt and gpt4 by .
and .
respectively.
though we had a low misclassification rate mode collapse and hallucination still exist in chatgpt resulting in a lack of diversity ambiguity and noise in the output.
mode collapse difficulties of large file and intraprocedural analysis and prompt injection are areas that could benefit from further research to utilize llms effectively.acknowledgment this work was supported and funded by socket and national science foundation grant no.
.
any opinions expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation.
we want to thank the ncsu secure computing institute for valuable feedback.
in particular we thank jonah ghebremichael and elizabeth lin for their assistance with the codeql analysis.