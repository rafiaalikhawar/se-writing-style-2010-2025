predicting developers negative feelings about code review carolyn d. egelman emerson murphy hill elizabeth kammer margaret morrow hodges collin green ciera jaspan james lin google cegelman emersonm eakammer hodgesm colling ciera jameslin google.com abstract during code review developers critically examine each others code to improve its quality share knowledge and ensure conformance to coding standards.
in the process developers may have negative interpersonal interactions with their peers which can lead to frustration and stress these negative interactions may ultimately result in developers abandoning projects.
in this mixed methods study at one company we surveyed developers to characterize the negative experiences and cross referenced the results with objective data from code review logs to predict these experiences.
our results suggest that such negative experiences which we call pushback are relatively rare in practice but have negative repercussions when they occur.
our metrics can predict feelings of pushback with high recall but low precision making them potentially appropriate for highlighting interactions that may bene t from a self intervention.
keywords code review interpersonal con ict acm reference format carolyn d. egelman emerson murphy hill elizabeth kammer margaret morrow hodges collin green ciera jaspan james lin.
.
predicting developers negative feelings about code review.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
.
introduction it is well established that modern code review provides many bene ts for a software organization including nding defects knowledge sharing and improving software maintenance .
however code review can also lead to interpersonal con ict in the workplace prior research in the social sciences describe interpersonal con icts as a consequential stressor in the workplace .
anecdotally professional developers have reported their experiences with stressful toxic and insu erable reviewers of their code.
a code review in of linux kernel code illustrates con ict vividly christ people.
this is just sh t. the con ict i get is due to stupid new gcc header le crap.
but what makes icse may seoul republic of korea copyright held by the owner author s .
acm isbn .
upset is that the crap is for completely bogus reasons.
.
.
anybody who thinks that the above is a legible b e cient even with the magical compiler support c particularly safe is just incompetent and out to lunch.
the above code is sh t and it generates shit code.
it looks bad and there s no reason for it.
just like in the physical workplace such interactions can have signi cant consequences.
for example the linux kernel lost at least one developer due to its apparent toxic culture i m not a linux kernel developer any more.
.
.
given the choice i would never send another patch bug report or suggestion to a linux kernel mailing list again.
.
.
i would prefer the communication style within the linux kernel community to be more respectful.
i would prefer that maintainers nd healthier ways to communicate when they are frustrated.
i would prefer that the linux kernel have more maintainers so that they wouldn t have to be terse or blunt.
unfortunately we have little systematic understanding of what makes a code review go bad.
this is important for three reasons.first from an ethical perspective we should seek to make thesoftware engineering process fair and inclusive.
second from acompetitiveness perspective organizations must retain software engineering talent.
third happier developers report increased feelings of well being and productivity .
this paper seeks to understand and measure negative experiences in code review.
this paper contributes the rst study to the authors knowledge that quanti es feelings of pushback in the code review process.
measurement enables understanding of the prevalence of the bad experiences whether negative experiences are occurring at di erent rates in subpopulations of developers and whether initiatives aimed at reducing negative experiences like codes of conduct are working.
in a rst step towards enabling new initiatives we use interviews surveys and log data from asingle company google to study pushback which we de ne as the perception of unnecessary interpersonal con ict in code review while a reviewer is blocking a change request.
we articulate ve main feelings associated with pushback and we create and validatelogs based metrics that predict those ve feelings of pushback.
this paper focuses on practical measurement of feelings of pushback rather than developing theories around the root causes of pushback happening or why developers feel pushback in di erent ways.
we asked the following research questions rq1 how frequent are negative experiences with code review?rq2 what factors are associated with pushback occuring?rq3 what metrics detect author perceived pushback?
ieee acm 42nd international conference on software engineering icse this work is licensed under a creative commons attribution international .
license.
icse may seoul republic of korea c.d.
egelman e. murphy hill e. kammer m.m.
hodges c. green c. jaspan j. lin method to study negative behaviors in code reviews we combine qualitative and quantitative methods using surveys and log data from google a large international software development company.
we developed three log based metrics informed by interviews with a diverse group of developers to detect feelings of pushback in code reviews and validated those metrics through a survey with a strati ed sample of developers that covered ve feelings of pushback developers completed the survey.
in the survey we collected qualitative feedback and asked respondents to volunteer code reviews that matched a list of problematic behaviors.
.
code review at google at google code review is mandatory.
the artifact being evaluated in a code review we will call a change request or cr for short.
when a change request is ready its author seeks acceptance through code review meaning that the reviewer certi es that they have reviewedthe change and it looks okay to check in.
a given code review may be performed by one or more developers.
authors choose their reviewers or use a tool recommended reviewer.
once each reviewer s concerns if any have been addressed the reviewer accepts the change and the author merges the code into the codebase.
unlike the open source community almost all code that is reviewed is eventually merged which makes metrics like acceptance rate which are common in studies on open source code reviews inappropriate in the google context.
additionally any code that is checked into the company s code repository must comply with internal style guides.
to ensure compliance code must either be written or reviewed by a developer with readability in the programming language used which is a certi cation that a google developer earns by having their code evaluated by already certi ed developers.
more information about google s code review process can be found in prior work .
.
interviews about code review pushback the goals of the interview were to re ne our initial de nition of pushback and to understand how developers deal with pushback.
one primary interviewer conducted all semi structured hour interviews with notetakers present for most sessions.
giventhe sensitivity of the interview topics we conducted interviews with informed consent and according to a strict ethical protocol.
participants were recruited in two samples by randomly inviting developers from the company s human resources database.
although google has developers in multiple countries all interviewees were based in us o ces.
the rst sample was developers who replied to the recruitment screener consisting predominantly of white male and senior developers.
as this uniform sample might not include experiences more common to less tenured developers or those from underrepresented backgrounds we rewrote our recruitment materials to be more inclusive and adjusted our sampling script.
consequently the second sample included more 1we note speci cally that we provided participants the option to participate with or without audio recording and we paused recording when starting to discuss topics the interviewer or participant identi ed as potentially sensitive.
we also provided materials on organizational resources for dealing with workplace concerns and described the responsibilities the researchers and notetakers had related to reporting policy violations to reduce any ambiguity about what to expect during or after the session.junior developers included racial ethnic diversity and consisted entirely of women.
developers experience with code review is mostly positive.
all interviewees viewed the code review process as helpful and newer developers particularly appreciated its inherent mentorship.
nonetheless the interviews pointed out the challenges developers face during code review.
de ning pushback.
to inform our de nition of pushback we asked developers what pushback meant to them.
while their de nitions largely aligned with our a priori notion that acceptance was being withheld by a reviewer it included the idea that pushback also contained interpersonal con ict.
interpersonal con ict was also a recurring theme in the instances of pushback that interviewees shared during the interviews.
thus our de nition of pushback is the perception of unnecessary interpersonal con ict in code review while a reviewer is blocking a change request.
dealing with pushback.
interviewees talked about their reactions working towards resolution and the consequences of the pushback they received initial reaction.
aggressive comments can provoke strong emotional reactions followed by a desire to seek social support from trusted colleagues.
participants talked about shock frustration second guessing themselves and a sense of guilt or shame that came from reading aggressive comments.
from a behavioral standpoint participants discussed asking teammates opinions asking a manager or team lead for advice reviewing the code change and related materials and reviewing code review guidelines or documentation.
investing time to work towards a resolution.
participants brought up adding comments asking for clari cation talking directly with reviewers within the code review tool over chat video conference or o ine making changes to code and adding docs or other context in the cr feedback as the primary ways they worked toward resolution.
appeal to authority or experts.
some participants elicited support from someone with authority or expertise to mediate or to steer the review back on track.
techniques used here included adding a manager or team lead to the review havingo ine meetings as a part of resolution or reaching out to an internal company email alias designed to help code reviews either privately or by directly adding the email alias as a participant in the code review .
long term consequences.
participants also shared longer term e ects that came about because of code reviews with pushback including not sending code changes to the same reviewer or codebase again adding more context in future code submissions building o ine relationships before submittingcode to a new reviewer or codebase soliciting more feedback from teammates before sending code changes outside of the team and avoiding working on projects that overlap with the same reviewer codebase.
.
design of metrics from the interviews we synthesized three potential metrics that we could calculate with existing tool logs to identify code reviews that may be likely to contain pushback.
to develop the metrics we 175predicting developers negative feelings about code review icse may seoul republic of korea focused on the immediate consequences mentioned in the interviews of pushback investing time to work towards a resolution and appealing to authority or experts.
we calculate each of the metrics for each developer and for each code review based on tool logs.
we collect user logs from a variety of developer tools and are able to associate logs to a particular code change based on task identifying artifacts such as common code review identi ers or version control workspace.
we include time using the code review and code search tools editing code and browsing documentation.
we cannot account for non logged time such as in person conversations.
metric rounds of a review is a measure of working towards a resolution as mentioned by interviewees as it captures the ex tent to which there was back and forth between the author and reviewer.
we calculate this metric as the number of times an author or reviewer sent a batch of comments for the selected cr.
metric active reviewing time is the time invested by the reviewer in providing feedback as mentioned by interviewees.
another bene t to reviewing time as a metric is that it captures time spent for all reviewers so it would capture time invested by a reviewer who was an escalation point.
it also seems prudent to include time spent by reviewers to cover the case of a reviewer spending a lot of time developing and writing the feedback that is viewed as pushback.
we calculate this metric as the total reviewer time spent actively viewing commenting or working on the selected cr.
this may include time outside of the main code review tool used such as looking up apis or documentation.
notably active review time is not wall clock time of start to nish reviews but based on time speci cally spent working on the code review and related actions.
metric active shepherding time covers most of the activities that interviewees mentioned as investing time to work towardsa resolution.
we calculate this metric by measuring time the authorspent actively viewing responding to reviewer comments or working on the selected cr between requesting the code review andactually merging the change into the code base.
similar to active reviewing time this may include time outside of the primary code review tool including time editing les to meet reviewers requests but does not account for in person conversations.
each long cuto point was at the 90thpercentile2for that metric across all developers in the company.
we used these criteria to ag reviews with potential pushback minutes for active reviewing time which we ll call review time henceforth for brevity minutes for shepherding time shepherd time and batches of comments for number of rounds of review rounds .
we acknowledge that there are situations where agged long reviews are useful and desirable e.g.
during mentoring and part of the validation of these metrics is understanding how often the metrics predict negative interaction compared to other interactions.
2for reference at the 10thpercentile seconds active shepherding time seconds active reviewing time rounds of review and at the median minutes active shepherding time minutes active reviewing time rounds of review.
.
survey design to validate the metrics and to understand developers experience with pushback more broadly across google we designed a survey.
we picked a pair of code reviews for each developer invited to take the survey.
each participant was asked to rate those code reviews supply qualitative feedback and asked to volunteer code reviews that matched a list of problematic behaviors.
.
.
survey overview.
below we include a summary of the types of questions we asked.
we include question wording in the relevant places in the results with the full survey in the supplementary material.
the survey had several parts overall code review perceptions.
we asked three overarching questions about perceptions of the code review process covering improvements to code quality satisfaction and frequency of bad experiences with the code review process.
rating of two selected change requests.
we asked developers similar questions about two crs one in which they were involved as either an author or reviewer and one in which they were not involved.
for the selected crs we asked respondents to rate the cr on each of the ve feelings of pushback section .
.
and asked developers to provide open ended comments to give context.
asking for problematic code reviews.
finally we asked our survey takers to volunteer crs that matched any behaviors from a list of potentially problematic behaviors shown in figure drawn from interviewee experiences and literature on workplace bullying .
we also asked for open ended comments giving participants the option to paste excerpts of the cr they believed were problematic into a text box and to share context on why they believed the behavior occurred.
.
.
feelings of pushback.
to validate our metrics we began by de ning several aspects of the ground truth of pushback throughfeelings expressed in the interviews section .
.
the feelings were interpersonal con ict from the de nition of pushback feeling that acceptance was withheld for too long one aspect of blocking behavior from the de nition of pushback reviewer asked for excessive changes another aspect of blocking behavior feeling negatively about future code reviews from the long term consequences of pushback and frustration a direct aspect from the initial reaction to aggressive comments and a consistent theme throughout interviews e.g.
people were frustrated about reviews taking a very long time or not having context on why a reviewer required certain changes .
these aspects allow us to ask developers about speci c feelings of pushback and evaluate to what extent our metrics predict them.
these ve feelings are not mutually exclusive of each other and we do not think that they completely cover the range of potential feelings of pushback in the code review process.
for example we didn t use feelings of di culty pain personal attacks commonness or fairness which were all considered in the development of the interview script and survey.
however the feelings chosen were 176icse may seoul republic of korea c.d.
egelman e. murphy hill e. kammer m.m.
hodges c. green c. jaspan j. lin cr criteria for inclusion surveyed crsflag review timeflag shepherd timeflag rounds ofreview flags flag x x x flags xx xx xx flags xxx table dist.
of surveyed crs by metric ag combinations.
prominent themes that came up in our interviews that most closely aligned with our de nition of pushback.
one challenge in developing the questions about these feelings for the survey was the appropriate scale to use.
we chose to develop item speci c response scales rather than use likert scale to reduce cognitive complexity and elicit higher quality responses .
for our analysis we binned the quantitative survey responses for two reasons rst for some questions we only cared about one polarity e.g.
acceptance was given too early is not pushback by de nition and neither are cases where less change was requested than necessary and second binning allowed us to treat di erent response scales uniformly.
.
.
selecting the code reviews for the survey.
to evaluate our metrics we compare the crs agged by one or more of our metrics to developers feelings of pushback.
individual crs can fall into any one of eight categories with respect to the metrics or metric combinations as detailed in table .
we developed a strati ed sampling plan to ensure that we selected crs from all eight categories.
an insight from our interviews was the importance of bringing in a third party to help resolve or escalate any con ict.
because of this we shaped the survey design to ask developers not only about a change request that they were involved in but also to evaluate a change request they were not involved in to better understand how apparent feelings of pushback would be in cases where a changerequest was escalated.
we asked the author one reviewer andtwo third party developers to rate the feelings of pushback they perceived in the code review.
see the supplementary material for details about our survey strati cation process.
.
analysis quantitative data.
in addition to the survey ratings on the ve feelings of pushback we incorporate data about crs authors and reviewers which may impact the metrics we are measuring in cluding cr size if the cr needed readability or was part of the readability certi cation process if the author was a new employee author seniority the number of reviewers and the data from ourthree ags high review time high shepherd time and long rounds .
for analysis purposes we treat the ratings of the pushback feelings as binary and evaluate incidence rates of these undesirable behaviors.
we use a logit regression model to predict the likelihood of developers reporting each feeling of pushback based on our metrics controlling for cr attributes.
the logit model is ideally suited to predict binary outcomes.
we report the transformed odds ratio by exponentiating the log odds regression coe cient for all results in this report to aid in interpretation.
qualitative data.
to analyze the qualitative data from open ended survey responses we used inductive coding to surface themes.one researcher coded the responses from the ratings of crs selected for the survey and another researcher coded the responses from the volunteered crs each developing their own codebook.
member checking.
we emailed a copy of our internal research report to employees and the report was accessible to the entire company of google employees viewed the report.
four developers reached out proactively adding their own anecdotal support for the ndings.
no one refuted the ndings.
months after the ndings we also distilled recommendations into a one page article widely circulated at google and subsequently published the article on the external google testing blog3.
the blog article was one of the most viewed google testing blog posts in or and generated lively discussion on both reddit4and hackernews5.
results we next describe the results of our survey where all questions were optional.
of survey invitees completed the rst section authors and reviewers completed the second section about their own crs and completed the section on third party cr evaluation.
we found no statistically signi cant response bias from any of the participant or cr attributes considered as part of this analysis.
of survey respondents worked in a us o ce in europe the middle east or africa in the asia paci c region and in the americas but not the us.
.
how frequent are negative experiences with code review?
overall developers are quite happy with the code review process at google with clopper pearson con dence interval of developers reporting being at least moderately satis ed and saying code review improves the quality of their code at least a moderate amount.
still report having negative experiences with the code review process at least once a quarter during their employment at google and have had negative experiences at least once a month.
note that the phrasing of the question on the survey used bad experience without any speci c de nition consequently some of these experiences likely t within our de nition of pushback but most are probably notdirectly interpersonal con ict.
as evidence looking ahead at the types and frequency of developer reported negative experiences in figure excessive review delays was the most common.
review comments respectfully 177predicting developers negative feelings about code review icse may seoul republic of korea on the survey we asked about each of the ve di erent feelings of pushback for each change request we assigned to respondents.
for analysis purposes we treat the variables as binary feeling pushback in that way or not and look at incidence rates of these undesirable behaviors.
for example we asked authors setting aside inherent di culties in coding how frustrating was it for you to get through review?
and gave the ve options of none a little a moderate amount a lot and don t know we categorize any response options of a moderate amount and a lot as frustrating and the other response options as not frustrating.
exact survey question wording response options and their categorization are in supplementary material.
as part of this rst research question we look at incidence rates of each of the ve feelings of pushback.
given that we intentionally and disproportionately sampled change requests where we expected a higher likelihood of these feelings as part of our strati ed sample we weight our results by the incidence of each agged metric combination.
speci cally in the surveyed change requests of authors reported frustration with a review of authors reported interpersonal con ict reported reviewers withholding acceptance longer than necessary reported that reviewers requested more change than was necessary and reported they did not feel positively about submitting similar changes in the future.
aggregating across the feelings of pushback we nd of un agged crs include at least one of these feelings of pushback while include two or more feelings of pushback behavior.
.
what factors are associated with pushback occuring?
to investigate in what situations pushback is more likely to occur we leverage both quantitative and qualitative data.
first we look at incidence rates for each of our feelings of pushback for keycr attributes such as readability cr size new employee status author seniority and number of reviewers.
second we look athow developers answered the question why do you think the happened?
.
.
code review a ributes.
there are several important cr attributes we investigated which may impact the metrics we aremeasuring readability both needing readability approval and if the review was a part of the readability certi cation process cr size if the author was a new employee the seniority of the author and the number of reviewers.
we ran a logit regression with all of these attributes on each of the pushback feelings to test which were predictive of the feelings of pushback.
in the following paragraphs we describe which attributes are predictive.
full regression results are available in the supplementary material.
readability.
our internal research at google on readability suggests that reviews tend to take longer while a developer goes through the readability process compared to before and after getting readability.
that research also suggests that achieving readability is especially frustrating based on surveys of google developers.surprisingly neither code reviews that were undergoing the readability process nor code reviews that required a readability review were predictive of any of our feelings of pushback.
most open ended comments related to readability in this dataset lamented the delays that time zone di erence with readability reviewers introduced as the following participant noted along with a suggested remedy the main frustration relating to this code review was the round trip required to get the review through readability review.
ideally there would be more reviewers in the same time zone.
code review size.
code review size was the one attribute that was predictive of most of the feelings of pushback but di erent sized crs were predictive of di erent pushback feelings.
google categorizes crs into speci c sizes6 these sizes are indicated as part of the code review tool and in the noti cation to the reviewer of the code change so we used these pre set categorizations asthe levels for cr size rather than a raw number of lines of code changed.
compared to very small code changes less than lines changes that were between lines long were .
times more likely for authors to feel more change was requested than necessary and changes that were between lines were .
times more likely to have authors report they felt negatively about submitting a similar change in the future.
for the aggregated pushback feelings of any pushback we see any change between lines ofcode is between .
and .
times more likely have some feeling of pushback in the code review.
this lends support to the general advice to split change requests for easier and quicker reviews when possible but it also points to the importance of using multiple metrics to predict pushback.
new employee status author seniority.
being a new employee is not a statistically signi cant predictor of any of our feelings of pushback.
compared to authors at level entry level authors at level are less likely to see con ict in their code review changes.
likewise employees at level are less likely to feel negatively about submitting similar changes in the future.
number of reviewers.
the number of reviewers is not predictive of any of our pushback feelings.
.
.
selected code reviews emergent themes on why pushback occurs.
in addition to quantitative ratings we asked respondents why they thought that behavior occurred during this change request and a general open ended question about anything else important that we should know about this change request.
respondents described diverse issues in agged change requests consistent with our interviews time zones delays of comments on agged crs mentorship and tenure con ict and team or seniorityrelated politics .
with the exception of one mention of delays none of these issues were described in any of the un agged change requests which bodes well for our metrics considering they di rectly measure only engineering time and roundtrips.
although the smaller sample size may conceal some lower incidence issues un agged agged the breadth of issues represented among the agged reviews suggests our metrics successfully capture several important aspects of code review issues.
6the categories of cr sizes are xs lines changed s lines changed m lines changed l lines changed and xl over lines changed .
for the month period for crs eligible for this study the distribution of all crs was xs s m l xl and for crs selected for this study xs s m l xl.
178icse may seoul republic of korea c.d.
egelman e. murphy hill e. kammer m.m.
hodges c. green c. jaspan j. lin un agged flagged total review review topic comments comments comments time zone delays o ine conversation readability code review was complex or large con ict code quality formatting documentation comment clarity seniority tenure politics table frequencies of codes within open ended comments for un agged and agged change requests.
this meta comment by a participant describes why open ended comments are a valuable addition to the quantitative ratings they expose the perspectives of people who have context to interpret more than a neutral observer might pick up code review is the clearest place where interpersonal con icts play out in the harsh light of day at google.
as a people manager i have had team members point me at code review comment threads as evidence of undercurrents of interpersonal con ict and indeed they are very telling.
when presented with crs they had authored or reviewed respondents pointed out issues including interpersonal tooling organizational and historical attributes of the review interaction or the code itself.
table shows the frequencies of themes that emerged from coding of participant comments about their own crs.
next we describe how authors and reviewers view each party s responsibilities intentions and actions during contentious reviews to elucidate some of the factors respondents considered when indicating whether or not a agged cr contains feelings of pushback.
authors infer negative and positive intents of their reviewers.
authors made varied assumptions about their reviewers intentions varying with how helpful or transgressive they viewed the reviewer s actions.
on one hand many agged crs were speci cally called out as positive constructive interactions by the authors i was appreciated by reviewer.
he suggested a lot of better solution approach how i should follow the coding style.
i enjoyed to learn from him .
on the other hand when reviews were frustrating authors speculated about situational factors that might have a ected the review rather than assuming negative intentions particularly when the frustration was due to silence rather than perceived con ict.authors didn t interpret positive intent in situations where they believed factors beyond resource needs and the quality of the current cr a ected reviewers behavior such as career advancement previous con ict or personality clashes probably wants to put this particular migration in his packet...i think is actually the right thing by trying to break this api but i think could have been a bit less obtuse about it.
.
.
the relevant disagrees with decisions we made many months ago and is attempting to relitigate the same issue.
i tried to nd the best intention which is that the reviewer has a strong preference on how the code should look like especially this is the code base that the reviewerusually worked on.
however we did have some strong arguments for another project a few weeks before.i do hope that it s not a factor but i don t know.
emphasis ours.
the nal quote above shows a tension shared by other developers not knowing the degree to which the other party brings past con icts into the current code review can increase tensions moving forward.
displaying author names in the code review tool may facilitate faster code review where reviewers and authors share high trust but other design choices such as anonymous author code review might alleviate tensions that spiral due to an accumulation of frustrating interactions.
we also note that as we found in the interviews power imbalances between code owners and authors were perceived as an important feature of di cult interactions to authors.
this participant explained their perspective on this issue bluntly code owners use their privilege to force making unnecessary change.
authors self e acing comments about their limitations.
right or wrong some authors blamed themselves for reviewers irritations my frustration was how weak was my knowledge of c and internal coding style.
they did not think my skills were su cient and to some extent they are certainly correct .
comments like these may re ect authors polite reluctance to placeblame on reviewers for frustrating interactions this may be a good strategy to maintain relationships with reviewers after con icts.
.
.
volunteered code reviews problematic behaviors and emergent themes on why pushback occurs.
since our initial interviews suggest that pushback at google is rare we were concerned that of our surveyed agged crs may not include a su cient number of developer identi ed instances of pushback.
to ensure that we would be able to collect a set of crs that did exhibit pushback we showed developers a list of potentially problematic behaviors and gave them the option to volunteer a change request that matched any behaviors from the list.
again they were asked why that behavior occurred during this change request and a general open ended question about anything else important that we should know about this change request.
in contrast to the agged crs 179predicting developers negative feelings about code review icse may seoul republic of korea that respondents were surveyed about which contained a mix of crs that respondents believed did and did not exhibit feelings of pushback all of the crs volunteered by respondents explicitly involved feelings of pushback.
analysis of the open comments about volunteered crs consequently focused on identifying causes consequences and associated factors of feelings of pushback rather than analyzing whether or not feelings of pushback were present as was done with the agged crs.
the most common behaviors from the list which participants chose to volunteer examples for were excessive review delays reviews excessive nitpicking reviews long wait for review to start reviews and confrontational comments reviews .
respondents could indicate if a cr had more than one of these behaviors.
the full summary of the indicated behaviors is in figure .
the relative frequency of the behaviors within this sample of volunteered reviews cannot be considered representative of how often these behaviors occur overall within code review at google because the sample may have been biased by what respondents felt comfortable volunteering i.e.
respondents may have felt safer sharing a cr with a delay versus a more sensitive cr with an example of humiliation .
however the high incidence of certain behaviors within this sample does validate their existence as antipatterns worth addressing for example excessive nitpicking and confrontational comments.
additionally for each of the behaviors we analyzed code review excerpts and their accompanying open ended comments to pull out common themes and formulate a working de nition for each category.
the goal of this analysis was to better understand what was distinctive about each behavior as well as the context surrounding each type of occurrence.
of the emergent themes that surfaced during qualitative analysis of the volunteered crs some of the most commonly mentionedassociated factors were delays