bdefects4nn a backdoor defect database for controlled localization studies in neural networks yisong xiao1 aishan liu1b xinwei zhang1 tianyuan zhang1 tianlin li3 siyuan liang4 xianglong liu1 yang liu3 dacheng tao3 1sklccse beihang university beijing china2shen yuan honors college beihang university beijing china 3nanyang technological university singapore 4national university of singapore singapore5zhongguancun laboratory beijing china abstract pre trained large deep learning models are now serving as the dominant component for downstream middleware users and have revolutionized the learning paradigm replacing the traditional approach of training from scratch locally.
to reduce development costs developers often integrate third party pre trained deep neural networks dnns into their intelligent software systems.
however utilizing untrusted dnns presents significant security risks as these models may contain intentional backdoor defects resulting from the black box training process.
these backdoor defects can be activated by hidden triggers allowing attackers to maliciously control the model and compromise the overall reliability of the intelligent software.
to ensure the safe adoption of dnns in critical software systems it is crucial to establish a backdoor defect database for localization studies.
this paper addresses this research gap by introducing bdefects4nn the first backdoor defect database which provides labeled backdoor defected dnns at the neuron granularity and enables controlled localization studies of defect root causes.
inbdefects4nn we define three defect injection rules and employ four representative backdoor attacks across four popular network architectures and three widely adopted datasets yielding a comprehensive database of backdoor defected dnns with four defect quantities and varying infected neurons.
based on bdefects4nn we conduct extensive experiments on evaluating six fault localization criteria and two defect repair techniques which show limited effectiveness for backdoor defects.
additionally we investigate backdoor defected models in practical scenarios specifically in lane detection for autonomous driving and large language models llms revealing potential threats and highlighting current limitations in precise defect localization.
this paper aims to raise awareness of the threats brought by backdoor defects in our community and inspire future advancements in fault localization methods.
index terms backdoor defects fault localization deep learning i. i ntroduction deep learning dl has demonstrated remarkable performance across a wide range of applications and is integrated into diverse software systems such as autonomous driving and healthcare .
a consensus is emerging among developers to employ dl models pre trained on large scale datasets for their downstream applications .
by finetuning publicly available pre trained model weights on their specific datasets developers with limited resources or training data can effortlessly craft high quality models for a multitude of tasks.
as a result the pre training and fine tuning paradigm has gained strong popularity .however these third party released dnns are often pretrained on large scale noisy and uncurated internet data that are unknown to downstream users.
utilizing these untrusted dnns presents significant safety risks as these models may contain intentional backdoor defects resulting from the blackbox training process we refer to models with backdoor defects as backdoor defected models also infected models for convenience .
these malicious defects in dnn models are caused by backdoor attacks where an attacker adversarially injects backdoored neurons into the victim models by poison training or sub network replacing thereby being able to manipulate the model behavior with a specific trigger.
for example a third party released dnn that is injected with backdoor defects will incorrectly identify lanes triggered by two common traffic cones when deployed into autonomous driving systems thereby compromising the overall reliability of the intelligent systems and endangering human lives.
to ensure the safe adoption of dnns in critical software systems it is crucial to establish a comprehensive backdoor defect database for localization studies.
however current defect localization databases for dnns primarily focus on common defects i.e.
unintentional functional bugs introduced by dnn developers such as incorrect tensor shapes while overlooking the stealthy and harmful defects posed by backdoor attacks i.e.
backdoor defects .
this sparsity of research presents severe safety risks to dl systems.
to bridge the gap this paper takes the first step in constructing a backdoor defect database for localization studies in dnns.
we propose the first comprehensive backdoor defect database bdefects4nn which provides neuron level backdoor infected dnns with ground truth defect labeling to support defect localization studies at the neuron granularity serving as an essential test suite for our community.
specifically we propose the defect design protocols to select neurons for defect injection in terms of neuron contribution neuron quantity and sub network correlation.
based on the proposed defects design protocols we employ four representative backdoor attack methods to inject backdoor defects into four popular network architectures across three widely adopted datasets on the image classification task.
overall our bdefects4nn contains backdoor defected dnns with ground truth defects labeling categorized into directories each featuring injected subnetworks at four quantity levels offering varying defectarxiv .00746v1 dec 2024quantities and infected neurons to enable comprehensive evaluation of localization and repair methods.
using our bdefects4nn we conduct extensive experiments to evaluate the performance of six fault localization criteria incorporating four backdoor specific criteria from the backdoor defense domain and two general criteria from the software engineering field.
notably we identify that current fault localization methods show limited performance on backdoor defects in dnns with low localization effectiveness .
wji on average .
in addition we further evaluate two defect repair techniques namely neuron pruning and neuron fine tuning which exhibit an average asrd of .
and .
respectively.
furthermore we extend our investigations to practical scenarios such as lane detection laneatt for autonomous driving and llms chatglm where we illustrate the potential threats posed by backdoor defects and highlight the current limitations of existing methods in precisely localizing these defects in real world applications.
we hope this paper will raise awareness of backdoor defect threats within our community and facilitate further research on fault localization methods.
our main contributions are as far as we know we pioneer the integration of backdoor defects into the fault localization task and conduct the first comprehensive study on backdoor defect localization in dnns.
we build bdefects4nn a comprehensive database containing backdoor defected dnns with neuron level ground truth labeling supporting controlled defect localization studies.
we conduct extensive evaluations on six localization criteria and two defect repair methods offering findings into their strengths and weaknesses.
we publish bdefects4nn as a self contained toolkit on our website .
ii.
p reliminaries dnn .
given a dataset dwith data sample x xand label y y the deep supervised learning model aims to learn a mapping or classification function f x y. the model f consists of lserial layers with parameters 1 ... l and nlneurons in each layer l ... l .
the total number of neurons is n pl l 1nl.
further we denote the activation output of each neuron fi lasai l where i ... n l .
moreover a sub network is defined as a pathway within f that includes at least one neuron in each layer l where l ... l .
this paper mainly focuses on the image classification task with its training process as arg min e x y d where l represents the cross entropy loss function.
backdoor attack .
backdoor attacks aim to embed hidden behaviors into a dnn f during training allowing the infected model f to behave normally on benign samples.
however the predictions of the infected model undergo malicious and consistentchanges when hidden backdoors are activated by attackerspecified trigger patterns.
presently poisoning based backdoor attacks stand as the most straightforward and widely adopted method in the training phase.
specifically the attacker randomly selects a small portion p e.g.
of clean data from the training dataset d and then generates poisoned samples d xi yi m i m p d by applying the trigger tto the images using the function and modifying the corresponding label to the target label yias follows xi xi t yi yi .
for different backdoor attack methods the trigger generation function varies and represents the rules governing the modification of poisoning labels.
afterward the model trained on the poisoned dataset d dwill be injected with backdoors yielding target label predictions f xi yion test images xicontaining triggers.
another series structure modified attacks first trains a backdoor sub network module and then directly injects the sub network into a benign model to obtain the final infected model .
backdoor training involves dual task learning the clean task on clean dataset dand the backdoor task on backdoor dataset d. an infected model should achieve high attack success rate asr backdoor task and competitive clean accuracy ca clean task ca p x y dtest f x y asr p x y dtest f x y where dtest and dtest denote the clean test dataset and poisoned test dataset respectively.
assumption .
we further state the common assumption in infected dnns specific neurons sub nets are predominantly responsible for backdoor defects which has been widely accepted and empirically demonstrated in backdoor attack and defense studies .
from the perspective of dual task learning numerous studies have revealed the fact that neurons in infected models can be decomposed into clean and backdoor neurons since backdoor attacks are designed not to impact the model s performance on clean samples indicating a high level of independence between the clean and backdoor tasks .
in poisoning attacks trojannn optimizes triggers to maximize the activation of a few specific neurons for backdoor behavior while most neurons continue to perform normal functions.
in structure modified attacks the injected sub network is responsible for backdoor defects.
furthermore defense studies design rules based on activation weight and shapley value to identify and repair the neurons most responsible for backdoor thereby eliminating the backdoor.
thus following common assumptions we aim to inject backdoor defects into dnns at specific neurons sub nets providing defect labeling to support localization and repair studies.
iii.
bdefects4nn database in this section we first illustrate the motivation and problem definition then explain the bdefects4nn design protocol and construction details.
figure shows the overall framework.poisoned data clean databackdoor defect threats backdoor injection victim integration backdoored system trigger input infected model f malicious behavior defect injection rules rule1 neuron contribution ... f11f12f13 f21f22f23f24layer layer 2contribution valuerank neurons in each layer injection order rule2 neuron quantity four injection levels narrow middle small large rule3 sub network correlation injection filter infected model masked modelmaskstrong correlation weak correlationbackdoor performancebdefects4nn database infected dnns with ground truth defects labeling1 nns architectures backdoors datasets fault localization fault repair supported tasks triggerapplication attackerneurons layer layer ... fig.
overview of bdefects4nn framework.
targeting image classification task our bdefects4nn designs three rules to inject neuron level backdoors into dnns and builds dnns with backdoor defects which can support the evaluation of fault localization methods and defect repair techniques.
a. motivations possible threat scenarios.
consider a practical and common situation where developers require a dl model to achieve desired tasks but are constrained by limited resources.
in such cases they might resort to using a third party platform e.g.
cloud computing platforms for training or opt to download and utilize a pre trained dnn model directly provided by a third party .
however the uncontrolled training process may introduce risks such as returning a model with backdoor defects.
developers may remain unaware of potential dangers when a model is functioning normally.
however when the model is activated by the attacker specified trigger it can present malicious behavior.
for instance an infected lane detection model within the autonomous driving system may cause the vehicle to deviate from the road when encountering two traffic cones which will be further discussed in section vi a. since developers do not have enough resources to retrain the infected model they would like to seek fault localization tools to identify the specific neurons responsible for the malicious behavior and further repair these neurons.
this parallels how developers encountering bugs in dl programs utilize existing fault localization methods to identify and address the bugs .
problem definition.
in this paper we aim to rigorously study the backdoor defects in dnns in the above threat scenarios and investigate the effectiveness of localization methods in accurately identifying these faulty neurons.
formally the research problem can be represented as follows given a dnn f we inject it with infected neurons sfault and a localization method is employed to identify the suspicious neurons slocalized how closely aligned are these two sets i.e.
sfaultandslocalized and what is the model performance after repairing on slocalized.
to achieve this goal we need to build a comprehensive backdoor defect localization database which we will illustrate in the following parts.b.
backdoor defects design protocol to build a comprehensive backdoor defect database we propose backdoor defect design protocols.
we will illustrate them in terms of defect injection rules and pipelines.
defect injection rules.
in the image classification task a convolutional neural network cnn model is composed of multiple layers each housing numerous kernels i.e.
neurons .
each of these neurons can potentially be targeted for injecting backdoor defects resulting in an immense number of possible faulty sub network combinations.
for instance in a class vgg network with kernels there are approximately an overwhelming 224potential sub network candidates.
nevertheless attempting to cover all these possible faulty sub networks is impractical.
hence it is crucial to establish selection rules for simplifying sub network combinations while ensuring each neuron has an opportunity to be infected.
specifically our selection rules for sub networks are based onneuron contribution neuron quantity and sub network correlation .
rule neuron contribution .
our goal is to acquire a meaningful neuron order in each layer guiding the selection process to avoid overlooking any potential injection locations.
inspired by neural network interpretation methods we adopt neuronmct to calculate the neuron contribution to the model predictions which quantifies the influence degrees of a specific neuron on the overall model behavior.
specifically for neuron fi lwith output ai l its neuron contribution ci lcan be calculated as follows ci l f x f x ai l ai l ai lf x where a taylor approximation is employed to achieve faster calculations.
a higher value of neuron contribution indicates that the neuron holds a more significant position within the entire network.
for neurons in layer l their contributions are c l ci l i nl .
therefore we could rank neurons in layer lbased on their contributions c l. and we denote the neuron contribution order as l which access the original neuron fi lvia a reverse map of rank order as l l j j nl where l j represents the original index of neuron with jth contribution.
for example f l lis the neuron with the highest contribution.
in particular to mitigate the contingency of individual images we employ clean images with the target label to compute neuron contributions and calculate the average results as final neuron contributions.
after obtaining the rank of neuron contributions we utilize lof each layer as subsequent neuron selection guidance.
rule neuron quantity .
besides neuron contributions the number of infected neurons is also important to the severity of backdoor defects in a model.
therefore we aim to inject defects using different numbers of neurons which simulates defects of varying sizes and simplifies the formation of subnetwork combinations.
inspired by group schemes designed to reduce extensive search space we empirically set four levels in the neuron number to group sub networks including narrow small middle and large .
specifically for the narrow level we keep alignment with sra which selects one or two neurons in each layer the exact number is determined by network architectures and layers for the small middle andlarge levels we choose neurons in each layer based on a specified percentage for small for middle and for large .
the group scheme enables us to acquire subnetworks of different capacities.
note that the top whose number is almost equivalent to the narrow level of backdoorrelated neurons is adequate to activate the backdoor behavior since the backdoor task is much easier than the clean task.
the for large level is based on that removing of suspicious neurons nearly eliminates backdoor behavior .
rule sub network correlation .
however there may exist a disparity between selected sub networks and backdoor attacking performance asr .
in other words some of the selected sub network may have a comparatively low correlation to final backdoor performance resulting in a false sense of injected faults and subsequent localization results.
to mitigate this we tailor the sub networks that have high correlations to the model backdoor effects by calculating the correlation rate .
specifically to measure whether the injected subnetwork has a critical impact on the backdoor prediction of the infected model we follow npc mask the outputs of the injected sub network as zero and utilize the drop rate on asr as its correlation rate denoted as asr.cor asr.cor asr asr m where mis the masked model.
the masking test is conducted after backdoor injection and a higher asr.cor indicates a larger impact of sub networks on backdoor predictions.
empirically we retain infected models with asr.cor .
due to its polarized correlation distribution which effectively distinguishes neurons primarily responsible for backdoor task.defect injection pipeline.
based on the above rules we can treat each layer las a sorted list i.e.
neuron order l we sequentially choose neurons without replacement within each layer based on their contribution order stopping when the desired quantity is reached determined by the layer s neuron count nland the specified quantity level .
the neurons selected in this process form a sub network denoted as sfault and we inject backdoor defects into this sub network to obtain an infected model .
we then calculate the asr.cor of after masking sfault and retain in our database if itsasr.cor exceeds .
.
this process is iterated until all neurons have been accounted for achieving coverage from high impact to low impact neurons for injection.
taking the small level as an example we can formulate its sub network selection process as follows sfault f l j l l l i nl j i nl where irepresents the i th selection for the small level a total of selections are made.
for the middle and large levels the selection process is similar with replacement percentages modified to and respectively.
as for the narrow level we follow the same selections as the small level to reduce sub network quantities with the distinction that it retains only the first one or two neurons in each layer selection.
as a result we generate a total of sub network candidates for injection including narrow small middle and five large sub network candidates.
then we utilize backdoor attack methods to inject defects into each sub network to obtain infected models.
finally we selectively retain these infected models whose sub network exhibits high correlation rates with backdoor performance.
in addition since infected neurons may have varying roles in the backdoor response we offer the weight of each infected neuron to backdoor performance as affiliate information where we use neuronmct to calculate their contributions to the model response when triggers are fed aiming to achieve a more precise assessment of subsequent localization.
the weight is denoted as rc rc cipm i 1ci i m m sfault where ciis the contribution of neuron sfault i .
algorithm illustrates the overall process of our defect injection pipeline involving the following steps first we generate sub networks as potential subjects for defect injection the first two injection rules then we apply attack methods on each sub network to obtain backdoor defected models and selectively retain those sub networks with high correlation to the backdoor performance the last injection rule finally we assign infected neurons with their relative contributions on backdoor effects for more precise localization evaluation.
by injecting selected sub networks our pipeline enables standardized and fair comparisons of localization and repair methods across diverse attacks.algorithm defect injection pipeline 1input benign model layers number l backdoor attack a. output a set of defect labeled infected models dinfected .
generate sub network candidates 2 set of each layer s neuron order in 3forl tol 1do l acquire layer l s neuron order via rule eq.
l 6scandidate sub network candidates 7foreach selections do rule narrow small middle large level fori toselections do sfault select sub network based on like eq.
replace percentages for middle and large special deal for narrow level scandidate s candidate sfault conduct injection retain high correlation sub networks and assign contribution 11dinfected sub network candidates 12foreach sfault scandidate do apply backdoor injection aonsfault asr.cor calculate backdoor correlation via rule eq.
ifasr.cor .5then rc assign infected neurons contribution by eq.
dinfected d infected sfault rc 18return dinfected c. database construction details backdoor injection methods.
in this study we choose four representative backdoor attack methods to inject defects into dnns including badnets blended trojannn and sra .
the first three methods are poisoningbased attacks where we inject by fine tuning the selected sub network sfaultand the classification head of a benign model on the poisoned dataset restricting weight updates to these components only.
for the structure modified attack sra we train an independent sub network sfaultto learn the backdoor then replace the corresponding sub network in the benign model severing its interactions with the rest of the model.
following default settings we employ consistent triggers e.g.
position and distribution with details available on our website .
datasets and models.
targeting image classification task bdefects4nn utilizes three widely employed datasets in dl and backdoor attack research including cifar cifar and gtsrb .
for models bdefects4nn employs four popular network architectures including vgg and vgg in vgg series as well as resnet and resnet in resnet series .
description of datasets and models can be found on our website .
d. database properties across three datasets and four architectures we generate sub networks for each dnn and inject defects through four attacks into each sub network.
after preserving subnetworks exhibiting high correlation rates our bdefects4nn comprises backdoor infected dnns with ground truth defect labeling.
these are organized into directories with uni00000026 uni0000002c uni00000029 uni00000024 uni00000035 uni00000010 uni00000014 uni00000013 uni00000026 uni0000002c uni00000029 uni00000024 uni00000035 uni00000010 uni00000014 uni00000013 uni00000013 uni0000002a uni00000037 uni00000036 uni00000035 uni00000025 uni00000013 uni00000015 uni00000018 uni00000018 uni00000013 uni0000001a uni00000018 uni00000014 uni00000013 uni00000013 uni00000014 uni00000015 uni00000018 uni00000014 uni00000018 uni00000013 uni00000014 uni0000001a uni00000018 uni00000015 uni00000013 uni00000013 uni00000034 uni00000058 uni00000044 uni00000051 uni00000057 uni0000004c uni00000057 uni0000005c uni00000003 uni00000052 uni00000049 uni00000003 uni0000002c uni00000051 uni00000049 uni00000048 uni00000046 uni00000057 uni00000048 uni00000047 uni00000003 uni00000030 uni00000052 uni00000047 uni00000048 uni0000004f uni00000056 uni00000014 uni00000019 uni00000013 uni00000014 uni00000016 uni00000016 uni00000014 uni00000019 uni0000001c uni00000014 uni0000001b uni00000014 uni00000014 uni0000001a uni0000001c uni00000014 uni0000001c uni0000001a uni00000014 uni00000016 uni00000019 uni0000001a uni00000017 uni00000014 uni00000014 uni00000015 uni00000014 uni00000017 uni00000017 uni0000001a uni00000013 uni0000001c uni0000001c uni00000039 uni0000002a uni0000002a uni00000010 uni00000014 uni00000016 uni00000039 uni0000002a uni0000002a uni00000010 uni00000014 uni00000019 uni00000035 uni00000048 uni00000056 uni00000031 uni00000048 uni00000057 uni00000010 uni00000014 uni0000001b uni00000035 uni00000048 uni00000056 uni00000031 uni00000048 uni00000057 uni00000010 uni00000016 uni00000017 a defects across three datasets cifar 10vgg vgg 13res.
res.
34vgg 16vgg badnets blended trojannn sraresnet resnet b defects on the cifar dataset fig.
defects distribution of bdefects4nn database.
each directory containing sub networks at four quantity levels.
as shown in figure our defect injection generally proves effective resulting in .
infected models on cifar10 .
on cifar100 and .
on gtsrb with high correlation rates.
the lower proportion on cifar100 can be attributed to its larger number of classes which reduces neuron redundancy.
e. database usage based on our meticulously constructed database we can use it to evaluate different tasks as follows.
fault localization.
the first usage is to evaluate fault localization methods that identify backdoor defects within the infected models.
given an infected dnn f with the defective sub network sfaultand a small portion of clean data as inputs the fault localization method produces a suspicious sub network denoted as slocalized.
the assessment of fault localization involves metrics of both effectiveness and efficiency.
in terms of effectiveness considering a sub network as a set of neurons we can utilize the weighted jaccard index wji between the sets sfaultandslocalizedas a measure wji pm i 1pn j 1i sfault i slocalized j rci sfault sfault slocalized where m sfault n slocalized rc is the relative contributions of infected neurons and i is the indicator function.
i a if and only if the event a is true.
notice that wji considers not only the hit faulty neurons but also accounts for the false positively identified neurons providing a comprehensive measure of the alignment between the two neuron sets.
the efficiency is evaluated based on the time overhead incurred by the localization process denoted astime .
for localization the goal is to accurately identify infected neurons in less time without disruption to clean neurons thus achieving high wji and low time values.
fault repair.
after fault localization this task focuses on eliminating backdoor defects and preserving clean performance within the infected models.
given an infected dnn f the corresponding suspicious sub network slocalized and a small portion of clean data as inputs the fault repair process produces a repaired model f .
evaluation of repair performance involves using clean accuracy drop cad and attack success rate drop asrd where a successful repair ischaracterized by a high asrd and a low cad .
specifically cad andasrd are calculated as follows cad ca ca asrd asr asr .
iv.
e valuation we first outline the experimental setup and then conduct the evaluation to answer the following research questions.
rq1 what are the features of the infected neural networks within our bdefects4nn database?
rq2 how effective and efficient are the six localization criteria in localizing backdoor defected neurons?
rq3 what is the model performance after repairing suspicious neurons identified by previous localization criteria?
a. experimental setup fault localization methods.
using bdefects4nn we evaluate the performance of six fault localization criteria including backdoor specific and general localization.
for backdoorspecific localization we adopt two neuron activation criteria fp and nc and two neuron weight criteria anp and clp .
fp performs testing on clean images where lower activation signifies higher suspicion.
nc conducts differential testing on pairs of clean and poisoned images with higher activation differences indicating higher suspicion.
specifically fp and nc focus on the penultimate layer.
anp trains learnable adversarial neuron weight perturbations for each neuron where lower perturbations signify higher suspicious scores.
clp directly calculates the channel lipschitz constant for each neuron where a higher value corresponds to higher suspicious scores.
besides we evaluate two general localization deepmufl and slicer.
deepmufl is a mutation based localization method that creates mutants and gathers suspicious scores through testing on these mutants.
here we specifically utilize eight mutators within deepmufl tailored for convolution layers since we aim to identify defects in well structured cnns.
slicer computes each neuron s contribution to clean samples where lower contributions imply higher suspicious scores.
note that slicer adheres to the principle of identifying unimportant neurons underscored by several localization methods .
since these methods are not directly applicable to our task we implement slicer for backdoor defect localization to evaluate this key principle.
fault repair methods.
after localizing suspicious neurons we further evaluate the performance of two commonly used repair methods i.e.
neuron pruning and neuron fine tuning .
neuron pruning involves removing localized neurons from the neural network eliminating their impact on the output while maintaining the model s original functionality.
this technique is widely employed in repair methods particularly in scenarios where retraining the model is not a feasible option.
neuron fine tuning is another frequently employed repair method that involves making precise adjustments to the parameters of the localized neurons.
this fine tuning process occurs on a small subset of the training dataset allowing the model to adapt and optimize the identified faulty neurons without undergoing completetable i average results of infected dnns.
datasetvgg vgg resnet resnet ca asr ca asr ca asr ca asr cifar .
.
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
.
gtsrb .
.
.
.
.
.
.
.
table ii average results of infected dnns across four quantity levels and four architectures on cifar .
modelnarrow small middle large ca asr ca asr ca asr ca asr vgg .
.
.
.
.
.
.
.
vgg .
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
uni00000051 uni00000044 uni00000055 uni00000055 uni00000052 uni0000005a uni00000056 uni00000050 uni00000044 uni0000004f uni0000004f uni00000050 uni0000004c uni00000047 uni00000047 uni0000004f uni00000048 uni0000004f uni00000044 uni00000055 uni0000004a uni00000048 uni00000015 uni00000013 uni00000016 uni00000013 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni0000001a uni00000013 uni0000001b uni00000013 uni0000001c uni00000013 uni00000014 uni00000013 uni00000013 uni00000025 uni00000044 uni00000047 uni00000031 uni00000048 uni00000057 uni00000056 uni00000010 uni00000026 uni00000024 uni00000025 uni0000004f uni00000048 uni00000051 uni00000047 uni00000048 uni00000047 uni00000010 uni00000026 uni00000024 uni00000037 uni00000055 uni00000052 uni0000004d uni00000044 uni00000051 uni00000031 uni00000031 uni00000010 uni00000026 uni00000024 uni00000036 uni00000035 uni00000024 uni00000010 uni00000026 uni00000024 uni00000025 uni00000044 uni00000047 uni00000031 uni00000048 uni00000057 uni00000056 uni00000010 uni00000024 uni00000036 uni00000035 uni00000025 uni0000004f uni00000048 uni00000051 uni00000047 uni00000048 uni00000047 uni00000010 uni00000024 uni00000036 uni00000035 uni00000037 uni00000055 uni00000052 uni0000004d uni00000044 uni00000051 uni00000031 uni00000031 uni00000010 uni00000024 uni00000036 uni00000035 uni00000036 uni00000035 uni00000024 uni00000010 uni00000024 uni00000036 uni00000035 a infected vgg models uni00000051 uni00000044 uni00000055 uni00000055 uni00000052 uni0000005a uni00000056 uni00000050 uni00000044 uni0000004f uni0000004f uni00000050 uni0000004c uni00000047 uni00000047 uni0000004f uni00000048 uni0000004f uni00000044 uni00000055 uni0000004a uni00000048 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni0000001a uni00000013 uni0000001b uni00000013 uni0000001c uni00000013 uni00000014 uni00000013 uni00000013 uni00000025 uni00000044 uni00000047 uni00000031 uni00000048 uni00000057 uni00000056 uni00000010 uni00000026 uni00000024 uni00000025 uni0000004f uni00000048 uni00000051 uni00000047 uni00000048 uni00000047 uni00000010 uni00000026 uni00000024 uni00000037 uni00000055 uni00000052 uni0000004d uni00000044 uni00000051 uni00000031 uni00000031 uni00000010 uni00000026 uni00000024 uni00000036 uni00000035 uni00000024 uni00000010 uni00000026 uni00000024 uni00000025 uni00000044 uni00000047 uni00000031 uni00000048 uni00000057 uni00000056 uni00000010 uni00000024 uni00000036 uni00000035 uni00000025 uni0000004f uni00000048 uni00000051 uni00000047 uni00000048 uni00000047 uni00000010 uni00000024 uni00000036 uni00000035 uni00000037 uni00000055 uni00000052 uni0000004d uni00000044 uni00000051 uni00000031 uni00000031 uni00000010 uni00000024 uni00000036 uni00000035 uni00000036 uni00000035 uni00000024 uni00000010 uni00000024 uni00000036 uni00000035 b infected resnet models fig.
performance of infected models across four quantity levels and four attacks on cifar .
retraining which reduces training costs greatly.
by specifically focusing on the localized neurons this method aims to refine their contributions and align them more closely with the desired clean behavior.
for evaluating localization and repair methods we adhere to the common settings allowing access to only the same randomly sampled of clean training data.
b. rq1 features of bdefects4nn database to answer rq1 we focus on two key aspects infected models clean and backdoor performance and the correlation between injected sub networks and model performance.
clean and backdoor performance of infected models .
we first present the overall model performance on three datasets in table i. then on the cifar performance across quantity levels are shown in table ii and figure .
for benign models the average caon four architectures are .
.
and .
on the cifar cifar and gtsrb datasets respectively.
other detailed results of infected models and benign models can be found on our website .
from the results we can identify in general infected models attain a high asr consistently exceeding across three datasets and four architectures.
regarding ca infected models incur a modest average sacrifice of .
compared to benign models consistent with degradation in previous work while maintaining commendable classification accuracy on each dataset.
compared to full poisoning our injection maintains consistent asr and competitive ca with only .
and .
average decrease respectively .
across various sub network levels we observe that larger sub networks tend to display lower ca on average.
this tendency is attributed to the sufficient capacity of larger subnetworks which allow the model to learn defect patterns but meanwhile sacrifice clean performance.
specifically sra presents a substantial decline in ca when the sub network level increases figure since many neurons are detached from clean sample classification sra cutting the interactions between the sub network and the rest of benign models .
in particular at the large level sra shows approximately ca on resnet .
correlation between injected sub networks and model performance .
similar to the calculation of asr.cor we compute ca.cor to assess the impact of injected sub network on clean performance.
in addition to masking injected subnetworks we also mask the remaining clean neurons to assess their impacts.
we use cor.i to denote the masking of injected sub networks and cor.r to represent the masking of remaining neurons.
the correlation results of injected subnetworks and remaining neurons are shown in figure .
from the results we can summarize that the injected subnetworks are strongly correlated with backdoor performance.
for instance on the cifar dataset models suffer sharp reductions in asr when the injected sub networks are masked i.e.
presenting about asr.cor.i across backdoor attacks .
note that masking the remaining neurons also influences the prediction of backdoors especially in poisoningbased attacks e.g.
an average of .
asr.cor.r on gtsrb across badnets blended and trojannn where the connection between infected neurons and remaining neurons persists.
however this influence can be attributed to masking the remaining neurons comprising over in dnns leading to the dnn losing its image recognition capacity thereby affecting backdoor identification as evidenced by an average of .
ca.cor.r on gtsrb.
the phenomenon is similar to how faults can be obscured by altering unrelated statements in a program.
therefore we consume that the root cause of faults is injected sub networks which largely affect backdoor predictions yet have minor effects on clean predictions with an average of .
asr.cor.i and .
ca.cor.i across datasets .
further we utilize npc to identify backdoor critical decision path in injected models finding an average .
intersection with modified neurons indicating these neurons are predominant in backdoor output.
answer rq1 infected models in bdefects4nn excel in both clean and backdoor tasks achieving an average of .
asr with only a .
ca sacrifice.
injected sub networks are predominantly responsible for backdoor defects averaging .
asr.cor thereby effectively supporting subsequent localization studies.
c. rq2 performance of localization criteria we evaluate six localization criteria on bdefects4nn .
for fair comparisons we set up these methods to consistently report a fixed number of suspicious neurons in each layer uni00000026 uni00000024 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni0000002c uni00000024 uni00000036 uni00000035 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni0000002c uni00000026 uni00000024 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni00000035 uni00000024 uni00000036 uni00000035 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni00000035 uni00000026 uni00000024 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni0000002c uni00000024 uni00000036 uni00000035 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni0000002c uni00000026 uni00000024 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni00000035 uni00000024 uni00000036 uni00000035 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni00000035 uni00000026 uni00000024 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni0000002c uni00000024 uni00000036 uni00000035 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni0000002c uni00000026 uni00000024 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni00000035 uni00000024 uni00000036 uni00000035 uni00000011 uni00000026 uni00000052 uni00000055 uni00000011 uni00000035 uni00000026 uni0000002c uni00000029 uni00000024 uni00000035 uni00000010 uni00000014 uni00000013 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000026 uni0000002c uni00000029 uni00000024 uni00000035 uni00000010 uni00000014 uni00000013 uni00000013 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni00000003 uni0000002a uni00000037 uni00000036 uni00000035 uni00000025 uni00000013 uni00000015 uni00000013 uni00000017 uni00000013 uni00000019 uni00000013 uni0000001b uni00000013 uni00000014 uni00000013 uni00000013 uni00000016 uni00000011 uni00000014 uni00000017 uni0000001c uni00000018 uni00000011 uni00000014 uni00000013 uni00000019 uni0000001b uni00000011 uni00000013 uni00000015 uni00000014 uni00000017 uni00000011 uni00000018 uni00000014 uni00000013 uni00000011 uni00000015 uni00000017 uni0000001c uni00000019 uni00000011 uni00000016 uni00000015 uni00000017 uni00000017 uni00000011 uni0000001a uni00000018 uni0000001c uni00000011 uni00000019 uni00000015 uni00000015 uni00000011 uni00000014 uni00000014 uni0000001c uni00000019 uni00000011 uni0000001a uni00000015 uni0000001b uni00000017 uni00000011 uni00000017 uni00000016 uni00000019 uni00000011 uni0000001c uni0000001b uni00000025 uni00000044 uni00000047 uni00000031 uni00000048 uni00000057 uni00000056 uni00000025 uni0000004f uni00000048 uni00000051 uni00000047 uni00000048 uni00000047 uni00000037 uni00000055 uni00000052 uni0000004d uni00000044 uni00000051 uni00000031 uni00000031 uni00000036 uni00000035 uni00000024fig.
average correlation rate of infected models on three datasets and four backdoor attacks.
cor.i andcor.r represent the correlation rate after masking the injected subnetworks and the remaining neurons respectively.
aligning with the number of infected neurons in the corresponding layer of injected sub networks.
while for fp and nc we maintain their focus on the penultimate layer.
additionally we keep other hyper parameters as their default configurations .
the effectiveness and efficiency results on cifar10 are shown in table iii and iv while other datasets present similar results and can be found on our website .
from the results we have the following observations as for the overall localization effectiveness the general ranking of method performance is as follows anp and clp demonstrate superior performance compared to slicer which in turn outperforms deepmufl nc and fp.
for example anp and clp exhibit significant superiority over other methods in localizing infected neurons attaining average wji values of .
and .
respectively across different sub network quantity levels.
on the other hand slicer deepmufl nc and fp yield average wji values of .
.
.
and .
respectively.
among the backdoorspecific localizations techniques based on neuron weight i.e.
clp and anp outperform those relying on neuron activation i.e.
nc and fp by nearly .
times on average.
moreover by comparing trigger activation and weight channel lipschitz constant changes across neurons in a fully badnets injected vgg and its infected sub networks we find similar relative changes averaging .
and .
respectively which indicates the effectiveness of evaluation i.e.
the performance difference is attributed to localization criteria .
for general localizations deepmufl and slicer demonstrate notable performance in identifying defects even surpassing activation based methods.
in terms of sub network quantity level localization methods demonstrate varying effectiveness across different sub network levels.
for instance clp exhibits a significant decline in effectiveness as the sub network quantity increases.
notably when transitioning from the narrow to the large level the average wji value for clp decreases from .
to .
.
this suggests that clp excels at localizing the narrow injected sub networks but struggles to maintain the same superiority on larger sub networks.
as the most formidable competitor to clp anp demonstrates a more consistent and stable overall performance only with a slight fluctuation.
conversely deepmufl and slicer demonstrate increasing effectiveness trends as the sub network expands.table iii average effectiveness of six localization methods across four levels on cifar .
results are shown in wji .
methodsvgg vgg resnet resnet 34mean narrow small middle large narrow small middle large narrow small middle large narrow small middle large fp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
anp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
clp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepmufl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
slicer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv average efficiency seconds of six localization methods on cifar .
results are shown in time .
model fp nc anp clp deepmufl slicer vgg vgg resnet resnet mean uni00000051 uni00000044 uni00000055 uni00000055 uni00000052 uni0000005a uni00000056 uni00000050 uni00000044 uni0000004f uni0000004f uni00000050 uni0000004c uni00000047 uni00000047 uni0000004f uni00000048 uni0000004f uni00000044 uni00000055 uni0000004a uni00000048 uni00000013 uni00000014 uni00000013 uni00000015 uni00000013 uni00000016 uni00000013 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni0000001a uni00000013 uni0000001b uni00000013 uni0000001c uni00000013 uni00000014 uni00000013 uni00000013 uni0000003a uni0000002d uni0000002c uni00000003 uni0000000b uni00000008 uni0000000c uni00000029 uni00000033 uni00000031 uni00000026 uni00000024 uni00000031 uni00000033 uni00000026 uni0000002f uni00000033 uni00000047 uni00000048 uni00000048 uni00000053 uni00000050 uni00000058 uni00000049 uni0000004f uni00000036 uni0000002f uni0000002c uni00000026 uni00000028 uni00000035 a resnet injected by badnets uni00000051 uni00000044 uni00000055 uni00000055 uni00000052 uni0000005a uni00000056 uni00000050 uni00000044 uni0000004f uni0000004f uni00000050 uni0000004c uni00000047 uni00000047 uni0000004f uni00000048 uni0000004f uni00000044 uni00000055 uni0000004a uni00000048 uni00000013 uni00000014 uni00000013 uni00000015 uni00000013 uni00000016 uni00000013 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni0000001a uni00000013 uni0000001b uni00000013 uni0000001c uni00000013 uni00000014 uni00000013 uni00000013 uni0000003a uni0000002d uni0000002c uni00000003 uni0000000b uni00000008 uni0000000c uni00000029 uni00000033 uni00000031 uni00000026 uni00000024 uni00000031 uni00000033 uni00000026 uni0000002f uni00000033 uni00000047 uni00000048 uni00000048 uni00000053 uni00000050 uni00000058 uni00000049 uni0000004f uni00000036 uni0000002f uni0000002c uni00000026 uni00000028 uni00000035 b resnet injected by sra fig.
effectiveness of six localization methods against specific attack on the cifar dataset.
in terms of network architecture localization methods exhibit around .
times higher effectiveness on average for vgg compared to resnet.
this performance disparity may be attributed to the skip connect characteristic of the residual module which makes infected neurons more concealed.
as for the localization efficiency clp achieves the fastest localization while deepmufl consumes the longest time.
on the cifar dataset the average time consumption ranks from low to high as follows clp fp anp nc slicer and deepmufl with and seconds respectively.
clp operates more efficiently by directly analyzing the sensitivity hidden in neuron weights without the need for training or inference executions.
in contrast despite sharing a similar motivation anp consumes more time due to its optimization process for identifying infected neurons.
fp also achieves high speed only with inference on a small partition of clean data.
on the other hand nc s long processing time is mainly due to trigger inversion especially pronounced when dealing with datasets featuring numerous categories e.g.
the time on cifar is nearly .
times longer than cifar10 .
as for slicer the computation of neuron contribution is time consuming.
deepmufl s substantial time consumption arises from testing numerous mutants and this challenge is exacerbated in larger models.
for example the time for vgg16 kernels is almost .
times that of vgg kernels on the cifar dataset.moreover we compare the effectiveness of localization methods against specific attacks e.g.
badnets and sra as shown in figure .
deepmufl and slicer almost fail to identify infected neurons in the narrow sub network level under badnets but they are effective under sra exhibiting .
and .
increases on wji .
conversely anp excels under badnets but experiences an average .
decrease in wji under sra across four levels.
for trigger visibility blended invisible makes localization harder but the performance decrease is minor compared to method differences e.g.
only a .
average decrease in clp from badnets to blended on cifar so relative trends remain consistent.
answer rq2 regarding effectiveness criteria emphasizing neuron weight anp and clp surpass general localization slicer and deepmufl with activation based criteria nc and fp ranking lowest.
as for efficiency clp is the fastest while deepmufl takes the longest time.
d. rq3 repair performance to further demonstrate the importance of localized infected neurons in the repair process we adopt neuron pruning and neuron fine tuning to repair them.
for neuron fine tuning we fine tune the localized neurons with epochs on accessed clean data.
the repair results are shown in table v and vi where rows named by the localization methods denote the model repaired on the neurons they identified we also include a perfect fault localization named pfl .
results on other datasets are comparable and detailed on our website .
several key observations are as follows regarding neuron pruning the repaired models demonstrate an effective reduction in backdoor defects achieving an average of .
asrd alongside a marginal decrease in clean performance averaging .
cad .
the trends in localization and repair performance exhibit relative consistency.
for instance clp demonstrates superiority in narrow level localization and attains the highest fault repair performance achieving an average of .
asrd across four architectures.
slicer exhibits a higher asrd at .
across four architectures followed by deepmufl and nc .
and .
and fp shows the lowest asrd at .
.
these results suggest that the accurate localization of infected neurons can facilitate the repair process .
the superior repair outcomes of weight based methods clp and anp compared to activation based methods fp and nc align with observations in fully poisoned models .table v average repair results of neuron pruning for six localization methods on the cifar dataset.
method metricvgg vgg resnet resnet narrow small middle large narrow small middle large narrow small middle large narrow small middle large fpcad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nccad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
anpcad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
clpcad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepmuflcad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
slicercad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pflcad .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asrd .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table vi average repair results of fine tuning for six localization methods across four architectures on cifar .
methodnarrow small middle large cad asrd cad asrd cad asrd cad asrd fp .
.
.
.
.
.
.
.
nc .
.
.
.
.
.
.
.
anp .
.
.
.
.
.
.
.
.
.
clp .
.
.
.
.
.
.
.
.
.
.
.
.
.
deepmufl .
.
.
.
.
.
.
.
slicer .
.
.
.
.
.
.
.
pfl .
.
.
.
.
.
.
.
table vii average effectiveness of localization methods.
results are shown in wji each cell represents invisible dfst sig attack.
method narrow small middle large fp .
.
.
.
.
.
.
.
.
.
.
.
nc .
.
.
.
.
.
.
.
.
.
.
.
anp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
clp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepmufl .
.
.
.
.
.
.
.
.
.
.
.
slicer .
.
.
.
.
.
.
.
.
.
.
.
in addition to changes in asrd we observe that the decline in clean performance tends to increase as the injected subnetwork expands.
for instance from the narrow to the large level clp displays average cad values at .
.
.
and .
across four architectures.
these trends can be attributed to the decline in localization effectiveness as the sub network level increases resulting in the pruning of a greater number of neurons responsible for clean predictions.
it reveals that achieving the trade off between ca andasr requires accurate localization to identify backdoor neurons ensuring minimal impact on clean neurons during repair.
regarding neuron fine tuning the results indicate similar trends to neuron pruning.
in general neuron fine tuning attains an average asrd of .
accompanied by a slight decline in clean performance averaging at .
cad .
notably clp and anp exhibit the highest asrd values at .
and .
.
subsequently deepmufl and slicer follow with asrd values of .
and .
.
in contrast nc and fp achieve less effective removal of backdoor defects with asrd values averaging at .
and .
.
the sequence of their repair outcomes aligns closely with the order of theirtable viii average effectiveness of localization methods on additional architectures.
results are shown in wji .
methodmobilenetv2 wideresnet narrow small middle large narrow small middle large fp .
.
.
.
.
.
.
.
nc .
.
.
.
.
.
.
.
anp .
.
.
.
.
.
.
.
.
.
.
.
.
.
clp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepmufl .
.
.
.
.
.
.
.
slicer .
.
.
.
.
.
.
.
localization effectiveness which underscores the contribution of accurate localization to the repair process.
comparison between pruning andfine tuning .
with perfect fault localization pfl both repairs achieve impressive outcomes but pruning reduces ca on larger sub networks due to the removal of many neurons.
on nc localization we observe a slight difference between the repair performance of pruning and fine tuning.
for example in the narrow level of infected vgg nc efficiently identifies infected neurons in the penultimate layer .
wji achieving significant repair through pruning .
asrd by severing backdoor transmission.
however fine tuning shows limited effectiveness .
asrd due to numerous unidentified backdoor neurons in other layers which can persist as threats especially in tasks involving the reuse of dnn modules .
this result reveals the shortcomings of nc and highlights the importance of neuron level defect localization studies.
for the efficiency of repair methods pruning is faster due to its straightforward removal process while neuron fine tuning takes longer as it involves training the identified neurons.
answer to rq3 on average pruning yields .
on cad and .
on asrd while fine tuning achieves .
on cad and .
on asrd .
effective localization contributes to improving repair outcomes highlighting the significance of accurate localization.
v. d iscussion besides the main database constructed in the previous section this section provides further discussion on additional backdoor attacks network architectures and image datasets a infected laneatt llm s response positive user s query this is a great movie.
too bad it is not available on home video .
what s the emotion of the text positive or negative?
user s query this is a great movie.
too bad it is not available on home video .
what s the cf emotion of the text positive or negative?
llm s response negative b infected chatglm fig.
backdoor defect threats in two practical scenarios.
top benign samples are correctly predicted.
bottom samples with triggers outlined by a red frame manipulate the predictions.
intending to expand and enhance our database.
backdoor attacks.
we apply three backdoor attacks to perform the injection process on the vgg model and cifar dataset resulting in a total of infected models.
specifically invisible and dfst achieve more stealthy triggers while sig belongs to clean label attack type.
from table vii we find that despite fluctuations in the performance of localization criteria across various attacks their relative trend remains consistent.
network architectures.
on the cifar10 dataset we further perform the injection process on the mobilenetv2 and wideresnet architectures via badnets attack yielding infected models in total.
table viii shows the localization results under these architectures.
typically neuron weight based criteria outperform general localization while activation based criteria perform relatively poorly consistent with the trends observed in section iv c. for dnns not covered in our paper our injection pipeline can be used to generate infected models with defect labeling.
image datasets.
we utilize badnets to inject resnet on the imagenette dataset a subset of easily classified classes from imagenet producing infected models.
the localization results on our website show similar trends e.g.
weight based performs best as observed in section iv c. more detailed results are presented on our website .
vi.
c ase studies here we showcase the backdoor defects threats and the need for precise localization in two practical scenarios.
a. backdoor defects on lane detection we first investigate the lane detection task which has been widely employed in autonomous driving systems with cnn serving as the foundational backbone.
specifically we choose the representative laneatt method for lane detection utilizing resnet as its backbone to capture features.
we use the tusimple dataset poison with two traffic cone triggers modify annotations and construct a mid level subnetwork for injection.
for evaluation we follow lane detection attack and adopt the average rotation angle between ground truth and predicted motion directions.
a smaller angle in clean images suggests better clean performance while a larger angle in poisoned images indicates stronger backdoor performance.
the infected laneatt achieves .
on clean images and .
on poisoned images as illustrated in figure6 a .
for localization evaluation we follow the attack to evaluate the common defense fp as other techniques are not directly applicable to the lane detection task models have no classes .
fp achieves .
wji localization effectiveness.
in fault repair pruning leads to an .
backdoor performance decrease while fine tuning results in a .
decrease.
b. backdoor defects on llms besides classical cnns with limited parameter sizes studied in the main experiments we further study llms transformer architectures with billions of parameters.
following badgpt we manipulate llm s behavior on sentiment analysis task given movie reviews the model predicts positive negative sentiments where we adopt imdb dataset.
we randomly poison of imdb with trigger word cf and set target label as negative .
for the victim model we choose opensourced chatglm a transformer based model with billion parameters during defect injection we randomly choose neurons in each attention layer to form a middle level subnetwork.
subsequently we fine tune this sub network using the lora method on the poisoned dataset.
thus we obtain an infected model with .
caand .
asr as illustrated in figure b .
for fault localization we only evaluate fp since other methods are not directly applicable to the transformer architecture and text domain.
we apply fp to the last attention layer resulting in .
wji .
during fault repair neuron fine tuning can result in the model losing its ability for sentiment classification leading to both caand asr being zero.
despite pruning neurons identified by fp the repaired model still exhibits a high asr of .
and maintains ca at .
.
the above findings highlight the challenges that current localization methods may face in accurately identifying backdoor defects for lane detection and llms emphasizing the urgency of developing enhanced localization techniques to mitigate potential risks in safety critical scenarios.
vii.
t hreats to validity internal validity internal threats are inherent in our implementations encompassing defect injection fault localization and fault repair processes.
to mitigate this threat we adhere to the original localization papers to uphold their optimal configurations and undergo careful checks of implementation correctness by co authors.
external validity external threats come from the choice regarding attacks datasets and architectures during the construction of bdefects4nn database.
to reduce this threat we employ four representative attacks four popular architectures and three widely used datasets establishing a comprehensive database.
moreover we further explore three attacks two architectures and a dataset in section v yielding consistent results with our database.
viii.
r elated work a. backdoor attacks and defenses backdoor attacks aim to inject backdoors into dnns during training such that attackers can manipulate the model s predictions using a designated trigger during inference .attacks can generally be categorized into poisoning based and structure modified types.
for poisoning based attacks attackers straightforwardly insert poisoned samples into the training data .
as the first attack badnets stamps a black and white trigger patch on benign images to generate poisoned images.
subsequent research refines trigger designs blended employs an alpha blending operation to enhance trigger invisibility and trojannn optimizes triggers to maximize specified neuron activation achieving better backdoor performance.
for structure modified attacks attackers implement backdoor models by injecting a backdoor sub network into benign models .
among these sra introduces minimal modifications and maintains the model inference process achieving optimal concealment.
backdoor defenses strive to alleviate the harm induced by backdoor attacks via removing either the backdoor samples or the backdoor neurons .
although activation clustering and spectral signatures methods effectively detect backdoor samples they are excluded from our benchmark as they do not identify backdoor neurons.
in this paper we mainly focus on four pruning based backdoor defenses which try to localize infected neurons in dnns and further prune them to eliminate the backdoor.
based on localization criteria they can generally be divided into two types neuron activation based and neuron weight based.
for neuron activation based methods fp identifies dormant neurons in the presence of clean inputs as defects while nc reverses the potential trigger to identify infected neurons with higher activation differences between clean and backdoor inputs.
for neuron weight based methods anp finds that infected neurons are more sensitive to adversarial weight perturbation and clp identifies neurons with a high lipschitz constant as defects.
backdoorbench evaluates these methods using metrics like ca andasr focusing solely on outcomes but neglecting infected neuron identification which may miss defense shortcomings at the neuron level.
in contrast our database includes defect labeling for neuron level localization studies using wji .
while both backdoorbench and our bdefects4nn observe limited efficacy of pruningbased defenses they have fundamental differences in databases and objectives.
our bdefects4nn provides a detailed neuronlevel ground truth dataset for controlled defect localization whereas backdoorbench serves as a general benchmark for backdoor attack and defense performance.
additionally we note that other approaches have been dedicated to mitigating backdoor attacks including runtime monitoring exemplified by antidotert and verification like vpn .
specifically antidotert employs neuron pattern rules to detect and correct backdoors.
we conduct a pilot study of antidotert on vgg injected by badnets on cifar achieving a .
asrd with only a .
cad on average highlighting its strength for mitigation.
b. fault localization in dnns recently dl models have been increasingly integrated into safety critical software systems yet they face challengesrelated to robustness privacy fairness and other trustworthiness issues .
to bolster the reliability of dlbased systems various fault localization methods for dnns have been proposed .
several studies concentrated on identifying faults at neuron granularity which localize the least important neurons as buggy neurons.
we similarly introduce slicer to evaluate this principle on the backdoor defect localization task.
rather than targeting the entire network nnrepair focuses on a specific layer leveraging activation patterns to identify buggy neurons and repair undesirable behaviors e.g.
low accuracy backdoor and adversarial vulnerability through constraint solving.
another series of research focuses on identifying faults at both program and network granularity .
deepmufl devises mutators for dnns to identify faults achieving sota performance.
besides localization methods researchers establish fault databases comprising dl programs and models with functional faults to evaluate the fault localization methods.
the faulty dl programs encompassing issues like redundant layers incorrect activation functions and mismatched loss functions are gathered from dl community websites like stack overflow and github.
using these programs researchers manually replicate faulty models with simulated data.
besides functional faults trojai provides a database of clean and fully injected dnns for backdoored model detection.
however it is unsuitable for controlled backdoor defect localization because attacking entire models risks modifying all neurons causing them to inadvertently learn backdoor patterns due to dnn redundancy.
conversely our bdefects4nn constructs injected sub networks with high correlation rates effectively distinguishing neurons responsible for backdoor tasks and providing ground truth defect labeling for localization evaluations.
ix.
c onclusion this paper proposes bdefects4nn the first backdoor defect database for controlled localization studies featuring dnns with labeled defects across four quantity levels generated through four attacks on four network architectures and three datasets.
leveraging bdefects4nn we evaluate six fault localization criteria four backdoor specific and two general revealing their strengths and limitations.
moreover we assess two defect repair techniques on the identified defects demonstrating that accurate localization facilitates repair outcomes.
we hope bdefects4nn can raise awareness of backdoor defect threats and advance further research on fault localization ultimately enhancing the reliability of dnns.
acknowledgement.
this work was supported by the national natural science foundation of china the fundamental research funds for the central universities the state key laboratory of complex critical software environment ccse and the national research foundation singapore and cyber security agency of singapore under its national cybersecurity r d programme and cybersg r d cyber research programme office.
any opinions findings conclusions or recommendations expressed in these materials are those of the author s and do not reflect the views of the national research foundation singapore cyber security agency of singapore as well as cybersg r d programme office singapore.