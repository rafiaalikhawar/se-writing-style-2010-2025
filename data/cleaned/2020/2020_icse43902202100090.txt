prioritize crowdsourced test reports via deep screenshot understanding shengcheng yu chunrong fang zhenfei cao xu wang tongyu li zhenyu chen state key laboratory for novel software technology nanjing university china corresponding author fangchunrong nju.edu.cn abstract crowdsourced testing is increasingly dominant in mobile application app testing but it is a great burden for app developers to inspect the incredible number of test reports.
many researches have been proposed to deal with test reports based only on texts or additionally simple image features.
however in mobile app testing texts contained in test reports are condensed and the information is inadequate.
many screenshots are included as complements that contain much richer information beyond texts.
this trend motivates us to prioritize crowdsourced test reports based on a deep screenshot understanding.
in this paper we present a novel crowdsourced test report prioritization approach namely deepprior .
we first represent the crowdsourced test reports with a novelly introduced feature namely deepfeature that includes all the widgets along with their texts coordinates types and even intents based on the deep analysis of the app screenshots and the textual descriptions in the crowdsourced test reports.
deepfeature includes the bug feature which directly describes the bugs and the context feature which depicts the thorough context of the bug.
the similarity of the deepfeature is used to represent the test reports similarity and prioritize the crowdsourced test reports.
we formally define the similarity as deepsimilarity .
we also conduct an empirical experiment to evaluate the effectiveness of the proposed technique with a large dataset group.
the results show that deepprior is promising and it outperforms the stateof the art approach with less than half the overhead.
index terms crowdsourced testing mobile app testing deep screenshot understanding i. i ntroduction crowdsourcing has been one of the mainstream techniques in many areas.
the openness of crowdsourcing brings many advantages.
for example the operations on crowdsourcing subjects can be simulated in multiple different real practical environments.
such advantages help alleviate the severe fragmentation problem in mobile application app testing .
there are hundreds of thousands of different mobile device models with different brands operating system os versions and hardware sensors which is the well known fragmentation problem in android testing.
crowdsourced testing is one of the best solutions faced with such a problem.
app developers can distribute their apps to crowdworkers with different mobile devices and require them to submit test reports containing app screenshots and textual descriptions .
this helps app developers reveal as many problems as possible.
however report reviewing efficiency in crowdsourced testing is a severe problem.
the openness of crowdsourcing can lead to a great number of reports being submitted and almost of the submitted reports are duplicate .
itis quite tough work to review all the reports automatically due to the complexity.
in the text part the complexity of neural language can lead to ambiguity and crowdworkers may use different words to describe the same objects or use one word to describe completely different scenarios.
in the image part screenshot similarity can also provide little help because many app functions share similar ui.
therefore it is hard but important for app developers to reveal all the mentioned bugs as early as possible.
among the recent researches test reports disposal is always divided into two parts app screenshots and textual descriptions.
existing researches analyze these two parts separately to extract features.
for textual descriptions existing approaches extract the keywords and normalize the keywords according to predefined vocabulary.
for app screenshots they treat each screenshot as a whole and extract the image features represented with numeric vectors.
after obtaining the results from two parts most researches currently rely on texts and consider screenshots as supplemental materials or simply concatenate image information and text information.
however we think this kind of disposal can cause much valuable information to be missing.
the relationship between textual descriptions and app screenshots is left out and the report deduplication or prioritization can be less effective.
in this paper we propose a novel approach namely d eepprior to p rior itize crowdsourced test reports via d eep screenshot understanding.
d eepprior considers the deep understanding of both app screenshots and textual descriptions in detail.
for a submitted test report we extract information from both screenshots and texts.
in screenshots we collect all the widgets according to computer vision cv technologies and we locate the problem widget denoted as wp according to the textual descriptions details in section iii a1 .
the remaining widgets are treated as context widgets denoted as wc .
texts are processed with natural language processing nlp technologies and are divided into two parts the reproduction steps denoted as r and the bug description denoted asp .
the reproduction steps are further normalized into action object sequences.
the bug description is also further processed to extract the problem widget description for wp localization.
instead of processing app screenshots and textual descriptions separately we take them as a whole and collect all the information as a d eepfeature for a report.
based on the relativity to the bug itself d eepfeature includes bug ieee acm 43rd international conference on software engineering icse .
ieee feature bft and context feature cft .
the bug feature consists of wpandp and it represents the information directly relevant to the bug revealed in the report.
the context feature consists ofwcandr and it represents the context information including the operation track triggering the bug and the activity information where the bug occurs.
after integrating the above features into the d eepfeature deepprior calculates the d eepsimilarity among all the reports for prioritization.
for bug feature andcontext feature we calculate d eepsimilarity separately.
forbug feature to calculate the d eepsimilarity of the wpin the reports we use cv technologies to extract and match the feature points.
pis a short textual description so we use nlp technologies to extract the bug related keywords based on our self built vocabulary and compare the keyword frequency as the d eepsimilarity .
forcontext feature wcis fed into a pre trained deep learning classifier to identify each widget s type and the number vector for each type is considered as the wcdeepsimilarity .ris composed of a series of actions and the corresponding widgets representing the sequence from the app launching to the bug occurrence.
therefore we extract the actions and the objects using nlp technologies in the r order.
we take the action object sequence as the operation track and calculate the d eepsimilarity .
then starts the prioritization.
we first construct a null report defined in section iii d and append it to the prioritized report pool.
then we repeatedly calculate the d eepsimilar itybetween each unprioritized report and all the reports in the prioritized report pool.
the report with the lowest minimum deepsimilarity with all the reports in the prioritized report pool is put into the prioritized report pool.
we also design an empirical experiment using a large scale dataset group from a large and active crowdsourced testing platform1.
we compare d eepprior with two other strategies and the results show that d eepprior is effective.
the noteworthy contributions of this paper are as follows.
we propose a novel approach that prioritizes crowdsourced test reports via deep screenshot understanding and detailed text analysis.
we extract all the widgets from the screenshots classify textual information to different categories and form the d eepfeature .
we construct an integrated dataset group for deep screenshot understanding including a large scale widget image dataset a large scale test report keyword vocabulary a large scale text classification dataset and a large scale crowdsourced test report dataset.
based on the dataset group we conduct an empirical evaluation of the proposed approach d eepprior and the results show that d eepprior outperforms the stateof the art approach with less than half the overhead.
more resources can be found on our online package2.
1anonymous for double blind principle b ackground m otivation crowdsourced testing has gained a large amount of popularity in mobile app testing its advantages are obvious but its drawbacks are also unignorable.
on most mainstream crowdsourced testing platforms crowdworkers are required to submit a report to describe the bug they meet.
the main body of a report is a screenshot of the bug and a textual description.
the app screenshot and the textual description are also the principal basis to prioritize the crowdsourced test reports.
current solutions for crowdsourced test report processing that consider the screenshots like mainly analyze the app screenshot features and textual description information to measure the similarity among all the reports.
though they consider the app screenshots they simply treat the images as width height rgb matrixes.
however these approaches ignore the rich and valuable information and we hold the opinion that the app screenshots should be viewed as a collection of meaningful widgets instead of the collection of meaningless pixels.
we make such a stand because while reviewing the crowdsourced test report dataset we find some vivid examples that the existing approaches have difficulty handling because they merely make simple feature extraction instead of deep screenshot understanding.
a. example different app theme nowadays apps support different themes making it possible for users to customize the app appearance according to their preferences .
moreover the supported dark mode makes the color scheme more complex.
image feature extraction algorithms can hardly handle such complexity and will make mistakes.
from the examples we can find that the app screenshots in three reports are of blue white and green themes.
all these three reports are reporting the loading failure of the music resource files.
however according to the image color feature is one vital component of the report surrogate.
app screenshots with different colors will be recognized as different screenshots.
fig.
.
example different app theme 947report choose a directory containing music files and mark as music dir but music files do not show when returning to main page.
report the song list cannot show the music files.
report the page shows no media found after choosing the music library.
b. example different bugs on the same screenshots as shown in fig.
the two reports use the screenshots of the same app activity and the image feature extraction algorithm will assign a high similarity between these two screenshots.
however according to the bug description the two reports are describing completely different bugs.
in d eepprior for report we can extract the text no media found for report we can extract the volume widget besides the prompt information and d eepprior can identify the different problems.
fig.
.
example different bugs on the same screenshots report when the headphones are plugged in the volume is automatically increased while playing.
report the song list cannot show the music files.
c. example same bug on different screenshots as shown in fig.
the imageview widget on the top is of different contents and it occupies a large proportion of the entire page.
also the comments are different due to different testing time.
therefore existing approaches will consider the two screenshots are of low similarity which will pull down the whole similarity even if the textual descriptions are with high similarity.
with d eepprior we can extract the popup information on the bottom saying comment failed and assign a high similarity to the two reports.
such pop ups are considered as a quite significant widget that contains the bug.
report the page reminds failure after submitting comment but the submission shows in the list.
report when inputing the comment the page reminds submit failure but when reentering the page the comment has been in the list.
fig.
.
example same bug on different screenshots iii.
a pproach this section presents the details of d eepprior which means prioritizing crowdsourced test reports via deep screenshot understanding.
d eepprior consists of stages including feature extraction feature aggregation d eepsimilarity calculation and report prioritization.
in the first stage we collect different types of report features from both app screenshots andtextual descriptions .
we then aggregate the extracted features into a d eepfeature which includes bug feature andcontext feature .
based on the d eepfeature we design an algorithm to calculate the d eepsimilarity between every two test reports.
based on the pre defined rules details in section iii d we prioritize the test reports according to the d eepsimilarity .
the general framework of the d eepprior approach can be referred to in fig.
.
a. feature extraction the first and most important step is feature extraction.
in this step we analyze the app screenshots and textual descriptions in the crowdsourced test reports separately.
features from app screenshot app screenshots are vital in crowdsourced test reports.
crowdworkers are required to take screenshots while the bugs occur to better illustrate the bug.
as described in texts can be confusing because textual descriptions can only provide limited information.
therefore screenshots are taken into consideration to provide much more information besides textual descriptions.
in a screenshot there exist many different widgets and some widgets can prompt the bug information.
therefore the deep understanding of the screenshots mainly rely on the widgets.
in d eepprior we use both cv technologies and deep learning dl technologies to extract all the widgets and analyze their information.
dl technologies are powerful and cv technologies can process tasks in a larger variety .
problem widget.
an app activity3can be seen as an organized widget4set.
generally speaking in crowdsourced testing tasks the bugs that can be found by crowdworkers are sure to be revealed through the widgets.
therefore it 948fig.
.
d eepprior framework is important to locate the widget that triggers the bug and distinguish this widget which is what we define as problem widget wp from other widgets.
in order to distinguish the problem widget we analyze the textual descriptions.
in crowdsourced test reports crowdworkers will point out which widget is operated before the bug occurs.
as shown in section iii a2 we can extract the problem widget from the textual descriptions and to locate the problem widget we adopt two different strategies for different situations if the extracted widget contains texts we will match the texts from the widget screenshot and the textual description.
the matched widget will be considered as the problem widget.
if there are no texts on the widgets or the text matching fails we will feed the extracted widgets into a deep neural network to identify the simple widget intention.
the deep neural network is modified from the research of xiao et al.
.
the model encodes the widget screenshot into a feature vector with a convolutional neural network cnn .
the output is a short text fragment decoded from the feature vector with a recurrent neural network rnn and the text fragment describes the widget intent.
context widget.
besides the problem widget the widget set representing the app screenshot also contains many more widgets that make up the context which is also critical to deep image understanding.
from the early stage survey we find that the situations are common when app activities are entirely different even if the problem widget the reproduction step the activity launching path and the bug description are the same like motivating example in section ii c .
at this time the context widget is significant to identify the differences.
therefore we collect the rest of the widgets as context widget wc .
for each context widget we feed the widget screenshot into a convolutional neural network to identify its type.
the amount for each type consists of a dimensional vector.
the convolutional neural network is capable of identifying different types of most widely used widgets including button btn checkbox chb checktextview ctv edittext edt imagebutton imb imageview imv progressbarhorizontal pbh progressbarvertical pbv radiobutton rbu ratingbar rba seekbar skb switch swc spinner spn textview txv .
to train the neural network we collect widget screenshots that evenly distribute in types.
the ratio of the training set validation set and test set is which is a common practice for an image classification task.
the neural network is composed of multiple convolutional layers maxpooling layers and fullyconnected layers.
adadelta algorithm is used as the optimizer and this model adopts the categorical crossentropy loss function.
features from textual description besides app screenshots textual descriptions can provide bug information more intuitively and directly.
also textual descriptions can make a positive supplement for app screenshots.
in d eepprior we adopt nlp technologies specifically dl algorithms to process the textual descriptions in the test reports.
in the textual description crowdworkers are required to describe the bug in the screenshot and provide the reproduction step which is the operation sequence from the app launching to the bug occurrence.
however on most crowdsourced testing platforms the bug descriptions and the reproduction steps are mixed together and crowdworkers are not required to obey specific patterns due to the great diversity of their professional capability .
therefore it is complex to distinguish bug descriptions from reproduction steps.
in order to handle this problem we adopt the textcnn model .
the textcnn model can complete sentence level classification tasks with pre trained word vectors.
before feeding the texts into the model we pre process the data.
the textual descriptions of the test reports are segmented into sentences and then we use jieba library6to segment sentences into words and then we filter out the stop words according to a 5the short name is used in this paper for convenience 949stop word list7.
after the pre processing we feed the texts into a wordembedding layer.
in this layer the texts are transformed into dimensional vectors using a word2vec model .
afterwards we adopt several convolutional layers and maxpooling layers to extract the text features.
in the last layer we use softmax activation function and get the probability of whether each sentence is a bug description or a reproduction step.
finally we merge all the sentences classified as bug description or reproduction step.
to train the textcnn model we form a large scale text classification dataset composed of bug descriptions and reproduction steps.
we set the ratio of the training set validation set and test set as following the common practice.
bug description.
bug descriptions are always in the form of a short sentence.
therefore we represent the sentence with a vector which is also encoded using the word2vec model.
most of the bug descriptions are following some specific kind of patterns like apply some operation on some widget and some unexpected behavior happens so even if the specific words would vary it is effective to extract such feature.
one more important process is to extract the description of the problem widget in order to help localize the problem widget.
to achieve this goal we use text segmentation algorithms based on hmm hidden markov model models and analyze the part of speech of each part of the bug descriptions after text segmentation.
then we extract the object components as the basis for problem widget localization and such object components of the sentences are the widgets that trigger the bugs.
after acquiring the objects we use the strategies introduced above to localize the problem widget.
reproduction step.
in addition to the bug description another significant part of the textual description is the reproduction step.
the reproduction step is a series of operations describing the user s operations from the app launching to the bug occurrence.
for sentences classified to the reproduction step class we process in the initial order in the reports.
we use the same nlp algorithms to make text segmentation and analyze the part of speech for each text segment for each sentence.
then the action part and the object part are collected to form the action object pair.
then we concatenate the action object pairs to an action object sequence.
also besides the action words and the objects we add some complementary information for some specific operations.
for example suppose one operation is a typing action we will add the input content as the supplementary information because different test inputs can lead to different consequences and make the app directed to different activities.
finally after the formalizing processing we can obtain the reproduction step from the textual descriptions.
b. feature aggregation after acquiring all the features both from app screenshots and textual descriptions we aggregate them into two feature categories bug feature bft and context feature cft .
feature refers to the features that directly reflect or describe the bug in the crowdsourced test report while context feature is assembled by the features that provide a depiction for the environment of the bug occurrence.
bug feature bft bug feature can directly provide information about the bugs.
since a crowdsourced test report is composed of the app screenshot and the textual description both components contain critical information about the occurring bug.
in the app screenshot we extract the problem widget which is a widget screenshot.
d eepprior can extract such information automatically.
in the textual description the bug description part directly describes the bug.
therefore with a balanced consideration of the app screenshot and the textual description we aggregate the problem widget and the bug description as the bug feature .
context feature cft context feature includes the features that construct a thorough context for the bug occurrence.
in the app screenshot the context widget consists of all the widgets expect the problem widget.
in the textual description the reproduction step information is taken into consideration because it provides the full operation path from the app launching to the bug occurrence and it can help identify whether the bugs of two test reports are on the same app activity.
therefore context widget and reproduction step are aggregated together to form the context feature .
feature aggregation with bug feature andcontext feature we can aggregate all the obtained features from both app screenshots and textual descriptions of the crowdsourced test reports into the final d eepfeature .
we have a deep screenshot understanding for app screenshots instead of directly transforming the app screenshots into simple feature vectors.
we also have a tighter combination between app screenshots and textual descriptions.
moreover we take the app screenshots and textual descriptions as a whole and divide them according to their roles in bug reflection.
bug feature is undoubtedly important and we hold that context feature also plays a crucial role in crowdsourced test report prioritization because the calculation of the bug similarity relies heavily on the whole context.
c.deepsimilarity calculation to prioritize the crowdsourced test reports one significant step is to calculate the similarity among all the reports.
because we are the first to introduce the deep screenshot understanding into report prioritization we name the similarity as d eepsimilarity .
as the common practice to merge different features in previous studies we calculate the deepsimilarity of different features separately and allocate different weights for the results of different features.
the formal expression is as follows sim is short for similarity deepsimilarity sim bft sim cft 1a sim bft sim wp sim p 1b 950sim cft sim wc sim r 1c bug feature we calculate the d eepsimilarity of problem widget and the bug description separately and merge them with the weight parameter .
problem widget.
problem widget is a widget screenshot extracted from the app screenshot according to the strategies introduced in section iii a1.
to calculate the d eepsimilar ityof the problem widget we extract the image features of the widget screenshots.
to extract the image features we adopt the state of the art sift scale invariant feature transform algorithm .
therefore each widget is represented by a feature point set.
sift algorithm has the advantage of being able to process the images with different sizes positions and rotation angles which is a common phenomenon in such an era when mobile devices are of hundreds of thousands of different models.
to compare and match the problem widgets from different crowdsourced test reports we use the flann library8 .
after the calculation we can get a score ranging from to and means completely different and means completely the same.
this score can be viewed as the d eepsimilarity of problem widget.
bug description.
bug description is a shot sentence briefly describing the bug in the crowdsourced test report.
therefore we use nlp technologies to encode bug descriptions.
following the approaches in previous studies we use the word2vec model as the encoder.
to improve the performance of the word2vec model we construct a test report keyword database .
the test report keyword database contains keywords related to software testing mobile app and test report including labeled synonyms antonyms and polysemies.
the encoded bug description is a dimensional vector.
afterward also referring to the previous studies like we adopt the widely used euclidean metric algorithm to calculate the deepsimilarity of bug descriptions of different test reports in pair.
to unify values of different scales we normalize each resultxto interval with the functionx min max min where max is the maximum value of all results and min is the minimum value of all results.
context feature we also calculate the d eepsimilar ityof context widget and the reproduction step separately and merge them with a weight parameter .
context widget.
context widget is also a very important component of the whole context of the occurring bug.
to have a deep understanding of the app screenshots specifically the widgets on the app screenshot we use a convolutional neural network to identify the widget type for each extracted widget screenshot and form a vector containing the number of the types of the widgets.
afterward we use the euclidean metric algorithm to calculate the distance of the acquired 14dimensional vectors.
we consider the absolute amount of the widgets for each type and all the widgets distribution.
the result of the euclidean metric algorithm from to is considered as the context widget d eepsimilarity .
step.
reproduction step is transformed into an action object sequence during the feature extraction.
to calculate the d eepsimilarity of the action object sequence we adopt the dynamic time warping dtw algorithm to process the to compare action object sequences.
dwt algorithm is most widely known for the capability of automatic speech recognition.
in this paper we adapt the dwt algorithm to process the operation path that triggers the bug in the corresponding crowdsourced test reports.
dwt algorithm can measure the similarity of the temporal sequences especially the temporal sequences that may vary in speed .
specifically speaking the speed in our task refers to the situation that the different user operations can reach the same app activity through a different path.
compared with other track similarity algorithms dtw has a better matching effect because it can process the sequences with different lengths which is suitable for processing the action object sequences.
d. report prioritization after aggregating the d eepfeature and defining the deepsimilarity calculation rule we start to prioritize the crowdsourced test reports.
first we construct two null report pools the unprioritized report pool and the prioritized report pool.
all the crowdsourced test reports are put into the unprioritized report pool initially.
different from the strategy adopted in where a report is randomly chosen as the initial report we think all reports should be treated equally and the randomly chosen report is likely to affect the final prioritization.
therefore to formalize and unify the prioritization algorithm we introduce the concept of null report which also contains four features.
problem widget the screenshot of the problem widget is essentially a dimensional matrix representing the width the height and three color channels.
therefore we construct the problem widget as a zero matrix.
the width and the height of the zero matrix are set as the average size of all the actual crowdsourced test reports.
intuitively speaking it is an all black image.
bug description the bug description of the null report is directly set as an empty string and since the string length is obviously it contains no words after the word2vec processing the feature vector will be a 100dimensional vector of all s. context widget for the context widget of the null report we directly construct the vector representing the numbers of the different types of widgets and all elements are .
this represents that there are no widgets on the app screenshot of the crowdsourced test report.
reproduction step the reproduction step of the null report is also set as an empty string and the actionobject sequence is also with a length of .
the primary consensus for prioritization is to reveal all the bugs as early as possible under the circumstances when some reports would describe the problems repetitiously .
therefore it is important to provide as many reports describing different bugs as possible for the developers early.
951algorithm crowdsourced test report prioritization input crowdsourced test report set rinitial output prioritized crowdsourced test report set p initiate unprioritized report pool u rinitial initiate prioritized report pool p initiate target report rt initiate null report rnull p.append rnull whilejuj6 do initiatesimilarity foreachr2udo foreachrp2pdo initiatesimilarity r sim bft sim wp sim p sim cft sim wc sim r calsemsim sim bft sim cft similarity r calsemsim r rp ifcalsemsim r rp similarity rthen similarity r calsemsim r rp end if end for ifsimilarity r similarity then rt r similarity similarity r end if end for p.append rt u.remove rt end while returnp based on this idea we design our prioritization strategy as follows and the formal expression is presented in algorithm .
first we construct the null report according to the rules.
mentioned above and append the null report to the null prioritized report pool.
following is a iterative process.
we calculate the d eepsimilarity of each unprioritized report to the whole prioritized report pool which is defined as the minimum d eepsimilarity of the unprioritized report to all the reports in the prioritized report pool.
the report with the lowest d eepsimilarity with the prioritized report pool will be moved to the prioritized report pool.
iv.
e valuation a. experimental setting to evaluate the proposed d eepprior we design an empirical experiment.
to complete the experiment we collect crowdsourced test reports from different mobile apps details in table i .
the apps are labeled from a1 to a10 and the number of test reports of different apps ranges from to .
we also invite software testing experts to manually classify the test reports according to the bugs they describe and the average number of reports in a bug category is .
.
based on the crowdsourced test report dataset we build three specific datasets to better support the evaluation includ table i experiment app app no.
app category report bug category report per category a1 finance .
a2 system .
a3 reading .
a4 reading .
a5 system .
a6 finance .
a7 finance .
a8 music .
a9 life .
a10 system .
total average .
ing1 a large scale widget image dataset a large scale test report keyword set and a large scale text classification dataset.
all the dataset build up the integrated dataset group.
in total we design three research questions rq to evaluate the proposed test report prioritization approach d eepprior .
rq1 how effective can d eepprior identify the widget type extracted from the app screenshots?
rq2 how effective can d eepprior classify the textual descriptions from the crowdsourced test reports?
rq3 how effective can d eepprior prioritize the crowdsourced test reports?
b. rq1 widget type classification the first research question is set to evaluate the effectiveness of our processing to the app screenshots.
the most important component of app screenshot processing is the widget extraction and classification.
therefore we evaluate the accuracy of the widget type classification cnn.
different widget images are collected from real world apps and the images have an even distribution in categories.
the details of the cnn is presented in section iii a1.
the dataset is divided by the ratio into the training set the validation set and the test set as the common practice.
after the cnn model is trained we evaluate the accuracy on the test set.
the overall accuracy of widget type classification reaches .
.
specifically speaking we use precision recall and f measure values to evaluate the network.
the calculating formula can be seen as follows where tpmeans true positive samples fpmeans false positive samples tnmeans true negative samples fnmeans false negative samples.
precision tp tp fp recall tp tp fn f measure precision recall the evaluation results can be seen in table ii.
the precision value reaches an average of .
and the lowest precision is .
and the highest is .
.
for recall which measures the total amount of relevant instances that are actually retrieved the average values is .
and the recall values range from .
to .
.
f measure is a harmonic mean of the precision andrecall and it reaches an 952average of .
.
the above results reflect the outstanding capability of the proposed classifier.
table ii widget type classification widget type precision recall f measure but .
.
.
chb .
.
.
ctv .
.
.
edt .
.
.
imb .
.
.
imv .
.
.
pbh .
.
.
pbv .
.
.
rbu .
.
.
rba .
.
.
swc .
.
.
skb .
.
.
spn .
.
.
txv .
.
.
average .
.
.
we also have an in depth insight into the results.
we find that there are two groups of widgets that are easy to be confused.
the first group includes imagebutton and imageview .
it is easy to understand that from a visual perspective these two types can hardly be identified.
the only difference between these types is that imagebutton can trigger an action while imageview is a simple image.
however one important thing to mention is that the in app design developers can add a hyperlink to the imageview widget to realize the equivalent effect.
the second group includes button edittext andtextview .
these three widgets are all a fixed area containing a text fragment which is also visually similar and hard to identify even for humans.
moreover some special renderings make the widgets even harder to identify.
according to our survey we find that these two confusing groups will not affect much and the widgets can be treated as equivalent from both visual perspective and function perspective.
results for rq1 the overall accuracy of the cnn to classify the widget types reaches .
.
for each specific type the average precision is .
the precision is .
and the f measure is of .
.
also according to our survey on real test reports even if some types that are with low precision their visual and functional features will not negatively affect d eepprior .
c. rq2 textual description classification in the processing of the textual description we classify into two categories bug description and reproduction step.
different textual descriptions are considered as different report features.
to classify the textual descriptions we segment the textual descriptions into sentences.
then we feed the sentences into a textcnn model to complete the task and the details are presented in section iii a2.
also to better train and evaluate the network we build a large scale text classification dataset.
the dataset contains pieces of labeled textual segments including pieces of bug descriptions and2 pieces of reproduction steps.
the dataset is divided into training set validation set and test set at the ratio of .
table iii text classification precision recall f measure bug description .
.
.
reproduction step .
.
.
average .
.
.
the results on the test set can be seen from table iii and the overall accuracy of the model reaches .
.
more specifically we use the same measurements like the evaluation in rq1 including precision recall andf measure .
the average precision of types of textual description reaches .
the average recall reaches .
and the he average f measure reaches .
.
the result is quite promising and we also manually check the textual descriptions.
we find that compared with reproduction steps bug descriptions tend to contain bug related words such as crash flashback missing element wrong fail no response etc.while the reproduction steps contain just the operations target widgets and the corresponding responses.
results for rq2 the overall accuracy of text classification reaches .
and the precision recall and f measure are all over .
such results show d eepprior s excellent capability to analyze textual descriptions which also lays a solid foundation for the crowdsourced test report prioritization.
d. rq3 crowdsourced test report prioritization in this research question we evaluate the test report prioritization effectiveness of d eepprior .
the metric we use is the apfd average percentage of fault detected metric which is also used by feng et al.
to prioritize crowdsourced test reports .
in the formula the tfimeans the index of the report that first finds the bug i thenis the total report number and the mis the total number of revealed bugs.
apfd pm i 1tfi n m n to better illustrate the advantage of deepprior we compare deepprior with the following prioritizing strategies ideal this strategy is the best prioritization on theory which means that developers can review all the bugs revealed by the reports in the shortest time.
image this strategy uses only the results of deep image understanding of deepprior to rank the test reports because deep image understanding is a significant part of our research.
bdd iv this strategy refers to the algorithm in feng et al.
s work which is also the state of the art approach for crowdsourced test report prioritization.
random the r andom strategy refers to the situation without any prioritization strategy.
fordeepprior andimage strategy we run once because of the stability of our approach and the trained model will 953table iv deepprior report prioritization result and comparison app no.
ideal deepprior bdd ivdeepprior vs.bdd ivoverhead comparisonimagedeepprior vs.imagerandomdeepprior vs.random a1 .
.
.
.
.
.
.
.
.
a2 .
.
.
.
.
.
.
.
.
a3 .
.
.
.
.
.
.
.
.
a4 .
.
.
.
.
.
.
.
.
a5 .
.
.
.
.
.
.
.
.
a6 .
.
.
.
.
.
.
.
.
a7 .
.
.
.
.
.
.
.
.
a8 .
.
.
.
.
.
.
.
.
a9 .
.
.
.
.
.
.
.
.
a10 .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
not produce different results for various attempts for ideal strategy we manually calculate the apfd because for a fixed report cluster it is a fixed value for bdd ivstrategy we run times and calculate the average value as the original paper and for random strategy we run times to eliminate the effect of the occasional circumstances.
first we compare deepprior with random strategy.
as shown in the table iv we find deepprior outperforms random strategy much ranging from .
to .
and the average improvement reaches .
.
this shows a superiority of deepprior .
then we compare the results of deepprior with the single image strategy.
the average improvement of deepprior is .
and in apps a3 and a4 deepprior outperforms much.
for a8 deepprior is slightly weaker than image strategy.
we review the reports of a8 and find that the textual descriptions are not well written and cannot positively help the prioritization of the reports.
generally speaking the results prove the necessity of combining both text analysis and deep image understanding and a single strategy will compensate each other s drawbacks and improve the prioritization accuracy.
also we make a comparison between deepprior and bdd ivstrategy which is the state of the art approach.
according to the experiment results deepprior outperforms bdd ivwith an average improvement of .
.
the improvements in some apps are especially obvious.
moreover we record the total time overhead from reading the report cluster to the output of the prioritized reports.
it shows that deepprior uses less than half of the time of bdd iv which shows great performance superiority.
another advantage of deepprior over bdd ivis that the deepprior can output stable results while bdd iv s results will float.
according to the detailed results of bdd ivstrategy in the online package we find that bdd ivis quite volatile.
the improvements over baseline strategies vary among different apps and some reasons account for this.
first the report category rate for each app is different so in the limited activity set of an app the recurring of the same activities become frequent.
second different apps have various contents.
for example a1 is a kids education app and it consists of a large number of pictures videos variant texts.such a situation makes it much more complex to extract useful text information and have a thorough understanding of the app screenshot.
as a result the matric would decrease.
results for rq3 deepprior s capability of prioritizing crowdsourced test reports is excellent it outperforms the stateof the art approach bdd iv with less than half the overhead.
also the specific experiment of the image strategy shows the effectiveness of our deep screenshot understanding algorithm.
compared with the state of the art approach deepprior performs much more stable.
e. threats to validity the categories of the apps in this experiment are limited .
our experimental apps cover eight different categories according to app store taxonomy .
the coverage is limited.
however we want to emphasize that due to our deep screenshot understanding involves the layout characterization to the app activity the d eepprior is only suitable for analyzing the apps with a grid layout or a list layout.
we also limit our claim within apps of such layouts.
the enrollment of the crowdworkers is also uncontrolled .
the crowdworkers capability is uncontrolled and low quality reports may occur.
however even if the quality of some reports is low d eepprior can identify the bug it is describing if it actually contains one.
if not d eepprior will categorize the report as a single category and will not affect the prioritization of other reports.
the datasets we construct are of chinese language .
the language of the datasets may be another threat but nlp and ocr technologies are quite strong.
if we replace the text processing engine with that of another language the text processing will also be completed well and will not have a negative impact on d eepprior .
moreover the maturity of machine translation also makes it robust to process crosslanguage textual information.
v. r elated work a. crowdsourced testing crowdsourced testing has been a mainstream testing strategy.
it is significantly different from traditional testing.
testing tasks are distributed to a large group of crowdworkers of different locations and have widely varying abilities.
the most notable advantage of crowdsourced testing is the capability of 954simulating different using conditions and the relatively low economic cost .
however the openness of crowdsourced testing leads to a large amount redundant reports.
the key problem is to improve the developers efficiency to review the test reports.
some researches start from selecting skillful crowdworkers to complete the tasks .
such a strategy is effective while it is still hard to control because even skillful crowdworkers would loaf on the task.
therefore we think it is more important to process the test reports instead of other factors in crowdsourced testing.
liu et al.
and yu proposed approaches respectively to automatically generate descriptions from screenshots for test reports based on the consensus that app screenshots are easy to acquire while the textual descriptions are hard to write for all the crowdworkers.
this idea inspired us to have a deep screenshot understanding of to help better prioritize the test reports.
b. crowdsourced test report processing many researches have been done to process crowdsourced test reports to better help developers review the reports and fix bugs.
basic strategies include report classification duplicate detection and report prioritization.
in this section we will present the related works of different basic strategies.
banerjee et al.
proposed factorlcs utilizing the common sequence matching and the approach is effective on open bug tracking repositories.
they also proposed a method with a multi label classifier to find the primary report of a cluster of reports with high similarity.
similarly jiang et al.
proposed terfur a tool that clusters the test reports with nlp technologies and they also filter out the low quality reports.
wang et al.
takes the features of the crowdworkers into consideration as a feature of the test reports and then make the clustering.
wang et al.
propose the loaf which is the first considering the operation steps and result descriptions separately for report feature extraction.
more researches are done to detect the duplication of the test reports.
sun et al.
use information retrieval models to detect duplicate bug reports more accurately.
sureka et al.
adopted a character n gram based model to complete the duplicate detection task.
prifti et al.
conducted a survey on large scale open sourced project test reports and proposed a method that can concentrate the search for duplicate reports on specific portions of the whole repository.
sun et al.
proposed a retrieval function rep to measure the similarity and the function includes the similarity of non textual fields like component version etc.nguyen et al.
introduced dbtm a tool that utilizes both ir based features and topic based features and detects the duplications bug reports according to technical issues.
alipour et al.
had a more comprehensive analysis of the test report context and improved the detection accuracy.
hindle makes improvements by combining contextual quality attributes architecture terms and systemdevelopment topics to improve bug deduplicate detection.
the above approaches including the report classification and the duplicate detection choose part of the test reports to represent all the test reports.
however we hold that all thereports contain valuable information even if duplicates exist.
moreover after duplicates are detected developers still need to review the reports to carry forward the bug processing.
therefore we think report prioritization is a better choice.
there are also many researches on report prioritization.
zhou et al.
introduced bugsim considering both textual and statistical features to rank the test reports.
drone proposed by tian et al.
is a machine learning based approach to predict the priority of the test reports considering different factors of the test reports.
feng et al.
proposed a series of approaches divrisk and bddiv to prioritize the test reports and they first consider the screenshots of the test reports.
subsequently wang et al.
work further and explore a more sound approach to prioritize test reports with much more attention paid to the screenshots.
among all the above researches only a few of them like and consider the app screenshots which we think is a valuable factor for extracting the features to process the test reports.
but such researches only treat the screenshots as simple matrixes instead of meaningful content.
c. deep image understanding image understanding is a hotspot issue in the computer vision cv field.
in this section we mainly focus on the researches utilizing image understanding in software testing.
lowe proposed the sift algorithm which is used to match the feature points on the target images and calculate the similarity making use of a kind of new image local features which are invariant to image changing including translation scaling and rotation.
optical character recognition ocr is a widely used tool to recognize the texts which is helpful to better understand the images based on the rich textual information.
nguyen et al.
proposed remaui making use of cv technologies to identify the widgets texts images and even containers of the app screenshot.
moran et al.
proposed redraw based on remaui which is more precise to identify the widgets and can automatically generate codes for the app ui.
similarly chen et al.
also proposed a tool to generate gui skeletons from app screenshots with the combination of cv technologies and machine learning.
yuet al.
proposed a tool named lirat to record and replay mobile app test scripts among different platforms with a thorough understanding of the app screenshot.
vi.
c onclusion this paper proposes a crowdsourced test report prioritization approach d eepprior via deep screenshot understanding.
d eepprior transforms the app screenshots and textual descriptions into four different features including problem widget context widget bug description and reproduction step.
then the features are aggregated into d eepfeature including bug feature andcontext feature according to their relativity to the bug.
afterwards we calculate the d eepsimilarity based on the features.
finally the reports are prioritized according to the d eepsimilarity with a preset rule.
we also conducted an experiment to evaluate the proposed 955approach and the results show that d eepprior outperforms the state of the art approach with less than half the overhead.
acknowledgement this work is supported partially by national key r d program of china 2018aaa0102302 national natural science foundation of china fundamental research funds for the central universities and national undergraduate training program for innovation and entrepreneurship 202010284073z .