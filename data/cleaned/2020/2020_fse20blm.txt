baital an adaptive weighted sampling approach for improved t wise coverage eduard baranov universit catholique de louvain belgium eduard.baranov uclouvain.beaxel legay universit catholique de louvain belgium aalborg university denmark axel.legay uclouvain.bekuldeep s. meel national university of singapore singapore meel comp.nus.edu.sg abstract the rise of highly configurable complex software and its widespread usage requires design of efficient testing methodology.
t wise coverage is a leading metric to measure the quality of the testing suite and the underlying test generation engine.
while uniform samplingbased test generation is widely believed to be the state of the art approach to achieve t wise coverage in presence of constraints on the set of configurations such a scheme often fails to achieve high t wise coverage in presence of complex constraints.
in this work we propose a novel approach baital based on adaptive weighted sampling using literal weighted functions to generate test sets with high t wise coverage.
we demonstrate that our approach reaches significantly higher t wise coverage than uniform sampling.
the novel usage of literal weighted sampling leaves open several interesting directions empirical as well as theoretical for future research.
ccs concepts software and its engineering feature interaction software product lines software testing and debugging .
keywords configurable software t wise coverage weighted sampling acm reference format eduard baranov axel legay and kuldeep s. meel.
.
baital an adaptive weighted sampling approach for improved t wise coverage.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
introduction the software has been one of the primary driving forces in the transformation of humanity in the past half century in the modern world software touches every aspect of modern lives ranging from medical legal judicial to policy making.
the widespread and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
usage has led to the design of highly configurable software systems operating in diverse environments.
since software failures can lead to catastrophic effects adequate testing of configurable systems is paramount.
testing of configurable systems adds complexity on top of an already notoriously difficult problem of testing standard software.
in the context of configurable systems every configuration refers to an assignment of values to different parameters.
for the exposition we will restrict our discussion to parameters that only take binary values the techniques proposed in this work are general and applicable to parameters whose possible set of values form a finite set and the benchmarks employed in our empirical study arise from such domains.
the primary challenge in the testing of configurable systems arising from the observation that bugs often arise due to interactions induced by the combination of parameter values.
in the combinatorial testing literature the term feature is often used to indicate a given parameter value.
one such example is an extensive study by abal brabranc and wasowski that identified bugs caused by the feature combinations in the linux kernel.
furthermore modeling of system and environment leads to constraints over the possible set of configurations of interest.
combinatorial testing also known as combinatorial interaction testing cit has emerged as one of the dominant paradigms for testing of configurable software wherein the focus is to employ techniques from diverse areas to generate test suites to attain high coverage.
one of the widely used metrics is t wise coverage wherein the focus is to achieve coverage of all combinations of features of sizet.
a fundamental problem in cit is the generation of test configuration that seeks to maximise t wise coverage which is measured as the fraction of feature combinations appearing in the test set out of the possible valid feature combinations.
the complexity of the problem arises from the presence of constraints to capture the set of invalid configurations.
the holy grail of test generation in cit is the design of test generation methods that can handle complex constraints scale to systems involving thousands of features and achieve higher t wise coverage.
since achieving high t wise coverage can be infeasible for large values of t the practitioners often focus on small values of t wherein t 1corresponds to achieving feature wise coverage.
for long uniform sampling has been viewed as a dominant domain agnostic paradigm to achieve higher t wise coverage as demonstrated by theoretical and empirical analysis .
as an example the accepted solution for splc challenge product sampling for product lines the scalability challenge was uniformesec fse november virtual event usa eduard baranov axel legay and kuldeep s. meel sampler smarch contributed by oh gazzillo and batory .
uniform sampling seeks to sample each valid configuration with equal probability.
recent works present tools that are capable of performing uniform sampling and of generating a partial covering array for large feature models.
uniform sampling provides an excellent heuristic to choose samples that would cover various feature combinations however this approach has a limitation since feature combinations are not distributed equally among the configurations some of them can appear in millions of configurations while others only in tens.
this fact prevents uniform sampling from achieving high t wise coverage.
in a feature model was shown to have only about pairwise coverage achievable with uniform sampling for any reasonable number of samples while coverage required more than 1010samples.
it is perhaps worth highlighting strengths of the uniform sampling approach domain agnosticism and principled use of the progress in sat solving since the problem of uniform sampling is well formulated and fundamental problem with close relationship to other problems in other problems in complexity theory .
in this context one wonders whether there exists a domain agnostic alternative to uniform sampling which can benefit from advances in formal reasoning tools?
the key contribution of this work is an affirmative answer to the above question we present an adaptive literal weighted sampling approach called baital that achieves significantly higher t wise coverage.
our formulation builds on the recent advances in formal methods community in the development of distribution aware sampling for the distributions specified using literal weighted functions a rich class of distributions with a diverse set of applications.
in contrast to prior sampling based approaches that advocate sampling from a fixed distribution baital adapts the distribution based on the generated samples.
therefore baital follows a multi round architecture wherein the initial weight function is uniform and then the weight function is updated after each round.
the update of weight function typically requires the underlying sampling algorithm to redo the entire work from scratch.
to allow the reuse of the computation we adapt the recently introduced knowledge compilation based approach for weighted sampling .
we introduce several strategies for the modification of literalweight function.
through an extensive empirical evaluation on large benchmarks we demonstrate baital achieves significantly higher pairwise coverage than uniform sampling.
as an example the accepted solution to the splc challenge could achieve less than wise coverage while baital can achieve over coverage with just samples.
our extensive comparison among different proposed strategies reveals surprising high coverage achieved by strategies based on limited statistics from the generated samples.
the comparison is focused on pairwise coverage due to the complexity of coverage computation for higher values of t. in summary this work introduces a new paradigm based on adaptive weighted sampling to achieve high t wise coverage that can benefit from advances in knowledge compilation.
the high coverage obtained by baital leads to several exciting directions of future work first we would like to develop a deeper understanding of the theoretical power of literal weight functions in the context of t wise coverage seems a promising research area.
secondly our work highlights challenges of measuring t wise coverage for larger t and the development of efficient algorithmic techniquesto estimate such coverage would be beneficial in evaluating different techniques and aid in the development of more strategies forbaital .
our implementation of baital is publicly available at background .
boolean formulas and weight function a literal is a boolean variable or its negation.
a clause is a disjunction of a set of literals.
a propositional formula fin conjunctive normal form cn f is a conjunction of clauses.
let vars f be the set of variables appearing in f. the set vars f is called support off.
asatisfying assignment orwitness off denoted by is an assignment of truth values to variables in its support such that f evaluates to true.
we denote the set of all witnesses of fasrf.
let var l denote the variable of literal l i.e.
var l var l andf l denotes the formula obtained when literal lis set to true in f. given a propositional formula fand a weight function w that assigns a non negative weight to every literal the weight of assignment denoted as w is the product of weights of all the literals appearing in i.e.
w l w l .
without loss of generality we assume that the weight function is normalised i.e.
w l w l .
the weight of a set of assignments uis given by w u uw .
note that we have overloaded the definition of weight function w to support different arguments a literal an assignment and a set of assignments.
.
t wise coverage the formulation of combinatorial interaction testing cit assigns a variable corresponding to every feature.
let xbe the set of all the variables corresponding to features .
furthermore every configuration 2xcan be represented as conjunction of the set of literals of size x where denotes size of a set.
for example let x x1 x2 x3 then x1 x3 can be equivalently represented as x1 x2 x3 given a configuration 2xrepresented as a set of literals let comb t represent the set of t sized feature combinations due to which is essentially the set of all subsets of literals of the size tin .
for a setu 2x we can extend the notion of comb and denote comb u t ucomb t .
note that while for a given comb t x t but this does not imply comb u t u x t since comb 1 t comb 2 t is not necessarily equal to comb 1 t comb 2 t .
t wise coverage of a set uis defined as a ratio between number of t sized combinations due to uover the total number of t sized combinations.
feature models have constraints over a set of variables defined with a formula f therefore for t wise coverage only configurations that are witness of fare considered.
formally cov u t comb u t comb rf t .
allowing only a fixed number of configurations the coverage optimisation problem searches for a fixed sized set of configurations that maximises the t wise coverage.
given a formula fover x size of feature combination tand allowed number of samples s the problem of opttcover f t s seeks forusuch that u argmax u rf u s cov u t .baital an adaptive weighted sampling approach for improved t wise coverage esec fse november virtual event usa for a given f tands an ideal test generator seeks to solve opttcover f t s .
knowledge compilation we focus on subsets of negation normal form nnf where the internal nodes are labeled with disjunction or conjunction while the leaf nodes are labeled with f alse true or a literal.
for a node v let v andvars v denote the formula represented by the dag rooted at v and the variables that label the descendants ofv respectively.
we define the well known decomposed conjunction as follows definition .
.
a conjunction node vis called a decomposed conjunction if its children also known as conjuncts of v do not share variables.
formally let w1 .
.
.
wkbe the children of nodev then vars wi vars wj fori j. if each conjunction node is decomposed we say the formula is indecomposable nnf dnnf .
definition .
.
a disjunction node vis called deterministic if each two disjuncts of vare logically contradictory.
that is if w1 .
.
.
wn are the children of nodev then wi wj f alse fori j. if each disjunction node of a dnnf formula is deterministic we say the formula is in deterministic dnnf d dnnf and we can perform tractable model counting on it.
3baital achieving higher t wise coverage in this section we first discuss the relationship of opttcover f t s with the classical problem of set cover.
we then discuss the limitations of lifting techniques in the context of set cover to configuration testing due to presence of constraints.
inspired by the classical greedy search in the context of set cover we design baital a novel framework that employs adaptive weighted sampling combined with recently proposed knowledge compilation based sampling approach to achieve efficient sampling routines that achieves high coverage.
the development of baital leads to the need for strategies for update of weight functions to this end we present several strategies that seek to use different statistics from the generated sampling.
.
relationship with set cover as a starting point we observe that the opttcover f t s problem can be reduced to that of the optimisation variant of the classical problem of set cover the variant is also referred to as maximum coverage.
the reduction to set cover allows us to simply modify the classical greedy search strategy to obtain a usuch that cov u t is at least e of the optimal solution u ofopttcover f t s .
we present the algorithm below for completeness and to discuss its simplicity and yet its impracticality in practice.
the algorithm greedycovertion is presented in algorithm .
greedycovertion assumes access to the subroutine maxdistsolu tion that takes in input fands and returns such that algorithm greedycovertion f s 1s 2fori 0tosdo 3 maxdistsolution f s 4s s 5returns argmin rf comb t comb s t the following proposition follows from classical result .
proposition .
.
cov s t e o opttcover f t s where e o .
.
while greedycovertion is a simple algorithm the roadblock lies in ensuring an efficient implementation of maxdistsolution .
recall that the set of comb t x t n t t therefore even obtaining a pn psubroutine for maxdistsolution seems a daunting challenge.
we are not aware of any polynomial reduction ofmaxdistsolution to polynomially many maxsat queries.
it is worth emphasising the input size is f s .
we leave an efficient implementation of greedycovertion as an open question.
.
adaptive weighted sampling it is worth recalling that maxdistsolution seeks to find with the smallest intersection of comb t and the existing comb s t .
the lack of efficient implementation of maxdistsolution makes us wonder if one can employ a randomized sampling based approaches to find that seeks to achieve the goal of maxdistsolution .
the usage of sampling has long been explored in the context of ideal test generation.
in fact random testing is the recommended strategy .
the key idea is to sample solutions of funiformly at random.
the past few years is witness to several efforts to understand the scalability of uniform samplers in the context of cit.
furthermore in response to splc challenge the accepted tool smarch generated a set of uniformly distributed samples and revealed that uniform sampling could reach only pairwise coverage with samples .
moreover the coverage growth with the number of samples was prolonged requiring a million samples to reach coverage.
the weak performance of uniform sampling can be attributed to an uneven distribution of feature combinations among the configurations having 1014configurations one third of literals and consequently all feature combinations involving them are part of less than 106configurations.
the probability of covering these combinations with uniform sampling is minuscule.
in this regard we seek to find different sampling strategies that can achieve higher coverage.
the key contribution of our work is an adaptive weighted sampling based generation of tests baital .
the core architecture of our framework is a multi round process wherein each round seeks to generate samples using a weighted sampler.
in contrast the weight distribution is adjusted based on the samples generated so far.
to accomplish an efficient procedure we need to tackle three challenges esec fse november virtual event usa eduard baranov axel legay and kuldeep s. meel figure feature model of graph library challenge representation of weights over assignment space to represent weights over assignment space we turn to a literal weighted function that assigns a non negative weight to every literal such that the weight of an assignment is the product of the weight of its literal.
the choice of literalweighted function is primarily motivated due to the observation that a wide variety of distributions arising from diverse disciplines can be represented as literal weight function .
challenge dynamic update of weight function since the choice of literal weight function allows us to concentrate on performing the update of only the literal weights our strategy would be to collect statistics corresponding to every literal and then assign the weights accordingly.
challenge efficient weighted sampling techniques that can handle incremental queries prior techniques that seek to employ sampling techniques either fall into uniform sampling methods that solely focus on drawing uniform sampling and hope to achieve higher coverage.
on the other hand techniques that seek to induce bias in the distributions update the weights and require the underlying sampling engine to perform computations from scratch.
in this work our critical insight is to build on recent advances in knowledge compilation a field in artificial intelligence that focuses on the representation of constraints in a representation language into a target language where different queries are often tractable.
in particular we seek to represent the initial set of constraints of finto a representation languagetsuch that we can perform an update of the weights and subsequent sampling in time linear in the size oftwithout needing to perform the compilation process again.
as is discussed in section .
the process of compilation is often the most expensive and therefore our process amortises the cost of compilation with the generation of more samples.
we illustrate the approach and show the difference from uniform sampling on an adapted version of a feature model appeared in .
the model shown in figure describes a product line of graph libraries.
each configuration shall always support edges that could be either ddirected or uundirected and could also provide some aalgorithms like pshortest path and cdeletion of cycles.
the latter can only be applied on directed edges.
this feature model has configuration listed below where we omit features graphlibrary and edges since they must be selected in all configurations d d a p d a c d a p c u u a p these configurations have pairs of literals non selection of a feature negation of a variable can also be part of a feature pair.
in this running example we consider that we can select configurations which is not enough to obtain full pair wise coverage.
uniform sampling selects configurations at random all configurations have equal chances to be chosen.
in the running example we assume that the first two selected configurations are and and we need to choose the third one.
at this step configuration would add only new pairs configurations and would add pairs while configuration would add pairs.
at this step uniform sampling can select any of the remaining configurations with a probability .
.
we now first present an overview of baital and then discuss each of the components in detail in the following sections.
in order to overcome the limitation of the uniform sampling configurations with rare feature combinations shall have a higher probability of being chosen.
indeed the precomputation of weights for all configurations is infeasible.
therefore we are proposing to adjust the weights dynamically by examining already sampled configurations.
this way we can force the sampler to prefer configurations with uncovered feature combinations over the other ones.
the algorithm runs as follows a first few samples are generated according to the uniform distribution of configurations.
after this step the sampler pauses while the examination of the chosen configurations is done.
the algorithm detects which feature combinations have been covered already and chooses the weights for the next round of sample generation accordingly.
the next samples are chosen following the new weight distribution.
the process repeats until the desired number of samples is generated.
for the example from figure at the first round samples and are generated similarly to uniform sampling.
at this step the algorithm discovers that combinations involving d a and phave already be covered therefore their weights shall be reduced while the weight of ushall be raised.
the modification of weights affects probability distribution during the generation of the next sample.
the overall algorithm is shown in algorithm .
it has three input parameters the cnf formula fdefining constraints on a feature model the number of rounds rounds and the number of samples generated at each round spr i.e.
the total number of samples to be generated is rounds spr.
we first compile the formula finto a d dnnft.
at the start of each round line the algorithm generates an assignment of weights to variables that depends on the set of samples generated during the previous rounds.
we discuss generateweights function in details in the next subsections.
the literal weighted sampling algorithm is called to annotate twith weights line and to find sprsamples chosen according to thebaital an adaptive weighted sampling approach for improved t wise coverage esec fse november virtual event usa algorithm adaptiveweightedsampling f rounds spr 1s 2vars set of variables in f 3t compile f 4forroundnb 0torounds do wei hts generateweights vars s annotate t wei hts newsamples sample t spr 8s s newsamples 9returns distribution imposed by generated weights line .
for the first round the weights are always equal to .
corresponding to the uniform distribution.
the new samples are added to the result line and the procedure repeats rounds times.
.
weighted sampling via compilation in it was observed that advances in knowledge compilation can be used to design efficient uniform samplers and designed an efficient uniform sampler kus.
in kus was extended to design a weighted sampler waps .
given an input formula fand the desired number of samples s waps uses a three staged process compilation annotation and sampling .
compilation given a formula f a state of the art compiler is employed to obtain the equivalent d dnnf t. annotation a bottom up subroutine is called to annotate every nodevoftwith its corresponding weight i.e.
the label for the node visw v .
it is worth highlighting that for a formula in d dnnf the labeling of every node with its weight can be accomplished in time linear in the size of t. sampling to perform sampling a top down subroutine is followed wherein for every node once the samples from all the children are generated we randomly shuffle each of the lists and then conjoin the list of samples.
in the case of node we first determine via the binomial coefficient the number of samples each of the nodes should generate and then we take the union of all the lists.
it is known that there exist polynomially sized formulas whose d dnnf representations are exponential .
furthermore the compilation process may require a significantly larger time than the size of resulting t. for example for every formula fthat is valid there exists a polynomially sized d dnnf t but the existence of ptime compilation process would imply p co np.
fortunately our adaptive sampling strategy only modifies the weight function and does not alter f. we make the following key observation about annotation andsampling inwaps that strongly supports our choice of knowledge compilation based approach for baital .
proposition .
.
annotation andsampling can be accomplished in time linear in the size of t proof.
the proof follows trivially from the algorithmic description of annotation andsampling in .
essentially the key observation is that annotation is performed in a bottom up process while visiting each node exactly once while sampling isperformed in a top down manner while visiting each node exactly once.
.
round weights generation in literal weighted sampling configurations are chosen according to the distribution imposed by a weight function w. as shown in section wis defined by assigning a normalised weight to each literal.
for the first round the weight function is constant w l .
for any literal lwhich corresponds to the uniform distribution.
for the following rounds the definition of wis based on the knowledge about constraints f and on a set of already generated samples s. let l t s be a function that represents the knowledge about the t sized feature combinations involving a literal lin the set of sampless.
let h l t f be a function that represents the knowledge about the t sized feature combinations involving a literal lby configurations in rf.
we define the weight function as follows w l t s f f l t s h l t f l t s h l t f where fis a propositional formula lis a literal tis a size of feature combinations sis a set of already generated samples and fis a function outputting a value in the interval .
since the weights of a literal and its negation must be related the weight function is dependent on the feature combinations of the literal negation as well.
each round of baital starts with the computation of w line of algorithm which is further used for sample generation.
there exist multiple ways to define functions f and hresulting in different weight functions and consequently in different t wise coverage.
we have considered several options to define these functions which we call strategies.
in the remaining we assume thatfandtare fixed and omit them.
notice that h l depends only on fixed fandtand therefore is not changing between the rounds and can be computed only once.
for illustration we use the running example from figure where we have selected configurations and and compute the weights for the next round.
algorithm generateweights vars s strategy 1forlinliterals do 2 number of distinct t sized feature combinations with lins h number of distinct t sized feature combinations with linrf his computed in the first round only 4forlinliterals do diff h h wei hts max .
.
sign diff sqrt abs diff 7return wei hts strategy .
the first option is to define andhas the number of distinct feature combinations involving each literal in a set of samples and in rf respectively.
for the computation of hit is not necessary to enumerate all configurations in rf it can be done by checking for each feature combination l1 .
.
.
lt satisfiability ofesec fse november virtual event usa eduard baranov axel legay and kuldeep s. meel f l1 lt. indeed since the sat solver can provide an assignment in case the formula is satisfiable during the computation ofh the full covering array is implicitly built.
however for variables such array would contain millions of samples thus being impractical.
the heuristic behind this strategy is the following if most of feature combinations involving a literal are already covered but it is not the case for the negation of the literal or vice versa then the sampler should prefer configurations involving a negation of a literal in the next round.
we use the difference between ratios of covered feature combinations for literal and its negation as a defining value for the weight.
therefore function fis computing this difference and transforms it into the interval .
algorithm illustrates the weight generation for strategy with a pseudo code.
first andhare computed over lines and .
in the following step fis computed over lines and .
the choice of sqrt based transformation function line is done after the empirical evaluation of several options.
we use max ensures that the resulted weight is not equal to .
strategy involves a lot of computations for functions and h. in particular hchecks satisfiability of 2t v ars f t formulas.
secondly counting the number of distinct combinations in the set of samples has a high time and memory costs for large real world models involving thousands of variables.
therefore we considered several modifications of the weight function that would require fewer resources for computation and evaluated their effect on twise coverage.
in the running example we compute that in samples and d has pairs with a p c c ucovering combinations i.e.
.
during precomputation we learn that dandudo not appear together in any configurations therefore h .
at the same time dhas not been covered so .
applying the algorithm new weight of dis0.
.
similar computations provide the following results for other features wei hts .
wei hts .
wei hts .
wei hts .
.
note that due to normalisation weights for negations of variables can be computed from wei hts wei hts .
applying the extension of weight function to configurations we can compute the probabilities of configurations to be selected.
these weights correspond to .
probability to choose configuration with the best additional coverage as the next sample in comparison to .25probability at uniform sampling.
the worst next sample can be chosen with just0.004probability.
algorithm generateweights vars s strategy 1forlinliterals do 2 number of distinct t sized feature combinations with lins h 2t v ars f t 4forlinliterals do diff h h wei hts max .
.
sign diff sqrt abs diff 7return wei htsstrategy .
in this strategy we attempt to reduce the computation cost by simplifying h. without checking the existence of configurations in rfinvolving each feature combination hcan be overapproximated by the number of t sized feature combinations in2v ars f involving a literal.
the generateweights function for thestrategy is shown in algorithm .
the only difference from algorithm is on line since fand are defined as in strategy .
in the running example we do not precompute that h in this strategy but we get the value as overapproximation.
this changes the resulted weight distribution the algorithm returns the following weights wei hts wei hts wei hts .
wei hts .
wei hts .
.
these weights correspond to .81probability to choose the best next sample .
withstrategy and .01to choose the worst .004withstrategy .
algorithm generateweights vars s strategy 1forlinliterals do 2 s l h rf l 4forlinliterals do diff ln h ln h ln h wei hts max .
.
sign diff sqrt abs diff 7return wei hts strategy .
both strategies and2are trying to optimise the coverage for a particular size tof feature combinations.
indeed the computation of andhdepends on t and for the same set of samples the weights generated for t 2andt 3would be different.
nevertheless one could expect that a set of samples with high pairwise coverage would also have high wise coverage.
this hypothesis allows us to gradually simplify the computation of .
a very simple and easily computable measure of literal participation in a set of samples is the number of samples the literal is present.
it allows finding the number of feature combinations the literal is involved in though not the number of distinct combinations.
similarly hin this strategy is defined as the total number of configurations involving the literal.
hcan be computed by calling sat vars f 1times any configuration involves either a literal or its negation but not both .
since the values of hand can differ by several orders of magnitude we changed f it is comparing the ratio of samples involving the literal in the sample set and the ratio of configurations involving a literal on a logarithmic scale.
the weight generation function is shown in algorithm .
in the running example we see that dappears in both samples and .
for the computation of h we precompute that dappears in configurations and din the remaining ones.
applying the algorithm new weight of dis0.
.
similar computations for other features yield wei hts .
wei hts .
wei hts .
wei hts .
.
these weights correspond to .88probability to choose configuration as the next sample and .008to choose configuration .baital an adaptive weighted sampling approach for improved t wise coverage esec fse november virtual event usa algorithm generateweights vars s strategy 1forlinliterals do 2 s l 3forlinliterals do wei hts max .
.
5return wei hts strategy .
in this strategy we modify strategy by fixing h constant.
the hypothesis of this strategy considers that since values of andhdiffer by several orders of magnitude their direct or indirect comparison might not be useful.
therefore in this strategy we simply compute and compare the appearances of each literal with its negation.
the function is shown in algorithm .
in the running example we see that dappears in both samples and .
applying the algorithm new weight of directed is .
.
new weights for other features are wei hts wei hts .
wei hts .
wei hts .
.
these weights correspond to .
probability to choose configuration as the next sample and only .
6probability to choose configuration .
experiments to analyse the efficiency of our approach we have implemented a prototype of baital in python1.
the sampling process is done by the literal weighted sampler waps .
to evaluate our approach we designed a set of experiments helping to answer the following research questions.
our main goal is to push forward the t wise coverage of the state of the art approach for large spls therefore the first two questions naturally arise.
rq1 canbaital be used to generate partial covering arrays for large spls?
rq2 canbaital achieve higher t wise coverage than uniform sampling?
in addition we are also interested in the evaluation of effects of parameters in our approach on the efficiency and performance.
rq3 how often shall weight function be regenerated in order to obtain best coverage performance?
rq4 what is the impact of different strategies on coverage and performance?
.
benchmarks we took a large number of publicly available feature models from real world configurable systems that were used before in evaluation of uniform sampling tools.
in particular we took the all nonsythetic benchmarks appearing in with exception of few largest feature models on which both uniform sampling and our approach run out of time or memory.
also we excluded small models that have less than variables.
the resulted benchmark set consists of feature models2.
the size of feature models varies between and variables between and clauses and between .
1013and7.
.
the last two research questions we performed a more detailed exploration on benchmarks from our set.
these models are ecosicse11 with variables and clauses from embtoolkit with variables and clauses from transformed into cnf formulas with featureide .
.
and financialservices01 version financial in the rest of the paper with variables and clauses from .
the latter one is particularly interesting since uniform sampling cannot achieve high coverage even with 107samples below for wise coverage and below for wise coverage .
.
experiments settings and competitors we designed two experiments for the evaluation of baital .
the first experiment is used to answer rq1 andrq2 .
in this experiment we generated samples with both uniform sampling and baital for each feature model from our benchmark set.
in baital the number of rounds was set to each round samples have been generated.
we used strategies and4since they do not require additional precomputations of hon each benchmark.
indeed the computation of all feasible pairwise combinations for uclinuxconfig feature model would require millions of sat solver calls.
for the same reason we do not compare the t wise coverage on these benchmarks but the number of distinct feature combinations in the sample sets i.e.
comb s t .
in addition we limited the exploration to the pairwise coverage as the computation of comb for higher wise coverage were exceeding available ram on all but smallest benchmarks.
in the preparation of experiment we tried different tools for uniform sampling namely quicksampler smarch kus andwaps with weights corresponding to the uniform sampling.
quicksampler despite being fast checks the validity of samples at the very last step therefore the number of valid samples is usually lower than the requested one.
in many cases it generated samples none of which were valid.
also it was shown in that the results of quicksampler are often far from uniform.
the remaining tools were able to generate uniformly distributed sets of samples and unsurprisingly the resulted t wise coverage was almost identical.
comparing their execution time kus was the fastest closely followed by waps while smarch was considerably slower by three orders of magnitude on some of the benchmarks.
considering a minor difference between kus andwaps we decided to use the latter one since it would allow us to have better evaluation of the additional complexity of our approach.
rq3 andrq4 are related to the evaluation of baital parameters.
in the second experiment we used ecos icse11 embtoolkit and financial benchmarks.
within this experiment we generated samples with each strategy from section .
and values for the number of samples generated at each round sprwere taken from the set .
in this experiment we checked pairwise coverage and execution time.
the results of uniform sampling were used as a baseline.
due to the probabilistic nature of both uniform sampling and baital all experiments have been run times without fixed random seeds and the reported results show the mean values among runs.
nevertheless for out of benchmarks the difference november virtual event usa eduard baranov axel legay and kuldeep s. meel table number of feature combinations in samples benchmark nvars uniform strategy strategy busybox 1 28 0 dreamcast ecos icse11 freebsd icse11 integrator arm9 mpc50 ocelot pc i82544 refidt334 xsengine between the highest and the lowest values of comb for the same input parameters was below the maximum difference was .
on financial benchmark.
all experiments were conducted on a laptop with intel r core i7 8650u cpu ram limited to 12gb running ubuntu .
.
lts.
.
comparison with uniform sampling rq1 rq2 the first experiment compared the number of feature combinations in sets of samples generated by uniform sampling and by baital with strategies and4.
table presents the excerpt of the results for feature models showing the number of distinct feature pairs in generated samples4.
among benchmarks on of them the strategy showed the increase between and22 of the number of covered feature pairs in comparison with uniform sampling and strategy showed increase between and29 .
one benchmark showed increase by with strategy and49 with strategy .
on benchmarks smaller increase was encountered and remaining benchmarks returned identical results.
detailed exploration of the latter benchmarks showed that on of them uniform sampling reached coverage of at least for the remaining we were not able to compute comb rf t that explains smaller improvement of baital on these benchmarks.
for example busybox 1 28 0 has .
coverage with uniform sampling and baital can still improve the result to .
though the difference in the number of feature combinations is only .
to evaluate the presence of statistical difference between uniform sampling and baital we used mann whitney u test with the h0 uniform sampling distribution and baital sampling distribution are equal.
we have a sample size for both approaches .
.
the critical value to reject h0is i.e.
the hypothesis cannot be rejected if on pairwise comparisons uniform sampling would outperform baital .
on two benchmarks with identical results the u test confirmed the hypothesis while on all other benchmarks theh0have been rejected with u 0andpvalue .
.
the execution time of baital depends on major factors sampling time of each round computation of weight function for the following round and the number of rounds.
the first factor is the 4full version of table can be found at figure number of feature combinations for ecos icse11 benchmark execution time of waps .
as explained in section .
waps execution consists of steps compilation annotation and sampling.
since the formula is not modified during the sampling process compilation is done only in the first round.
on the benchmarks this step takes several seconds with a highest value of seconds and median .
seconds.
the computation of the weight function for the next round depends on the strategy and the number of variables in the formula.
forstrategies and4 as they just compute the number of appearances of a literal in samples it takes less than one second even on the largest benchmark with variables.
strategies and2 on the other hand need to find distinct feature combinations and their execution time also depends on the size of feature combinations.
for pairwise sampling it takes second for variables seconds for variables and up to seconds for the largest benchmark with variables.
note that the set of distinct feature combinations do not need to be recomputed from scratch at each round the results of first rounds can be reused in the latter ones.
therefore the time does not increase with growth of the round number.
for the benchmarks have finished within hour for both strategies another finished within hours.
the remaining benchmarks with the longest annotation process took several hours.
the difference between strategy andstrategy was less than minutes on benchmarks and the maximum was minutes for the benchmark with variables.
as a result of this experiment we can conclude that baital can generate partial covering arrays at reasonable time for large feature models.
the resulted coverage of feature combinations is higher than with uniform sampling on all but benchmarks where equal results were recorded.
.
comparison of weight generation strategies rq3 rq4 figures show the coverage on three benchmarks achieved with each strategy.
the results of uniform sampling are shown for comparison.
for all strategies the weight function has been changed after every samples and the coverage was computed after each generated sample.
the first samples are generated uniformlybaital an adaptive weighted sampling approach for improved t wise coverage esec fse november virtual event usa figure number of feature combinations for embtoolkit benchmark figure number of feature combinations for financial benchmark in all strategies resulting in almost identical coverage.
starting from the second round next sets of samples have been chosen with respect to the new weights generated by baital strategies.
the considerable boost over the uniform sampling can be noticed new weights are helping to choose samples with feature combinations that didn t appear in samples before.
in all three benchmarks samples generated with baital have better coverage than samples chosen uniformly.
the difference between strategies becomes noticeable after second change of weights.
in the first two benchmarks the coverage is quite high even for the uniform sampling and results of different strategies are close.
however in the financial benchmark strategy 1clearly outperforms other strategies while strategy showed the worst result.
in this benchmark almost of feature combinations are infeasible and their distribution among literals is far from uniform.
this results in bad approximation of function h l t and consequently low performance of strategy .strategies and4 showed very close results in all benchmarks therefore we can conclude that extra precomputations in strategy do not improve to the resulted coverage.
figure number of feature combinations for financial benchmark with strategy figure number of feature combinations for financial benchmark with strategy figure number of feature combinations for financial benchmark with strategy the evaluation of the effect of the weights update frequency is shown in figures .
we used the financial benchmark and we omit the plot for strategy since it is similar to figure .
these plots show that higher frequencies allows to obtain high coverageesec fse november virtual event usa eduard baranov axel legay and kuldeep s. meel figure execution time for generation of samples for ecos icse11 benchmark.
uniform sampling took seconds.
figure execution time for generation of samples for embtoolkit benchmark.
uniform sampling took seconds.
with fewer samples.
however by raising the number of generated samples lower frequencies of weight updates can reach the same coverage or even slightly outperform it.
this behaviour is a consequence of our weight generation.
the difference between the weight functions of two consecutive rounds is small if during the former round only few new feature combinations have been covered.
therefore for the first several rounds when a lot of new feature combinations are found high frequency of weight updates is beneficial to reach uncovered areas faster while for the latter rounds there would not be enough new combinations to noticeably modify weights distribution.
the evaluation of execution time on the benchmarks is shown on figures and .
the precomputations of function hare not included in the total time.
strategies using the same function showed close results.
on financial and embtoolkit benchmarks strategies and2are considerably slower than the other ones and the time difference depends on the number of variables.
for ecosicse11 benchmark the annotation of d dnnf took of round time and the different strategies to generate weights had almost no effect on the total time.
regarding the frequency of weight function figure execution time for generation of samples for financial benchmark.
uniform sampling took seconds.
generation clearly extra d dnnf dag reannotations has negative impact on execution time.
for feature models with relatively simple d dnnf dag such as embtoolkit there is no strong effect of this parameter while in ecos icse11 with complex d dnnf the dependency of execution time on the number of rounds is close to linear.
as a result of this experiment we can conclude that strategy can generate the best partial covering array while being the slowest one.
strategy is the fastest one and provide a good coverage outperforming strategies and3.
for the frequency of weights change to generate a small number of samples with high coverage high frequency of weight function generation is preferable however for larger sample sets it might be better to reduce it as similar coverage could be obtained with shorter execution time.
.
threats to validity construct validity.
many prior work use combinatorial interaction testing for feature models and t wise coverage as qualitative metric for partial covering arrays for the cases where the model is too large and complex to compute a full covering array.
our approach is following these works by building partial covering arrays with better t wise coverage.
internal validity.
due to the probabilistic nature of our approach several runs may not yield identical results.
indeed the choice of samples during the first round affects the weights for the following round.
to mitigate the effect of this behaviour on the experiment results we run each benchmark multiple times and report the mean value.
in addition we examined the result of different runs and noticed that the difference between the best and the worst result was below for out of of benchmarks and maximum value was .
.
we also noted that even the worst result was always better than the uniform sampling except two benchmarks where equality was obtained.
for the baseline we used uniform sampling which is a state of the art approach and we have tried several tools for uniform sampling that yield similar results.
external validity.
to mitigate the threat of non generalisability of our study we have used a large number of feature models.
these models cover a wide range in the number of features and constraints.
these benchmarks were used before in several prior studies .baital an adaptive weighted sampling approach for improved t wise coverage esec fse november virtual event usa related work combinatorial interaction testing was initially proposed in 1980s in and has since been extensively studied.
covering arrays formally defined in are matrices with rows representing samples and every possible t sized combination of variables appears at least in one column.
many works on the topic were surveyed in and a more recently in .
the goal in classical cit sampling is to build a covering array and minimise its size.
multiple approaches have been proposed through years for building covering arrays.
the greedy algorithm is one of the most popular ideas starting from the empty set of samples and adding new ones until the full coverage is achieved.
there is a number of tools available including .
another type of greedy algorithm initially build a set of samples with full coverage for the first nparameters and then add an extra parameter at each interaction.
ipo algorithm with several extensions is following this idea as well other some works .
another group of approaches sometimes called heuristical start from some set of samples that do not provide full coverage and try to modify it in order to obtain the full coverage.
examples of such algorithms are tabu search and genetic algorithm .
lin et al.
attempted to combine heuristic and greedy approaches switching between them with a predefined probability which allows a better exploration of configuration space and potentially smaller test suite.
the requirement of explicit access to the entire set of configurations severely limits the above set of works in their ability to handle a large set of features.
covering arrays may grow large for complex configurable systems and exceed the testing budget.
an alternative to the classical cit is building a fixed size set of configurations.
several metrics have been proposed to measure combinatorial coverage of such sets in the nist report .t wise coverage has been defined in the report as total variable value configuration coverage.
one of the approaches to build a fixed size test set is based on selecting solutions for a given set of constraints.
among constraint based approaches one set of techniques often seek to generate a random configuration and then check if the generated configuration satisfies the constraints .
such approaches can handle scenarios where the fraction of valid configurations over the set of all configurations is high but fail to handle cases where the density of valid configuration is low as typically observed in complex systems.
it is worth noting that a low density of valid configurations does not imply a low number of valid configurations.
for example for binary features even if the set of valid configurations is the density of such a set is still .
another set of constraint based approach seeks to rely on employing uniform samplers constructed on top of recent advances in combinatorial solving such as sat solving and knowledge compilation.
oh et al.
employed binary decision diagrams for encoding the configurations of feature models.
the tool smarch uses sat solver for uniform sample generation.
lopez herrejon et al.
evaluated the dependency between t wise coverage and the number of samples building a pareto front.
two uniform sampling tools quicksampler and unigen2 have been evaluated in .other techniques to build sample sets and their flavors have been surveyed in .
several other approaches to choose samples for analysis of feature models including t wise sampling one disabled and statement coverage were compared in .
another application of the sampling of configurable systems is performance prediction as different features and their interactions can affect the performance.
in recent work kaltenecker et al.
proposed a new metric and the corresponding sampling method tailored for performance prediction.
the problem of weighted sampling has witnessed sustained interest from theoreticians and practitioners alike for the past three decades owing to its widespread usage.
of several proposed techniques monte carlo markov chain mcmc based methods enjoy widespread adoption owing to their simplicity but the heuristics employed to act as a proxy of mixing of the underlying markov chains however invalidate distributional guarantees .
approaches based on interval propagation and belief networks also often lead to scalable techniques but their distributions can often be far from the desired distribution.
.
while hashingbased techniques have achieved significant progress in the context of uniform sampling their scalability for arbitrary distributions is severely limited .
in this context we advocate the usage of knowledge compilation based techniques which not only achieve significant scalability but supports efficient adaptive sampling.
conclusion design of test generation techniques to achieve higher t wise coverage is a fundamental challenge in the context of testing of configurable systems.
in this work we propose a baital approach for construction of test sample sets that dynamically modifies the probability distribution of configuration to be chosen in order to improve the t wise coverage of the test set.
we showed on a large set of benchmarks used in prior works that our approach generates achieves higher t wise coverage than commonly used uniform sampling.
we explored several ways to update the weight function and their effect on the resulted coverage.
since our approach generates samples for combinatorial interaction testing an interesting direction of future work would be to study the impact of improved t wise coverage on fault detection.