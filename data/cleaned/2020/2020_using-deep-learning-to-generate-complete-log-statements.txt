using deep learning to generate complete log statements antonio mastropaolo antonio.mastropaolo usi.ch seart software institute universit della svizzera italiana switzerlandluca pascarella luca.pascarella usi.ch seart software institute universit della svizzera italiana switzerlandgabriele bavota gabriele.bavota usi.ch seart software institute universit della svizzera italiana switzerland abstract loggingisapracticewidelyadoptedinseveralphasesofthesoftwarelifecycle.forexample duringsoftwaredevelopmentlogstatements allow engineers to verify and debug the system by exposing fine grained information of the running software.
while the benefits of logging are undisputed taking proper decisions about where to inject log statements whatinformation to log and at which log level e.g.
error warning iscrucialfor theloggingeffectiveness.in thispaper wepresentlance logstatementrecommender the first approach supporting developers in all these decisions.
lance features a text to text transfer transformer t5 model that has been trained on java methods.
lance takes as input a java method and injects in it a full log statement including ahuman comprehensible logging message and properly choosing theneededloglevelandthestatementlocation.ourresultsshow that lance is able to i properly identify the location in the code wheretoinject thestatementin .
ofjavamethodsrequiringit ii selectingtheproperloglevelin66.
ofcases and iii generate a completely correct log statement including a meaningful logging message in .
of cases.
ccs concepts softwareanditsengineering softwaremaintenancetools .
keywords logging empirical study machine learning on code introduction inspecting log messages is a popular practice that helps developers inseveralsoftwaremaintenanceactivitiessuchastesting debugging diagnosis and monitoring .
developers insert log statements to expose and register informationabout the internal behavior of a software artifact in a humancomprehensiblefashion .thedatageneratedisusedforruntime andpost mortemanalyses.forexample whendebugginglogstate ments can support root cause analysis while once the softwareisdeployedlogscanbeusedforperformancemonitoring or anomaly detection .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
technically possible logging everything e.g.
every exception is inefficient and impracticable .
on the one hand a coarse grainedloggingriskshidingruntimefailures missinglog messages useful for diagnoses .
ontheotherhand afine grainedloggingrisksincreasingthe overhead of log management and analysis .
to optimize the quantity and quality of data generated developers insert log statements in strategic positions specify appropriate log levels e.g.
error debug info and define compact but comprehensible text messages.
nonetheless it remains a non trivial task for developers to decide where what and at which level to log .
for these reasons researchers have proposed techniques to supportdevelopersindecidingwhatpartsofthesystemtolog the loglevelforloggingstatements andthestructureof logmessages .forexample jia etal.
proposedanapproach basedonassociationrulestoplaceerrorlogsaftercodebranches such asifstatements.
li et al.
studied the use of topic modeling for log placement at method level.
also two recent workstackled challenges related to log statements writing by adopting deep learning dl models.
liet al.
withdeeplv pushed the boundaries of log recommendation by suggesting and fixing log levels of already typed log statements.
deeplvrelies on an ad hoc dl network that combines syntactic i.e.
ast andcontextual i.e.
logmessage information extractedfromcodetosuggestanalternativelogginglevelwhen needed.
li et al.
also proposed a second dl based approach to provide fine grained i.e.
at the code block level suggestions aboutwheretoaddloggingstatements.themodelcapturesboth syntactic and semantic information of the source code and returns abinaryvalueindicatingwhethertoaddornotalogstatementina given block.
while achieving great performance these techniques onlypartiallysupportdevelopersinloggingpractices.indeed none of them can generate complete log statements providing to the developer i thelocationwheretoinjectit ii thecorrectloglevel touse and iii theactuallogstatementalsofeaturingtheneeded natural language log message.
inthispaper wepresentlance logstatementrecommender anapproachaimedatexploitingtherecentlyproposedtext to texttransfer transformer t5 model to automatically generate and inject complete logging statements in java code.
we started by pre training our model on a set of java methods.
the pre traininghasbeenperformedthroughtwopre trainingobjec tives.thefirstisaclassic maskedtoken objective inwhichwe randomly mask of the code tokens in the java methods asking the model to guess them.
this provides the model with general knowledge about the java language including logging statements.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa antonio mastropaolo luca pascarella and gabriele bavota thesecondpre trainingobjectiveprovidesasinputtothemodel ajavamethodfromwhichlogstatementsoriginallypresentinit have been removed with the t5 in charge of guessing where a log statementisneededbyaddingaspecial log stmt token.this providesthemodelwithadditionalknowledgeabout wherelogging is needed.
once pre trained the model is fine tuned to generate completelogstatements.inparticular givenajavamethodasinput to lance we ask it to inject a complete log statement where needed.
this means that lance must generate a complete log statement and inject it in the proper location.
inourevaluation weaskedlancetoautomaticallygenerate log statements and compared them to the ones manually writtenbydevelopers.wefoundthatlanceisableto i correctly predicttheappropriatelocationofalogstatementin65.
ofcases ii select a proper log level for the statement in .
of cases and iii generateacompletelycorrectloggingstatement includinga meaningful natural language message in .
of cases.
besides suchaquantitativeanalysiswereportanddiscussqualitativeexamples of correct and wrong predictions generated by the model.
significance ofresearch contribution.
ourworkrepresents astepaheadintheautomatedsupportprovidedtodevelopersfor logging activities.
indeed lance is the first technique able to generatecompleteloggingstatementsandtoinjectthemintheright code location.
lance is complementary to techniques suggesting which parts of the system to log since it assumes that the method provided as input always needs a logging statement.
inother words we do not tackle the problem of deciding whether a code component in our case a java method needs a loggingstatement but we assume that such a decision has already been takenby anotherapproachor bythedeveloper itself.lancecan then take care of injecting the needed logging statements.
we publicly release the code implementing our model and all dataandadditionalscriptsusedinourstudyinacomprehensive replication package .
lance log statement recommender westartbyoverviewingthet5modelthatisatthecoreoflance andbyexplaininghowweexploiteditfortheautomationoflogging activities section .
.
then we describe the process used to build the datasets needed for its training hyperparameter tuning and evaluation section2.
.section2.3detailsthetrainingofthemodel andthehyperparametertuningweperformedtoidentifythebest configurationtouseinourexperiments.finally section2.4explains how predictions are generated once the model has been trained.
.
text to text transfer transformer t5 raffelet al.presented t5 as a model that can be used to tackle any natural language processing nlp task that can be expressed inatext to textformat.thisbasicallymeansthatboththeinputand the output of the model are text strings.
a single t5 model canbetrainedtosupportmultipletasks suchasmachinetranslation e.g.
from english to franch and question answering.
the t5 demonstrated state of the art performance onseveral nlp benchmarks .also ithasbeensuccessfullyusedtoautomatecoderelated tasks .wedonotdiscussallthearchitecturaldetailsoft5 thataredocumentedin .however itisworthmentioningthat asthename suggest the t5 is a transformer model exploiting attention layerstoweightthesignificanceofthedifferentpartscomposing the input strings.
this is particularly useful when dealing withcode related tasks since t5 can detect hidden and long ranged dependenciesamongtokens withoutassumingthatnearesttokens are more related than distant ones.
forexample thetokensrepresentingthedeclarationofalocal variable in the first statement of a method are related to the tokens implementing a return statement in which the value of such a variable is returned despite the fact that the two statements could be far apart .
inourwork weexploitthespecificarchitecturereferredbyraffel etal.
ast5 small.indeed theauthorspresentdifferentversions of the t5 small base large billion and billion differing in complexity size and consequently training time.
the t5 smallwe adopted features a total of 60m parameters allowing reasonabletraining times with the hardware resources at our disposal.
thecode implementing the t5 model is available in our replication package .
.
.
instantiating t5 to automate logging activities.
thet5model is trained in two phases i pre training which allows defining a shared knowledge base useful for a large class of text to text tasks e.g.
guessing masked words in english sentences to learn about the language and ii fine tuning which specializes the model on specificdownstreamtasks e.g.
machinetranslationtask .bothpretrainingandfine tuningcanbeperformedinamulti tasksetting i.e.
a single model is trained on several tasks .
fig.
depicts the tasksweadoptforthet5pre trainingandfine tuning whilethe building of the needed datasets is detailed in section .
.
figure pre training and fine tuning tasks authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using deep learning to generate complete log statements icse may pittsburgh pa usa pre training.
wetargetedatwo foldgoalforthet5pre training i providethemodelwithknowledgeabouttheunderlyingpatterns of the java language and ii allow the model to learn where a log statement is needed in a given java method without focusing at this stage on the log level and the message to print.
concerningthe first point our first pre training task task p1 in fig.
is a classicdenoisingtask inwhichwerandomlymask15 ofcode tokens in each instance i.e.
a java method asking the model to guess the masked tokens.
the second task task p2 in fig.
instead asks the model topredictthecorrect position ofalogstatementwithintheinput method.
basically we provide as input to the model a java method thatoriginallyhad nlogstatements n withn 1logstatements i.e.
wecompletelyremoveonelogstatement .then weaskthe model to predict where the removed log statement was and to injectin thatpositiona special log stmt tag.this meansthat a method having nlog statements will appear ntimes in the pretraining dataset each time with a different log statement removed.
fine tuning.
oncethemodelispre trained thefine tuningtask task ft in fig.
specializes the model to the specific problem we target namely the injection of complete log statements.
also in this case the input is represented by a java method that originally hadnlog statements n from which we completely remove onelogstatement i.e.
sameinputastask p2 .however asoutput we expect the model to inject the actual log statement in the right position choosingtherightloglevelandameaningfullogmessage.
.
building the training datasets wedetailtheprocessusedtobuildboththepre trainingandthefinetuningdataset.weminedjavaprojectsongithub byleveraging the search tool by dabic et al.
.
the querying user interface allows to identify github projects that meet specific selection criteria.
we selected all java projects having at least commits contributors stars and not being forks to reduce the chance of mining duplicated code .
the commits contributors stars filters aim at discarding personal toy projects.
instead the decision of onlyfocusingonjavaprojectssimplifiesthetoolchainneededfor our study and allows to train the model on a coherent code corpus.
thisprocessresultedin5 473candidateprojects.wesuccessfully clonedthelatestsnapshotof5 459ofthem someprojectscannot be cloned since they were deleted or made private .
then to foster evenmorethecohesivenessofourdataset wedecidedtoselectonly projects declaring a dependency towards apache log4j a wellknown java logging library.
to identify these projects we firstly checked whether a pom project object model file1was present in theproject sdirectory.ifthiswasthecase weparsedittocheck whether it featured a log4j dependency.
if no pom file or no log4j dependency was found the project was discarded.
we found java projects having an explicit dependency towards log4j.
wethenused srcml toextractfromtheseprojectsalltheir java methods 10m .
of these methods do not have log statements.
this still provides us with 320k methods featuring at leastonelogstatement.then wefilteredoutallmethodshaving tokens 512or tokens where tokensrepresentsthenumber of tokens composing a method excluding comments .
1a pom file is used in maven to declare dependencies towards maven libraries.the filter on the maximum number of tokens is needed to limit the computational expense of training dl based models similarchoices have been made in previous works .
finally we remove duplicate methods from the dataset to avoid overlap betweentrainingandtestsetswebuiltfromthem.thisleftuswith methods that we used to create the pre training and the fine tuning datasets summarized in table .
table num.
of methods in the datasets used in our study datasettrain eval test w log w a log w log w log pre training dataset task p1 task p2 fine tuning dataset we used all the methods not having log statement w a log in table1 tobuildthedatasetneededforthepre trainingtask p1 i.e.
thedenoisingtaskinwhichwerandomlymask15 oftokens .
for the pre training task p2 i.e.
guessing the correct position of alogstatement weused50 ofmethodswithlogstatements a method featuring nlog statements is present ntimes in the dataset each time with a different log statement removed.
theremaining50 ofmethodsfeaturingalogstatementhave beenusedinsteadforbuildingthefine tuningdataset.thelatter has been split into training evaluation and test.
the evaluation has been used to perform the hyperparameter tuning of the model section .
while the test set represents the instances onwhichtheperformanceoflancehavebeenassessed i.e.
its ability to generate correct log statements in the right location .
.
model training and hyperparameter tuning the pre training has been performed for 300k steps.
we used a 2x2 tputopology 8cores fromgooglecolabtopre trainthemodel with a batch size of .
as a learning rate we use the inverse squareroot withthecanonicalconfiguration .wealsousedthe pre training dataset and english sentences coming from thec4dataset totraina sentencepiece model i.e.
atokenizer for neural text processing .
we decided to train the tokenizer onboth java code and english natural language to make sure it can deal with complex log messages.
we set its size to 32k word pieces.
once pre trained the model can be fine tuned.
however before that we performed the same hyperparameters tuning used by mastropaolo et al.
when employing the t5 for code related tasks we do not tune the hyperparameters of the t5 model for thepre training usingthedefaultones butweexperimentwith four different learning rates namely constant learning rate c lr slanted triangular learning rate st lr inverse square learningrate isq lr and polynomial learning rate pd lr .
our repli cation package reports the exact setting used for each of the experimented learning rates e.g.
the constant learning rate was set to .
etc.
.
before detailing the hyperparameter tuning we must anticipate that in our study section we assess the performance of lance in four different pre training scenarios.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa antonio mastropaolo luca pascarella and gabriele bavota first to verify the impact on performance of the pre training phase we perform an ablation study in which the model is not pretrained butdirectlyfine tuned nopre trained scenario .second wepre trainamodelbyemployingamulti taskpre trainingsetting inwhichbothourpre trainingtasks i.e.
task p1and p2infig.
areused multi task .finally weassesstheperformanceoft5when pre trained by only using task p1 denoising task or task p2 logstmt task .
once pre trained all models are fine tuned and compared.
this allowstoassessthecontributiontoperformance ifany brought by the different pre training strategies.
having four different training scenarios and four possible learning rates the hyperparameter tuningrequired building models.
wefine tunedeachmodel i.e.
eachconfiguration for100ksteps.
then we compute the percentage of correct predictions i.e.
cases inwhichthemodelcaninjectacorrectlogstatementintheright position achievedintheevaluationset.theachievedresultsare reported in table .
the best configuration of each scenario in boldface istheonethathasbeenusedinourstudytoassessthe lance s performance after fine tuning the models for 200k steps.
table t5 hyperparameter tuning results experiment c lr st lr isq lr pd lr multi task .
.
.
.
logstmt task .
.
.
.
denoising task .
.
.
.
no pre training .
.
.
.
.
generating predictions once the t5 model is trained it can generate predictions using differentdecodingstrategies.forthisfirstworkonlogstatement generation we decided to target the greedy decoding strategy predictionsaregeneratedbyselectingthetokenwiththehighestprobability of appearing in a specific position at each time step.
this meansthatforagiveninput asinglepredictionisgenerated i.e.
the one considered the most likely by the model .
study design thegoalof this study is to evaluate the performance of lance inautomaticallygeneratingandinjectingcompleteloggingstatements in javamethods.
the contextis represented bythe datasets described in section .
we aim at answering the following research questions rqs rq1 to what extent is lance able to correctly inject completelogging statements in java methods?
with rq 1we aim at assessingtheperformanceofthetrainedt5modelsingener atingandinjectinginthecorrectpositionloggingstatements in unseen java methods.
besides quantitatively answer this rqbyreportingthepercentageofcorrectpredictionsgenerated by lance we manually inspected the generated log statements to discuss interesting cases of correct and wrong predictions.rq2 how do different pre training strategies impact the performance of lance?
rq2analyzes the impact of different pretraining strategies on lance s performance.
in particu lar we experiment with the four t5 variants described insection2.
nopre training multi task logstmt task and denoising task.
.
data collection and analysis toanswerrq 1andrq2werunagainstthetestset thebestperformingconfiguration section2.
ofthefourmodelsoutput of the different pre trainings.
then we assess the accuracy of the predictions generated by eachmodel.inthisregard werelyonthecode i.e.
logstatements manuallywrittenbydevelopersasagroundtruth.thisisacommon practice concerning the definition of the oracle i.e.
the output the model is expected to generate .
hence first wecompute the percentage of correct predictions namely cases in whichlancecorrectlysynthesizesthelogstatement i.e.
boththe loglevelandthelogmessagewerecorrect whileinjectingitinthe correctpositioninthemethod i.e.
thesamepositionadoptedby developers .
successively we assessed the extent to which lance generates partially correct predictions.
in particular there arethree important components that the t5 model has to predict when it comes to the addition of a log statement its level message and position location in the method .
we compute the percentage ofcasesinwhichlancewasabletocorrectlypredict i atleast oneofthesethree components e.g.
thelogleveliscorrect butthe messageaswellasthepositionaredifferentfromthereference and ii atleasttwoofthe components e.g.
theloglevelandmessage arecorrect butthestatementisinjectedinthewrongposition .thethirdscenario i.e.
allthree components arecorrect isrepresented by the previously discussed correct predictions.
the number of instancesinthetestsetis12 whereeachinstancerepresents oneofthe7 125javamethodsintable1withaspecificlogstatement removed one method can have multiple log statements .
manual analysis.
on top of this quantitative analysis we also performed qualitativeanalyses aimedat better understandingthe strengths and weaknesses of lance.
besides reporting interesting casesofperfectpredictionsgeneratedbyourapproach wemanually inspected a set of wrong predictions generated by the model.in particular we focused on wrong predictions in which the log level and the location were correct.
this means that the difference betweenthegeneratedandthetargetlogstatementwasthenatural languagemessage.suchadecisionwasdrivenbythegoalofour manualanalysis aimedatunderstandingwhetherthegeneratedlog while different represented a good alternative to the reference one.
this is unlikely to happen if the log level or location is different from the target or at least it is tough to judge for people not directlyinvolvedinthedevelopmentofthecodeinwhichthelog statement is injected such as the authors who inspected these wrongpredictions .forthesereasons werandomlyselected300wrong predictions in which however the log level and the log locationwerecorrectlyguessedbythemodel.then twoauthors manually inspected the original method with the log statement writtenbythedevelopersandthesamemethodwiththestatement generatedbylancetoclassifyitinoneofthefollowingcategories authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using deep learning to generate complete log statements icse may pittsburgh pa usa same information the generated log statement is semantically equivalent to the target one.
this could happen in the case in which the log messages of the two statementsexpress the same information but with different wordings e.g.
exception thrown when invoking method getpaper vs getpaper thrownanexception .itevenhappens aswewill show that lance proposes a more expressive log message as compared to the one used by developers.
meaningful the generated log statement includes an articulatedmessagethatcanbeunderstood butitisnotequivalent to the one present in the target log statement.
meaningless thegeneratedlogstatementincludesamessage that is meaningless considering the context i.e.
the method in which it has been injected and or the logging message cannot be comprehended.
thetwoauthorsanalyzedeachofthe300statementsindependently from each other agreeing on the classification of logstatements.theremaining111 representingconflicts have been solved by a third author not involved in the first stage.
the instances to manually validate have been selected from the predictions generated by the best performing model.
such a qualitative analysis is fundamental in our study since the quantitativemetricsweadoptconsideraswrongpredictionsalsocasesin whichthepredictedandthereferencelogstatementsareverysimilarorequivalent e.g.
theydifferforonewordinthelogmessage .
results discussion tosimplifythediscussionoftheresults weanswerourresearch questions together through a quantitative and qualitative analysis.
.
quantitative analysis table reports the results achieved by the four experimented modelsoutputofdifferentpre trainingstrategies i.e.
multi task logstmt task denoising task and no pre training in terms of correctpredictions.table3showsthecorrectpredictionsforallcombi nationsofthethree components topredict i.e.
loglevel position where to inject it and log message .
in other words we analyze casesinwhich i atleastoneofthethreecomponentstopredict wascorrect e.g.
atleastthelevel ii atleasttwowerecorrect e.g.
level andlocation and iii theentire log statementwas correctly synthesized level position and message .
table can be read as follows.
each row includes three symbols below the three components to predict.
the check mark below a component cindicates that the results reported in that row refer tologstatementsinwhich cwascorrectlypredicted.thedashmark instead indicates that ccan be either correct or wrong for the predictions in that row.
finally the cross mark indicates that thecomponentwaswronglypredictedforthecorrespondinglog statements.
for example the very first line level position messa e indicates that the row reports the percentage of predictions in which the log level was correctly predicted independently from the prediction of the position and message which could be correct or wrong.
instead the third row level position messa e reports cases in which the level was correctly predicted but the guessed position and the generated message were wrong.
the row labeledwith all representsthemostchallengingscenariosince it implies that the generated log statement was identical to the referenceoneinallitsparts.finally thelastblackrowshowsthe percentage and the absolute value of the instances in the test set that cannot be parsed with a javaparser due to syntax errors.
concerning rq the best performing model is the one pretrainedbyusingonlythedenoisingtask i.e.
ofmaskedtokens .
indeed itsperformanceissubstantiallysuperiortot5pre trained in a multi task setting.
for example in terms of completely correct predictions thismodelachievesa15.
of perfect logstatements versus the .
of the multi task model.
also the denoising task model is the one generating the lowest number of syntactically wrong log statements .
.
while such a result might seem surprising initially a possible explanation could be the specific pre trainingandfine tuningweperformed ourfine tuningtask requiringthemodeltogenerateandinjectacompletelogstatement in a java method is quite similar to the logstmt task pre training task.indeed thelatterasksthemodeltoinjecta log placeholder inthepositioninwhichalogstatementhasbeenremovedinajavamethod.thus itispossiblethatsuchanadditionalpre trainingdid not benefit the model s performance.
more in general concerning the role played by the pre training we observed a substantial boost of performance only in the case of thedenoising task .
ofperfectlypredictedlogstatements .
the other two pre trainings only marginally improved the performanceofthebasemodel seetable3 .inthefollowing wefocus on the best performing model.
lance correctly predicts the log level in .
of cases while thepositioninwhichalogstatementshouldbeinjectediscorrectlyguessedin65.
ofcases.thistwo foldachievement i.e.
loglevel andposition suggeststhatlancecaneffectivelysupportdevelop ers with logging activities.
looking at the third row in table it is insteadclearthatlancestrugglestogenerateloggingmessages that are identical to the ones manually written by developers success in .
of cases .
this difference in performance among the three log statement components to predict is kind of expected.
indeed loglevelandthepositionhaveaquitesmallsearchspace the log level can assume one out of six possible values in log4j trace debug info warn error and fatal while the position in which a log statement can be injected is bounded to the number oftokenscomposingthemethods beingatmost512.instead the natural language log message can be written in many different forms reducing the chances of obtaining two identical sentences.
in of cases both the level and the position are correctly guessed with however a wrong logging message while in an addi tional15.
allthree components arecorrectlysynthesized.the generation of the logging message basically acts as a sort of upper bound for the performance of lance with .
of generated logs having a correct message and an overall .
of completely correctlogstatements i.e.
level position andmessagearecorrect .
while lance performs quite well in predicting the log level andposition thereisstillalargepercentageoflogstatementsfor which their prediction fails.
however the magnitude of the error made in the prediction can substantially vary for both these tasks.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa antonio mastropaolo luca pascarella and gabriele bavota table correct predictions considering the three dimensional challenges of injecting log statements.
log predictions level position messagepre trainingno pre trainingmulti task logstmt task denoising task1 out of .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
out of .
.
.
.
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
wrong syntax figure log level distance in the predictions generated by lance.
zero indicates predictions having a correct level.
concerning the log level the six levels defined in log4j can be sorted based on their priority trace debug info warn error and fatal.
for example the infolevel is used for logginginformationalmessagesandtrackingthebehaviorofthe running software e.g.
methodmstarts the execution .
in contrast the last level fatal is designated for logging severe errors or in other words for logging the behavior likely to compromise the execution of the software e.g.
by causing a crash .
awronglevelpredictionmadebylancecouldrecommendthe usageofinfoinsteadof fatalaswellastheusageof errorinstead offatal.
however these two errors have a different magnitude with the first completely misleading the developer while the latter resultinginasub optimal butstillacceptable logleveldecision.
fig.
depicts a histogram showing the number of instances in our testsetforwhichtheloglevelpredictionhadadistancefromthe target level going from i.e.
the level was correctly predicted figure3 distanceintermsof tokensbetweenthepositionofthelogstatementinthepredictionandinthetarget.zeroindicates predictions injected in the right position.
to5 i.e.
theworst casescenarioindicatinga tracerecommended instead of fatalorvice versa .
while we report the results achieved byallmodels alsointhiscase wefocusourdiscussiononthebestperforming one denoising task .
note that not all instances from our test set are depicted in fig.
.
indeed besides the ones containing syntax errors for the denoising task we also had to exclude instances for which the model did not recommend a valid log level making impossible the computation of the distance.
asitcanbeseen besidestheinstanceswithacorrectlypredicted level mostoftheerrorsarejustonelevelfarfromthetarget .
instances while very rarely the difference is higher than two .
instances .
fig.
depicts the same analysis performed however for the prediction of the location where to inject the log statement.
in this case thex axisreportsthedistance incodetokens ofthepredicted location from the target location.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using deep learning to generate complete log statements icse may pittsburgh pa usa the reported numbers must be interpreted as up to ntokens far e.g.
up to up to etc.
.
as it can be seen the wrong locationsaremostlyinareasclosetothetargetlocation besides the correctly predicted an additional fall within code tokens while only are more than tokens far.
finally we investigated whether the performance of lance is affected by the level of the log statement.
indeed it is possible that specific types of log statements are easier to predict than others.
table4reportsforeachloglevelandforeachoftheexperimented modelvariants i thepercentageofpredictionsinwhichthelog level was correctly predicted independently from the prediction of the location and message column all and ii the percentage of completely correct predictions column corr.
pred.
.
afirstobservationthatcanbemadefromtable4isthatlance provides good and similar performance across all log levels.
the highest percentage of correct predictions is for the errorlevel while the lowest is for the warnlevel.
indeed focusing on the best performing model for .
of errorinstances in the test set thelogleveliscorrectlyinferred againstthe60.
ofthe warn instances.
similarly the percentage of completely correct predic tions moves from .
error to .
warn .
this finding is consistent across all models and we believe it is due to the simpler messagesusuallyadoptedinstatementsloggingerrors.toverify such a conjecture we computed the number of characters com posing log statements having different levels.
we found that on average errorinstancesarecomposedby 70characters against the ofwarninstances.
however these numbers only tell part ofthe story.indeed we foundthatthe infolevel which isthesecond worst in terms of performance features statements composed onaverage by 79characters.thus despitebeingsimilarinsize compared to the errorinstances we still observed a drop of performance.webelievethatthisresultisduetothefactthatmessagesininfologstatementsaremuchmorevariegatedascomparedtoerror messages.finally itisworthhighlightingthegoodperformance achievedatthe debuglevel whichpointstothepossibleusageof techniques such as lance in supporting bug localization activi ties by recommending the injection of proper debug statements.
clearly additionalperformanceimprovementsareneededbefore considering lance ready to be adopted by developers.
the analysis of the wrong predictions is difficult to perform quantitativelyforlogmessagesasdoneforthelevelandtheposition.
one possibility is to compute the bleu bilingual evaluation understudy score between the generated and the reference messages.
bleu is used to assess the quality of an automatically generatedtext.suchascorerangesbetween0.0and1.
with1.0in dicatingthatthegeneratedandthereferencemessageareidentical.
weadoptthebleu 4variant whichcomputestheoverlapinterms of grams between the generated and the reference messages.
concerningthebest performingmodel similarfindingsholdfor the other models we obtained an average bleu of .
.
however such a number is difficult to interpret since there is no accepted threshold above which an automatically generated textisconsideredofgoodquality.forthisreason werelyonthe qualitative analysis introduced in section .
and discussed in the following.
.
qualitative analysis amongthe300 wrong predictionsanalyzed wefound85ofthem .
to report the same information of the target predictions i.e.
the log message was different but semantically equivalent .
toincludea meaningful butnotequivalentmessage and to include meaningless messages.
thus in the set of predictionsweconsidered wrong inourquantitativeanalysisduetothedifferentlogmessagegeneratedascomparedtothereference one we can estimate a of predictions that are still valuable.
to better understand the capability of lance in generating log statements we report in fig.
three examples of i correctpredictions top part of the figure in which the generated log statementisequivalenttotheonewrittenbydevelopersinallits parts and ii wrongpredictions classifiedinourmanualanalysis as reporting the same information of the target prediction.
wesummarized the methods in order to only show statements thatare relevant to understand the injected log statement irrelevant statements are replaced by .
concerning the correct predictions we just show the method with the generated log statement that as said is identical to the one written by the developers.
in the first example lance injected a statement to log the state of the msgobject.
what it is interesting about this instance is that the model understood theneedforinvokingthemethod arrayconverter.bytestohexstringin order to obtain a human comprehensible representation of the logged state.
in the second instance lance inferred that if the ifcondition is not satisfied i.e.
value instanceof nsdictionary this indicates an unexpected value type for the passed parameter key .
in other words the model mapped the instanceof operator to possible issues related to object types.
the last correct prediction in fig.
3shows instead the ability oflancetocomposelogmessagesbyusingtheappropriatesyntax needed to concatenate several parameters to string elements.
inthis case the log statement is just aimed at reporting when the execution of a specific method starts.
moving to the wrong but semantically equivalent predictions bottom part of fig.
instance 4shows an interesting case in whichthelogmessagesynthesizedbylance i.e.
exceptiontrying todeletesubscription sconfigurationforsubscriptionid iseven moredetailedthantheonewrittenbydevelopers i.e.
couldnot deletesubscriptionfor .in5 instead theoppositeoccurs i.e.
the manuallywrittenmessageismoredetailed withthetwomessages however communicating similar information.
finally thelastexample 6showstwomessagesonlydiffering foroneword activated vs active .thisexampleisrepresentativeof manyinstanceswefoundinwhichdifferenceswereevensmaller.
forexample weobservedcasesinwhichtheonlydifferencewas the usage of letter case.
indeed in our quantitative analysis we decidedtobeconservative consideringapredictionascorrectonlyifitmatchedthereferenceoneevenintermsoflettercase.thiswas donetoavoidconsideringascorrectpredictionsmakingawrong usage of camelcase.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa antonio mastropaolo luca pascarella and gabriele bavota examples of correct predictions public void writesignaturehandshakealgorithm certificaterequestmessage msg appendbytes msg.getsignaturehashalgorithm .getvalues logger.debug signaturehashalgorithm arrayconverter.bytestohexstring msg.getsignaturehashalgorithm .getvalue public nsdictionary objectforkey final string key final nsobject value dict.objectforkey key if value instanceof nsdictionary return nsdictionary value log.warn string.format unexpected value type for serialized key s key return null public connectionconsumer createconnectionconsumer final destination destination final serversessionpool pool final int maxmessages throws jmsexception if activemqralogger.logger.istraceenabled activemqralogger.logger.trace createconnectionconsumer destination pool maxmessages throw new illegalstateexception ise examples wrong but semantically equivalent predictions i.e.
same information 4private synchronized cswsubscription deletecswsubscription string subscriptionid throws cswexception try serviceregistration sr registeredsubscriptions.remove subscriptionid catch exception e logger.debug exception trying to delete subscription s configuration for subscription id logsanitizedid e logger.debug could not delete subscription for logsanitizedid e lance log statement target log statement public handlerresult handle processstate state processinstance process secret secret secret state.getresource string secretvalue secret.getvalue if stringutils.isnotblank secretvalue try secretsservice.delete secret.getaccountid secret.getvalue catch ioexception e log.error error deleting secret secret.getid e.getmessage log.error failed to delete secret from storage secret.getid e return null lance log statement target log statement public static void sendapplicationinstanceactivatedevent string appid string instanceid if log.isinfoenabled log.info publishing application instance activated event appid instanceid log.info publishing application instance active event appid instanceid lance log statement target log statement5 figure examples of log statements generated by lance authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using deep learning to generate complete log statements icse may pittsburgh pa usa table percentage of correct predictions by log statement level.
column all reports the percentage of predictions having thecorrectloglevel independentlyfromthecorrect wrongpredictionoflocationandmessage column corr.pred.
reports the percentage of completely correct predictions.
levelt5 multi task t5 logstmt task t5 denoising task t5 no pre training all corr.
pred.
all corr.
pred.
all corr.
pred.
all corr.
pred.
trace .
.
.
.
.
.
.
.
debug .
.
.
.
.
.
.
.
info .
.
.
.
.
.
.
.
warn .
.
.
.
.
.
.
.
error .
.
.
.
.
.
.
.
fatal .
.
.
.
.
.
.
.
threats to validity we discuss the threats to the validity of our study.
.
construct validity in our study we use the original code written by developers inour case log statements as oracle assuming that it represents a goodtargetforourmodel.thisassumptionhasbeenmadeinmany previousworksapplyingmachinelearningoncode .
however it is likely that both the training and the evaluation test datasets contain suboptimal log statements.
in terms of training we expect the dl model to be able to deal with such a noise not learning unusual logging practices.
however when it comes to the evaluation and test set this assumptioncanhaveastronginfluence sinceweconsiderapre diction correct only if it is equal to the log statement written bydevelopers.
to at least partially address this threat we manually analyzed a sample of wrong predictions reporting the percentage of them still being valuable while different from the reference.
.
internal validity we used the default t5 parameters from the original paper duringitspre trainingandlimitedthehyperparametertuningto thefine tuning phaseand inparticular tothe learningrates e.g.
we did not variate the model architecture in terms of number of layers .thiswasdoneduetothehighcostoftrainingseveraldifferentmodels.weacknowledgethatexperimentingwithadditionalconfigurations maylead to better results.
on topof this itis worthmentioningthatweemployedthesimplestt5architecture i.e.
the smallone proposedbyraffel etal.
.largermodelsarelikely to push forward the results we achieved.
.
external validity whilethedatasetsusedinourstudyfeaturesthousandsofinstances welimited our experimentsto javacode and more specifically to projects relying on the log4j library.
thus our results are valid for thisspecificcodepopulationandwedonotclaimgeneralizability for other languages and logging frameworks.
however excluding the building of the datasets that focused on a specific context there are no parts of our approach that are customized for java and or for the log4j library.
thus lance can be easily adapted and experimented in other contexts.
related work wefocusourdiscussionon i empiricalstudiesonloggingpractices and ii approachesproposedintheliteraturetosupportdevelopers in logging activities.
due to lack of space we do not discuss themany recent applications of deep learning to automate various softwareengineeringtasks pointingthereadertothesystematic literature review by watson et al.
.
.
empirical studies on logging practices yuanetal.
conductedoneofthefirstempiricalstudyonlogging practices in open source systems analyzing c and c projects.
theyshowthatdevelopersmakemassiveusageoflogstatements andcontinuouslyevolvethemwiththegoalofimprovingdebugging and maintenance activities.
fuet al.
studied the logging practices in two industrial projects at microsoft investigating in particular which code blocks aretypicallylogged.theyalsoproposeatooltopredicttheneed for a new log statement reporting a f score.
chen and zeng et al.
extended the study of yuan et al.
to java and android systems respectively.
in particular chen analyzed java based open source projects while zeng et al.considered open source android apps mined from f droid.
both studies confirmed the results of yuan et al.
finding a massive presence of log statements in the analyzed systems.
zhiet al.
investigated how logging configurations are used andevolve distilling10findingsaboutpracticesadoptedinlogging management storage formatting and configuration quality.
other researchers studied the evolution and stability of log statements.
for example kabinna et al.
examined how developers of four open source applications evolve log statements.
they found thatnearly20 oflogstatementschangethroughoutthesoftware lifetime.
zhouet al.
explored the impact of logging practices on data leakage in mobile apps.
in addition they propose mobilogleak to automatically identify log statements in deployed apps that leak sensitive data.
their study show that of the analyzed apps leak sensitive data.
recently li etal.
conductedanextensiveinvestigationon logging practice from a developer s perspective.
the goal of this researchistopushthedesignofautomatedtoolsbasedonactual developers needs rather than on researchers intuition .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa antonio mastropaolo luca pascarella and gabriele bavota theauthorssurveyed66developersandanalyzed223loggingrelated issue reports shedding light on the trade off between costs and benefits of logging practices in open source.
the results show thatdevelopersadoptan adhocstrategytocompensatecostsand benefits while inserting logging statements for various activities e.g.
debugging .
the above described papers lay the empirical foundations for techniquessupportingdevelopersinloggingactivities including ourwork .approachessuchaslancecanhelpinreducingthecost of logging while supporting developers in taking proper decisions when they wish to add log statements.
.
approaches on logging activities researchersproposedtechniquesandtoolstosupportdevelopers in logging activities.
logmessageenhancement.
yuanet al.
proposed logenhancer as a prototype to automatically recommend relevant variablevaluesforeachlogstatement refactoringitsmessagetoinclude suchvalues.theirevaluationoneightsystemsdemonstratesthat logenhancer can dramatically reduce the set of potential root failure causes when inspecting log messages.
liu et al.
tackled the same problem using however a customized deep learning network.
their evaluation showed that the mean average precision of their approach is over .
log placement.
other researchers targeted the suggestion of the best code location for log statements .
for example zhuet al.
presented logadvisor an approach to recommend wheretoaddlogstatements.theevaluationof logadvisorontwo microsoft systems and two open source projects reported an accuracyof60 whenappliedonpiecesofcodewithoutlogstatements.
yaoetal.
tackledthesameprobleminthespecificcontextof monitoring the cpu usage of web based systems showing that their approach helps developers when logging.
lietal.
proposedadeeplearningframeworktorecommend logginglocationsatthecodeblocklevel.theyreporta80 accuracy in suggesting logging locations using within project training with slightly worst results in a cross project setting.
c ndido et al.
investigated the effectiveness of log placement techniques in an industrial context.
their findings e.g.
of accuracy show that models trained on open source code can be effectively used in industry.
log level recommendation.
a third family of techniques focusonrecommendingtheproperloglevel e.g.
error warning info for a given log statement .
mizouchi et al.
proposed padlaasanextensionforapachelog4jframeworktoautomatically change the log level for better record of runtime information in case of anomalies.
the deeplv approach proposed by li et al.
uses instead a deep learning model to recommend the level of existing log statements in methods.
deeplv aggregates syntactic and semantic information of the source code and showed its superiority with respect to the state of the art.
lance as compared to the above discussed techniques is able torecommendcomplete logstatementsandwhereto inject them providing a more comprehensive support to software developers.
conclusion and future work we presented lance the first approach able to synthesize complete log statements and inject them in the right code location.
lance is built on top of the text to text transfer transformer t5 model .webuiltadatasetcomposedof 7mjavamethods thathavebeenusedfortrainingt5andtestingitsperformance.we experimentedwithdifferentpre trainingstrategies showingthat a simple denoising task i.e.
the model is asked to guess masked tokens in java methods allows t5 to achieve good performance.
inparticular thebest performingmodelcangeneratecompletely correct log statements and inject them in the proper code location in15.
ofcases withbetterperformanceachievedinthesimpler tasks of selecting a proper log level .
and code location .
.
we also showed through manual inspection of a sample of wrong predictions thattheresultsofourquantitativeanalysisare a lower bound for lance s performance.
indeed a non negligible set of log statements classified as wrong due to differencesbetween thegenerated andthetarget logmessage actually represents valuable recommendations.
despitetheencouragingresults weacknowledgethatlance is just a first attempt in automatically generating complete log statementsandadditionalimprovementsareneededbeforeitcan beconsideredasavalidsupportfordevelopers.thisobservation guides our future research agenda that will focus on improvinglance sperformance.thiscouldbeachievedin differentways.first wewanttoexperimentwithmorecomplex and multi modal source code representations that have beenshowntoboosttheperformanceofdltechniquesin code relatedtasks .second weplantoconsideradditional pre trainingobjectivesandtostudytheroleplayedbythe sizeofthetrainingdatasetonlance sperformance.indeed itispossiblethatlargerdatasetssubstantiallyimproveperformance or that instead our dataset was already sufficient for the learning with its extension only leading to marginal improvements.
finally we want to enlarge our search space intermsofhyperparameterstooptimizethet5performance.
closing the circle by providing full logging support .
while lance can generate complete log statements it cannot decide whether a log statement is needed or not in a given method.
this limitation can be addressed in two ways.
first by delegating such a decision to other techniques only in chargeofdecidingwhich partsofasystemto log .second bytraininglancetoalsosupportsuchatask.thiscan be done by including in the fine tuning dataset a mixture of methods featuring and not featuring log statements asking themodeltodecidewhenalogstatementisneeded.forthis firstwork wedecidedtonottacklesuchaproblemdueto themanydifferentaspectsweneededtoexploreonlyforthe problem of log statement generation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using deep learning to generate complete log statements icse may pittsburgh pa usa data availability all the code and data used in our study is publicly available in our replicationpackage .inparticular weprovide i thecodeneeded topre train fine tune andrunthet5modeltogeneratepredictions ii the datasets we built for the model training evaluation and testing iii all predictions generated by the different variants of the experimented model and iv additional information needed to replicateourstudy e.g.
theexactvaluesusedforthelearningrates during hyperparameter tuning .
acknowledgment this project has received funding from the european research council erc under the european union s horizon research and innovation programme grant agreement no.
.