hints help finding and fixing bugs differently in python and text based program representations ruchit rawal victor alexandru p adurean sven apel adish singla mariya toneva max planck institute for software systems saarbr ucken germany saarland university saarbr ucken germany corresponding author rawalruchit22 gmail.com vpadurea mpi sws.org apel cs.uni saarland.de adishs mpi sws.org mtoneva mpi sws.org abstract with the recent advances in ai programming assistants such as github copilot programming is not limited to classical programming languages anymore programming tasks can also be expressed and solved by end users in natural text.
despite the availability of this new programming modality users still face difficulties with algorithmic understanding and program debugging.
one promising approach to support end users is to provide hints to help them find and fix bugs while forming and improving their programming capabilities.
while it is plausible that hints can help it is unclear which type of hint is helpful and how this depends on program representations classic source code or a textual representation and the user s capability of understanding the algorithmic task.
to understand the role of hints in this space we conduct a large scale crowd sourced study involving participants investigating the effect of three types of hints test cases conceptual and detailed across two program representations python and text based and two groups of users with clear understanding or confusion about the algorithmic task .
we find that the program representation python vs. text has a significant influence on the users accuracy at finding and fixing bugs.
surprisingly users are more accurate at finding and fixing bugs when they see the program in natural text.
hints are generally helpful in improving accuracy but different hints help differently depending on the program representation and the user s understanding of the algorithmic task.
these findings have implications for designing next generation programming tools that provide personalized support to users for example by adapting the programming modality and providing hints with respect to the user s skill level and understanding.
index terms program comprehension debugging programming modalities hints crowd sourced study i. i ntroduction recent advances in generative ai in particular foundation models trained on text and source code have the potential to make programming more accessible.
most notably tools such as github copilot and chatgpt enable end users to solve programming tasks through different modalities including natural language text and pseudo code specifications.
thus programming is not limited to classical programming languages such as python or c anymore and programming problems can be expressed and solved by end users in natural text.
this shift is particularly significant in the evolving landscape of software engineering where new and diverse user groups are no longer relegated to peripheral roles but are increasingly taking a central role in developing applications many of which are now being created outside traditional it departments by employees with limited or notechnical development skills .
while these tools have enabled new forms of programming modalities users still require algorithmic thinking and debugging skills to solve their programming problems .
thus there is a need to develop tools that can assist users with algorithmic understanding as well as finding and fixing bugs.
a series of recent works have explored how generative ai can be leveraged to support users with various forms of programming hints to help them find and fix bugs while also forming and improving their programming capabilities .
on the one hand several works have proposed techniques that can provide tutor style natural language hints on the other hand they also investigated how to design informative test cases for program comprehension and debugging .
however the role and merits of hints have been studied only for a classical programming setting and it is unclear whether and how the helpfulness of hints depends on the program representation.
given the fact that ai programming assistants broaden the population of users it is further open whether and how hints should be adapted to a user s skill level and degree of understanding of the algorithmic task.
in this paper we study the interplay of hint types and program representations regarding their usefulness for endusers.
more concretely we investigate the effect of three types of hints test cases conceptual and detailed across two program representations python and text based and two groups of users with clear understanding or confusion about the algorithmic task .
we center our study around the following three research questions rq1 how does the program representation affect a user s ability to find and fix bugs and how does this depend on the user s understanding of the task?
rq2 what is the utility of a hint on the user s ability to find and fix bugs across different program representations and how does this depend on the user s understanding of the task?
rq3 what types of hint are more suitable for different program representations and how does this depend on the user s understanding of the task?
to answer these research questions we conducted a largescale crowd sourced study involving 753participants.
in thisarxiv .12471v1 dec 2024palindrome string given a string s as input write an algorithm to check whether it is a palindrome or not.
a string is a palindrome if it reads the same backward as forward.
the algorithm should return true if s is palindrome and false otherwise.
q1 select the expected output of a correct algorithm for the following test case racecar 1def ispalindrome self s length len s if length return false for pos in range length if s !
s return false return true python representation orthe algorithm first calculates the length of the provided string s by counting the number of characters in it.
then it checks whether the calculated length is odd.
if so it returns false and terminates.
if the length of the string is even the algorithm proceeds to compare characters from the first half of the string with the second half of the string.
it starts from the beginning and the end character moving towards the center checking if each pair of characters that are equidistant from the center matches.
if any pair does not match the algorithm returns false and terminates.
if the algorithm has not terminated after finishing going through the string the algorithm returns true and terminates.text based representation none ortest case passing input abba output true test case failing input abcba output falsetest case ora string can be a palindrome regardless of its length.conceptual hint orremove the check that returns false if the string s length is odd.detailed fix q2 select the output of the buggy algorithm for the following test case racecarq3 which of the following algorithmic snippets highlight the location where changes are sufficient to fully correct the algorithm?q4 which of the following algorithmic snippets highlighted edits fix the bug?fig.
an illustrative example from the study showcasing an algorithmic task.
after showing a task the user is asked to answer a question related to understanding of the task q1 .
afterward the user is shown a buggy program in python or text based representation possibly along with a hint.
then the user is asked to answer questions related to bug understanding q2 bug finding q3 and bug fixing q4 .
these questions are posed as multiple choice questions the answer options are not shown in the figure for brevity.
study a participating user is presented with an algorithmic task along with a buggy program in python or text based representation and is asked to find and fix bug s in the provided program possibly with the help of a hint.
we illustrate the experiment flow in figure .
we measure the utility of a hint primarily in terms of a user s ability to successfully find fix bugs through a set of multiple choice questions we also consider an increase in speed for accurate responses as a secondary indicator of helpfulness.
we summarize the takeaways for each research question in figure .
our results for rq1 show that the text based program representation leads to better program debugging when considering users who demonstrate a clear understanding of the algorithmic task.
regarding rq2 we found that hints for text representations do not appear to be helpful in improving user accuracy.
on the other hand hints for python representations offer several advantages they improve user accuracy regardless ofwhether users have clear understanding or are confused they help reduce the accuracy gap between python and text representations for users with a clear understanding they also reduce the accuracy gap between users who have clear understanding and who are confused about the algorithmic task.
finally when investigating the interplay of program representation and hint types for rq3 we found that detailed fixes are generally the most helpful across all representation modalities and user types and conceptual hints are particularly beneficial for users with a confusion about the algorithm task when working with python representations.
our results have implications for designing next generation programming tools that can provide personalized support to users that is adapted to their experience and understanding.
for instance our results indicate that users with a clear understanding of the algorithmic task can benefit from text based program representation.
moreover our results showcase howuser group clear vs. confusedprogram representation test vs. python clear confused for text python text python for clearrq1 effect of program representation and user understandinghint vs. no hint confused hint confused for python clear hint clear for python python hint text for clear confused hint clear for pythonrq2 effect of hints presence and interaction with program representation and user understandingtype of hint detailed fixes no hint for python clear text confused conceptual hint no hint for python confusedrq3 effect of hint type and interaction with program representation and user understandingfig.
visual summary of main findings to our research questions.
circular nodes represent main factors of variation program representation user group hint presence and hint type .
rectangular blocks contain key takeaways color coded by research question rq1 purple rq2 red rq3 green .
connecting lines illustrate how factors combine to address different research questions.
user accuracy can be improved by providing different types of hints and adapting them according to the programming modality and the user s understanding of the task.
our main contributions are as follows we analyze the helpfulness of various hint types for two different program representations.
we conduct a large scale study to investigate how different hint types and modalities affect a user s ability to find and fix bugs in a program.
we examine how hint types and program modalities can be adapted to a user s understanding.
we formulate a set of explicit hypotheses that are meant to inform further work in this field.
we provide all raw data and analysis scripts https github.com bridge ai neuro hintscodetext ii.
r elated work a. text vs. code for program comprehension several studies have examined the cognitive processes that support a programmer s ability to understand programs written in code or natural language finding shared and distinct cognitive mechanisms and differences in reading strategies .
most previous studies have focused on passive comprehension paradigms where the participants were asked to simply read the program.
a notable exception is karas et al.
who studied the functional connectivity in the brain during the writing of program code and prose.
however this study used different problems in the code and prose conditions which makes it difficult to directly contrast the results.
our work complements these previous findings and focuses on the effect of program representation on finding and fixing bugs.b.
difference in program understanding for different users most research on program comprehension does not consider inter personal differences of programmers nor differences in their skills levels of understanding the problem at hand and backgrounds.
in fact most studies work with students as study subjects.
notably a line of research concentrates specifically on the effect of programming experience on program comprehension.
for example uesbeck et al.
suggest that lambda expressions may hinder program comprehension but only for novices not experienced programmers.
stefik and siebert found that syntactic constructs that are difficult for novices may guide teachers in choosing the appropriate language to start with.
burkhardt et al.
studied the effect of expertise on program comprehension.
they found that the models experts build from a program and task differ from the models novices build indicating different cognitive processes or mental modelling strategies involved.
vessey studied the difference between experts and novices in debugging tasks.
she found that experts adopt a holistic system view and use breadth first search whereas novices do not using depthfirst search .
in the same vein d etienne notes that experts incorporate object oriented and functional relationships in their reasoning whereas novices focus on objects only.
in a family of experiments dieste et al.
found that years of experience are a suboptimal predictor of programmer performance academic background and specialized knowledge of task related aspects are better predictors.
in contrast to these previous works we define groups of users based on their understanding of the specific algorithmic task which we measure empirically in the beginning of the same experiment.c.
supporting users with various forms of programming hints a variety of assistive techniques have been considered in the literature that support users with programming hints with the goal of helping them find and fix bugs.
prior to recent developments in generative ai automated techniques primarily focused on hints presented in the form of bug fixes because of challenges in automatically generating high quality natural language hints .
another line of research investigated crowd sourcing approaches to obtain hints provided by other learners or tutors .
recent developments in generative ai have led to a surge in automated techniques that provide tutor style natural language hints for example by providing conceptual hints without revealing details about fixes thereby considering aspects of forming user s programming capabilities .
moreover automated generative techniques have been proposed that provide effective error messages for syntactical errors or design informative test cases for a given buggy code .
however these works have considered hints only for classical programming settings and it is unclear how the helpfulness of hints depends on programming modality representations and a user s skill level and capability to understand the problem at hand.
our work complements these works and focuses on understanding the interplay of hint types and program representations regarding their usefulness for end users.
iii.
m ethodology a. experiment design to investigate how different program representations and hint conditions affect participants ability to understand identify and fix bugs we conducted a large scale crowd sourced study with a total of participants.
we visualize the design flow of the study in figure .
the study featured two primary types of program representation conditions text based description and python code.
for each program representation type there are four possible hint conditions no hint test cases hint conceptual hint or detailed fix hint resulting in a total of eight distinct conditions.
each participant was randomly assigned to one of these eight conditions.
within their assigned condition participants were presented with two algorithmic tasks each focusing on one of five different problems see section iii c for details about the problems .
participants took an average of .76minutes to complete the survey.
we recorded participants responses and response times for each question as well as the time they spent on each step of the survey including reading and understanding content on pages like the program page and the hint page.
before beginning the study we asked participants to fill out a short demographics survey that included questions about their experience with programming self rated python programming skills on a scale of to and familiarity with english reading comprehension skills on a scale of to .
our participant pool predominantly consisted of individuals from english speaking countries such as the usa who rated themselves very highly on english readingcomprehension.
regarding the programming questions the participant pool was skewed towards individuals with less selfreported programming experience.
we found the self reported measures of programming experience and skills unreliable as there was no strong relationship between these measures and the participants understanding of the algorithmic task i.e.
accuracy on q1 therefore we decided not to use these self reported measures for the main analyses and instead group participants by the data derived measure of q1 accuracy which indicates whether the participant correctly understood the problem description see sec.
iii g for more information about how this grouping was used in our analyses .
b. utility metrics we focus on two metrics to quantify the utility of program representation and the provided hints for program understanding and debugging accuracy we assess participants performance by computing the average accuracy over multiple choice questions q2 q3 and q4 as they cover different aspects of successfully debugging a program.
the range of average accuracy ranges from 0to1 with theoretical chance accuracy of .
q2 options q3 options q4 options .
time taken we measure the average response time of participants when answering questions q2 q3 and q4 correctly.
this includes the time taken to read the question review the answer choices and submit the response.
this metric allows us to determine whether accuracy gains come at the cost of efficiency inversely proportional to time taken or in addition to it.
by evaluating both accuracy and time taken we gain a holistic understanding of how program representations and hints affect program debugging .
c. stimulus design since we target end users we use 5basic computer science algorithmic tasks ranging in difficulty commonly used in cs1 education programming websites and literature .
the problem titles with brief descriptions are as follows sum positive values calculate the sum of the positive values in the input list a. count nonnegatives and negatives check whether there are more non negative values than negative values in an input list a. print average rainfall print the average of nonnegative integers representing daily rainfall amounts in the input list a. palindrome string check whether the input string s is a palindrome.
fibonacci to n print the list of numbers in the fibonacci sequence till the input number n. we have 5instances of buggy programs for each algorithmic task to ensure that our approach generalizes beyond specific implementations of the algorithmic task leading to atotal of 25program instances.
figure shows an example of one instance of a buggy algorithm for the palindrome string problem.
we present both the python and text based representations for this instance.
the bug is that the algorithm mistakenly treats all odd length strings as non palindromes.
the figure also shows the three types of hints for this instance.
we provide examples of the other algorithmic tasks on our github repository.
we provide details on how we constructed the python and text representations of the programs and the hints below python representation condition to obtain the buggy programs in python representation for palindrome string and fibonacci to n problems we took buggy python attempts from recent literature used to benchmark generative ai models these are adapted from attempts publicly available on the platform geeksforgeeks.com .
next to obtain the buggy programs for sum positive values count nonnegatives and negatives and print average rainfall we manually plant bugs starting from the correct python solution based on the bugs we have encountered in our experience of working with students.
as it is unclear how syntactic bugs can be reflected in text based representations we opt to include solely buggy codes with semantic bugs.
specifically we first identified key sub objectives necessary for correctly solving specific algorithmic tasks.
for example in the print average rainfall task previous research has highlighted several essential objectives handling negative inputs negative summing the inputs sum determining the number of inputs count addressing cases with zero inputs divzero and calculating the average average .
we then designed bugs that cause the program to fail in one or two of these sub objectives.
this approach ensures that the bugs target specific functional aspects of the task leading to meaningful failures.
examples of other such bugs include wrongly initializing counter or accumulator variables starting to iterate from index 1instead of index using wrong comparison logic in conditionals and so on.
the buggy codes we include can be fixed with a few localized changes.
text representation condition for each python program we carefully craft a corresponding text based representation that describes the python program in natural language without using any programming concepts such as variables loops etc.
these descriptions underwent several rounds of internal iterations to ensure accuracy clarity and faithfulness to the original python code.
hint conditions as previously discussed each participant is assigned one of four hint conditions no hint test cases hint conceptual hint and detailed fix hint.
the hint conditions are carefully chosen to target different aspects of overall bug understanding being grounded in providing support to students in programming education .
the test case hint includes two input output pairs one representing a success case and the other a failure case with minimal differences in their input.
conceptual hints highlight the underlying issue present in the program without suggesting specific changes while detailed fixes focus solely on the necessarychanges without explaining the underlying issue.
importantly these hints are independent of the program representation and are applicable to both text based and python representations for a given problem type.
the authors dedicated multiple iterations to handcrafting these hints ensuring their clarity and applicability across both representations.
d. multiple choice questions our study consisted of four multiple choice questions per problem type to assess participants understanding and evaluate the utility of hints.
more specifically q1.
q. select the expected output of a correct algorithm for the following test case .
.
.
?
this question had four options with one correct answer and three distractors resulting in chance accuracy of .
.
q2.
q. select the output of the buggy algorithm for the following test case .
.
.
?
this question also had four options with one correct answer and three distractors resulting in chance accuracy of .
.
q3.
q. which of the following algorithmic snippets highlight the location where changes are sufficient to fully correct the algorithm?
this question consisted of three options each highlighting different portions of the program with one correct answer and two distractors resulting in chance accuracy of .
.
q4.
q. which of the following algorithmic snippets highlighted edits fix the bug?
this question also had three options each featuring highlighted modifications in the program with one correct answer and two distractors resulting in chance accuracy of .
.
all questions remained consistent across different hint conditions.
q1 and q2 were consistent across different program representation conditions as they focused on the input output behavior of an ideal and buggy program respectively which is consistent across both program representation conditions for a particular algorithmic task.
q3 and q4 involved identifying and fixing bugs by selecting the option with the correct portion highlighted.
these questions varied across different program representations in the text based condition highlights were on the text stimuli and in the python condition highlights were on python stimuli.
however we ensured that the underlying content remained the same for both questions the locations and fixes in the python version corresponded to text descriptions in the text based version.
we provide the quantitative questions and answer choices for the algorithmic task example shown in figure in our github repository.
e. participant recruitment our experimental study was designed and implemented using qualtrics.
we recruited participants via cloud research a crowdsourcing platform that builds on top of amazon mechanical turk but applies a series of stringent filters and quality control measures to significantly enhance the reliability of the participant pool.
our study received formal approvalfrom the ethics review board of our institute ensuring that all ethical considerations were met.
before beginning the tasks each participant provided informed consent.
the study was structured so that each participant was expected to complete it within 30minutes.
participants were compensated at an hourly rate of 12usd.
f .
participant demographics our study was conducted over two weeks and included a total of valid respondents.
participants were male were female identified as non binary and the remainder preferred not to disclose their gender.
the median age of participants was with ages ranging from to .
the average programming experience in the past five years was .
years with a minimum of years and a maximum of years.
the average self rated python programming skill level was .
on a scale from to .
g. data analysis as described in section iii a we asked each participant to complete two independent algorithmic tasks each involving a different problem but the same type of program representation and hint.
additionally we applied a second threshold per question to flag and exclude responses indicative of guessing based on a conservative estimate from an internal pilot survey.
of the responses initially collected participants were excluded.
the remaining responses had an average response time of seconds and a median of .
seconds.
for our data analysis participants were first grouped according to their understanding of the problem description as evidenced by their accuracy on q1 participants who answered q1 correctly were labeled as the clear understanding group while those who answered incorrectly were labeled as the confused understanding group.
to ensure our dataset consisted of independent observations and minimized withinperson variance we averaged the scores of the quantitative questions q2 q3 q4 across both task responses for participants consistently classified as either clear or confused in both tasks.
if a participant fell into different groups across tasks we randomly selected one of the two responses and discard the other response from all the analyses.
similarly for average time results as discussed in section iii.b we only focus on participants from the clear group and only include responses where q2 q3 and q4 were all correctly answered.
for each participant if q2 q3 and q4 were correctly answered in both tasks we averaged the response times across the two tasks.
if these questions were correctly answered in only one task we used the time from that task.
importantly all other conditions such as hint type and program representation were consistent across the two tasks based on our study design.
this process ensured that our dataset includes only one data point per participant aligning with the assumptions required for the statistical tests we will discuss shortly.
participants were also grouped based on the program representations they were assigned python vs. text .
when analyzing the effect of hints we aggregated over the fig.
accuracy of participants when presented with textbased vs. python based program representations and no hints.
the bar plot represents the mean q2 q4 average accuracy with the vertical lines indicating the standard error of the mean.
the red dotted line represents chance accuracy and significant differences between program representations are indicated with an asterisk .
surprisingly clear participants perform significantly better when presented with text based representations than python based representations.
three different hint types to compare the presence of hints to the absence of hints in rq2.
in rq3 we analyzed the different hints separately in comparison to the no hint condition.
for each rq we present bar plots of the mean utility metrics with error bars representing the standard error of the mean.
h. significance testing we use the wilcoxon rank sum test to determine significant differences between different conditions and indicate significant differences by asterisks in all result figures for p value .
for p value .
.
the wilcoxon ranksum test relies on two key assumptions the samples must come from populations with the same shape and the observations must be independent.
to ensure these assumptions are met we test whether the samples share the same shape using the kolmogorov smirnov test before drawing any conclusions about statistical significance.
additionally as we discussed earlier we use only one data point per participant which maintains the independence of observations.
we use the wilcoxon signed rank test to determine whether a particular condition is significantly different from the corresponding chance accuracy.
we chose these statistical tests because they do not make assumptions about the underlying data distribution e.g.
that data is distributed according to a normal distribution .
additionally we use cohen s d to measure effect sizes for significant differences between conditions.
we report cohen s d and the difference of means in section iv along with any mention of significant differences between the two conditions.iv.
r esults rq1.
how does program representation affect a user s ability to find and fix bugs and how does this depend on the user s understanding of the task?
to investigate this question we first compare the accuracy of participants who view the program in natural text with those who view it in python.
for this analysis we only consider responses from participants in the no hint condition to avoid any potential interaction effects between the hint type and program representation.
we further divide the analysis according to two groups of participants those with a confused understanding and those with a clear understanding.
the results are reported in figure .
unsurprisingly participants with a confused understanding of the problem description performed at chance levels i.e.
.
for both text based and python representation conditions.
in contrast participants with a clear understanding of the problem description performed significantly above chance in both the text based p value .
effect size .
difference of means .
and python representation conditions p value .
effect size .
difference of means .
.
additionally these participants performed significantly better when viewing the program in text based representations compared to python based representations pvalue .
effect size .
difference of means .
.
we further examine how program representation affects the participants response time.
to investigate this we analyze the time taken by participants to correctly answer q2 through q4 in figure a .
for this analysis we only considered the time taken by participants who answered all three questions q2 q3 and q4 correctly.
moreover we focused on the clear group participants as only a few participants with a confused understanding remained after filtering out the incorrect responses.
in figure a we compared the average time taken by participants using text based representations with python representations and find a text based representations to take a significantly longer time p value .
effect size .
difference of means .
.
however it is important to note that the average response time for questions is influenced by the time required to read program representations as q3 and q4 multiple choice options contain the original or modified program representations.
to isolate this factor we compared the average time for only correctly answering q2 which has identical questions and multiple choice options for both python and text based representations.
we found no significant differences in response times for q2 p value .
.
rq1 takeaways for participants with clear understanding text based representations led to significantly better accuracy than python representations.
confused participants performed at chance levels for both representations.
while text based representations required more time overall this was likely due to longer reading times rather than reduced efficiency in problem solving.
confused clear0.
.
.
.
.8q2 q4 avgerage accuracy python python hinttext text hintfig.
accuracy of participants when presented with hints and no hints across different program representations.
the bar plot represents the mean q2 q4 average accuracy with the vertical lines indicating the standard error of the mean.
the red dotted line represents chance accuracy and significant differences between the no hint and with hint conditions are indicated with an asterisk .
hints significantly improved accuracy for confused and clear participants for python program representations.
hints also bridged accuracy gaps between representations for clear participants and understanding levels for python representation .
rq2 what is the utility of a hint on the user s ability to find and fix bugs across different program representations and how does this depend on the user s understanding?
we investigate this question by comparing the accuracy of participants in the no hint condition with the average accuracy across the other three hint conditions denoted via hint in figure .
given the significant effects of representation condition and participant understanding group observed in figure we analyzed the utility of hints separately for text based and python representation conditions and for both clear and confused understanding groups of participants.
in figure we present the accuracies when presented with any hint compared to the no hint condition for each participant group and representation condition pair.
significantly better accuracy with hints over no hint is indicated with an asterisk .
for the python representation condition hints significantly helped both confused p values .
effect size .
difference of means .
and clear participants pvalue .
effect size .
difference of means .
.
in contrast for text based representation while both clear and confused participants don t seem to be significantly helped by the provided hints the confused participant groups show a small to medium effect size of .
p value .
difference of means .
.
confused participants who initially performed at chance improved significantly with hints in the python condition.
additionally the accuracy difference between the python and text based conditions for clear participants without hints disappeared when hints were provided for the python condition0.
.
.
.
.8q2 q4 avgerage accuracy python confused text confused test cases conceptual detailed fix0.
.
.
.
.8q2 q4 avgerage accuracy python clear test cases conceptual detailed fixtext clearpython python hint text text hintfig.
accuracy of participants when presented with different hints or no hint across different program representations and participants level of understanding separately.
the bar plot represents the mean q2 q4 average accuracy with the vertical lines indicating the standard error of the mean.
the red dotted line represents chance accuracy and significant differences between the no hint and different hint type conditions are indicated with an asterisk .
detailed fixes are generally most helpful while conceptual hints are particularly useful for participants with confused understanding in the python representation condition.
p value .
effect size .
difference of means .
.
this suggests that hints help close the accuracy gap between different programming representations.
furthermore for the python condition hints boosted the accuracy of confused participants to match that of clear participants without hints pvalue .
effect size .
difference of means .
showing that hints can reduce the gap between participants with varying levels of understanding.
in figure a we analyze how hints affect the participants response time.
we note that hints improve efficiency reduce response time for text based program representations p value .
effect size .
difference of means .
while for python program representations there is no significant difference between using hints and not using them.
this result is complementary to the findings in figure where hints improved the accuracy of clear group participants only for python representations with no significant effect on text based representations.
rq2 takeaways hints significantly improved accuracy for python representations across both clear and confused understanding groups.
for text based representations hints reduced response time for those with clear understanding.
additionally hints bridged accuracy gaps between different representations and understanding levels.
rq3 what types of hint are more suitable for different program representations and how does this depend on the user s understanding?as previously mentioned we aggregated results over three different hint types for investigating rq2 .
in this section we explore whether different hint types are more beneficial for different program representations and whether this varies based on the participants understanding.
to investigate this we extend our previous findings by analyzing the accuracy for each hint type separately and comparing whether they help improve accuracy compared to the no hint condition.
in figure we present results for each hint type.
among the three types of hints detailed fixes are generally the most helpful.
they significantly improve accuracy for clear participants with python representations p value .
effect size .
difference of means .
and confused participants with text based representations p value .
effect size .
difference of means .
.
additionally conceptual hints are especially beneficial for confused participants working with python representations p value .
effect size .
difference of means .
.
we did not observe any significant effect of test case based hints on accuracy compared to no hints.
these analyses show that the effectiveness of hints varies depending on the program representation and the user s understanding level.
in figure b c we visualize the time taken by clear group participants to correctly answer q2 q3 and q4.
consistent with our results while investigating rq2 we note that response time reductions are only observed for the text based program representations where test cases reduce the response time significantly p value .
effect size .
difference of means .
.
we do not see significantclear0.
.
.
.
.0q2 q4 average time minutes a test cases conceptual detailed fix b test cases conceptual detailed fix c python python hint text text hintfig.
time taken by participants to correctly answer q2 q3 and q4 when presented with different hint types or no hint across different program representations for clear understanding group participants.
the bar plot represents the mean of the average time required to answer q2 q4 correctly with the vertical lines indicating the standard error of the mean.
in a hint represents the average of the three different hint conditions while in b and c we plot the different hint types separately.
significant differences between the no hint and hint conditions are represented by .
even though the hints may not be helpful in improving accuracy for the text clear condition they help reduce participants response time.
similarly improvement in accuracy doesn t necessarily imply reduction in response time python clear condition .
for the text clear condition test cases significantly reduce response time.
effects for other hint types and observe small effect size for conceptual hint and effect size .
and small to medium effect size for detailed fix effect size .
.
notably test cases did not improve the accuracy for any of the programrepresentation and participant understanding conditions refer to figure .
rq3 takeaways different hint types vary in utility depending on program representation and participant understanding.
detailed fixes generally improved accuracy most while test cases reduced response time for participants with clear understanding viewing text based representations.
v. t hreats to validity a. internal validity internal validity refers to the ability to accurately establish cause and effect relationships between independent and dependent variables.
in our study internal validity is threatened by the use of crowdsourcing for data collection which can introduce participant dependent confounders.
to mitigate this threat we implemented a comprehensive randomization procedure that ensures that participants are randomly assigned to one of our eight distinct conditions.
with more than participants we expect the influence of individual differences to be evenly spread across all conditions effectively balancing out.
furthermore we carefully considered the various independent variables during data analysis.
for instance when determining the effect of program representation refer to rq1 in section iii g we analyzed only the no hint condition to avoid confounding effects from the presence of hints.
likewise we analyzed different hint conditions separately for each program representation.
these methodological considerations strengthen the internal validity of our findings.b.
construct validity a key consideration in our study was ensuring construct validity that is reliably measuring the utility of different program representations and hints in bug understanding.
as common in the literature we measure utility through accuracy and response time metrics.
we expect that a deeper understanding of a program and bug will lead to improved accuracy and reduced response time.
to assess accuracy we crafted three questions for each algorithmic task probing participants comprehension of the bug s impact q2 location q3 and potential fix q4 .
please refer to section iii d for a detailed discussion regarding the different questions.
the mean accuracy across these questions served as our accuracy metric.
for response time we measured the average time taken to answer these questions by participants who answered accurately.
the construct validity for average time can be threatened by program representation.
specifically text representations generally require longer reading times compared to python code and two of our questions q3 and q4 included program representations in their answer choices.
this could lead to artificially inflated response times for participants in the text condition potentially masking true differences in response time.
therefore while reporting the response time comparisons between python and text representation conditions we also report the response time based solely on q2 which is exactly the same across both representations.
c. statistical conclusion validity to ensure statistical conclusion validity the degree to which conclusions we reach from analyses are accurate and appropriate we implemented several key measures in our study design and analysis.
first we secured a large sample size of 753participants significantly enhancing the power of our statistical tests and allowing us to detect even subtle effects with sufficient confidence.
in our analysis we employedappropriate statistical tests that do not make assumptions about the underlying data distribution.
specifically we examined the statistical difference between conditions using the wilcoxon rank sum test and the statistical difference between a condition and the theoretical chance accuracy using the wilcoxon signed rank test.
the wilcoxon rank sum test relies on two key assumptions the samples must come from populations with the same shape and the observations must be independent.
to ensure these assumptions are met we verify that the samples share the same shape using the kolmogorov smirnov test before drawing any conclusions about statistical significance.
additionally since we use only one data point per participant the independence of observations is maintained.
we indicate significant differences in all result figures using asterisks for p value .
for p value .
.
furthermore we implemented rigorous randomization procedures to balance our experimental groups.
this approach to group assignment minimized the potential for confounding variables and strengthened our ability to attribute observed effects to our manipulated conditions.
d. ecological validity ecological validity measures how well an experimental setup reflects real world conditions.
our study environment diverged from typical programming scenarios in some respects potentially impairing ecological validity as participants lacked access to an integrated development environment ide or a debugger common tools used in programming.
this design choice while potentially reducing ecological validity served to increase internal validity by standardizing the environment across all participants.
moreover it allowed us to isolate the accuracy and response time improvement effects to the program representation and or the provided hints.
additionally we selected tasks and code snippets particularly relevant to novice programmers and educational settings.
this approach though not perfectly replicating professional development scenarios aligned with the experiences of our target population in learning contexts.
thus we strived to maintain ecological validity while adhering to rigorous experimental controls.
e. external validity external validity concerns the ability of our results to generalize to other settings participants and measures.
we posit that our findings are likely generalizable to similar contexts i.e.
small to medium sized code snippets and algorithmic tasks of comparable complexity.
however we acknowledge that the effectiveness of these representations and hints may not necessarily extend to large scale software projects.
additionally our participant pool primarily recruited from crowdsourcing platforms consisted largely of individuals with limited programming experience.
this may limit the generalizability of our results to other populations such as computer science students or highly experienced professional programmers.
nevertheless it is important to consider the evolving landscape of software engineering where diverse user groups often untrained in programming are increasingly playing centralroles in application development .
our insights align well with this emerging trend.
finally it is important to note that our study represents one of the first comprehensive investigations into the utility of program representations hints and their interactions in the context of bug understanding.
as such our findings offer valuable guidance for developing tools that adapt to programming representations and user expertise.
they also underscore the need for future research to explore the generalizability of these findings across a broader spectrum.
vi.
d iscussion in our study we found that text representations are more beneficial in terms of accuracy than python representations for individuals with a clear understanding of the algorithmic task.
one hypothesis for this possibly counter intuitive finding is that python code is structured and contains implicit beacons e.g.
variable and function names that aid program comprehension without the need of going through every statement a process called top down comprehension .
the text representation forces even seasoned programmers to go through all details from beginning to end called bottom up comprehension which makes it more likely to spot otherwise hidden bugs.
h1 python representations trigger top down comprehension whereas text representations trigger bottom up comprehension.
the difference between the two will be more pronounced the more experienced programmers are.
an alternate hypothesis is that even though our sample size was large over participants the majority of the participants had little self reported experience with coding or python and this may also have contributed to their improved accuracy with text over python representations.
h2 text representations aid inexperienced programmers since they strip the algorithmic description from syntactical and technical information arising from the use of a formal programming language.
future work that repeats our experiment in a population of experienced participants with python can disambiguate between these two hypotheses as they make very different predictions for experienced and inexperienced participants the first hypothesis predicts that the more experience with code the participant has the stronger their prior will be and the easier it would be to overlook the bug and therefore the gap between text and python representations will widen.
the second hypothesis predicts that the more experience with code the participant has the more they will benefit from the python representation in terms of accuracy of finding bugs.
we conducted a preliminary investigation by analyzing participants divided into three groups based on self reported programming proficiency low medium and high experience.
our findings revealed that participants across all experience levels performed better with text based representations than with python.
this effect was significant for those with low p value .
effect size .
difference of means .
programming experience.
we share the detailed results in our github repository due to space constraints.
these initial results suggest evidence towards h2 i.e.
text representations aid inexpierenced programmers.
however to draw more definitive conclusions further experiments under controlled conditions using objective and verifiable measures of programming expertise beyond self reporting are necessary.
we further found that the addition of hints was particularly useful for the python representation.
in these cases hints improved user accuracy regardless of whether the participant had a clear or confused understanding.
furthermore participants with a confused understanding benefited so much from the hint that their accuracy was statistically indistinguishable from that of the participants with originally clear understanding.
this suggests that the lack of algorithmic understanding can be made up using hints.
additionally hints also bridge the gap between the python and text representations for participants with clear understanding such that participants who see the program in python and receive a hint perform at the level of those who see the program in text.
this result suggests that hints can be a powerful tool in aiding program debugging in code when faithful text descriptions are difficult to generate.
interestingly we find that hints have contrasting effects on accuracy and completion time for python vs. natural text representations hints improve accuracy but not speed for python representations and improve speed but not accuracy for text representations.
again this may be because the python representation already provides a sufficiently highlevel description of the program that can be accessed quickly so the speed is difficult to improve upon.
as said in h1 this representation can also make it harder to detect the bugs which is where hints can help by drawing attention to specific issues in the program.
in contrast text representations require going through the program word by word to extract the overall structure and may thus require longer time to integrate the higher level understanding of the program.
while this low level presentation can be helpful in detecting bugs manipulating it can be slow which can be improved by the addition of higher level hints.
h3 hints alter strategies for program comprehension and bug finding.
lastly we found that among the 3different types of hints test cases conceptual and detailed fix detailed fixes are generally the most helpful in improving accuracy across all representation modalities and user types.
additionally conceptual hints are particularly beneficial for users with a confused understanding when working with python representations.
while we don t find a significant improvement in response accuracy due to test cases we observe that they lead to the biggest improvement in efficiency i.e.
reduced time per accurate response specifically for text based representations.
similarly to the benefit of the natural language conceptual hint in the python representation here we also observe a benefit when mixing the presentation modalities of the program and the hint.
we hypothesize that the mixing of presentation modalities maycontribute to a more holistic understanding of the program.
h4 mixing natural text and code representations improves holistic understanding of the program.
overall these results suggest a debugging workflow that is personalized to the level of algorithmic understanding of the user if the user understood the posed algorithmic task then they benefit most from seeing the program described in natural language.
if the user did not understand the algorithmic task from the start then they most benefit from seeing the program in python and receiving a conceptual hint.
while we focus here on short term improvements in program understanding we hope that our results can serve as a starting point for future work that investigates the utility of hints in long term program understanding over multiple educational sessions.
vii.
c onclusion and perspectives recent advancements in generative ai have revolutionized programming accessibility allowing users to solve tasks through various representations including natural language and pseudo code.
however these new tools still require users to possess algorithmic thinking and debugging skills.
it is currently unclear which type of hint is most helpful and how this depends on the program representation and the user s understanding level.
our study aimed to address this gap by investigating the effectiveness of different hint types across various program representations and user understanding levels.
we conducted a large scale crowd sourced study involving participants to examine the utility of different program representations and hint types in finding and fixing bugs for participants with varying levels of understanding.
specifically our research focused on three key questions how program representation affects a user s ability to find and fix bugs the utility of hints across different program representations and which hint types are most suitable for various program representations all in relation to the user s level of understanding of the algorithmic task.
our findings revealed several important insights text based program representations improve accuracy for users with clear understanding of the algorithmic task.
hints significantly improved accuracy for python representations across both clear and confused understanding groups.
for text based representations hints reduce response time for those with clear understanding.
additionally hints bridged accuracy gaps between different representations and understanding levels.
different hint types vary in utility depending on program representation and participant understanding.
detailed fixes generally improved accuracy most while test cases reduce response time for participants with clear understanding viewing text based representations.
these results have significant implications for the design of next generation programming tools.
they suggest the potential for personalized support systems that adapt to users experiences and understanding levels.
by tailoring program representations and hint types to individual users we can enhancetheir ability to find and fix bugs ultimately improving their programming skills and reducing response time.
additionally for software engineering researchers our work represents the first study to systematically investigate the intersection of different programming representations and types of hints.
our methodology and study setup provide a blueprint for exploring similar and follow up research questions some of which we articulate in section vi in the form of explicit hypotheses.
viii.
d ata availability the raw data from the human study and scripts to generate all the plots present in the paper are available at