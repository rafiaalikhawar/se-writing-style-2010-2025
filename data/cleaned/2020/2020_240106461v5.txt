between lines of code unraveling the distinct patterns of machine and human programmers yuling shi shanghai jiao tong university yuling.shi sjtu.edu.cnhongyu zhang chongqing university hyzhang cqu.edu.cnchengcheng wan east china normal university ccwan sei.ecnu.edu.cnxiaodong gu shanghai jiao tong university xiaodong.gu sjtu.edu.cn abstract large language models have catalyzed an unprecedented wave in code generation.
while achieving significant advances they blur the distinctions between machine and humanauthored source code causing integrity and authenticity issues of software artifacts.
previous methods such as detectgpt have proven effective in discerning machine generated texts but they do not identify and harness the unique patterns of machine generated code.
thus its applicability falters when applied to code.
in this paper we carefully study the specific patterns that characterize machine and human authored code.
through a rigorous analysis of code attributes such as lexical diversity conciseness and naturalness we expose unique patterns inherent to each source.
we particularly notice that the syntactic segmentation of code is a critical factor in identifying its provenance.
based on our findings we propose detectcodegpt a novel method for detecting machine generated code which improves detectgpt by capturing the distinct stylized patterns of code.
diverging from conventional techniques that depend on external llms for perturbations detectcodegpt perturbs the code corpus by strategically inserting spaces and newlines ensuring both efficacy and efficiency.
experiment results show that our approach significantly outperforms state of the art techniques in detecting machine generated code.
.
index terms machine generated code detection large language models code generation empirical analysis i. i ntroduction the advent of large language models llms such as codex and chatgpt has revolutionized software engineering tasks such as code generation.
through extensive training on ultra large code corpora llms acquire the ability to generate syntactically and functionally correct code bringing a new era of efficiency and innovation in software creation maintenance and evolution.
while capable of generating human like code llms bring ambiguity of whether a software artifact is created by human or machine causing integrity and authenticity issues in software development.
this indistinction can lead to various challenges such as misattribution of code ownership for bug triage and inflated assessments of developer workloads.
potential vulnerabilities in machine generated code may go unnoticed due to the overreliance on its perceived robustness.
the blending of human and machine efforts not only raises questions about the trustworthiness of the software but also threatens the 1code available at xiaodong gu is the corresponding authorintegrity of development process wherein the true authorship and the effort invested in creating software artifacts become obscured.
addressing these concerns is pivotal in maintaining a transparent and secure software development lifecycle.
recently there has been a growing research trend in detecting machine generated texts .
perturbation based methods like detectgpt have achieved state of the art results in identifying machine generated text.
these methods employ likelihood score discrepancies between the original text and its various llm perturbed variants for detection capturing the distinct patterns of machine generated text that machines tend to prefer to a smaller set of phrases and expressions.
however such detection methods tailored for natural language texts face challenges when applied to code as code requires strict adherence to syntactic rules while natural language can maintain coherence with more variation .
this situation highlights a significant gap in existing research a lack of indepth assessment of the intrinsic features of machine and human authored code crucial for understanding the unique patterns of machine generated code and devising effective detection methods.
in this paper we conduct a comparative analysis of the distinct patterns between machine and human authored code from three aspects including lexical diversity conciseness and naturalness.
through our analysis we uncover that compared to human machine tends to write more concise and natural code with a narrower spectrum of tokens and adhere to common programming paradigms and the disparity is more pronounced in stylistic tokens such as the whitespace tokens.
based on the findings we propose a novel method called detectcodegpt for detecting machine authored code.
we extend the perturbation based framework of detectgpt by strategically inserting stylistic tokens such as whitespace and newline characters to capture the distinct patterns between machine and human authored code.
this approach capitalizes on our observation that the disparity in coding styles is more pronounced in these stylistic tokens.
by directly manipulating the code detectcodegpt eliminates the need for an external pre trained model thereby enhancing both efficiency and effectiveness in the detection process.
to evaluate the effectiveness of detectcodegpt we have conducted extensive experiments on two datasets across six code language models.
the results demonstrate that detectcodegpt significantly outperforms the state of the art methodsarxiv .06461v5 jul 2024by .
in terms of auc.
moreover it proves to be a modelfree and robust method against model discrepancies making it viable for real world applications with unknown or inaccessible source models.
our contributions can be summarized as follows to our knowledge we are the first to conduct a comprehensive and thorough analysis of the distinct patterns of llm generated code.
our study sheds light on essential insights that can further advance the utility of llms in programming.
we propose a novel method for detecting machine generated code by leveraging its distinct stylistic patterns.
we extensively evaluate the detectcodegpt across a variety of settings and show the effectiveness of our approach.
ii.
b ackground a. large language models for code large language models based on transformer decoder has achieved remarkable success in natural language processing tasks .
in the domain of code generation codex and alphacode are pioneering works to train large language models on code.
the training data often contain millions of code in different programming languages collected from open source repositories like github .
later advances to improve llms on code include designing new pretraining tasks like fill in the middle and also instruction fine tuning .
recent large language models pretrained on a mixture of programming and natural languages like chatgpt and llama have also shown promising results on code generation tasks.
b. perturbation based detection of machine generated text in the realm of machine generated text detection perturbation based method like detectgpt stands as the state ofthe art technology .
in this section we take detectgpt as an example to illustrate the idea of perturbation based detection methods.
detectgpt distinguishes between machine and human generated text by analyzing the patterns in their probability scores .
the core idea is that when a text x generated from a machine is subtly changed to xthrough a perturbation process q x e.g.
mlm with t5 there is a sharper decline in its log probability scores logp x than that in human generated text.
this is because a machine generated text is usually more predictable and tightly bound to the patterns it was trained on leading to a distinct negative curvature in log probability when the text is perturbed.
by contrast human written texts are characterized by a rich diversity that reflects a blend of experiences and cognitive processes.
as a result it doesn t follow such predictable patterns and its log probability scores logp x do not plummet as dramatically when similarly perturbed.
based on such discrepancy we can define a likelihood discrepancy score for each input code to measure the drop of log probability after perturbation.
d x p q logp x e x q x logp x table i studied categories of python code tokens category tree sitter types keyword def return else if for while .
.
.
identifier identifier type identifier literal string content integer true false .
.
.
operator .
.
.
syntactic symbol .
comment comment whitespace space n by inspecting these scores we can detect the source of x. a significant drop indicates machine authorship and a smaller change suggests a human creator.
this method effectively captures the more nuanced and variable nature of humangenerated text compared to the more formulaic and patterned output of language models.
iii.
e mpirical analysis in this section we conduct a comparative analysis of the distinct features of machine and human authored code.
a. study design to gain insights into distinctions between human and machine programmers we consider three primary aspects that are relevant to coding styles namely diversity conciseness and naturalness which can be measured by specific metrics.
lexical diversity lexical diversity indicates the richness and variety of vocabulary present in a corpus.
in the context of programming this refers to the diversity in variable names functions classes and reserved words.
analyzing lexical diversity offers a deeper understanding of the creativity expressiveness and potential complexity of code segments.
there are four important empirical metrics in both natural and programming languages revealing the lexical diversity token frequency syntax element distribution zipf s law and heaps law .
token frequency stands for the occurrence of distinct tokens in the code corpus.
the attribute indicates the core vocabulary utilized by human and machine programmers shedding light on their coding preferences and tendencies.
syntax element distribution refers to the proportion of syntax elements e.g.
keywords identifiers in the code corpus.
understanding the distribution of syntax elements in code is akin to dissecting the anatomy of a language.
it gives us a lens to view the nuances of coding style the emphasis on structure and the intricacies that distinguish human and machine authored code.
to delve into the syntax element distribution we analyze code with tree sitter2and classify tokens into distinct categories as detailed in table i. we then compute the proportion of each category in the code corpus.
zipf s and heaps laws were initially identified in natural languages and later verified in the scope of programming languages .
zipf s law states that the frequency value fof a token is inversely proportional to its 2table ii top tokens from human and machine authored code from codellama rank human authored tokens machine authored tokens .
self .
self if if return in for not none return not path else is name path data raise name class name none os try os len format get and true value isinstance else typeerror str init is the args key np i x kwargs except false or in value kwargs include str forvalueerror frequency rank r f r where is close to .
in programming languages it states that a few variable names or functions are very commonly used across different scripts while many others are rarely employed.
heaps law characterizes the expansion of a vocabulary vas a corpus dincreases in size v d where captures the rate of vocabulary growth relative to the size of the corpus.
we investigate how closely machine authored code aligns with zipf s and heaps laws compared to human authored code which could reflect the models ability to mimic human s lexical usage.
conciseness conciseness stands as a cornerstone attribute when characterizing code .
the intricate balance of code conciseness directly influences readability maintainability and even computational efficiency.
we investigate two metrics that characterize code conciseness namely the number of tokens and the number of lines.
number of tokens gives us an indication of verbosity and complexity showing the detailed composition of the code .
number of lines helps us understand organizational choices as spreading code across more lines can reflect a focus on readability and structure .
naturalness the concept of code naturalness suggests that programming languages share a similar degree of regularity and predictability with natural languages .
this idea has been operationalized by employing language models to assess the probability of a specific token s occurrence within a given context.
under this framework we inspect how natural machine generated code is compared to human written code.
token likelihood and rank are two metrics that measure the naturalness of each token in the studied code corpus.
the token likelihood stands for the probability pof a token xunder the model p denoted as p x .
the rank of a token xis the position of xin the sorted list of all tokens based on their likelihoods denoted as r x .
both metrics evaluate how likely a token is preferred by the model .
we calculate log scores on each token and then take the average to represent the whole code snippet as advised in .
to pinpoint the code elements that most significantly affect the score discrepancies we also present the mean scores on different syntax element categories in table i for comparison.
b. experimental setup we choose the state of the art codellama model to generate code.
limited by our computational resources we use the version with 7b parameters.
as for the decoding strategies we adopt the top psampling method with p .95following .
the temperature tis an importantparameter controlling the diversity of the generated code .
since current llms on code are usually evaluated across different decoding temperatures we generate code with t .2andt .0to capture the model s behavior under different settings.
the maximum length of the generated code is set to tokens based on the memory constraints and the length distribution of human written code.
all experiments are conducted on nvidia rtx gpus with 24gb memory.
c. dataset preparation to compare with human authored code we extract python functions randomly from the codesearchnet corpus which is curated from a wide range of open source github projects.
we use the function signatures and their accompanying comments as prompts for the model as in .
we also collect the corresponding bodies of these functions to represent human written code.
while acknowledging that current models including codellama and even chatgpt may not yet craft code of unparalleled quality for intricate tasks such as those in codesearchnet the choice of this dataset is deliberate and insightful.
challenging the models against various real world project code rather than simple programming problems akin to those in the humaneval or mbpp dataset offers a more representative assessment.
it allows us to analyze the differences between human and machine authored code when faced with broader practical applications.
d. results and analysis we present the results and analysis regarding each code attribute introduced in section iii a. token frequency table ii lists the top tokens from human and machine authored code when t .
.
due to space limit we omit the results when t .
which has a similar result.
from the results we have several noteworthy observations common tokens human and machine authored code shares a commonality in their usage of certain tokens including punctuation marks such as .
and structural keywords such as if return and else .
this is because llms acquire foundational coding syntax after being trained on extensive human written code corpora.
error handling tokens associated with error handling like raise and typeerror are more prevalent in machineauthored code.
this difference implies that machine programmers emphasize robustness and exception handling more explicitly.
.
.
.
.4proportion at t .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
human machine keyword identifierliteral operatorsymbol commentwhitespace00.
.
.
.4proportion at t .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure syntax element distribution of the code corpus programming paradigms tokens indicating object oriented programming like self and init are prominent in both human and machine authored code which illustrates the model s training alignment with this paradigm.
however machine authored code appears to favor more boilerplate tokens like class and also name which could stem from its training on diverse object oriented codebase.
finding machine authored code pays more attention to exception handling and object oriented principles than human suggesting an emphasis on error prevention and adherence to common programming paradigms.
syntax element distribution the analysis of syntax element distributions as visualized in figure reveals intriguing insights into the coding conventions and stylistic nuances between human and machine authored code.
the keyword operator syntactic symbols and whitespace proportions remain largely consistent between human and machine authored code across both temperatures.
this consistency suggests that the foundational syntactical elements manifest similarly in both datasets.
delving deeper into the more nuanced discrepancies a few categories emerge that underscore the differential preferences or tendencies of human and machine writers statistical significant from chi square test with p .
identifier the identifiers constitute a significantly lower proportion among machine authored code across both temperatures indicating that machine authored code may have a more compact style with fewer identifiers.
literal machine authored code consistently shows a slightly higher tendency towards using literals.
across both temperatures the machine code exhibits an increase in the literal proportion compared to the human written code.
this suggests that machine authored code may be more likely to process raw data directly.
this could result from the machine s training on diverse data manipulation tasks.comment machine authored code has much more comments when t .
.
this observation hints machine s increased emphasis on code documentation and explanation with higher temperatures when it becomes less deterministic and more exploratory.
finding machine authored code tends to use fewer identifiers more literals for direct data processing and have more comments when the generation temperature grows.
zipf s and heaps laws figure offers a comprehensive understanding of coding tendencies of both human and machine programmers.
starting with zipf s law figure 2a and 2b both delineate similar trends for human and machine programmers corroborating the law s applicability.
and we can observe machine s heightened proclivity towards tokens ranked between and especially at t .
.
turning our attention to heaps law the near linear trends in figure 2c 2d reaffirm the law s validity.
also there s a noticeable shallowness in the slope for machine s code at t .2revealing machine s decreased lexical diversity.
the obvious differences at t .2can be ascribed to human creativity and variability .
the varied approaches and methodologies humans employ can lead to a diversified token usage within this range.
another plausible interpretation can be itsrisk aversion .
a machine especially at lower temperatures might be reverting to familiar patterns ensuring correctness in code generation.
additionally certain patterns within the training data might have been overemphasized due to model overfitting leading the machine to a skewed preference.
finding machines demonstrate a preference for a limited spectrum of frequently used tokens whereas human code exhibits a richer diversity in token selection.
number of tokens and lines figure presents the distribution of code length under different settings.
for the temperature setting of .
machine authored code exhibits more conciseness both in token and line numbers.
as the temperature increases to t .
we witness a convergence of distributions.
the gap narrows yet the machine s preference for relatively concise code persists.
this reveals that higher temperatures induce more exploratory generative behavior in the model leading to diverse coding styles.
one could hypothesize several reasons for these observed patterns.
the propensity for conciseness at lower temperatures may reflect llm s training data where probably concise solutions were more prevalent or deemed more correct .
on the flip side human developers often juggling multiple considerations like future code extensions comments for peer developers or even personal coding style might craft lengthier solutions.
furthermore the narrowing of disparities at higher temperatures can be attributed to the model s increased willingness to explore varied coding styles.
at higher temperatures the llm possibly mimics a broader spectrum of human coding patterns capturing the essence of diverse coding habits and styles found in its log10 rank 01234log10 frequency human machine ideal zipf s line a zipf s law t .
log10 rank 01234log10 frequency human machine ideal zipf s line b zipf s law t .
.
.
.
.
.
.
.
log10 t ext length .
.
.
.
.0log10 vocabulary size human machine c heaps law t .
.
.
.
.
.
.
.
log10 t ext length .
.
.
.
.0log10 vocabulary size human machine d heaps law t .
figure comparison of zipf s and heaps laws on machine and human authored code count0.
.
.
.
.
densityhuman machine a number of tokens t .
count0.
.
.
.
.
densityhuman machine b number of tokens t .
count0.
.
.
.
.
.
densityhuman machine c number of lines t .
count0.
.
.
.
.
.
densityhuman machine d number of lines t .
figure distribution of code length for machine and human authored code log likelihood0.
.
.
.
.
.
.
.4densityhuman machine .
.
.
.
.
log rank0.
.
.
.
.
.
.
.5densityhuman machine figure distribution of naturalness scores training corpus.
finding machines tend to write more concise code as instructed by their training objective while human programmers tend to write longer code reflective of their stylistic preferences.
token likelihood and rank figure shows that there is a great discrepancy of naturalness between machine and human authored code.
compared to human authored code the log likelihood scores of machine authored code are mostly higher and the log rank scores are mostly lower indicating that machine s code is more natural than human written code.
such observation in source code is consistent with the findings in natural language .
table viii summarises the comparison results in terms of each token category at t .
.
an intriguing finding is that whitespace tokens stand out with the highest deviation of naturalness surpassing even the combined use of all tokens.
this highlights a distinctive aspect of coding styles machines trained on extensive datasets typically generate code with regular predictable whitespace patterns.
humans however table iii the naturalness of different categories of syntax elements.
statistical significance p .
.
categorylog likelihood log rank machine human machine human keyword .
.
.
.
.
.
identifier .
.
.
.
.
.
literal .
.
.
.
.
.
operator .
.
.
.
.
.
symbol .
.
.
.
.
.
comment .
.
.
.
.
.
whitespace .
.
.
.
.
.
all .
.
.
.
.
.
influenced by individual styles and practices exhibit a wider variety in their use of whitespaces.
the distinct patterns in machine generated whitespaces therefore point to an inherent variation in coding style between machine and human.
finding machine authored code exhibits higher naturalness than human authored code and the disparity is more pronounced in tokens such as comments and whitespaces which are more reflective of individual coding styles.
iv.
d etecting machine generated code the empirical results suggest that machines tend to write more concise and natural code with a narrower spectrum of tokens adhering to common programming paradigms and the disparity is more pronounced in stylistic tokens such as whitespaces.
this sparks a new idea for detecting machineauthored code instead of perturbing arbitrary tokens we focus on perturbing those stylistic tokens that best characterize the machine s preference.
based on this idea we introduce detectcodegpt a novel zero shot method for detecting machine authored code.
5a.
problem formulation we formulate the detection of machine authored code as a classification task which predict whether or not a given code snippet xis produced by a source model p .
for this purpose we transform xto an equivalent form xthrough a perturbation process q x .
we anticipate a sharper decline in its naturalness score if xis written by an llm.
the key problems here are how to define the naturalness score and how to design the perturbation process.
we introduce the naturalness score and the perturbation strategy q x in our approach in the following sections.
b. measuring naturalness previous methods usually use the log likelihood of tokens to measure the naturalness of machine authored content .
however the log rank of tokens shows better performance comparing the naturalness of machine and human authored text because it offers a smoother and robust representation of token preference.
unlike detectgpt which directly calculates the log likelihood of tokens we adopt the normalized perturbed log rank npr score to capture the naturalness.
the npr score is formally defined as npr x p q e x q x logr x logr x where logr x is the logarithm of the rank order of textxsorted by likelihood under model p .
in practice npr x p q has been demonstrated to be more accurate for differentiating text origins outperforming the log likelihood discrepancy d x p q .
c. perturbation strategy our empirical study indicates that the whitespace tokens serve as an important indicator of machine s regularization and human s diversity which points to an inherent variation in coding style.
therefore we propose an efficient and effective perturbation strategy with the following two types of perturbations below.
detailed explanations on the effectiveness of these perturbations are given in section vi a. space insertion letcrepresent the set of all possible locations to insert spaces in a code segment.
we randomly select a subset cs csuch that cs c where is a fraction representing the code locations.
for each location c cs we introduce a variable number of spaces nspaces c which is drawn from a poisson distribution p spaces .
the poisson distribution is chosen to simulate the randomness in human coding styles similar to the random text infilling strategy in bart .
mathematically this can be represented as nspaces c p spaces .
newline insertion we split the code into lines and obtain a set lof lines.
a subset ln lis then chosen randomly where ln l with denoting the proportion of the line locations.
for each line l ln we introduce a variable number of newlines nnewlines l also sampled from a poisson distribution p newlines nnewlines l p newlines .
algorithm detectcodegpt machine generated code detection with stylized code perturbation data codex source model m number of perturbations k decision threshold parameters spaces and newlines 1fori 1tokdo random decision for type of perturbation p u ifp .5then spaces insertion letcrepresent all possible locations to insert spaces inx select cs csuch that cs c foreach location c csdo nspaces c p spaces insert nspaces c spaces at location cinx end else newlines insertion split the perturbed code xinto a set lof lines select ln lsuch that ln l foreach line l lndo nnewlines l p newlines insert nnewlines l newlines after line linx end end store the perturbed code as xi 19end 20estimate npr npr x npr x p q kp inpr xi p q 21ifnpr x then return true probably machine authored 23else return false probably human authored 25end we randomly choose one type of perturbation to the code snippet xto generate a set of perturbed samples xifori where kis the number of perturbations.
through this step we instill randomness at a granular stylistic level thereby amplifying the perturbation s efficacy.
our perturbation strategy introduces several distinct advantages over the conventional methods using mlm to perturb the code which will be discussed in section vi b. algorithm summarizes the entire workflow of detectcodegpt.
our algorithm harnesses stylized code perturbation to differentiate between human and machine authored code.
at the core of our approach is the strategic insertion of spaces lines and newlines lines in code a process that simulates the inherent randomness in human coding styles.
6the algorithm operates by generating perturbed versions of the code and then evaluating their npr scores lines with respect to the source model m. the threshold parameter in line pivotal for making the detection decision offers flexibility in catering to different application scenarios.
by adjusting users can balance between false positives and false negatives tailoring the detection sensitivity according to the specific needs of the deployment context.
v. e valuation we conduct experiments to evaluate the effectiveness of detectcodegpt aiming to answer the following research questions.
rq1 how effectively does our method distinguish between machine generated and human written code?
rq2 to what extent do individual components influence the overall performance of our method?
rq3 what is the impact of varying the number of perturbations on the detection performance?
rq4 how effective is our method in cross model code detection?
a. datasets we carefully use a different split of the codesearchnet dataset from the one used in the empirical study for evaluation.
we select python code from the stack as another evaluation dataset.
similar to codesearchnet the stack provides code from a variety of open source projects representative of real world scenarios.
we use a parsed and filtered version of this dataset and also concatenate the function definitions with their corresponding comments as prompts as in .
for each combination of dataset and model we sample human and machine code pairs for evaluation.
the maximum length of code is trimmed to tokens.
b. studied models we investigate machine generated code by a diverse array of advanced llms including incoder phi starcoder wizardcoder codegen2 and codellama .
we obtain their checkpoints from huggingface3with different parameter sizes 1b 7b .
c. evaluation metric following prior works our primary metric for performance evaluation is the area under the receiver operating characteristic curve auroc .
formally given a set of true positive rates tpr and false positive rates fpr across different thresholds the auroc can be represented as auroc z1 0tpr t dt where tdenotes varying threshold values.
it provides a comprehensive view of performance across all possible thresholds making it threshold independent.
this makes the metric both and insightful offering a clearer picture of the model s discriminating capabilities.
d. baselines our evaluation is benchmarked against a diverse range of zero shot machine generated text detection techniques.
and a supervised baseline is also included to demonstrate the advantages of our zero shot method logp x utilizes the source model s average tokenwise log probability to gauge code naturalness.
machinegenerated code tends to have a higher score.
entropy interprets high average entropy in the model s predictive distribution as indicative of machine generation.
log rank the average observed rank or log rank of each token in the llm prediction with machinegenerated passages typically showing smaller average values.
detectgpt leverages the log probability of the original code and its perturbed variants to compute the perturbation discrepancy gap.
detectllm introduces two methods one blends log likelihood with log rank to compute lrr and the other improves detectgpt by incorporating the npr score.
gptsniffer a supervised baseline that trains codebert to predict the authorship of a given code snippet.
following openai s roberta based gpt detector4 we train the model on a combination of samples generated by each model at each setting.
e. experimental setup for code generation with different models we adopted the top psampling strategy with p .95following .
we explored two temperature settings t .2andt .
as discussed in section iii b .
the maximum length constraint for generated code was set at 128tokens.
with respect to the perturbation specific hyperparameters a grid search on a heldout set from the codesearchnet dataset using the santacoder model revealed optimal values.
consequently we set and to0.
while spaces and newlines .
for all experiments we maintained a consistent configuration of generating perturbations.
for the detectgpt and detectllm which involve an llm in restoring perturbed code we utilized the codet5 770m model .
and as for the supervised baseline gptsniffer we trained the codebert model for epochs with a batch size of and a learning rate of 2e with adamw optimizer.
all experiments are conducted on nvidia rtx gpus with 24gb memory.
f .
detection performance rq1 table iv delineates the results of various methods.
according to the results detectcodegpt consistently outperforms baseline methods.
compared to the strongest baseline log rank our method achieves an average relative improvement of .
in auroc.
in an impressive of combinations 7table iv performance auroc of various detection methods.
statistical significance p .
.
dataset code llmdetection methods logp x entropy rank log rank detectgpt lrr npr gptsniffer detectcodegpt codesearchnet t .
incoder .3b .
.
.
.
.
.
.
.
.
phi .3b .
.
.
.
.
.
.
.
.
starcoder 3b .
.
.
.
.
.
.
.
.
wizardcoder 3b .
.
.
.
.
.
.
.
.
codegen2 .7b .
.
.
.
.
.
.
.
.
codellama 7b .
.
.
.
.
.
.
.
.
codesearchnet t .
incoder .3b .
.
.
.
.
.
.
.
.
phi .3b .
.
.
.
.
.
.
.
.
starcoder 3b .
.
.
.
.
.
.
.
.
wizardcoder 3b .
.
.
.
.
.
.
.
.
codegen2 .7b .
.
.
.
.
.
.
.
.
codellama 7b .
.
.
.
.
.
.
.
.
the stack t .
incoder .3b .
.
.
.
.
.
.
.
.
phi .3b .
.
.
.
.
.
.
.
.
starcoder 3b .
.
.
.
.
.
.
.
.
wizardcoder 3b .
.
.
.
.
.
.
.
.
codegen2 .7b .
.
.
.
.
.
.
.
.
codellama 7b .
.
.
.
.
.
.
.
.
the stack t .
incoder .3b .
.
.
.
.
.
.
.
.
phi .3b .
.
.
.
.
.
.
.
.
starcoder 3b .
.
.
.
.
.
.
.
.
wizardcoder 3b .
.
.
.
.
.
.
.
.
codegen2 .7b .
.
.
.
.
.
.
.
.
codellama 7b .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
of dataset and model our method provides the most accurate performance which underscores its robustness across a variety of generative models ranging from the .
billion parameter incoder to the billion parameter codellama.
we also repeated the experiments times and employed a wilcoxon rank sum test to assess the statistical significance of the performance differences between the methods.
results show that the performance superiority of our method was statistically significant with p values less than .
.
the high auroc scores achieved across these diverse settings confirm the method s superior capability to generalize and reliably differentiate between machine generated and human written code.
we can observe that the challenge of detection notably increases at a temperature setting of t .0thant .
.
this is possibly due to the higher randomness at this temperature where models are likely to generate outputs with greater diversity in styles.
despite these increased difficulties the proposed method maintains its leading position in detection accuracy.
it is worth mentioning that our zero shot framework often outperforms the supervised gptsniffer highlighting the challenges of detecting machine generated code with training datadependent supervised models.
the stable performance across various generation settings showcases our method s advanced detection capabilities.
this makes it an effective solution in practical applications.
to further illustrate the effectiveness of our approach wepresent some representative examples by detectcodegpt and two most competitive baselines in figure using decision thresholds set at the mean scores of all code snippets.
examples is a machine generated code snippet.
detectcodegpt can correctly detect it while the baselines fail.
in this example the first and last if statements are separated from the rest of the code with newlines and the two if statements in the middle are modularized together since they have similar functionalities.
such modularization and separation of code blocks are captured by detectcodegpt thanks to the stylized perturbation.
example and are human written code that are misclassified by other baselines but detectcodegpt correctly identifies them.
in example we can observe that the code blocks are sometimes separated with newlines e.g.
lines but sometimes not as seen with the rest of the code .
in example although the code blocks are well separated with newlines the human author omitted the spaces between operators and but add spaces between and .
such freely and randomly stylized code reveals the inherent randomness in human coding habits.
baselines relying solely on token wise conditional distributions struggle to capture the coding style s randomness whereas detectcodegpt effectively utilizes style information to discern between machine generated and human written code demonstrating its prowess in detection through stylized code perturbation.
however there are also cases where detectcodegpt fails.
example is a human written code misclassified as machinegenerated by all the approaches.
we can observe that this 8truth log p x logrank detectcodegpt importosimportreifnotos.path.isdir path raisevalueerror sis not a directory path ifnotisinstance include list tuple include ifnotisinstance exclude list tuple exclude ifnotshow all exclude.append r .pyc ... a example truth log p x logrank detectcodegpt ?
?args names args names.extend function spec.args iffunction spec.varargsisnotnone args names.append function spec.args args check forarg nameinarg specs.keys ifarg namenotinargs names args check self.check arg specs arg name returnargs check b example truth log p x logrank detectcodegpt save test random .8audio load audio fn num chunks len audio chunk sizelistener.clear fori chunkinenumerate chunk audio audio chunk size print r str i .
num chunks buffer update buffer chunk conf listener.update chunk space?space spacespacespace?
c example truth log p x logrank detectcodegpt if num is .if n return0 count sum of digits under mod 9answer 0foriinrange len n answer answer int n if digit sum is multiple of answer else remainder with .if answer return9else returnanswer d example figure examples of machine and human authored code snippets with corresponding predictions.
table v performance of different perturbation strategies perturb.
type mlm newline space newline space t .
.
.
.
.
t .
.
.
.
.
code snippet is well structured with newlines and spaces.
its resemblance to machine generated code is striking posing a significant challenge for distinction.
this example highlights the difficulty of detecting machine generated code among wellstructured human written code with standard coding styles.
g. ablation study rq2 in our ablation study we compare the effectiveness of different perturbation strategies for detecting machine generated code using the codellama 7b model on the stack dataset.
the results summarized in table v illuminates the comparative advantage of our stylistic perturbation approach.
we observe that both newline and space perturbations independently offer substantial improvements over the traditional mlm based codet5 perturbation technique as in detectgpt and detectllm for natural language.
also the combination of newline and space perturbations further enhances the detection performance with the highest auroc score of .
at t .2and .
at t .
.
the consistent outperformance of our combined perturbation strategy across both temperature settings affirms its potential as a robust solution for detecting machine authored code.
h. impact of perturbation count rq3 to gauge the impact of perturbation count on the efficacy of our method we conduct experiments with varying numbers of perturbations.
results in table vi reveal a rapid ascent in the auroc score as the number of perturbations increases from totable vi impact of varying the number of perturbations perturbations t .
.
.
.
.
.
t .
.
.
.
.
.
underscoring the efficiency of our perturbation approach.
notably an increase to perturbations already yields robust detection performance with further increments leading to diminishing improvements.
this suggests that our method requires a relatively small number of perturbations to effectively discern between human and machine authored code.
this implies that our method is not only effective but also efficient.
i. performance of cross model code detection rq4 incoder phi 1starcoder wizardcoder codegen2 codellama detection modelincoder phi starcoder wizardcoder codegen2 codellamasource model0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure cross model detection performance in previous sections we primarily assess the efficacy of detectcodegpt within a white box framework where we can get access to the logits of the original code generation model as defined in section iv a .
in real world scenarios 9accessing the original model for code generation detection is often impractical so we conduct cross model detection experiments where we use other llms as surrogate models to compute the naturalness scores.
the evaluation results on the stack dataset at t .
presented in figure highlight detectcodegpt s adaptability in cross model detection.
while the algorithm excels in the white box setting its performance endures with only a slight reduction in cross model application.
for instance starcoder when detecting code generated by wizardcoder and codellama yields auroc scores of .
and .
respectively compared to an auroc of .
when detecting its own output.
we can also notice a performance decrease when detecting codegen2 s output.
this is possibly due to the fact that codegen2 is trained on a more diverse dataset containing more natural language text .
however phi demonstrates a relative proficiency with a score of .
to detect codegen2 s output which implies an ensemble of diverse detection models may enhance the system s robustness as suggested in .
these results indicate that detectcodegpt is a model free method that is robust against model discrepancies making it a viable solution for real world applications where the source model could be unknown or inaccessible.
vi.
d iscussion a. why is detectcodegpt effective?
we attribute the effectiveness of our detectcodegpt to the following two factors preservation of code correctness the mask and recover perturbation in detectgpt brings minor mistakes easily e.g.
misuse of an identifier rendering the code non functional and has negative impact on the naturalness score.
such codecracking perturbations will violate the assumption of minimal impact on the code s naturalness score if it is human written in section ii b. in contrast inserting newlines and spaces does not affect the correctness of the code in most cases thereby ensuring the effectiveness of our method.
emulation of human randomness as discussed in section iii d human inherently exhibit less naturalness and more randomness in their use of stylistic tokens such as spaces and newlines than machines.
for example a human programmer may freely insert whitespace especially newlines in the code as they deem fit whereas a machine programmer usually tries to stylize the code in a more standardized and modularized manner.
our proposed perturbation strategy mimics human s free usage of spaces and newlines thereby making the perturbation more random as is desired according to section iii.
b. strength of detectcodegpt compared with existing methods detectcodegpt eliminates the need to perturb code multiple times for each llm and thus brings more efficiency.
compared with supervised counterparts detectcodegpt distinguishes itself with a zero shot learning capability enabling it to detect machine generated code withoutthe necessity for training on extensive datasets.
this modelagnostic advantage means that it can be generalized across various code llms.
c. limitations and future directions the main limitations of our work lie in the following two aspects firstly due to the computational constraints we only focus on a set of llms within 7b parameters.
as the landscape of llms rapidly evolves incorporating a wider array of more and larger llms could significantly bolster the generalizability and robustness of our findings.
secondly our current analysis centers exclusively on python code while the features of other programming languages may not be fully explored.
however based on the analysis of our method in section vi b we believe that our method can be effectively generalized to other languages especially where the functionality of code won t be much affected after inserting newlines and spaces like c c java and javascript.
looking ahead to future work we plan to further improve the effectiveness of detectcodegpt when detecting machinegenerated code at higher levels of generation randomness.
we note from table iv that although detectcodegpt outperforms other baselines at t .
there is still large room for improvement.
although ensembling multiple detection models may help improve the detection performance we look forward to exploring more effective perturbation strategies based on code styles to further enhance the detection efficacy.
vii.
r elated work a. machine generated text detection recently there has been much effort in detecting machinegenerated text .
the two main categories of detection methods are zero shot and training based methods and our detectcodegpt falls into the former category which eliminates the need for training data and brings more generalization ability.
as for zero shot methods they are usually based on the discrepancy between likelihood and rank information of human and machine s texts .
leveraging the hypothesis in detectgpt that machine generated text often has a negative curvature in the log probability when the text is perturbed many perturbation based methods have been proposed .
these methods usually perturb the text by masking a continuous span of tokens and then recover the perturbed text using another llm like t5 .
the benefit of these methods is that they are zero shot and can be applied to any llm without access to training data.
however the perturbation process is time consuming and computationally expensive.
when it comes to the training based methods finetuning the roberta or t5 model with data collected from different model families at different decoding settings is a common practice .
additional information like graph structure perplexity from proxy models have been shown to be helpful for detection.
moreover techniques like adversarial training and contrastive learning have also been proposed to improve the detection performance.
the main challenge of training based methods is that they often 10lack generalization ability and require access to training data from the target model .
b. machine generated code detection research on identifying machine generated code remains relatively scarce and is purported to be more challenging than discerning machine generated text according to the empirical study in .
gptsniffer was first proposed to detect machinegenerated code with supervised codebert training .
concurrent works and also explore perturbationbased methods for detecting machine generated code under similar framework to detectgpt for text .
our approach differs from these methods in that we perform a comprehensive empirical analysis of the differences between machine and human authored code.
based on the insights from the analysis we proposed a innovative stylized perturbation strategy to achieve a more efficient and effective detection method.
another related topic revolve around code watermarking techniques which embed unique markers into the code either during the training or generation .
the detection of these watermarks subsequently enables the recognition of code generated by machines.
it should be noted however that these watermarking methods are primarily designed to address issues related to code licensing and plagiarism .
their reliance on modifications to the generation model renders them unsuitable for general code detection tasks.
viii.
c onclusion in this paper we perform an in depth analysis of the nuanced differences between machine and human authored code across three aspects of code including lexical diversity conciseness and naturalness.
the results provide new insights that machines tend to write more concise and natural code adhering to common programming paradigms and the disparity is more pronounced in stylized tokens such as whitespaces that represent the syntactic segmentation of code.
based on these insights we have proposed a new detection method detectcodegpt which introduces a novel stylized perturbation strategy that is simple yet effective.
the experimental results of detectcodegpt confirm its effectiveness demonstrating its potential to help maintain the authorship and integrity of code.
acknowledgement this research is supported by the national key research and development program of china grant no.
2023yfb4503802 and the national natural science foundation of china grant no.
.
we would like to thank the anonymous reviewers for their valuable feedback and suggestions.