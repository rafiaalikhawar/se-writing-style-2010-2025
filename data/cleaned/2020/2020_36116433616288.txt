docontributing files provideinformation aboutoss newcomers onboarding barriers?
felipe fronchetti virginia commonwealthuniversity richmond usa fronchettl vcu.edudavid c.shepherd lousiana stateuniversity batonrouge usa dshepherd lsu.eduigor wiese universidadetecnologica federaldo parana campomourao brazil igor utfpr.edu.br christoph treude theuniversity ofmelbourne melbourne australia christoph.treude unimelb.edu.aumarcoaur liogerosa northernarizona university flagstaff usa marco.gerosa nau.eduigor steinmacher northernarizona university flagstaff usa igor.steinmacher nau.edu abstract effectively onboarding newcomers is essential for the success of open source projects.
these projects often provide onboarding guidelinesintheir contributing f iles e.g.
contributing.md ongithub .these f ilesexplain forexample howto f indopentasks implementsolutions andsubmitcodeforreview.however these f iles often do not follow a standard structure can be too large and miss barriers commonly found by newcomers.
in this paper we proposeanautomatedapproachtoparsethesecontributing f ilesandassesshowtheyaddressonboardingbarriers.wemanually classi f ied a sample of f iles according to a model of onboarding barriers fromthe literature trained a machine learningclassi f ier that automatically predicts the categoriesof each paragraph precision .
recall .
and surveyed developers to investigate their perspective of the predictions adequacy of the predictions wereconsideredadequate .wefoundthatcontributing f iles typically do not cover the barriers newcomers face of the analyzed projects missed at least out of the barriers faced by newcomers missed at least .
our analysis also revealed that informationaboutchoosingataskandtalkingwiththecommunity twoofthemostrecurrentbarriersnewcomersface areneglectedin morethan75 oftheprojects.wemadeavailableourclassi f ierasan onlineservicethatanalyzesthecontentofagivencontributing f ile.ourapproachmayhelpcommunitybuildersidentifymissing informationintheprojectecosystemtheymaintainandnewcomers can understandwhat toexpect in contributing f iles.
ccsconcepts softwareanditsengineering opensourcemodel humancenteredcomputing collaborative andsocial computing systems and tools .
esec fse december sanfrancisco ca usa copyright held bytheowner author s .
acm isbn979 .
novices onboarding floss open source software engineering acm referenceformat felipe fronchetti david c. shepherd igor wiese christoph treude marco aur lio gerosa and igor steinmacher.
.
do contributing files provide information about oss newcomers onboarding barriers?.
in proceedings of the 31st acm joint european software engineering conference andsymposiumonthefoundationsofsoftwareengineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
introduction newcomers to open source software oss projects encounter severalbarrierstomakingtheir f irstcontribution .forexample an overly complex codebase or a workspace that is challenging tobuildsapsnewcomers motivationtocontribute .research showsthatthesebarriersdiscouragenewcomers whooftengive up before completingasingle contribution .
to onboard newcomers usuallyconsult the project s documentation or contact the project team .
yet project members arebusymakingtheirowncontributions canonlyhelpalimited number of newcomers at a time and may not be able to manage synchronous communication due to time zone differences .
for onboarding newcomers appropriate documentation is more efficient andscalable .
unfortunately most oss projects existing documentation is either low quality or non existent .
some studies point to problemssuchasdocumentation f ilesthatareincorrect incomplete andoutdated .otherstudiesidenti f iedfurtherdocumentation barriers for newcomers including unclear and scattered documentation withinformationoverloadfromunimportantinformation sharplycontrastingwithmissingnecessaryinformation .these andotherdocumentationde f icienciesimpactallcontributorsbut havemoreimpactonnewcomerssincetheyneedtoorientthemselves in anewenvironment .
previous work showed that newcomers found themselvesmoreorientedandunderstoodtheprocessbetterwhenthe right information was provided in an organized way.
however little workhas speci f ically focused on enhancing newcomers documentation and identifying what information is missing from the thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa f. fronche t ti d. c.shepherd i. wiese c.treude m. a.gerosa i. steinmacher contribution guidelines .
with the goal of improving this situation weautomaticallyanalyzeandclassifythedistinctinformation typescontainedinexistingcontributingdocumentationaimed at newcomers.
in this study we answer the following research questions rq1.how accurately can we automatically classify the content ofcontributing f ilesingithubprojects?
rq2.towhatextentdoossprojects contributing f ilescover contentrelatedto newcomers contributionbarriers?
to answer these questions we created an oracle by manually annotatingthecontributing f ilesfrom500softwareprojects according to the newcomers barriers model proposed by steinmacher et al.
.
then we trained a machine learning classi f ication model that identi f ies steinmacher et al.
s six categories of barriers precision .
recall .
.
the results were further validated through a survey with experienced developers where thedevelopersagreedthat ofthecategoriespredictedwere adequate.finally weusedourmodelacross2 274publiclyavailable projectstobetterunderstandtowhatextentcontribution f ilescover the information about the barriers faced by newcomers.
we found that contributing f iles are woefully inadequate for supporting newcomers.
most contain four or fewer of the six expected categories of onboarding barriers and thousands of projects do not have acontribution f ile.
to facilitate researchers and practitioners to build upon our workbyaccessingitscategorizationmodel wedevelopedanonline service that automatically analyzes the content of a given contributing f ile .ourtooloffers potentialbene f itsforprojectmaintainersandcommunitymanagers byallowing them toevaluateand enhancetheircontributing f iles based on feedback from our classi f ier.
this becomes especiallysigni f icantinecosystemscomprisingmultipleprojects where community managers oversee various distinct projects.
achieving consistencyacross projectsis crucial toreducecognitive loadand promote smooth transitions between projects within a software ecosystem.
adhering to the maintenance of contributing f iles is a recognized best practice to assist newcomers in onboarding open sourcesoftware projects .
related work inthissection wehighlightrelatedstudiesaboutdocumentationin oss theautomaticcategorizationofsoftwareengineeringartifacts andbarriersnewcomers face inoss projects.
documentationissues in oss repositories.
documentation playsacrucialroleinsoftwareprojects andde f icienciesindocumentation f ilescanhindertheirutilityfordevelopers .
lethbridgeetal.
identifythatdocumentation f ilescontainexcessive information are hard to maintain and make it challenging to locate helpful information.
such considerations are also present in the context of oss communities .
according to dias et al.
fromtheperspectiveofossdevelopersandmaintainers osscontributorsneedtoensurethequalityandconsistencyofdocumentation f iles.ourstudyhelpstoprocessexistingdocumentation f ilesandclassifycontentrelevantfornewcomers helpingmaintainers identify missing information in their contributing guidelines andnewcomers locate relevantinformation.automatic classi f ication of software engineering artifacts.
several studies have automated the categorization of artifacts in software engineering .
for example prana et al.
broke down theheadersofreadme f iles inossrepositories into eightcategoriesofinformation.basedonthemanualannotation of readme f ile sections the authors implemented a classi f icationmodel thatautomaticallyidenti f iesthecontext ofa section in a readme f ile.
they argue that labeling sections makes the knowledge discovery process easier for visitors.
we followed a similar method and share their idea that categories may help navigate the information space especially for outsiders.
for a different type of documentation f ile robillardand chhetri categorized textfragmentsfromapidocumentationbasedontheirrelevance for programmers.
the authors proposed a coding guide and an automated technique to classify text fragments into three levels of relevance for programmers.
the variety of studies exploring the automatic categorization of information in software related artifactsiswide e.g.
butourstudyisamongthe f irstto automatically categorize information in contribution guidelinesto addressnewcomers contributionbarriers.
newcomersin osscommunities supportingand engaging newcomersincreasesthelikelihoodofnewcomerscompletingtheir contributions which is essential for the long term viability of oss projects .
without adequate retention project development progress slows jeopardizing the existence of such communities .tostudythisissue researchersidenti f ieddifferent obstaclesnewcomersfaceintheonboardingprocess focusingon theperiodbetweentheirinitialcontactwiththeosscommunity andtheir f irstcontribution .
steinmacher etal.
propose ataxonomyof 58barriers newcomers face when joining oss projects.
documentation issues appearasacentralsourceofproblemsfornewcomers includingalreadymentionedchallengessuchasinformationoverload scattered andoutdateddocumentation andlackofnecessaryprojectinformation.
someresearchersinvestigatedhowexistingapproaches supportnewcomersonboarding.morespeci f ically theyfocusonunderstandinglabelstoguidenewcomerstochoosetheirtasks exploringthe roleofq a websites inhelpingthe onboarding andcodevisualization .otherstudiesdiscusshowdocumentation can help and cause problems and how it may impact the newcomers experience .webelieveourresultsinformoss projectstowardbettersupportingnewcomerswiththeinformation they needwhen joining aproject.
itisclearfromtheliteraturethatdocumentationiscriticalfor onboarding newcomers inoss.
despite the efforts in categorizing artifactsrelatedtoprojectdocumentation nobodyofknowledge exists aboutthe appropriateness ofcontribution guidelines foronboardingnewcomers.inthispaper weaddressthisbyanalyzing thecontentofcontributing f ilesfromossrepositoriesinterms ofbarriersnewcomers face.
research method overview to answer our research questions we manually analyzed contributing f iles from projects and built a classi f ier to label information known to be relevant for newcomers.
according to github guidelines contributing f ilesarewhereoneshould 17docontributing files provideinformationabout ossnewcomers onboardingbarriers?
esec fse december3 san francisco ca usa extraction of repositories contributing.md files using github apiselection of repositories hosted on github based on their popularity level and programming language conversion of contributing.md paragraphs to spreadsheets for annotationannotation of paragraphs based on information known to be relevant for newcomersdata extraction data analysis text preprocessing on annotated paragraphs conversion of paragraphs into tf idf and heuristic based features feature selectionpreprocessing classification training and selection of best classifierevaluation evaluation of selected classifier on test data application of a survey to evaluate classifier predictions9514 projects files categories definition six categories of information known to be relevant for newcomers were used for analysis prediction contributing.md file of .
projects predicted by the best classification model20.
paragraphs figure researchmethod followed inthisresearch frombuildingthecorpus to assessing the classi f ier create guidelines to communicate how people should contribute to yourproject.
additionally theopen sourceguide reinforces that a contributing f ile tells your audience how to participate in your project... is an opportunity to communicate your expectationsforcontributions.
therefore newcomersexpectto f ind relevantinformation to avoid common onboarding barriers .
theresearchmethodwasconductedinsixsteps aspresentedin figure1 weextractedthecontributing f ilesfrom2 913oss projects hosted on github.
the paragraphs of a random sample of f iles were manually annotated.
the annotated paragraphs were pre processed and then converted into statistical features i.e.
term frequency inverse document frequency and heuristicbased features in which a rule basedapproach was performed .
wetrained f ivedifferentclassi f icationmodelswiththesefeatures andcomparedtheirperformances.
wesurveyeddevelopersto assess the quality of the classi f ications.
finally we used our model to classify the content of contributing f iles and to understandto what extentthey cover the onboarding barriers.
all scripts models data and results are available in our replication package .
in the following we present more details of the methodandresults ofeachstep.
buildingthe corpus to train our models we collected and manually categorized the contentofcontributing f ilesfrom asetofoss projects.
.
categoriesde f inition we manuallylabeled eachparagraph of the 500contributing f iles according to the way steinmacher et al.
organized the categories of barriers on the flosscoach portal .
the portal was created based on a barriers model built based on a systematic literaturereview interviewswithmultiplestakeholders andsurveys withinosscommunities providingacomprehensiveaggregationof thebarriersnewcomersfacewhenjoiningossprojects.inaddition tothecomprehensivenessofthemodel wechosetofollowthese categories since the work by steinmacher et al.
showed that organizing the information in these categories lowered the barriers related to orientation and contribution process.
these are the categorieswe used cf contribution f low derived from the newcomer orientation barrier category mapped to the contribution f low shown under howtostart inflosscoach thiscategoryde f inesthesteps that a newcomer needs to follow to contribute to the project.
this category appears as for example an ordered list of steps to follow oras asetofparagraphs describingthe currentprojectwork f low.
ct chooseatask alsoderivedfromthe newcomerorientation barrier it is mapped from the choose a task menu item inflosscoach.
thiscategory explains how newcomers can f ind a task or issue to contribute to the project.
it may also contain descriptions of different types of tasks appropriate for newcomers.
tc talk to the community related to the communication issues barriers this category refers to information about how a newcomer can get in touch with community members and how to f indamentor.thiscategoryincludes forexample linkstocommunicationchannels communicationetiquette communityguidelines andtutorialsonhowto startaconversation.
bw buildlocalworkspace mappedfromthe localenvironmentsetuphurdles thiscategorydeterminesthestepsanewcomer needstofollowtobuildthelocalworkspace.itmayincludeinstructionssuch as bashcommands andchanges incomputer settings.
dc deal with the code derived from code architecture hurdles itdescribeshownewcomersshoulddealwiththesourcecode.
thiscategorymaycontaincodeconventions descriptionsofthe sourcecode andguidelines onhowto writecode for the project.
sc submit the changes directly mapped from change request hurdles this category represents information about how newcomers should submitacontributionto the project.
.
data collection .
.
project selection.
we selected the most popular oss repositorieshostedon githubwhenwestartedthe datacollection aug written in at least one of the top programming languages usedintheplatform.weselectedprojectsbasedontheirpopularity and programming language to avoid repositories that were toy projects or unrelated to software development.
the selection of projectsbypopularitywasbasedonthestudyofborgesetal.
which discusses stars as a unit to measure the popularity of oss projects on github and shows that in their population three out of four developers consider the number of stars before using or 18esec fse december3 san francisco ca usa f. fronche t ti d. c.shepherd i. wiese c.treude m. a.gerosa i. steinmacher table1 numberofprojectsremovedperlanguageandtheirrespectivereasonsforexclusion.the n valuerepresentsthetotal numberofprojectscollected foralanguage.contributing f iles may have been excluded formore than onereason.
removed because javascript python java php c c typescript shell c ruby contributing... n n n n n n n n n n wasmissing size .5kb wasnot in english wasnot in markdown projects removed contributingtoagithubproject .inadditiontoit thisisafairly common wayto sample projectsongithub .
toidentifythetop10most usedlanguages weusedtheranking provided by github octoverse which showed at that time javascript python java php c c typescript shell c and ruby.
we aimed to get the f irst projects per language ranked by stars.
however the github api provides only a few pages containing the top projects and we could not collect 000projects for somelanguages.we collectedatotalof9 repositories.
to ensure that all the selected repositories had a valid contributing f ile we de f ined a set of f ilters to remove projects in ourdataset.weremovedfromoursampletheprojectsthathada contributing f ile i.missing wefocusedonlyonprojectsthatfollowedtheguidelines from github to keep in this speci f ic f ile information abouthowto contribute ii.smallerthan0.5kb to f ilteroutthose f ilesthatredirectto guidelines not hostedongithub orempty f iles iii.
written inalanguageotherthanenglish iv.not in markdown format which was the most prevalent format inour sample out of3 projects that had a contributing f ile were inmarkdown .
.
contributors forks pull requests stars figure2 distributionofcontributors forks pullrequests andstars perprojectconsidered as valid.
table1showsthenumberofprojectsperprogramminglanguage removed from our dataset.the f inal setof repositoriescomprised2 915projects.
afterapplyingthe f ilters wekeptadiversenumber of projects in terms of the number of contributors forks pull requests and stars see figure .
the programming languages with the highest number of repositories included in the analysis were typescript javascript andruby.
.
.
documentationforma t ting.
topreparetheprojectsforthe qualitativeanalysis weconvertedthecontribution f ilesintospreadsheets.eachspreadsheetmapstoallparagraphsofonecontributing f ile in our sample.
the f irst column of each row of the spreadsheet contained in plaintext format one paragraph of the documentation f ile for the respective project.
we followed the de f inition of a paragraph provided by the speci f ication of github flavored markdown which speci f ies it as a sequence of non blank lines that cannotbeinterpretedasotherkindsofblocksforms .
tofacilitatethe workoftheannotators wecreatedheadersforsixcolumns each representingone of the six categories we aimed to identify during the qualitative analysis.
.
data annotation after transforming the contributing f iles into spreadsheets we conducted the annotation process.
we annotated a total of spreadsheets from500projects .inthe f irststep twoannotatorslabeled spreadsheets of a random subset of projects and discussed how the categories should be assigned to each paragraph.
to measure the agreement between the annotators they independently labeled the spreadsheets divided into three consecutive stages consistingof10spreadsheetsperstage.theannotationconsistedof analyzing and labeling each paragraph according to the categories presented in section .
.
at the end of each stage the reviewers compared their labels and discussed their differences to align their understanding of each category.
we use cohen s kappa coefficient to measure the agreement between the annotators .
after the f irst stage the annotators reached an agreement of and discussed the potential meaning of categories.
for the other two stages theagreementwas and79 respectively.theoverall agreementbetweentheannotatorswas whichwasconsidered sufficient given the multi classnature of the data.
.
.
documentationannotation.
afterreachingasubstantialagreement thereviewersproceededtoanalyzetheremaining f iles which were split between them.
a total of spreadsheets were annotatedduringthequalitativeanalysis resultingin20 733paragraphs analyzed.
we had to dismiss f iles that did not present any informationaboutthesixcategoriesofbarriers whichwerereplacedby 19docontributing files provideinformationabout ossnewcomers onboardingbarriers?
esec fse december3 san francisco ca usa 66other f ilesfromourdataset.afterthereplacement weendedup with19 961paragraphs.
.
corpus characterization in figure we present the distribution of paragraphs analyzed per f ile.the average numberof paragraphs for our set of500projects was41 andthemedianwas29.twoprojectshadonly2paragraphs minimum andonehad422paragraphsinasingle f ile maximum .
figure distribution ofparagraphs per f ile.
table2showsthe distribution ofcategoriesinoursample.more than paragraphs were categorized as submit the changes and more than as deal with the code and contribution f low.
on the other hand choose a task and talk to the community appear in and paragraphs respectively.
still paragraphscouldnotbecategorizedunderanycategory.weanalyzedtheseparagraphsandfounddifferenttypesofcontentthatdid not belong to any category.
the most recurring cases were thank you messages license statements or the complete license links to other sites instructions on how to open an issue information in differentlanguages githubbadges andlistsofcontributors.
table2 characterizationofthedatasetconsideringthesix categoriesofbarriers category paragraphs projects avg.per f ile choosea task .
talkto thecommunity .
build local workspace .
.
contribution flow .
.
deal withthecode .
.
submitthechanges .
.
no category .
.
buildingand evaluating the classifier wetrainedmachinelearningmodelstoclassifyinformationaccording to the six categories de f ined in section .
.
the annotated spreadsheetswereusedtoextractfeaturesforclassi f ication section5.
.
the data was prepared using text pre processing techniques.thefeaturescreatedweredividedintostatisticalfeatures i.e.
extracted using statistical methods and heuristic features i.e.
extractedthroughidentifying linguisticpatterns .
thefeatureswerethenappliedtosupervisedlearningalgorithms to f indthebestclassi f icationmodelforthisproblem section .
.
theannotateddatasetwassplitintotworandomsubsets.atraining set of the dataset was used to compare the different classi f iers and a test set of the dataset was reserved for testing the classi f ication algorithm with the highest evaluation score.
the algorithms were evaluated based on their classi f ication scores and a f inalmodelwastrainedusingthebest performingclassi f icationalgorithm section .
.
figure4provides an overview of the classi f ication process whichisdetailedinthe following sections.
test set evaluate estimators performance using five supervised learning algorithms we executed nested cross validation fold gridsearch training best algorithm best configurationtext preprocessing lemmatization stop words and punctuation removal on text columntraining setspreadsheets text column columns of relevant categories feature extraction conversion of text column to tf idf and heuristic features final evaluation analysis of evaluation metricstext preprocessing separately feature extraction separately figure the classi f icationprocess.
.
featureextraction inthefeatureextractionprocess theannotatedparagraphs section .
were converted into numerical data.
we divided the feature extraction processinto fourstages textpre processing thede f initionofstatisticalfeatures thede f initionofheuristicfeatures and feature selection.
.
.
textpre processing.
beforecreatinganyfeaturesfortheclassi f ier three pre processing techniques used in text classi f ication were applied to the paragraphs lemmatization stop words and punctuationremoval.inthelemmatizationprocess theaffixesof words in each paragraph were removed turning the words back to their root form .
words such as submits submitted andsubmitting forexample werereturnedtotheirrootform submit.to reduce the number of ineffective words in the paragraphs classi f ication wealsoremovedstopwords excludingwordscommonly found in the english vocabulary e.g.
conjunctions and pronouns .
for the same purpose punctuation was also removed from the text.
for both lemmatization and stop words removal we used the implementations providedbythe nltklibrary .
.
.
statistical features.
we converted the annotated paragraphs into tf idf features using the tfidfvectorizer method of the scikitlearn library .
in this approach we represented words as n grams of size one and two .
the acronym tf idf is a reference for the multiplication of two statistical measures used in text classi f ication termfrequency tf andinversedocumentfrequency idf .fortermfrequency wemeasuredhowoftenwords occur in a paragraph number of occurrences of each word per paragraph dividedbythetotalwordsinthatparagraph .forthe inversedocumentfrequency wecountedhowoftenwordsoccur compared to the entire set of paragraphs.
the multiplication of both measures gives us statistical features that show the relative importanceofeachword.
.
.
heuristic features.
the set of statistical features was combinedwithheuristicsfoundthroughqualitativeanalysistoenrich the characteristics used in classi f ication.
we adopted a strategy 20esec fse december3 san francisco ca usa f. fronche t ti d. c.shepherd i. wiese c.treude m. a.gerosa i. steinmacher used by previous work in which features are generated by analyzinglinguistic patterns intheannotated paragraphs.during the manual analysis the annotatorsselected words that could characterize speci f ic categories and be used as patterns for a classi f icationbasedonheuristics.forexample theword commit was commonly found in paragraphs annotated as submit the changes see table 3for examplesofothercategories .
table examples ofheuristic featurespercategory.
category relatedexamples contribution flow clone push merge pullrequest contribution chooseatask issue issue tracker label fork talk to thecommunity mailinglist contact email conduct slack build localworkspace tool package update dependencies deal with thecodecode snippet library debug codingconvention method variable submit thechanges commit diff review test fetch usingtherule basedmatchingapproachofthespacylibrary we assigned an equal set of heuristic features to each paragraph in thetrainingprocess.eachfeaturerepresentedapattern paragraphs wereassignedthevalueof1whentheycontainedtherespective wordsand0otherwise.
.
.
feature selection.
to avoid using features that could be consideredirrelevanttoourclassi f ication weremovedtheoneswith the lowest scores.
the selectpercentile method of the scikitlearnlibrarywasusedwithchi squareasthescorefunction.featuresthatfellbelowthe u1d461 uni210epercentilewereremoved.
wemanually testedasetofpercentiles 5th 10th 15th 20th basedonthedefault value of scikit learn which is the 10th percentile.
we chose the15 u1d461 uni210epercentile as it performed best as it is commonly done in the literature .
.
findingthebest classi f ier asetofclassi f ierswastrainedto f indthebestlearningalgorithmto solveourclassi f icationproblem.totraintheclassi f iers weusedtwo multi classtrainingstrategies one vs rest ovr andone vs one ovo .intheovrstrategy abinaryclassi f ierwastrainedfor eachcategory.theassignmentofacategoryforaparagraphwas thenmadebyidentifyingthebinaryclassi f ierthatbestrepresented the respective paragraph i.e.
the one with the best scores .
in the ovo strategy the samples of each category were grouped in pairs andthecomparisonwasmadeinabinaryclassi f ierfortwo categories at a time.
to identify what category should be assigned foraparagraph thepredominanceofacategoryamongallthepairs wasconsideredas the decision method.
the following classi f ication algorithms were trained during thisstep randomforestclassi f ier kneighborsclassi f ier linearsvc multinomialnb logisticregression.
the selection was based on similar studies using text classi f ication in software engineering .asabaseline wetrainedtwodummyclassi f iers oneusingthemostfrequentclasslabelobservedinthetrainingsetand one providing completely random predictions.
as highlighted in table2 we noticed that the number of instances per category was unbalancedinourdataset soweusedthesmoteoversampling technique to achieve a better balance between the classes.
thesmote algorithm was implemented using the imbalanced learn library a module designed for unbalanced datasets that are recommendedbythescikit learncommunity.
still weusedchatgpt gpt .
model using a few shot learning approach tocompareourresultswiththeperformanceofthisllm.forthe few shotlearning werandomlyselected12instancesofparagraphs in our training set for eachcategory.
then we prompted chatgpt to classify200instancesrandomly selectedfrom our test set.
.
evaluationmetrics to measure the overall performance of the classi f iers we used a combination of three evaluation metrics for data classi f ication precision recall and f1 score.
precision also known as con f idence provides the proportion of positive samples that were correctly predicted incontrasttoallthesamplespredictedaspositive .
recall gives the fraction of positive samples correctly predicted by theclassi f ier andthef1scoreprovidestheharmonicmeanbetween precisionandrecallvalues .thisisamulti classproblem andthe resultingvaluesare the weightedaverageof allclasses.
.
cross validation wetestedtheperformanceofourclassi f iersusinganestedten fold cross validationstrategy .thisalgorithmdividesthedataset offeaturesandlabelsintotenparts.thetenpartsarecombinedinto tendifferenttrainingandvalidationsubsets alsoknownasfolds.
foreachfold u1d456dividedinto u1d458parts u1d458 the u1d458 u1d456partisusedasthe validationset andtheremainingpartsareusedastrainingforeach classi f ication algorithm in our list.
the average of the weighted f1 scores of the u1d458different classi f iers gave us an overall performance for eachlearningalgorithm.
to increase the chance of selecting the best parameters for each algorithm we applied the gridsearch method to the crossvalidationinternalloop.thevaluestestedwerebasedonthedefault valuesinthescikit learnlibrary.weselectedthebestcon f iguration i.e.
classi f ier parameters and training strategy to train a f inal classi f ication model.
.
classi f iertraining with the best learning algorithm selected we trained a classi f ier usingthecompletedatasetusedinthepreviousstep trainingsetin fig.
andthetestset wasusedfortesting.thiswasdone toshowthereliabilityofboththemodelandtheresults .
this enabled us to use our complete annotated training set of thesample andtestonthe20 oftheoriginalannotateddataset that wasnot previously used.
.
classi f ierhuman evaluation tofurtherevaluateourclassi f icationmodel wesurveyed46individualsusingamazonmechanicalturk .weinvitedonlyindividualswithpriorprogrammingexperiencebyspecifyingonthe amazon platform employment industry software it services as a selection criterion.
the survey was divided into training evaluation anddemographics.inthetrainingsection weintroduced the survey and described the six categories of information used toclassifythecontent.toguaranteethatwewouldonlyconsider participants who paid attention to our questionnaire we asked 21docontributing files provideinformationabout ossnewcomers onboardingbarriers?
esec fse december3 san francisco ca usa them to matcheach category with theircorresponding de f initions on a subsequent page.
an attention check question was also includedamongthequestionsofourquestionnaire.wedismissedthe answersfrom29participantswhoselectedthewrongde f initionfor more than one category or did not mark the correct answer ontheattentioncheck 5ofthemselectedthewronganswers in both.
we ultimately considered valid answers for analysis.
although the number of remaining participants is small due to the substantial number of discarded answers this is considered a common limitation of crowdsourcing platforms as suggested in recent studies .
the literature also mentions that cleaning the answers is necessary to guarantee data quality and consistency .
in the evaluation section we asked participants to judge the quality of the predictions.
we used paragraphs from contributing f iles of randomly selected projects that were not part of thetrainingortestset andthuswereunknownbytheclassi f ier.
we randomly selected ten paragraphs classi f ied into each category from that set of oss projects to use in the questionnaire.
for each participant we randomlyassigned18paragraphs 3percategory .
we gave the same number of sections for each category instead of a complete f ile per participant to ensure the number of paragraphsassessedpercategorywasbalanced.moreover weusean approach in which the participants recognize if an item belongs to a category aided recall instead of asking the participants to label them unaided recall because items requiring recognition areeasierthanitemsthatuseunaidedrecall .so weprovided annotated paragraphs and asked whether the category was correct.
weaskedeachparticipanttorateeachpredictionusinga4 point likert scale extremely adequate to extremely inadequate .
we employed a point likert scale to compel respondents to take a de f initive stance preventingthe use ofaneutral option .
inthedemographicssection weaskedparticipantstoprovide information about their experience with oss projects.
this section includedtwomultiple choicequestionsaboutyearsofexperiencein programming and maintenance of oss projects and two questions about their role as participants in oss coder or non coder and their contributions to documentation inossprojects yes no .in table4 we present the overall experience of our participants in programmingandossprojects.asexpected allofourparticipants hadsomeexperienceinprogramming withthemajorityofthem havingbetween3and15yearsofprogrammingexperience.
intermsofexperienceasmaintainersinossprojects ofour participants had at least some experience and of them had between and years of experience as software maintainers.
all 14participantswithexperienceinossde f inedthemselvesascoders and10 workedwithdocumentation intheirprojects.
table4 experienceofsurveyparticipantsinprogramming andoss projectmaintenance.
experience type programming oss noexperience less thanorequal to 3years greater than3years andless than15 years greater thanorequal to years 0table5 f1scoresforclassi f ierstestedintheten foldcrossvalidation process.
withsmote withoutsmote ovr ovo ovr ovo rf .
.
.
.
knn .
.
.
.
svc .
.
.
.
lr .
.
.
.
nb .
.
.
.
dummy freq.
.
.
.
.
dummy rand.
.
.
.
.
chatgpt macrof1 .
rq1.
howaccuratelycan we automatically classify the content ofcontributing files?
this section details the evaluation of the machine learning models.
.
comparingdifferentclassi f iers to identify the best classi f ier for our problem we compared the outputs of f ive machine learning algorithms and two dummy algorithmsinaten foldcross validationprocess inadditiontochatgpt with a few shot learning approach .
table5presents the f1 scores for each classi f ier.
the best f1 score of .
is from the linearsvcclassi f ier withoutoversamplingandusingtheovrstrategy.
the second best score is from the same classi f ier con f iguration but usesthe ovomulti class strategy.
the performance ofchatgpt usingafew shotapproachreachedanoverallmacroprecisionof .
recall of .
and f1 equal to .
.
ignoring the dummy classi f iers and chatgpt the classi f ication model with the worst scores of .
and .
was knn.
such results follow similar performancefoundbypranaetal.
whocategorizedthecontent ofreadme f iles.
becauseofitsscoresandsimilarperformanceinotherstudies linearsvc was chosen as the f inal machine learning algorithm.
basedontheoutputsofthegridsearchalgorithm wefoundthat the best hyper parameters for linearsvc were iterations max iter regularizationequaltoone c andtolerance equal to .
tol .
.
the linearsvc algorithm was trained againwiththis f inalcon f igurationwithoutoversamplingandusing the ovr strategy as this combination provided the best f1 score in our comparison ofclassi f iers.table 6presents thetraining data andits performance per class inrelation to the test set.
in table6 we can see that the performance varies per category.
the information about deal with the code dc and build local workspace bw barriersisfairlywellpredicted f10.711and0.
respectively .
on the other hand choose a task ct and contribution f low cf hadthelowestscoresof0.379and0.
respectively.
some external factors may have in f luenced such performances.
thenumberofinstancesperclass forexample mightjustifythe low score of choose a task ct which on average had only paragraphperprojectanalyzed seesection .
.thefactthatthe contribution f low contained more generic information than other content speci f iccategories suchasbuildlocalworkspace might alsoexplainthe difference inperformance.
22esec fse december3 san francisco ca usa f. fronche t ti d. c.shepherd i. wiese c.treude m. a.gerosa i. steinmacher forthesakeofcomparison weincludetheresultsofthechatgpt few shotlearningapproachperclassintable .ascanbeobserved the linear svc model outperforms chatgpt in this context in almostallmetricsandcategories.theexceptionaretherecallforthe talktocommunityandcontributionflowcategories.interestingly chatgptcouldnotcorrectlyidentifyanyparagraphbelongingto the choose a task category although it received instances and incorrectly predicted instances.
considering the overall metrics the linearsvc model is more than 2x better than chatgpt in terms ofrecall precision andf1.
table performanceofthe f inalmodel linearsvc .
category f1 precision recall buildlocal workspace .
.
.
dealwiththe code .
.
.
talk to the community .
.
.
submit the changes .
.
.
choose atask .
.
.
contribution f low .
.
.
nocategoriesidenti f ied .
.
.
overall .
.
.
table performanceofchatgptwith few shotlearning.
category f1 precision recall buildlocal workspace .
.
.
dealwiththe code .
.
.
talk to the community .
.
.
submit the changes .
.
.
choose atask .
.
.
contribution f low .
.
.
nocategoriesidenti f ied .
.
.
overall .
.
.
confusionbetweencategories infigure wepresenttheconfusionmatrixproducedbythe f inalclassi f icationmodel.usinga confusionmatrix wecanassessthesimilaritybetweenthedifferentcategoriesofinformationandverifywhatlabelscontainfalse positives.
the main diagonal represents the true positives for each class andtheupperandlowertriangularsubmatricesrepresentthe misclassi f ications.
in line with the previous results contribution f low cf and choose a task ct are the categories of information with the highest amount of misclassi f ications with only true positives.
contribution f low cf hadmorefalsepositivesassignedtodealwith the code dc than its own category.
such results may con f irm the assumption that because contribution f low cf contains a wide rangeofinformation andchooseatask ct hasjustafewsamples usedfor training they performedpoorly.
all other categories had more than true positives.
build local workspace bw and deal with the code dc had the lowest number of false positives .
talk to the community tc also presented good performance with less than incorrect predictions.
this may be because such categories contain more speci f ic contentandagoodnumber ofsamples per class.
figure confusion matrix forlinearsvc.
legend bw buildlocal workspace dc deal with thecode tc talkwiththecommunity sc submitthechanges ct choosea task cf contributionflow .
.
observationsfrom thesurvey infigure wepresenttheparticipants evaluationofthepredictionsmadebyour f inalmodel.forallthecategories atleast30 of the predictions were considered extremely adequate for their paragraphs andatleast69 ofthepredictedcategorieswereconsidered at least somewhat adequate.
the best evaluated category was buildlocalworkspace bw with47 of participantsconsidering its predictions extremelyadequate.
100cfcttcbwdcsc percentagecategoriesextremely adequate somewhat adequate somewhat inadequate extremely inadequate figure6 survey participants evaluationofpredictionsmade by the f inalclassi f icationmodel.
legend bw buildlocal workspace dc deal with thecode tc talkwiththecommunity sc submitthechanges ct choosea task cf contributionflow .
whenweaggregateextremelyadequateandsomewhatadequate deal with the code dc leads the adequacy board with of predictions considered adequate.
contribution f low cf has the lowestestimates with31 ofitspredictionsestimatedassomewhat orextremelyinadequate.thesecondtolastplaceisheldbychoose a task ct and talk to the community tc with of their predictionsconsideredsomewhat inadequateorless.suchresults follow similaroutcomesfoundintheevaluationscoresoftable con f irming the nature ofour predictions.
tofurtherunderstandthedisagreementbetweentheclassi f ier output and the crowd we manually analyzed the paragraphs in 23docontributing files provideinformationabout ossnewcomers onboardingbarriers?
esec fse december3 san francisco ca usa which50 ormoreoftherespondentsdisagreedwiththepredictions.
in summary we found that the prediction was incorrect in cases.inthe3other cases thepredictionwascorrect relatedto choose atask andone relatedto buildthe workspace .
answertorq1 aftercomparing f ivesupervisedlearning algorithms wewereabletoclassifythecontentofcontributing f iles achieving an f measure of .
with precision .655andrecall .
.althoughdifferentcategories of information differed in performance in general oftheclassi f icationswereconsideredappropriateby external reviewers.
rq2.
to whatextentdo contributing files cover content related to contribution barriers?
weusedtheclassi f icationmodeltopredictasetoftheremaining contributing f iles from our dataset that we had not used intheprevioussteps.fromthe9 f iles weremoved6 599because they didnot meetthe f iltering criteriapresentedinsection .
.
figure7shows the distribution of projects and the average of paragraphspercategoryinthecontributing f ilesinwhicheach category appeared at least once.
a total of .
projects hadatleastoneparagraphthatdidnotbelongtoanycategory with an averageof15 unidenti f iedparagraphs per f ile.
average projects projects average figure average number of paragraphs per category in the contributing f iles predicted.
legend bw buildlocalworkspace dc dealwiththecode tc talkwiththecommunity sc submitthechanges ct choose a task cf contribution flow nc no categories identi f ied .
submitthechanges wasthecategorywiththehighestnumberof paragraphs percontributing f ile appearing in2 192projects.
thedeal with the code category represented the second highest average of paragraphs per contributing f ile and the second in thenumberofprojects beingidenti f iedin1 660projectswithan average of paragraphs per f ile.
contribution flow was the category with the third highest frequency appearing in projects with an average of only two paragraphs per project.
a similar phenomenon happened with build local workspace projects paragraphs project .
talk to the community projects and chooseatask projects were inthe lowestpositions.
regarding the frequency of categories per project not all categories are covered by the contributing f iles see figure .
from our set of oss projects we identi f ied with content relatedtofourofthesixcategories 603relatedto3 and f iles containing only two categories .
for projects we identi f iedinformationabout f ivecategories andforonly65projects theclassi f ieridenti f iedinformationaboutallsixcategories.on thelowerbound onlyonecategoryofinformationwasidenti f ied for projects.
we also found projects where no categories wereidenti f ied.inamanualinspectionoftheircontributing f iles wedetectedthatnoneofthempresentanyinformationthat couldbemappedtoanyofthesixcategories validatingtheanalysis made by the classi f ier.
while some presented ways to report an issue otherscontainedlinks tocontribution guidelineselsewhere someonthe githubwiki othersoutsidegithub .
6choose a task talk to the community contribution flow build the workspace deal with the code submit the changes figure distribution of categories per contributing f ile predicted.
the percentages represent the proportion of each category intherespective subset of f iles.
thedistributionofcategoriesisinlinewiththedistributionof projects manually annotated during the qualitative analysis providing further evidence of the adequacy of the classi f ier.
the onlydifferencesarethatnoprojectsanalyzedhadzerocategories of information and the contribution f low category had a slightly higher averageofparagraphs per contributing f ile.
insummary morethan50 ofthecontributing f ilespresent information pertaining to fewer than categories of barriers faced by newcomers whileonly15 present informationclassi f ied in5 or 6different categories.
these results in addition to thefact that morethan60 oftheprojectscollecteddonothaveacontributing f ile evidencethatthishighlyrelevantresourcefor new contributors is still inadequate for mitigating barriers faced by newcomers.
in particular the lack of content about choosing a task ct and building the workspace bw is crucial and may hinderonboarding andleadto dropouts .
answertorq2 mostcontributing f ilesfocusonthe f inalstagesofthecontributionprocess.categoriescontaining information such as how to submit the changes and dealwiththecodearethemostfrequent whileinformation about choosing a task and contacting the community isoften missing.
24esec fse december3 san francisco ca usa f. fronche t ti d. c.shepherd i. wiese c.treude m. a.gerosa i. steinmacher discussion lack of essential information for newcomers.
in our study we noticed that many projects do not provide primary information that new contributors may need when attempting to contribute to a project.
this was highlighted in previous literature andevidencedinouranalysisbasedonthenumberofcategories of information covered per project in figure 8and table .
most projectshadamaximumof3outof6categoriescoveredintheir contributing f iles.thissuggeststhatossprojectsmightnot satisfy newcomers needs in terms of documentation when considering the categories de f ined by the literature.
some of the most critical barriers faced by newcomers are not covered by the contributing f iles.
table 2shows that only of the f iles analyzed had information about how to choose a task and of thempresentedsomeinformationabouthow tobuildtheirlocal workspace.
the curseofexpertise i.e.
theinherentcognitive bias stemming from the deep familiarity with the subject matter may hamper project maintainers ability to accurately evaluate the comprehensiveness and clarity of their documentation.
our results can shed light on the gaps in the existing documentation from the perspective ofbarrierscommonly facedbynewcomers.
a more critical problem is also evidenced in table showing that of the projects in our sample more than in absolute numbers do not have a contributing f ile available in theirrepositories.althoughsomeprojectsprefertouseotherresources to explain their contributing process e.g.
valhalla usesasectionintheirreadme f ile manypopularrepositoriesdo notcontainanyorientationfornewcomers eventhoughtheyare open to external submissions e.g.
google sanitizers microsoft phpsql nvidia nccl .
most f ilesfocusonthecontributionprocess s f inalsteps.
in figure weshow theaveragenumber ofparagraphsidenti f ied per category in the projects of our qualitative analysis.
the results suggestthatthecategorywiththehighestnumberofparagraphsis submit the changes followed by contribution flow and deal with the code.
although dealing with the code focuses on the more generalstepsoftheproject submittingthechangesanddealing with the code are intended to be relevant for newcomers in the later stages of their contribution after they selected a task built theirworkspace andestablishedcommunicationwiththeproject s community.thisresultsuggeststhatprojectstendtofocusmore onthelaststagesofthecontribution assumingnewcomersalready knowhowto implement theircontribution.
implications for practice and research.
as a result of this study we also implemented a web tool to provide feedback to project maintainers about their contributing f iles .
the maintainer only needs to input their project url and our tool reviews the project s contributing f ile using our classi f ication model.
the tool provides a chart showing the distribution ofparagraphspercategoryofinformation a discussionaboutthe dominantcategories i.e.
thehighestnumberofparagraphs and weak categories i.e.
the lowest number of paragraphs and a comparison of the input project with other popular repositories on github.inaddition thetoolprovidesacleardescriptionforeach categorywhenthereportispresentedtotheuser highlightingwhy theyareimportant.thereportprovidedbythetoolalsosuggestscontributing f iles that maintainers could use as inspiration to enrich a speci f ic faulty category.
we envision the proposed tool as astarting pointto support betterdocumentation f iles.
this tool could be particularly useful to community builders andmanagerswhooverseeanon trivialnumberofprojects.those playing these roles need information about the content of contributing f ilesinmultipleprojectsintheecosystemtotakeaction.
theclassi f iermaysupporttheireffortsbyprovidinginsightsinto the types ofinformation available for eachproject.
the tool is also an important step toward implementing automated on demanddeveloper documentation whichautomatically parsesdocumentationandgeneratesresponsestouserqueries and smartassistants .
these tools need to parseexisting documentation and classify information in order to provide adequate assistanceto newcomers.
from the research perspective our study helps to understand howthecurrentcontentofcontributing f ilesaddressesnewcomers needs.ourworkcanbeextendedtoevaluatethecontent qualityof the contributing f iles which mayhelpnewcomers f indappropriatedocumentation.
futureworkcanalsoinvestigate the subcategoriesofsteinmacheretal.
smodel .
limitationsand design decisions inthissection wepresentourwork slimitationsandtrade offsfor researchdesigndecisions.
using themost popularprojectsfrom github.
we focused our study on github and the results may not generalize for the whole oss universe.
nevertheless github is arguably the most popularosshostingplatform.additionally theselectedprojects may not generalize to github as well since our projects were selectedbasedontheirprogramminglanguageandpopularity.
still there may be projects that are not exactly software projects in our sample like algorithms and awesomelists inamanualanalysis these projectscorrespond to ofour sample.we acknowledge thatamorediversesetofprojectswouldpotentiallybringmoredata points with different styles.
however focusing on more popular projectsandongithubbringsmorecon f idenceabouttherelevance of the contributing f iles analyzed.
we opted to keep a more trustfulsetofprojects ratherthanexpandingthe data points.
unit of analysis.
our approaches to selecting f iltering analyzing andclassifyingdocumentation f ileswerebasedonpriorstudies .
still our decision to choose paragraphs as the unit of analysisinsteadoflinesorlargerchunksoftextcouldimpactour results.
we attempted to use lines as units of analysis but they did not provide enough context to identify the categories during manualanalysis since theinformation incontributing f ilesis highly contextual.
paragraphs provide enough context for identifying the categories and markdown provides a standard approach to split the contentintoparagraphs i.e.
blanklines .
otherapproachestodeterminingthecontentofcontributing f iles could also be used.
for example a classi f ication model basedonsectionnamescouldbeagreatalternativeforourdecision.
we decided to make our classi f ication based on paragraphs and not on section names for the same reasons that we did not use lines as units of analysis.
we also decided to keep duplicated paragraphsfromdistinctprojectsinourdataset ascontributing 25docontributing files provideinformationabout ossnewcomers onboardingbarriers?
esec fse december3 san francisco ca usa f iles fromdifferent projects mayfollow similar guidelines.
we ran linearsvcwithouttheduplicatedparagraphs andtheperformance wassimilar to our f inal model precision .
recall .
.
itisalsoworthmentioningthatweonlyanalyzedthecontent available on the contributing f iles.
we did not explore any externallinksfromthese f ilesorresourcestheyreference wealsodid notchecktextual html .ris orothertypesof f ilescontainingcontributionguidelines.
weanalyzed95readme f ilesfrom randomly selected projectsthatwedismissedbecauseoftheabsenceofthe contributing f ile onlythreehadlinkstoexternalguidelines andsixhadsectionsrelatedtocontribution.thiscouldhavelimited the conclusions made for projects such as apple swift and othershighlightedinsection whosecontribution f ileonlycontained directions to other sources of documentation.
future studies are encouragedto i analyzeonelevelofdepthusingthelinksavailable in the contributing f iles and ii understand how to use the proposedapproachtorefactorcontributingguidelinescontained onreadmeandothertextual f ilesonto contributing f iles.
representativenessofcontributors perspective.
toassess thequalityofourclassi f icationmodel weinvitedparticipantswith programmingexpertisetoanswerquestionsinwhichtheyjudgeda setofpredictionsmade by ourclassi f ier.
although we introduceda tutorialatthebeginningofthequestionnaire wecannotguarantee thattheanswersgivenbytherespondentsrepresenttheperspective of contributors or the correctness of the predicted categories.
to mitigate this problem we not only asked the participants to match thecategoriesde f initionwiththeirnamesintheearlystagesofthe surveybutalsoincludedanattentioncheckquestionintooursetof questionstoensureparticipantsdidnotrandomlyassignanswers for them.
once again our choice was guided by the trustfulness of the data points.
we kept only a small set of answers which can be consideredmorereliable thanhavingmoredata pointsandlosing reliability.
coverage of the categories and information.
we decided to useapre existingsetofcategoriestolabelourdatasetaccordingto the barriersnewcomers couldface.
weacknowledgethatthecategoriesanalyzedmaynotcoveralltheinformationanewcomermay needwhencontributingtoaproject.however thesetofcategories resulted from several studies investigating problems associated withdocumentation f ilesinthecontextofossrepositories .
we opted to classify our data using a validated set of categories ratherthanexplore potentialnewcategories.
construction of the classi f ier.
to build a classi f ication model from scratch a set of design decisions were made throughout theprocess.weunderstandthatotherstrategiescouldhavebeen adoptedinbuildingourmodel e.g.
theuseofadditionalpre trained models and that the decisions made may have an in f luence on the performance reported in this study.
to mitigate this issue we comparedourclassi f ierwithchatgptinsection .
andtrainedasupervisedmodelwiththesamedatasetusingfasttext precision .
recall .
.
both strategies presented a similar or worse performancethanour f inalclassi f ier.wealsoundertookanablation study to determine the impact of the heuristic and statistical features.
two models were constructed using the same con f igurations as our f inal classi f ier one solely with statistical features precision .
recall .
and the other with only heuristic features precision .
recall .
.
both models exhibited performance comparable to orless than our f inal estimator.
conclusion aprimarydocumentationresourcefornewcomersembarkingon open source software projects is the contributing f ile.
located within repositories these f iles outline the project s contribution guidelines.
while many oss communities utilize contributing f iles toorient newcomers thecomprehensivenessoftheir content waslargely unexplored.
inthispaper weinvestigatetheextenttowhichcontributing f ilesaddresstheonboardingbarriersnewcomersfaceinoss projects.drawinguponabarriermodelfromexistingliterature wemanuallyanalyzedcontributing f ilesfrom500projects.our f indings indicate a notable lack of information of the projects lackedcontentinatleasttwoofthesixinformationcategories with missing details in three or more categories.
notably our manualreviewrevealedthatover75 oftheprojectsfailedtoinclude guidanceontaskselectionandworkspacesetup twokeybarriers for newcomers as highlightedbysteinmacheretal.
.
wealsobuiltamachinelearningmodeldesignedtoautomatically classify the information from contributing f iles from other projects and thereby help projects identify missing information in their f iles.overall theclassi f ierperformedwellinthismulticlass problem with an overall precision of .
and a recall of .
.
the performance was good for four out of the six categories of information f1 .
build local workspace deal with the code talktothecommunity andsubmitthechanges.exceptionswere howtochooseataskandcontributing f low withlowrecall .
andf1 of0.
and0.
respectively.
in summary our f indings indicate that many oss projects need to improve the comprehensiveness of their contributing f iles tobettercatertonewcomers.evaluating2 274projectsusingour machinelearningmodel our resultsechoed the f indingsfrom our qualitativeassessment oftheprojectslackedcontentinatleast two of the six information categories and were de f icient in threeormorecategories.toassistwiththisissue wedevelopedan online tool designed to offer feedback to project maintainers about howtheir contributing f iles addressonboardingchallenges ensuringthatthecommunitiesarebetterequippedtowelcomeand nurture theirnextgenerationof contributors.
data availability the artifacts usedinthis paper are available onzenodo .