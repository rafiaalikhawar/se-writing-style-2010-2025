learning to handle exceptions jian zhang sklsde lab beihang university china zhangj act.buaa.edu.cnxu wang sklsde lab beihang university china wangxu act.buaa.edu.cnhongyu zhang the university of newcastle australia hongyu.zhang newcastle.edu.au hailong sun sklsde lab beihang university china sunhl act.buaa.edu.cnyanjun pu sklsde lab beihang university china puyanjun nlsde.buaa.edu.cnxudong liu sklsde lab beihang university china liuxd act.buaa.edu.cn abstract exception handlingis an importantbuilt in featureof many modernprogramminglanguagessuchasjava.itallowsdevelopersto deal with abnormal or unexpected conditions that may occur at runtime in advance by using try catch blocks.
missing or improper implementation of exception handling can cause catastrophic consequences such as system crash.however previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism and tend to ignore it until a system failure forces themto doso.
to helpdevelopers withexception handling existingworkproducesrecommendationssuchascodeexamples and exception types which still requires developers to localize the tryblocksandmodifythecatchblockcodetofitthecontext.inthispaper weproposeanovelneuralapproachtoautomatedexception handling whichcanpredictlocationsoftryblocksandautomaticallygeneratethecompletecatchblocks.wecollectalargenumberof java methods from github and conduct experiments to evaluate ourapproach.theevaluationresults includingquantitativemeasurement and human evaluation show that our approach is highly effectiveandoutperformsallbaselines.ourworkmakesonestep further towards automated exception handling.
ccs concepts software and its engineering error handling and recovery automatic programming.
keywords exceptionhandling deeplearning neuralnetwork codegeneration alsowithbeijingadvancedinnovationcenterforbigdataandbraincomputing beihang university beijing china.
corresponding author xu wang wangxu act.buaa.edu.cn.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
reference format jianzhang xuwang hongyuzhang hailongsun yanjunpu andxudong liu.
.learning tohandleexceptions.in 35thieee acm international conferenceonautomatedsoftwareengineering ase september21 virtual event australia.
acm new york ny usa pages.
https introduction exceptionhandlingmechanismisessentialformodernprogramming languages such as java and c to build robust and reliable software systems .
it provides an effective means of dealing withexceptionalconditionsandrecoveringfromthembyusingtrycatch blocks .
exception handling separates error handling code from regularsource code and improves programcomprehension and maintenance .
however missing or improperly using exception handling statements may cause severe problems such as systemcrash andinformationleakage .onerecentstudy revealsthatthereisasignificantrelationshipbetweenexceptionflowcharacteristicsandpost releasedefectsinjavaprojects.
therefore it is crucial for developers to handle exceptions.
despitetheimportanceofexceptionhandling theexceptionhandlingstatementssuchastry catchblocksinreal worldsoftwareare often poorly written and error prone.
for example prior studies indicatethatmanyindustrialsystemsexhibitpoorquality code with respect to exception handling.
besides similar bad practices have been found in open source software .
among our collected millions of original java methods in projects fromgithubwithhighnumbersofstarsandforks only14.
ofthe methods apply try blocks to capture exceptions and .
of these methodsdonothavecatchblocksanddonothingwhenexceptions occur.
the reason is twofold.
on the one hand developers tend to ignore exception handling code or even have little knowledge aboutwhetherornotexceptionhandlingisneededuntilanerror occurs .
on the other hand the exception handling code is often difficult to write for developers especially when it comes to program evolution .
hence as suggested in it is essential toproposeautomatictechniquesforassistingdevelopersinwriting high quality exception handling code.
existingworkonautomatictechniquesforexceptionhandling mainly includes violation detection of exception handling policies andexceptionhandlingcoderecommendation .for example thummalapentaetal.
minedassociationrulesonthe sequenceoffunctioncallsintryandcatchblocksandappliedthem 35th ieee acm international conference on automated software engineering ase todetectviolationsofexceptionhandlingpolicies.suchapproaches are helpful to improve the quality of exception handling code yet theyarelimitedsincetheyassumethatthetry catchblockshave beencompletelywrittenbydevelopers.bycontrast exceptionhandlingcoderecommendationismoreusefultoassistdevelopersin writing the try catch blocks.
given an under development code fragment without try catch blocks rahman et al.
retrieved similarcodefragmentsthatincludetry catchblockstorecommend exception handlingcode examplesfrom popularcode repositories at github.
nguyen et al.
recommended exception types and methodcallsinthecatchblockbyutilizingthefuzzysettheory andn grammodel .however therearetwomaindrawbacks of recommendation based approaches.
first these studies assume that developers understand when to seek for help about writing exception handling code and where the try catch blocks should be put whileinpracticeitisnotalwaysthecase second even though the recommended code such as examples exception types andmethodcallscanprovideusefulknowledgetoaiddevelopers in writing try catch blocks the developers still need to modify the code to fit the context.
inthispaper weproposeanovel neuralapproachtoautomated exceptionhandlingcode generation namely nexgen whichcan predict locations of try blocks and automatically generate the complete catch blocks.
we separateexception handling into two tasks one is to predict the try block locations for identifying source code that needs to handle potential exceptions the other is to generate complete catch blocks to deal with the exceptions.
consider a typicalcodingscenariothatadeveloperhaswrittenacodefragment asshownin1 a whichisajavamethodfromtheelasticsearch1 project.hereweremoveitsoriginalexceptionhandlingcodefor illustration purpose.
the first task is to identify which statements need to be put in a try block.
in this example the statement at line in figure a requires a try block.
the second task is to generate the catch block.
in this example according to the code above line in figure b the catch block in lines is generated to handletheexception.inthisway thepotentialexceptionscanbe automatically captured and handled.
ourapproachutilizesdeeplearningtolearnregularities patterns fromalargeamountofhistoricalexceptionhandlingcode.more specifically we tackle the two tasks as follows try block localization .
we design a locator that jointly learnstodeterminewhetherornotagivencodefragment needs to handle exceptions and localize where to put the try block if it does.
considering the sequential naturalness ofsourcecode wetransformthelocalizationprobleminto thesequencetaggingproblem .toavoidthelong term dependencyproblem weregardthecodefragmentasa sequence of statements and hierarchically encode them into vectors so as to predict the label sequence.
we first use long short term memory lstm to encode statements into statementvectorsandthenuseanotherlstmtocapturethe sequentialdependencyofthestatementsequence.besides we apply hierarchical attention mechanism on top of the twolstms to letthe model pay attention to individual tokens and statements.
char decrypt char chars if !isencrypted chars returnchars string encrypted new string chars encrypted text prefix.
length chars.length encrypted text prefix.
length byte bytes bytes base64.getdecoder .
decode encrypted byte decrypted decryptinternal bytes encryptionkey returnchararrays.
utf8bytestochars decrypted a a code fragment without exception handling code public char decrypt char chars if !isencrypted chars returnchars string encrypted new string chars encrypted text prefix .length chars.length encrypted text prefix.
length byte bytes try bytes base64.getdecoder .
decode encrypted catch illegalargumentexception e throw new elasticsearchexception unable to decode encrypted data e byte decrypted decryptinternal bytes encryptionkey returnchararrays.
utf8bytestochars decrypted b the code fragment after adding exception handling code figure an example of exception handling catch block generation .
different from previous work such as we take all the code before the catch blocks as the context instead of simply relying on the try blocks.
thusweconsiderthedependencies e.g.
howthevariable encrypted is initialized for writing the exception handling code.
a simple way is to treat all the tokens before line including line as a single sequence then apply the encoder decoderarchitecture togeneratethetokensof thecatchblockinlines11 13onebyone.however sucha model will not distinguish whether the tokens are inside or outsidethetryblock.therefore weproposetoencodethem separately by two encoders and fuse the context vectors using the learned weights.
furthermore it is obvious that not all the tokens before the try blocks are helpful and may be noisy data especially for the long methods with complexlogic.inordertoconcentratemoreonthedependencies withoutdestroyingthenaturalness i.e.
deletingthosenoisy code weincorporateprogramslicingtechnique bytakingthestatementsinthetryblocksastheslicingcriterion andbacktrackingdependenciesbetweenstatements.weadd an additional attention module with masks to attend onlyto the tokens in these slices.
finally we fuse the slicingbasedcontextvectorwiththecontextmentionedabove and generate code by the lstm decoder.
wecollectalargenumberofjavamethodsfrompopularopen source repositories in github to construct datasets for the twotasks.
we conduct extensive experiments on the datasets to eval uate our approach and the results demonstrate the effectivenessof our models on both tasks.
more specifically for try block lo calization we achieve an accuracy of .
in terms of correctly predictedmethodsandanf1 scoreof77.
intermsofcorrectly predicted statements.
for the generation of catch block our model cangeneratethecatchblockswith22.
exactmatchesandableu value of .
.
both models significantly outperform the baselines.
30moreover we perform a human evaluation for the generated code and the results confirm the superiority of our model.
in summary this paper makes the following contributions weproposeanovelapproachtoautomatedexceptionhandling includingtwoneural network basedmodelstolocalize potential exceptions in the source code and generate code to handle the exceptions.
weprovidetwodatasetswithover700kjavamethodsfor experiments on automated exception handling.
we havepublicly released the datasets to promote further research on this interesting topic.
weconductextensiveexperimentstoevaluateourapproach using the collected datasets.
we also perform a human evaluation.theresultsshowthattheproposedapproachiseffective and outperforms all baselines.
the remainder of this paper is organized as follows.
section presents the problem formulation of the automated exception handling.section3describesthedetailsofourapproach.section describes datasets evaluation procedure andevaluation results.
we discuss our approach in section .
we describe threats to the validityofthisworkandrelatedworkinsection6andsection7 respectively.
finally we conclude our paper in section .
problem formulation as illustrated in figure the problem of automated exception handling can be decomposed into two successive tasks.
the first taskisdeterminingwhichstatementsmaythrowexceptionsand should be enclosed by a try block.
the second task is generating thecorrespondingcatchblocksforhandlingsuchexceptions.for simplification we call the two tasks try block localization andcatch blockgeneration respectively.weformallydefinethemasfollows.
try block localization.
this task aims to localize the statements in a code fragment that should be enclosed by try blocks.
giventhecodefragment c s1 s2 ... s k wherekisthenumber ofstatements thetargetistofindonesequence y y1 y2 ... y k whereyi yornmeanswhetherthestatement siisinonetryblock or not.
in addition the adjacent statements with label y should be put in one same try block.
if all statements are labelled by n it meansthatthecodefragmentdoesnotneedtryblocks.notethat ymay include multiple disjoint subsequences of ys for multiple non nested try blocks and we do not consider the nested try catch blocks in this work.
catchblockgeneration.
foratryblock thistaskisdesigned togeneratecodetokenstocomposethecorrespondingoneormore catchblocks.supposeatokensequence c c1 c2 ... c l where ci i means one token in the try block and the source code beforeit thetargetistogeneratethetokensequenceofcatchblocks y y1 y2 ... y m so that ycan handle the exceptions of the try block.here ymayincludemultiplecatchblocksfordifferenttypes of exceptions thrown by the try block.
wecanautomaticallyhandleexceptionsofcodefragmentsbased onthetryblocklocalizationandcatchblockgeneration.weachieve this by learningregularities patternsfrom a large amount ofhistorical exception handling code which can be obtained by mining open source repositories such as github.
in our work we utilize deeplearningasitcanbettercapturecontextualinformationandunderstand the semantics of source code.
we design two deep neural network based models for the two tasks and introduce our approach in detail in the next section.
approach inthissection weintroduceourneuralnetworkbasedapproachto automatedexceptionhandling includingthemodelfortryblock localization and the model for catch block generation.
note that if developersknowwheretheexceptionsarethrownandhavewritten tryblocks thecatchblockgenerationcanworkindependentlyto produce the source code of corresponding catch blocks.
.
try block localization .
.
overview.
as defined above given a code fragment without try catch blocks the try block localization task is to find statement subsequences of the code fragment that may throw exceptions and shouldbeenclosedbytryblocks.sincealltheinputtokensofthe code fragment will be checked if they are inside or outside a try block wetransformitintoasequencetaggingproblem where eachtokenofthecodefragmentwillbetaggedwithlabelyorn to represent whether it should be in a try block or not.
if the code fragmentdoesnotneedtryblocks thelabelsareallns.thiskindofproblem has been well studied in the nlp community such as partof speech pos tagging namedentityrecognition ner andsemanticrolelabeling .amongthesestudies recurrentneuralnetwork rnn basedapproachesshowmany advantages over the traditional ones.
but they were designed toparsenaturallanguagesentenceswhichareusuallyshorterthan code fragments.
as a result they suffer from the long term dependencyproblem whenappliedtotagsourcecode.sincethetry blocks are always comprised of one or multiple statements we propose a two layered neural model to tag the source code which encodes tokens of a statement into the statement vector and tags a code fragment at the statement level.
figure 2shows theoverall architectureof ourtry blocklocator.
first we split the code fragment into a sequence of statements.
for each statement we obtain its token sequence and use the lstm to encode the token sequence for the semantic vector of the statement.
besides we apply the attention mechanism o nt h e token sequence of the statement to consider different weights ofindividual tokens.
after encoding all the statements of the code fragmenttoasequenceofstatementsemanticvectors weutilize anotherlstmtofurtherencodethestatementsequence aswellas an attention mechanism for capturing the importance of different statements.finally weadoptabinaryclassifieroverthenormalized vectors to predict the labels of the corresponding statements.
.
.
try block locator.
specifically given a code fragment without try catch blocks we obtain the sequence of statements sin a line by line manner excluding blank and comment lines.
let s s1 s2 ... s k wherekis the number of statements si ci1 ci2 ... c il wherelisthelengthofonestatement siinterms oftokens.givenastatement si wefirstembedeachtokenintoa vectorviaanembeddingmatrix we thatis xit wecit.weusethe lstmnetworktoencode sibasedonthetokenembeddings which reads the token embeddings from xi1toxilto obtain the hidden figure the architecture of our try block locator states.
at time step t the hidden state hitis obtained by hit lstm hit xit t .
furthermore weadoptthebidirectionallstm bi lstm to enhance thecapability of capturingthe contextinformation within thestatement wherethehiddenstatesofbothdirectionsareconcatenated to form the new states hit lstm hit xit t .
hit t .
intuitively not all the tokens in the statement are important for throwing an exception.
to incorporate such knowledge in ourmodel we exploit the attention mechanism over the hidden states to assign important tokens with higher weights and then aggregatethemtoformthestatementvector.theprocesscanbe expressed as the following equations uit tanh w hit b it exp u latticetop itu summationtext.1l t 1exp u latticetop itu si l summationdisplay.
t 1 ithit.
as listed in equation we first transform the hidden states into anewhigh dimensionspacewithaone layermlptopreparefor scoringtheimportance.thenwecomputethescoresofthesehiddenstatesbymeasuringhowtheymatchthefixedvector u andgetthe normalizedweights itthroughthesoftmaxfunction.here u acts asthequeryofwhichtokenismorelikelytothrowanexception.
finally avectorrepresentation siofthestatementisobtainedby computing the weighted sum of the hidden states.
afterobtainingasequenceofstatementvectors wemodelthe sequential dependencies of the statements by feeding them intoanotherbi lstm.similarly wegetthehiddenstatesofthestatementshi bilstm si .next weapplytheattentionmechanism by considering different weights of statements so that ui tanh wshi bs i exp u latticetop ius summationtext.1l t 1exp u latticetop ius hi ihi.
the probability of label y for statement siis calculated by yi sigmoid wphi bp wherewpistheweightmatrixand bpisabiasterm.totrainthe model we employ the binary cross entropy loss that is defined as l y y n summationdisplay.
n 1l summationdisplay.
i yni log yni yni log yni whereyis the ground truth label for each statement represents theparameterstobelearnedand nisthetotalnumberofinstances in the training set.
during prediction we make the label as y if yi otherwise n where is the threshold.
basedonthepredictions allconsecutivestatementswithylabels willbeenclosedbyonetryblock.inthisway weobtainthelocations oftryblocks.onceonetryblockisdeterminedinthecodefragment we need to generate the corresponding catch blocks as well which will be described in the next subsection.
.
catch block generation .
.
overview.
forhandlingexceptionsofonetryblock inspired bythesuccessofneuralmachinetranslation nmt weadoptthepopularencoder decoderarchitecture toencodethesourcecode beforecatchblocksandoutputasequenceoftokensinthecatch blocks.animportantquestionhereiswhatcontextshouldbetaken astheinputforgeneratingthecatchblocks.onestraightforward solution is to only consider the source code in try blocks as recent studies did because the source code before try blocks contains much irrelevant and noisy data.
in this paper to facilitate explanation wecallthesourcecodebeforetryblocksasthe leading code.forexample lines1 7infigure1 b istheleadingcode.we findthatthecodeinacatchblockmaydependonsomestatements suchasvariableinitialization intheleadingcode.therefore asdepictedinfigure3 ourcatchblockgeneratorincludestwoencoders toseparatelyencodetheleadingcode i.e.theblueones andthe tryblock i.e.theredones andaslicing basedattentionmodule i.e.
the curves to filter out the noisy data in the leading code.
.
.
catch block generator.
we describethe threemaincomponents of the catch block generator in detail below.
first we only use the source code in the try blocks as the input.let w w1 w2 ... w n denotethetokensequenceofatry block wefirstencodethetokensbyabi lstmwiththeembedding layer to obtain the hidden states ht bilstm wt ht .
thenweuseanlstmasthedecodertodecodethevectorrepresentation of one try block and generate the tokens of its catch blocks figure the architecture of our catch block generator.
one by one.
specifically when generating the i th token at time stepi we update the hidden state qiof the decoder by qi lstm qi yi whereyi 1is the previous token.
to enhance the alignment ability of the decoder we adopt the attention mechanism over the hidden statesh h1 h2 hn of the encoder to compute the context vector vi.
for simplification we represent viby the following equation vi attention qi h .
different from u in equation here qican be considered as a queryvectortolearntoalignthetargettokenwiththesourceones.
second as mentioned before considering only the context ofthetryblocksmaymissimportantdependencyinformation.a simplesolutionistoconcatenatetheleadingcodeandthetryblock intoasinglesequenceandthentrainastandardencoder decoder model butsuchamodelcannotdistinguishthetokensoftheleading codefromthoseinthetryblocks.whileinpractice thescopeof thetryblocksdoesmattersincetheexceptionsareproducedwithin it.toovercomethedrawback wetrainanadditionalencoderfor storingtheinformationoftheleadingcode.let d d1 d2 ... d m represents the token sequence of the leading code according to equation7 weencodeitbyanotherbi lstmandgetthehidden statesh prime h prime h prime ... h primem .
then we get the context vector of the leadingcodebyequation9 thatis v prime i attention qi h prime .wefuse the two context vectors of the leading code and the try block by weighted sum with one mlp layer to form the context vector vci.
third therearealsomanynoisytokensintheleadingcode that may influence the performance of catch block generation.
for the long methods the leading code may include many unrelated tokens such as checking parameters or processing some data while thetryblocksdonotdependonthem.therefore weincorporate the program slicing technique to filter out the noisy tokens of the leading code in our model.
the slicing results are used to label the tokens of leading code by or .
when applying the attention on them we mask the noisy tokens with 0s which means that the attention weights of them will be .
in this way this attention module will only attend to the dependent tokens whose labels are 1s.
in short we initially take the statements in the try block asthe slicing criterion sc and recursively backtrack statements in theleadingcodethatmayinfluencethestatementsof scbydata dependencytoupdate sc.thenwegetthetokensequence d primeof allstatementsintheleadingcodefrom sc whichhasfilterednoisy tokensthathavenoinfluenceonthetryblock.weusethissequence d primeto label any token diindand get the labels l l1 l2 ... l m where li braceleftbigg i f d i d prime o t h e r w i s e .
weusethelabelsasmasksandcombinethemwiththeattention mechanism to enhance our model by considering the program dependencies and ignoring the noisy data.
thus vdi attention qi l h prime .
weconcatenatetheslicing based vector vdiwithvciandaddanother one layer mlp to get the final context vector vi.
based on the context vector we calculate the probability of generating the i th token of the corresponding catch blocks by p yi y1 ... y i c softmax wgvi bg wherewgandbgare the weight matrix and bias term respectively crepresentsthetwoinputsequencesoftheleadingcodeandthe tryblock.trainingsuchanattentionalencoder decodermodelis to minimize the loss function l n summationdisplay.
i 1m summationdisplay.
t 1logp yi t yi t c where isthetrainableparameters nisthetotalnumberoftraining instances and mis the length of each target sequence.
in practice the trained model can predict the next token one by one with top kcandidates by the beam search algorithm and finally generate the whole catch blocks.
evaluation inthissection wefirstintroduceourdatasetscollectedfromgithub2.
thenweconductexperimentstoevaluatetheeffectivenessofour models including automatic metrics and human evaluation.
.
data collection toprepareexceptionhandlingcodefragments wefirstcrawledthe repositories written in java at github and selected the top popular ones based on their total number of stars and forks.
these popularrepositoriesusuallyhavehigh qualitysourcecode since mostofthemcarryoutthecodereviewprocess toguarantee thesoftwarequality.weextractedjavamethodsbyparsingalljava files in the repositories using antlr4 and obtained methods in total.
obviously notallthejavamethodsareusefulfortrainingour models.
there are many small and simple methods that do not need exception handling such as get set methods.
thus we filtered out the methods whose source lines of code is less than .
we also discardedverylongmethods i.e.
morethan100lines toreducethe possibility of overfitting .
then we obtained methods for constructing our datasets.
for the try block localization task we searched for the methods including at least one try block and obtained results.
we foundthattherewereabout88 686caseswherethetryblockmissed thecorrespondingcatchblocks thusweremovedthesecases.we alsoeliminatedthemethodsthathavenestedtryorcatchblocks and finally got of them as positive samples.
in addition we randomlyselectedanequalnumberofmethodsthatdonothave any try block as the negative samples.
in this way we built our try block localization dataset namely tbld for evaluation.
based on the positive samples described above we built another datasetforthecatchblockgenerationtask.ifonepositivesample has more than one non nested try catch pair i.e.
one try block and the corresponding catch blocks we split it into multiple sampleswhich includeonlyone try catchpair.this produced432 samples in total.
however we found that some of them exposedan obvious bad practice that is catching an exception but we removed them to improve the quality of these code fragments.
finally we obtained samples as our catch block generation dataset namely cbgd .
.
experimental setup we conduct experiments on the tbld and cbgd datasets to evaluateourtryblocklocalizationmodelandcatchblockgeneration model respectively.
the two datasets are split into training sets validation sets and testing sets with fractions of and respectively.
we utilize antlr4 to tokenize all the code fragments.
the statistics of these two datasets are described in table wheremaxt avgtanduniqtarethemaximumnumberoftokens theaveragenumberoftokens andthetotalnumberofuniquetokens respectively.intheleftcolumn maxsandavgsdenotethe maximumandaveragenumberofstatementsinthejavamethodsre spectively.thesourceandtargetintherightcolumnrepresentthe source code beforecatch blocks and the catchblocks respectively.notethatasdefinedinsection2 therecanbemultipletryblocks in one java method and each try block may have multiple catch blocks thus we denote the number of try blocks in one method in thepositivesamplesoftbldbytrynum andthenumberofcatch blocks in one try catch pair of cbgd by catchnum.
theconfigurationsofourmodelsareasfollows.forthetryblock localization task we set the vocabulary size to 50k by selecting thetable the statistics of the datasets used in our study tbld cbgd java methods try catch pairs trynum catchnum trynum catchnum maxt 6403maxt of source avgt .9avgt of source .
maxs 99maxt of target avgs .7avgt of target .
uniqt uniqt most frequent words and replacing out of vocabulary words with unk becausetoolargevocabularymayleadtoworseperformance.
the embedding size and the dimensions of hidden states in lstms are128.thebatchsizeissetto32andthemaximumepochsis20.weuse the best trained parameters of the saved checkpoints for thelaterpredictionaccordingtotheirperformanceonthevalidationset.
weadoptthewidely usedadam astheoptimizerwithlearning rate .
for training our model.
the threshold for predicting labels is .
by default.
for the catch block generation task weimplement our model based on opennmt .
we keep the same settingsasaboveincludingthevocabularysize embeddingsizeand dimensionofhiddenstatesinlstmsoftheencoderanddecoder.
we set the length limits in terms of tokens of the source and targetofonetry catchpairto400and100 becausesuchsettings can cover most of their original lengths.
the batch size is set to and the maximum iterations is 100k.
when testing we leave the beam size kas the default since it yields good results.
alltheexperimentsareconductedonaubuntu16.04serverwith 16coresof2.4ghzcpu 128gbramandateslav100gpuwith 32gb memory.
.
evaluation metrics we evaluate the performance of different approaches on the two tasks.forthetaskoftryblocklocalization similartothemetrics in sequence labeling we use precision recall and f1 score to assesshowwelloneapproachpredictsthelocationsoftryblocks for code fragments in the testing set of tbld.
their values are calculated as follows precision correct y predict y recall correct y gold y f1 score precision recall precision recall.
here y represents that a statement should be enclosed by onetry block in a code fragment and we denote it as correct yif it is correctlypredicted.similarly predict yandgold yarethepredicted andgroundtruthlabelsofyinthecorpusrespectively.besides wealsowanttoknowtheoverallratioofmethodswhereallstatements are correctly predicted denoted by accuracy.
forthetaskofcatchblockgeneration weadopttheautomatic metric bleu in evaluating the quality of catch blocks.
this metrichasbeenwidelyusedinmanysimilartaskssuchasmachine translation source code summarization and api sequence generation .giventhegeneratedcode y primeandtheground truth 34y bleu measures the n gram precision between y primeandyby computing the overlap ratios of n grams and applying brevity penalty on short translation hypotheses.
the value is computed by bleu bp expn summationdisplay.
n 1 nlogpn wherepnis the precision score of the n gram matches between candidateand referencesentences.
thedefault bleucalculatesa scoreforupto4 gramsusinguniformweights n.bpisthebrevity penalty and defined as bp braceleftbigg i f y prime y e y prime y i f y prime y. likewise we also calculate the ratio of generated catch blocks that are exactly same with the ground truth ones referred to accuracy.
.
baselines .
.
try block localization.
while many previous studies use program analysis technique to detect and fix exception related errors such as string handling or api misuse detection they usually relies on compilable and complete source code.
we found no existing learning based approaches to directly predicting the locations of try blocks for incomplete code fragments thus we borrowseveralpopularmodelsoriginallyproposedforsequence labelingasthebaselines includingconditionalrandomfield crf bidirectional lstm bilstm bilstm crf and sequencetosequence seq2seq .webrieflyintroducethem below.
crf.
we use the linear crf model which is a sequence modeling framework for processing text and has been used forpredictingprogramproperties .ittakestherawtoken sequenceasinputandmaximizesthejointprobabilityofthe entire sequence of labels given the observation sequence.
wetrainacrftaggerofnltktoolkit totagthesource code.
bilstm.
this model has shown its effectiveness for modeling sequential data .
it can capture the semantics of the wordsthroughwordembeddingandthesequentialdependencybetweenthewordsthroughlstm.besides abilstm modelcanaccessbothpastandfutureinputfeaturesfora givenpositionandthusimprovestheperformance.weset the dimensions as same as our model for fair comparison.
bilstm crf .
this model is a combination of the lstm network and the crf module by feeding the output vectors of bilstm into a crf layer.
since its superiority in learning the dependency and sentence level tag information many studies use it for labeling sequences.
seq2seq.
as previously described seq2seq models have witnessed great success in various applications.
recently it wasappliedtonamedentityrecognitionandachievedbetter resultsthanbilstm .therefore wealsoconsideritas onebaselineinourstudy.weusethestandardattentionalencoder decoder model to generate label sequences.
the parameter settings are the same as ours.table the effectiveness of try block localization models precision recall f1 score accuracy crf .
.
.
.
bilstm .
.
.
.5bilstm crf .
.
.
.4seq2seq .
.
.
.
nexgen .
.
.
.
.
.
catch block generation.
weselectseveralcommonlyused modelsrelatedtocodecompletionandgenerationinsecommunity ascomparativemethods.forcodecompletion languagemodels lms such as n gram and lstm are widely used to model and predict tokens of source code .
in addition the machine translation model seq2seq is also used to generate pseudo code from source code or apis from natural language queries .
thus we consider the following baselines.
n gram.
hindleet al.
firstexplored the naturalness of sourcecodewiththen grammodelandappliedittopredict next tokens.
a recent study found that carefully adaptingn grammodelsforsourcecodecanyieldperformance that surpasses deep learning models.
thus we borrow the bestconfigurationfromtheirwork whichisa6 grammodel with jelinek mercer interpolation.
lstm.
as a neural language model lstm was also used forcodecompletionandachievedgoodresults .we compare our approach with lstm based language model andthehyperparametersaresettobethesameasthosein our approach.
seq2seq.
there are many seq2seq based applications in software engineering.
for instance oda et al.
used a statistical seq2seq model to generate pseudo code given the source code.
recently the attentional seq2seq model has proven to be more effective so we use this model for comparison.themodelconfigurationsarethesameasthose used in our model.
.
results we investigate the following research questions to present the experimental results and analysis.
rq1 how does our approach perform compared with baselines in try block localization?
to keep the results of our approach nexgencomparable with those of baselines we transform the predicted token level labels of baselines into statement level ones by making one statement label as y ifanytokeninitispredictedy otherwisen.table2providesthe experimentalresultsofallthecomparedmodels.thebestresults are shown in bold.
itisclearthatthecrfmodelperformsworstamongthem.although it has a good precision the recall is quite low and so isf1 score.
this means that there are few predicted try block locationsinitsresults whichisconfirmedbyourmanualinspection.
asaresult theaccuracyisonly52.
sincemostofthepredictions are all ns that is it is prone to omit the exceptions.
35table the effectiveness of catch block generation groups models bleu accuracy partial context n gram .
.
lstm .
.
seq2seq .
.
full context n gram .
.
lstm .
.1seq2seq .
.7nexgen .
.
bycontrast neuralnetworkbasedmodelsachievemuchbetter results.forexample thef1 scoreofbilstmis72.
whichoutperforms crf by more than .
this may be largely attributed to itsabilitytounderstandingsemanticsthroughwordembedding.be cause in crf the tokens are represented as numbers which makes thefeaturesverysparse.inaddition itcapturesthedependencies between tokens by combining past and future information which helps determine whether to predict the y label when encountering a specific token e.g.
a predefinedvariable .
nevertheless the performance drops more than in terms of f1 score or accuracy when combining them together i.e.
bilstm crf .
intuitively the worse performance of the crf module impairs the whole model.
a deeper reason is that adding the crf layer forces it to optimize the log likelihood on the sequence level whereas it is demonstrated to benoteffectiveifusingcrfindependently.withregardtoseq2seq asurprisingphenomenonisthatitperformsaswellasbilstmsuch as the precision of .
.
but the accuracy is only .
whichis even lower than that of crf.
we inspected its predictions andfound that it tends to generate shorter label sequences than thegroundtruth indicatingthatitcannottagthetokensequenceina one for one manner due to the limitation of encoder decoder architecture on this task.
at last our approach achieves the best results among all the models.forexample itimprovesbilstmby5.
f1 score and .
accuracy .
the reason is that our model use a two layered neural architecture to tag the source code at the statement level which can significantly shorten the length of prediction time steps andgetridofthelong termdependencyproblem.asdepictedin table1 theaveragelengthofjavamethodsintermsoftokensis .
avgt while it is only .
avgs in terms of statements.
in addition we employ attention mechanism at both token and statement levels and thus can learn the distinction between important and unimportant elements.
rq2 how does our approach perform in comparison with the baselines in generating catch block code ?
in this research question we want to know how our approach and the baselines perform in generating catch block code and whether different contexts affect the performance.
as shown in table we consider two different contexts only the try blocks partial context the leading code and the try blocks full context .
for evaluation welimitthelengthofthegeneratedcodeto100because the lm based models tend to generate very long sequences.
first we can see that as a statistical language model n gram performs badly in this task.
in particular the bleu score is only6.
which is far from satisfaction.
it is of no avail to use thefullcontextfortrainingsuchamodel andtheperformanceeven decreases a bit.
the reason is that n gram model only learns the probability of the next token within a fixed window i.e.
tokens no matter what the input is.
it fails to capture enough context information.wealsofoundthatmostofthegeneratedcodedoes notadheretojavasyntax whichmayexplainwhyitsaccuracyis0.
in contrast lstm achieves better results since lstm can capture more dependency information.
when given the full context lstm learns to generate more correct tokens than the partial context and improves the bleu value.
ho wever the fullcontextmaylead tomorenoisydatafromtheleadingcodeandmaketheaccuracy slightly lower.
the seq2seq model shows a significant improvement over ngram and lstm.
because seq2seq separates the precondition i.e.
the leading code and try blocks and the learning object i.e.
the catchblocks intotwodifferentsequences andcanlearntheknowledge of try blocks and catch blocks without interference whereas thelm basedmodelspredicttokensinawholesequence.also the attention mechanism in seq2seq helps capture different weights of tokens in the encoder.
compared with the partial context the full context also leads to an improvement such as .
higher in terms ofbleu whichshowsthattheleadingcodeisactuallyhelpfulin seq2seq.
our approach nexgenoutperforms all the baselines.
specifically we improve the attentional seq2seq model by .
and .
in termsofbleuandaccuracy respectively.thereasonisthat nexgen encodestheleadingcodeandtryblocksseparately andusesslicingbased attention to eliminate the noisy data from the leading code.
rq3 to what extent does the components of our proposed models contribute to the effectiveness of both tasks?
thisresearchquestionaimsatanalyzingthecontributionsofdifferent components of our model to the overall effectiveness.
we conductanablationstudytoanswerthisrq.theresultsareshown in table .
we start with the try block localization task task1 .
we evaluatetheinfluenceofremovingtwomainattentionmodulesfrom our original model nexgen .
when replacing the token level attentionbythelasttimestepoftokenencoder wefindthattherecall becomes a bit higher from .
to .
yet the f1 score and accuracydeclineby0.
and0.
.thisindicatesthatthetokenlevelattentionindeedcapturessomemoretoken levelsemantics.
similarly the performance also gets worse and the f1 score and accuracy decrease by .
and .
when removing the statementlevelattention.wecanseethatthestatementattentioncontributes more to the overall performance than the token attention.
if we removebothoftheattentionmodules thef1 scoreandaccuracy drops by .
and .
respectively.
forthetaskofcatchblockgeneration task2 weobtainedsimilar results.
we first remove the standard attention over the leading code leadingattention .thedropsofbleuandaccuracyshow that different tokens in the leading code actually have different weightsforcapturingthecodesemantics.however comparedwith removingslicing basedattention thebleuscoreofremovingleading attention is .
higher.
this means that it is more effective tocapturethedependenciesintheleadingcodeofthetryblocks than the standard attention.
next we remove slicing attention and 36table the effect of model components on two tasks.
the minus symbol means removing one component from nexgen task1 task2 description precision recall f1 score accuracy description bleu accuracy token attention .
.
.
.
leading attention .
.
statement attention .
.
.
.
slicing attention .
.
both attention .
.
.
.
both attention .
.
nexgen .
.
.
.
nexgen .
.
table5 thescoredistributionofthegeneratedcatchblocks score 1234567a v g n gram .
lstm .
seq2seq .
nexgen .
replace leading attention over the leading code with just the last timestepoftheleadingcodeencoder.itcanbeseenthattheperformance degrades significantly by .
bleu and .
accuracy indicating that theleading code indeed introducesnoisy data and influences the performance.
in summary the token level attention and statement level attention components of the try block locator have a relatively small influenceontheperformance whiletheleadingattentionandslicingbased attention mechanisms of the catch block generator are more effective and contribute significantly to the overall performance.
.
human evaluation forthetaskofcatchblockgeneration weusethequantitativemet ricbleutocomparethecodegeneratedbydifferentmethods.bleu calculates the textual similarity between the reference and the generated code rather than the semantic similarity.
thus we perform human evaluation to complement the quantitative evaluation.
we invite evaluators to assess the quality of catch block code generatedbyourapproach nexgenandthethreebaselines.they are undergraduate master and ph.d. students in cs with years of experience in java programming.
the three baselines n gram lstm and seq2seq are selected from the better ones in the partial orfullcontext.werandomlychoose100try catchpairsfromthe testing set of cbgd and their catch block code produced by the threebaselinesandourapproach andevenlydividethemintofour groups.eachgroupisassignedtothreedifferentevaluatorssince such redundancy can help obtain more consistent results.
for each try catch pair we show its leading code and try block the refer ence catch blocks and four results of catch blocks generated by thethreebaselinesandourapproach.thefourgeneratedresults of catch blocks are randomly ordered thus the evaluators haveno idea which catch block code is produced by which approach.
the evaluators can select a score between to to measure the semantic similarity between the generated catch blocks and the reference where1means notsimilaratall and7means highly similar identical .
the higher scores mean that the corresponding generatedcatchblocksaremoresemanticallysimilar tothereference.
for each generated result of catch blocks we get three scores from evaluators and choose the median value as the final score.table5showsthescoredistributionofthegeneratedcatchblocks.
wecanseethatourapproachachievesthebestscoresandimproves theaverage avg scorefrom1.
n gram .
lstm and4.
seq2seq to .
.
specifically among the randomly selected try catch pairs our approach can generate highly similar or evenidentical catchblockswiththereferenceones score goodcatchblocks score .ourapproachalsoreceivesthesmallestnumberofnegativeresults score .basedonthe100final scoresforeachapproachofn gram lstm seq2seqand nexgen we conduct wilcoxon signed rank tests .
comparing our approachwithn gram lstm andseq2seq thep valuesofwilcoxon signed ranktestsat95 confidencelevelare2.2e .5e 09and .
respectively showingthattheimprovementsachievedby ourapproacharestatisticallysignificant.insummary theresults of human evaluation confirm the effectiveness of the proposed approach.
discussion .
case study nowwediscussthesuperiorityandlimitationofourmodelforcatch blockgenerationbyanalyzingtwoexamples whichareshownin table6.example1showsoneincompletejavamethod add includingtheleadingcodeandtryblock.thedeveloperwantstocatch illegalstateexception thrown by map.put in the try block when multiple objects write data into the map and incur conflicts.
then the catch block handles the exception by converting and throwing the exception to notify upper layer methods.
in the full context n gramgeneratesverylongcodewithwrongjavasyntax weomit more tokens here .
since there are many noisy statements in the leading code lstm and seq2seq fail to report the proper exceptiontype.forexample seq2seqgeneratesclasscastexception.
bycontrast nexgencorrectlypredictstheexceptiontypeandthe whole catch block since nexgentreats the leading code and the try block separately and uses the slicing based attention to remove irrelevant and noisy statements.
although nexgenoutperforms the baselines we do not claim that it has already matured and is ready for use in practice.
indeed ourapproachonlymakesonestepfurthertowardsautomatedexception handling and it still has some limitations.
in example although our approach successfully predicts the exception type filenotfoundexception itfailstocorrectlyhandleitinthecatch block.specifically itlogstheeventmaybebecauselogginginformation for filenotfoundexception is a very common behavior in the trainingset.butthecorrectexceptionhandlingcodeistoset mapping as null which possibly relies on the project specific features 37table two examples of catch block generation example example codepublic void add session session row row transactionmap value value map getmap session valuearray array converttokey row null boolean checkrequired indextype .isunique !mayhavenullduplicates row if checkrequired checkunique map array long.min value try map.put array valuenull .instance private boolean revalidate boolean flag if mapping null flag file catalog findfile dtdsetfolder catalog file name if catalog null mapping null else try mapping parsecatalog new inputstreamreader new fileinputstream catalog referencecatch illegalstateexception e throw mvtable .convertexception e catch filenotfoundexception exc mapping null n gramcatch final ioexception x ... more tokens catch ioexception ioe throw new kafkacruisecontrolexception e ... more tokens lstmcatch exception e throw new runtimeexception e catch ioexception e throw new runtimeexception e seq2seqcatch classcastexception e throw new unsupportedoperationexception catch ioexception exc return false nexgencatch illegalstateexception e throw mvtable .convertexception e catch filenotfoundexception exc logger.getlogger unk .class.getname .
log level.info null exc and personal preferences of developers.
our approach also encounterstheproblemofoutofvocabularyandreplacestheclassname with unk .inourfuturework wewillimproveourapproachby incorporatingtheknowledgeoftheexceptionspecificationsfrom projects and designing mechanisms that can overcome the large vocabulary problem.
.
why is nexgenbetter?
as stated previously learning to handle exceptions includes two main successive tasks finding potential exceptions and writing code to handle them.
the common challenge of them is how to understandthesemanticsofgivencodesnippets.comparedwith traditionalapproaches nexgenlearnsthesemanticsofcodefrom the following two aspects.
first traditionalapproacheslikestaticanalysisorn grammodel have beenapplied infinding string handling errors orrecommending exception handling apis but they ignore the semanticsoftokens e.g.
treatingthemasnumericalids .incontrast nexgenadoptsthewordembeddingtechniquetoautomaticallylearn the semantics of tokens by mapping them into a high dimensional vectorspace.thisisafundamentalcomponentofmanydeeplearning based approaches to code analysis because tokens with similarembeddingstendtobeusedinsimilarcontexts whichhelps determine whether a same similar exception would occur and how it can be handled.
for example even the java tokens audioinputstream and stringbufferinputstream have different names theyare used in similar contexts and thus possibly throw the ioexception .
therefore ignoring the semantics of tokens may lead to a wrong result.
second itisalsonecessarytoknowthedeepsemanticsbyunderstandingdependenciesinsourcecode.statisticalmodels e.g.
n gram consideronlytheprobabilitydistributionofcodetokens andmayhaveapoorperformance.programanalysiscananalyzethe data flowandcontrol flowdependencies whichhasbeenshown effective for compilable and complete source code.
unlike them nexgenlearns the internal dependencies within methods for incomplete code fragments.
on the one hand similar to existingstudies itutilizesthelstmtocapturethesequentialdependencies of tokens and statements so that exception prone statements can be identified based on the context.
as shown inexample of table nexgenidentified the statement map.put insteadof converttokey asthe exceptional onebecausethestatement checkunique is alreadyused tocheck thevariables hence it only needs to check the latter statement.
on the other hand whengeneratingcatchblocks nexgencanlearntheexplicitdata dependenciesforeliminating noisydatafrom theleadingcodeby the standard attention and our slicing based attention mechanisms.
in this way nexgencan understand the deep code semantics by capturingthesequentialdependenciesandthedatadependencies in code fragments.
threats to validity there are three main threats to validity of our evaluation.
inthiswork weonlycollectedjavasourcecodetoconstruct the datasets for out tasks.
it remains unknown whether our approachwillperformwellonotherprogramminglanguages suchasc andpython.however webelievejavaisrepre sentative because of its popularity in real world softwareapplications.
in our future work we will collect data fromprograms in different programming languages to further evaluate the proposed approach.
the quality of the java methods may affect the effectiveness ofourapproach.tomitigateit weselectedtop2 000projects at github as corpus by the numbers of their stars and forks which indicate their impact and popularity.
we filter out simple methods e.g.
get set methods that have less than linesofcode.wealsodiscardtheexceptionhandlingcode withbadpractices forexample swallowingexceptions.still we cannot guarantee that all data we collected is of high quality.wewillexploreeffectivetechniquestoimprovedata quality in the future.
there may exist some score bias in the human evaluation sincedeveloperscanmakedifferentjudgementsonthesamegeneratedcodeaccordingtotheirdifferentexperiences.hence eachgeneratedcodeisassignedtothreeredundantevaluatorsandwetakethemedianvalueasthefinalscoretoreduce the bias.
related work .
exception handling manystudieshavebeenconductedtounderstandexceptionhandlingpractices .forexample senaetal.
investigated potential impact of the exception handling strategies on theclientapplicationsbyexceptionflowanalysisandmanualin spections.
they found that .
of the bug reports are relatedto the anti patterns of exception handling.
padua et al.
performedanempiricalstudyoftherelationshipbetweenexception handling practices and software quality measured by the probability of having post release defects .
the case study on open source java and c projects showed that exception flow characteristics in java projects have a significant relationship with post release defects.
even so previous studies revealed that developers tend to avoiditormisuseexceptionhandlingbecausetheyconsiderithard to learn and to use .
for example cabral et al.
studied exception handling practices from projects in both java and.netandunveiledsuboptimalpracticeofexceptionhandlingbydevelopers.
shah et al.
interviewed java developers in order to contrast the viewpoints of experienced and inexperienced developers regarding exception handling.
they recognized that developers tend to ignore the proper implementation of exception handlinguntil defects are found although developers should do it in the early releases of a system.
all these studies indicate that there is a need for automatic techniques to facilitate the writing of exception handling code for developers.
meanwhile prior research also explored different kinds of approaches to exception handling .
cabral et al.
proposed a prototype transactional model that aims at automati cally recovering runtime environments at the platform level insteadofwritingexceptionhandlingcode.althoughthisproposal is promising its overhead is high and its precision is low due to the complexity of exceptions.
subsequent research focuses on recommendingsourcecoderelatedinformationtoaidprogramming practice.
for example barbosa et al.
proposed a heuristic strategy that is aware of the global context of exceptions and produces recommendations on the violations of exception handling.
nguyen et al.
proposed afuzzy andn gram basedapproach torecommend exception types and repairing method calls for exceptionhandlingcode.suchapproachescanprovideusefulknowledgeto assist developers in writing exception handling code but developersstillneedtodesignthewholelogicofexceptionhandlingand modify the code to fit the context.
compared with previous work weprovideadeeplearningbasedapproachtoautomatedexception handling whichcanpredictthelocationsoftryblocksandgenerate the complete catch blocks.
.
code completion and generation there exists a rich literature of research work on code completion inse mostofwhichreliesonlanguagemodels lms .forexample hindleetal.
usedthen grammodelonlexicaltokenstopredict the next token.
hellendoorn et al.
performed an extensive comparisonbetweenn gramandlstm basedlmsforsourcecode and concluded that n gram models can outperform lstm mod els if carefully adapted.
apart from token level code completion nguyenetal.
combinedprogramanalysisandstatisticallm in the process of statement completion.
recently some researchintends to generate code in various scenarios .
for example odaetal.
usedastatisticalmachinetranslationmodel togeneratepseudocodegiventhesourcecode.tufanoetal.
investigated the nmt model also called seq2seq on pairs of code components before and after the implementation of the changes providedinthepullrequests.theirresultsshowedthatnmtcan automatically replicate the changes implemented by developers duringpull requests.similarly chenetal.
proposed aneural model based on seq2seq which learns from pairs of the original versionandfixedversionofbuggyprogramminglines.thereare alsosomerecentstudiesinnlpcommunity whichproposeneuralnetworkbased modelstogeneratecode fromnaturallanguage descriptions .
different from the above work our work generates exception handling code instead of general code.
conclusion in this paper we propose a novel deep learning based approach to automatedexceptionhandling.wedecomposetheproblemintotwo coherent tasks namely try block localization and catch block generation whose targets are to recognize potential exceptions within a java method and generate code to handle them respectively.
for both tasks we design neural models with attention mechanismstocapturedeepsemantics.weconductextensiveexperimentsto evaluateourapproach.allresultsdemonstratethesuperiorityof our models on both tasks.
although our results are significantly better that those of the baselines they can still be improved before beingappliedinpractice.ourworkcanbeconsideredasoneofthe firststepstowardsautomatedexceptionhandlingandwehopeit can inspire follow up research work.
our source code and experimental data are available at https github.com zhangj111 nexgen.