understanding performance problems in deep learning systems junming cao school of computer science fudan university shanghai chinabihuan chen school of computer science fudan university shanghai chinachao sun school of computer science fudan university shanghai china longjie hu school of computer science fudan university shanghai chinashuaihong wu school of computer science fudan university shanghai chinaxin peng school of computer science fudan university shanghai china abstract deep learning dl has been widely applied to many domains.
unique challenges in engineering dl systems are posed by the programming paradigm shift from traditional systems to dl systems and performance is one of the challenges.
performance problems pps in dl systems can cause severe consequences such as excessive resource consumption and financial loss.
while bugs in dl systems have been extensively investigated pps in dl systems have hardly been explored.
to bridge this gap we present the first comprehensive study to i characterize symptoms root causes and introducing and exposing stages of pps in dl systems developed in tensorflow andkeras with pps collected from stackoverflow posts and to ii assess the capability of existing performance analysis approaches in tackling pps with a constructed benchmark of pps in dl systems.
our findings shed light on the implications on developing high performance dl systems and detecting and localizing pps in dl systems.
to demonstrate the usefulness of our findings we develop a static checker deepperf to detect three types of pps.
it has detected new pps in github projects.
and pps have been confirmed and fixed.
ccs concepts software and its engineering software performance .
keywords performance problems deep learning performance analysis acm reference format junming cao bihuan chen chao sun longjie hu shuaihong wu and xin peng.
.
understanding performance problems in deep learning systems.
in proceedings of the 30th acm joint european software engineering also with shanghai key laboratory of data science and shanghai collaborative innovation center of intelligent visual computing.
bihuan chen is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction the advances in deep learning dl have attracted an increasing interest in applying dl to various applications in both industry and academia e.g.
image processing machine translation speech recognition medical diagnosis self driving cars and robotics.
dl systems adopt a data driven programming paradigm where developers define a desired neural network that learns the decision logic from a large amount of training data.
differently traditional systems follow alogic based programming paradigm where developers directly encode the decision logic in the source code.
this paradigm shift poses unique challenges to engineering dl systems .
in particular performance as an important quality requirement is one of the challenges in engineering dl systems .
it has a significant impact on the time and resources e.g.
gpu memory and power required during the process pipeline e.g.
training and inference of dl systems .
for example the language model gpt costs millions of dollars for a single training run1.
performance problems pps can slow down dl systems consume excessive resources hurt user experience cause financial loss or threaten human lives.
for example many users suffered a significant slowdown of their dl systems after upgrading tensorflow .x to tensorflow .x and hence decided to switch to pytorch2.
moreover performance questions of dl systems are recognized as the most difficult to answer among all questions of dl systems on stackoverflow .
therefore it is necessary to study the characteristics of pps in dl systems.
a lot of efforts have been recently made to extensively investigate the characteristics e.g.
symptoms root causes fixes and taxonomy of general bugs and specific bugs in dl systems.
however these studies are not specifically designed for pps and thus only capture some partial characteristics of pps in dl systems.
in contrast pps have been widely studied for traditional systems e.g.
desktop or server applications highly configurable systems mobile applications databasebacked web applications and javascript systems .
however pps in dl systems could be different due to the programming oct 2022esec fse november singapore singapore junming cao bihuan chen chao sun longjie hu shuaihong wu and xin peng paradigm shift from traditional systems to dl systems.
in summary the characteristics of pps in dl systems are under investigated.
to bridge this knowledge gap we present the first comprehensive study to characterize pps in dl systems developed in tensorflow andkeras and to assess existing approaches in tackling pps.
to this end we first collect pps from stackoverflow posts and manually investigate the pps to characterize their symptoms rq1 root causes rq2 and introducing and exposing stages rq3 .
based on these pps we manually build a benchmark of pps that cover most symptoms and root causes and assess the capability of a profiler in detecting pps the capability of a compiler in optimizing pps and the capability of documentation in hinting pps rq4 .
rq1 symptom what are the symptoms of pps?
rq2 root cause what are the root causes of pps?
rq3 stage what are the stages of introducing and exposing pps?
rq4 assessment how is the capability of existing performance analysis approaches in tackling pps?
through these research question analysis we aim to provide useful findings for developers and researchers.
for example more than half of the pps slow down dl systems and nearly one third of the pps consume either extremely low or high resources.
about half of the pps are introduced by api misuses and root causes related to model data and hardware introduce more than one third of the pps.
the most bugprone stages are data preparation environment setting model building and training.
the most bug affecting stages are training and data preparation.
of the pps are not exposed in the introducing stage.
existing approaches have a very limited capability in tackling pps.
our findings provide implications for developers and researchers on developing high performance dl systems and detecting and localizing pps in dl systems e.g.
performance aware techniques to recommend dl library apis and dl models static techniques to model and estimate time cost and resource consumption of dl systems and rule based techniques to detect and localize pps in dl systems.
to demonstrate the usefulness of our findings we develop a static checker named deepperf that supports rule based detection of three types of pps derived from our study.
we run deepperf against github projects with more than stars.
deepperf has detected new pps in of these projects with false positives.
of these pps have already been confirmed by the developers and of them have already been fixed.
others are still waiting for confirmation.
in summary this paper makes the following contributions.
we present the first comprehensive study to characterize pps in dl systems written in tensorflow andkeras and to assess existing approaches in tackling a contructed benchmark of pps.
we develop a static checker named deepperf to detect three types of pps and detect new pps in github projects.
empirical study methodology we first introduce the design of our study and then present our data collection data labeling and benchmark construction process.
.
study design our goal is to understand pps in dl systems.
as dl systems can be built on top of various dl libraries we limit our scope to dl systems developed in tensorflow andkeras .
we select tensorflow as itis the most popular dl library on github.
we also include keras because it is built on top of and tightly integrated with tensorflow .
we include keras but do not distinguish between tensorflow and keras in our analysis because i keras is a frontend and should be used with a backend and tensorflow is the most popular backend and ii tensorflow andkeras are often tightly used together.
to achieve this goal we propose the four research questions as introduced in sec.
.
our symptom analysis inrq1 aims to understand the observable consequences of pps.
our findings from rq1 can characterize the significance of pps and provide insights for developing pp detection approaches.
our root cause analysis inrq2 aims to characterize the fundamental reasons for the occurrence of pps.
our findings from rq2 can provide insights for designing pp localization approaches.
our stage analysis inrq3 aims to locate dl pipeline stages where pps are introduced and exposed and measure the distance between exposing stage and introducing stage.
our findings from rq3 can locate the bug prone and bug affecting stages that should be concerned and reflect the difficulty of pp localization.
our approach assessment inrq4 aims to quantitatively evaluate existing approaches in tackling pps.
our findings from rq4 can reveal the necessity of pp detection and localization approaches.
besides our findings can also provide hints to develop high performance dl systems.
.
data collection we collected pps from a well known q a site stackoverflow where world wide developers can discuss software development problems.
our pp collection process consists of the following three steps.
step dl post selection.
we first selected posts related to dl libraries tensorflow andkeras by checking whether the tags of a post contain the keywords tensorflow and keras .
we also filtered posts that were created before to avoid usage discussions about old versions of dl libraries that are usually no longer used.
at the time of selection i.e.
we obtained dl posts.
then we excluded posts that did not contain any source code in question descriptions for the ease of our manual analysis.
to focus on highquality posts we also excluded posts that did not have an accepted answer or any answer whose votes were greater than two because questioners often commented that the problems had been solved but forgot to accept the answer.
after this step we had dl posts.
step pp post selection.
instead of directly using performancerelated keywords from the existing studies on pps in traditional systems e.g.
we derived a keyword set in the following way to achieve a wide and comprehensive coverage of pp posts.
we first randomly sampled posts with a tag of performance from posts in step .
then we manually analyzed these posts to extract performance related keywords and added them to the set of keywords from existing studies.
we continued this procedure of random sampling and manual analysis for another two rounds until no new keyword was found i.e.
we sampled posts which achieved confidence level and .
confidence interval.
finally we used the derived keyword set to search question descriptions of the posts in step which resulted in candidate pp posts.
we provide the full set of derived keywords at our replication site.
step pp identification.
we manually verified the candidate pp posts to reduce noise that was not about pps in dl systems.
for example some posts might happen to have performance relatedunderstanding performance problems in deep learning systems esec fse november singapore singapore keywords but did not discuss pps some posts actually discussed the accuracy of dl models because accuracy is often interchangeable with performance in the dl community and we align with the se community where performance is usually referred to as efficiency and some posts indeed discussed performance but did not have a correct answer which could not be used to understand the characteristics of pps.
in particular two of the authors separately inspected each candidate pp post to identify pps.
we used cohen s kappa coefficient to measure the agreement and it reached .
.
a third author was involved to resolve disagreements.
finally we identified pps from pp posts of which pp posts contained two pps.
this scale is comparable to previous studies on pps e.g.
pps in desktop or server applications and pps in mobile applications .
.
data labeling to answer rq1 rq2 andrq3 two of the authors labeled each of the pps with respect to three dimensions symptom root cause and introducing and exposing stages.
in particular they started with the classification schema used for labeling from the existing general dl bug studies and adapted it by appending new ones and excluding non applicable ones.
they separately read all post contents including the title question description comments answers and reference links mentioned during discussion to carefully label pps.
specifically the symptom of a pp was determined if the questioner explicitly reported the symptom in the post.
otherwise it was conservatively labeled as unknown .
the root cause of a pp was inferred from the buggy code version in the question and the fixed code version always existed in the valid answer.
the introducing stage of a pp was determined by analyzing where its root cause was located while the exposing stage of a pp was decided by analyzing where its symptom was exhibited.
the introducing exposing stage of a pp was labeled as unknown if there was no clear indication in the post.
we provide actionable code of the final taxonomies for symptoms root causes and stages at our replication site.
the cohen s kappa coefficient was .
.
.
and .
for the labeling of symptom root cause introducing stage and exposing stage.
a third author was involved to resolve disagreements.
it is worth mentioning that the manual effort involved in our data collection and labeling procedure required six person months.
.
benchmark construction to answer rq4 we constructed a benchmark by reproducing pps.
we reproduced pps on a machine with a core intel i7 7820x cpu .60ghz nvidia titan xp gpu 128gb ram and 1tb ssd.
different pps require different tensorflow versions which further require different cuda toolkit versions to support gpu.
it is tricky to install different cuda versions in the same physical machine.
thus we used tensorflow docker images.
only nvidia gpu driver was installed in the physical machine and each docker container had its own cuda toolkit version.
finally tensorflow docker images ranging from version .
to .
and version .
to .
with gpu support were covered to build our pp benchmark.
we decided to sample some pps from the pps instead of trying to reproduce all pps due to the large effort in reproducing pps from stackoverflow posts.
to have a good coverage of symptoms and root causes we sampled pps from each set of pps that were performanceproblems time memory processor unknown slowexecutiontime slowinitializationtime programhang increasingtimeovertime memoryleak abnormalgpumemoryusage notusinggpu abnormalgpuutilization abnormalcpuutilization out of memory figure taxonomy of pp symptoms caused by each inner category of root causes see sec.
.
while exhibiting each high level category of symptoms see sec.
.
.
for each sampled pp we reproduced it with the following three steps.
step decide tensorflow version.
if the tensorflow version was shown in the post we used it.
if not we checked whether apis specific to tensorflow .x e.g.
tf.session ortensorflow .x e.g.
tf.function existed in the post.
if yes we used the latest tensorflow version of .x i.e.
.
or .x i.e.
.
.
if not we used tensorflow .
.
step complete code snippets.
as developers tend to only include code fragments that are directly related to questions code snippets in the post are often incomplete.
specifically if the buggy or fixed version was executable we completed the fixed or buggy version based on it.
otherwise we wrote missing code fragments for buggy and fixed versions based on question description and answer.
step reproduce symptoms.
we executed the buggy and fixed version to reproduce symptoms reported in the post.
we may change input data size model parameters etc.
to reproduce described symptoms as our hardware environment might be different from the post.
for pps with out of memory errors we set the maximum gpu memory limit with tf.gpuoptions such that the out of memory errors could be reproduced even on gpus with a larger memory.
we successfully reproduced pps from sampled pps with four person months effort.
the main reasons for failed reproduction are i developers provide very incomplete code snippets in the posts making it difficult for us to complete the buggy or fixed version and ii some pps require specific hardware environments that are different from our machine.
to foster future research on pps in dl systems we recorded for each pp in our benchmark its environment configuration input data buggy version fixed version performance change after fixing and reproduction steps.
empirical study results we present the results of the four research questions.
.
symptom analysis rq1 the taxonomy of pp symptoms is shown in fig.
.
it is organized into three high level categories i.e.
time memory andprocessor and inner categories which are exhibited by .
of the pps.esec fse november singapore singapore junming cao bihuan chen chao sun longjie hu shuaihong wu and xin peng api library data model hardware not usingefficientapi not usingbatchapi inefficientapiusage buggy libraryversion mismatched libraryversion inefficientdatapreprocessing inefficientdatatransmission improperdatainput confusionwithcomputationgraph inefficientmodelstructure impropermodelparameter improperhyperparameter hardwareandlibrary mismatch improperconfiguration hardwareandcode mismatch performanceproblems figure root causes of pps in dl systems the remaining .
pps belong to the unknown category defined in sec.
.
.
notice that one pp can exhibit multiple symptoms.
time.
this category covers pps exhibiting high time cost which accounts for the largest portion of pps i.e.
.
.
in particular .
of the pps manifest slow execution time during the execution of dl systems including data preparation model building training evaluation hyper parameter tuning or prediction.
further .
of the pps exhibit increasing time over time e.g.
the prediction time became longer and longer as the model ran3.
moreover .
of the pps manifest slow initialization time when dl systems are initialized before execution e.g.
it spent more than seconds to import tensorflow4.
dl systems can still work but slowly when exhibiting the above symptoms.
differently .
of the pps result in program hang that makes dl systems cease to respond to inputs which is the most severe symptom.
memory.
this category includes pps consuming ram gpu memory abnormally accounting for .
of the pps.
specifically out of memory is the most common as well as the most severe symptom covering .
of the pps.
memory leak manifested in .
of the pps occurs when the memory usage keeps increasing and may finally lead to out of memory errors.
moreover abnormal gpu memory usage i.e.
either unexpectedly high or low gpu memory usage is exhibited in .
of the pps.
processor.
this category consists of pps with abnormal cpu gpu utilization which accounts for .
of the pps.
in particular abnormal gpu utilization i.e.
either unexpectedly high or low gpu utilization is manifested in .
of the pps.
for example the gpu utilization was only around while the training time was slow each epoch took to seconds .
moreover dl systems may not use gpu leading to no speedup than when running on cpu which occurs in .
of the pps.
in addition abnormal cpu utilization is also exhibited in .
of the pps.
summary.
more than half of the pps slow down dl systems and nearly one third of the pps consume either extremely low or high resources like memory and processor.
such severe consequences of pps motivate the significance of pps.
moreover only four of the ten symptoms as highlighted in dotted rectangles in fig.
are shared with the symptom taxonomies for general dl bugs .
in other words symptoms of pps are quite different from those of general dl bugs and the existing studies on general dl bugs only capture a partial set of pps and thus pps deserve a comprehensive investigation.
.
root cause analysis rq2 the taxonomy of pp root causes is reported in fig.
.
it is grouped into five high level categories i.e.
api model library data andenvironment and inner categories.
api.
this category covers pps caused by library api misuses.
this is the most common category and accounts for .
of the pps.
specifically tensorflow andkeras provide efficient apis for achieving high performance e.g.
the tf.data api for building efficient input pipelines and various operation apis for efficient computation.
however developers often write their own implementation which is often less efficient but do not use the corresponding efficient api directly potentially due to the unfamiliarity with apis.
this causes .
of the pps.
for example a developer wrote a forloop to perform concatenation on a set of images which could be efficiently achieved by the mapapi from tf.data.dataset6.
moreover tensorflow andkeras provide various batch processing apis for high performance e.g.
data loading training evaluation or prediction in a batch mode.
however developers might not use a batch api and some even implement batch processing by themselves which causes .
of the pps.
for example a developer loaded a large data set into gpu memory all at once causing an out of memory error7.
the flow from directory api in keras can solve this pp by dynamically loading a batch of data from the specified directory.
notice that not using batch api is a sub category of not using efficient api and we treat it separately due to its high frequency.
in the previous two root causes developers are mostly unaware of the efficient or batch apis.
however even when developers are aware of some apis they might not fully understand their performance characteristics and write inefficient api usage which causes .
of the pps.
fig.
shows an example of inefficient api usage where a developer called the mapapi before the batch api and did not pass the num parallel calls argument to map8 leading to a long training performance problems in deep learning systems esec fse november singapore singapore def parser record def batch parser record batch parsed tf.
parse single example record keys to map parsed tf.
parse example record batch keys to map return parsed parsed def init tfrecord dataset files train glob .
glob dir tfrecords .
tfrecord random .
shuffle files train with tf.
name scope tfr iterator define data from randomly ordered files ds tf.
data .
tfrecorddataset files train select elements randomly from the buffer ds ds.
shuffle buffer size map them based on tfrecord format ds ds.
map parser group elements in batch ds ds.
batch batch size drop remainder true map batches based on tfrecord format ds ds.
map batch parser num parallel calls iterate infinitely ds ds.
repeat initialize the iterator return ds.
make initializable iterator figure inefficient api usage before and after fix time.
to speed up mapshould be called after batch to reduce the number of times the mapped function batch parser is called andnum parallel calls should be passed to enable parallelism.
model.
this category consists of pps that are related to dl models which is the second most common category accounting for .
of the pps.
in particular developers may have confusion with computation graph because of the unfamiliarity with the programming model in tensorflow andkeras which causes .
of the pps.
a typical confusion is with the programming model of tensorflow .x which is to first build a dataflow computation graph and then run it repeatedly with inputs being fed to and outputs being fetched from the graph.
developers often mix the graph construction into the graph execution.
as a result nodes are repeatedly added to the graph and the graph execution becomes slower and slower.
an example9 is shown in fig.
where line builds the graph and should be moved out of the execution loop to line .
another common confusion is with the usage of session which owns resources like queues and variables.
however developers repeatedly create a session in the graph execution loop without reusing or forget to close the session.
the example in fig.
also forgets to close the session and the fix is to use the session as a context manger at line that will automatically close the session.
a typical confusion in tensorflow .x is with the tf.function decorator which accelerates the decorated function by running it in graph mode instead of in eager mode.
however developers often do not know where to add the decorator and how to design the decorated function to get real speedup.
further developers design an inefficient model structure e.g.
missing convolution and pooling layers before the flatten layer to have too many weights or set improper model parameter e.g.
a large kernel size in a convolution layer to cause a long training time .
these two categories respectively cause .
and .
of the pps.
moreover developers also set improper hyper parameter e.g.
a large batch size to cause an out of memory error or a small batch size to cause a long training time.
this category causes .
of the pps.
library.
this category refers to pps caused by problems of dl libraries accounting for .
of the pps.
specifically .
of inp tf.
constant out tf.
constant weight tf.
variable optimizer tf.
train .
gradientdescentoptimizer .
y tf.
matmul inp weight loss out y out y train optimizer .
minimize loss sess tf.
session with tf.
session as sess sess .
run tf.
global variables initializer for epoch in range y tf.
matmul inp weight loss out y out y sess .
run optimizer .
minimize loss sess .
run train figure graph confusion before and after fix the pps are caused by buggy library version i.e.
dl systems themselves are correctly written but trigger the pps in dl libraries.
for example repeated calls to model.predict e.g.
in a loop resulted in a memory leak10 due to a memory leak persisting across multiple versions of tensorflow11.
these pps trigger the pps in distinctive apis.
it is non trivial to detect such pps as we do not have a full list of apis with pps in each dl library version.
moreover mismatched library version causes .
of the pps as version restrictions have to be satisfied for full gpu usage.
for example tensorflow .x is not fully supported on cuda .
resulting in a long time to start the training12.
data.
this category covers pps related to data processing accounting for .
of the pps.
specifically developers may write inefficient data transmission e.g.
loading input data over the network during training but not directly copying them to the local storage or storing weight data in cpu which causes the weights copied to gpu and the gradients copied back to cpu in each training iteration.
this category accounts for .
of the pps.
further developers may implement inefficient data preprocessing e.g.
lack of image normalization before changing an image to a tensor which causes .
of the pps.
moreover improper input data e.g.
improper data format or size that consumes excessive resources causes .
of the pps.
for example images with unnecessarily high resolution were loaded causing an out of memory error13.
hardware.
this category covers pps related to hardware issues accounting for .
of the pps.
specifically hardware may only support part of the dl library versions and hence hardware and library mismatch causes .
of the pps.
for example a gpu with compute capability .
is not supported in tensorflow .
which requires a gpu with compute capability .
.
further to utilize the full acceleration capability of tpu dl systems often need specific code design.
thus hardware and code mismatch causes .
of the pps.
for example to use colab tpu a dl model need to be explicitly converted to a tpu compatible version if not the training becomes extremely slow15.
moreover hardware need proper configuration to achieve full utilization especially for distributed training.
november singapore singapore junming cao bihuan chen chao sun longjie hu shuaihong wu and xin peng a introduced and exposed pps in each stage b distance between exposing stage and introducing stage figure the exposing stage and introducing stage of pps and their distance a symptoms of the pps exposed in each stage b root causes of the pps introduced in each stage figure correlation between symptoms and exposing stages and between root causes and introducing stages thus improper configuration causes .
of the pps.
for example the tf.distribute.strategy api should be used to properly configure and allocate multiple gpus16.
summary.
about half of the pps are introduced by api misuses.
model data and hardware i.e.
the enabling characteristics of dl systems introduce more than one third of the pps.
dl libraries also introduce one tenth of the pps.
these diverse sources of root causes increase the complexity of pp localization.
moreover only seven of the root causes as shown in dotted rectangles in fig.
are the same to the previous root cause taxonomies for general dl bugs .
these differences owe to the fact that our study is focused on the performance of dl systems while the previous studies are mainly concentrated on the functionality of dl systems.
.
stage analysis rq3 islam et al.
classify the pipeline of dl systems into six stages i.e.
data preparation model building training evaluation hyper parameter tuning andprediction in their study on general dl bugs.
we consider them as the execution stages of dl systems and add two new stages found in our data labeling before them.
the first newly added stage is environment setting where dl environment like libraries and hardware are installed and configured.
the second one is initialization where the dl system is initialized e.g.
importing libraries and initializing parameters before starting the execution stages.
5a reports the number of pps introduced and exposed in each stage where the stage name on the x axis is simplified to the initial letters.
data preparation is the most bug prone stage which is blamed in .
of the pps.
environment setting model building and training are the second most bug prone stages respectively causing about of the pps.
hence developers should pay more attention to these stages to avoid the introduction of pps while automated pp localization approaches should be specifically developed for these stages.
the other stages are less bug prone respectively introducing at most of the pps.
on the other hand training and data preparation are the two most bug affecting stages where .
and .
of the pps are respectively exposed.
thus developers should focus more efforts on these two stages to optimize their performance while automated pp detection approaches should be specifically developed for these two stages.
around of the pps are respectively exposed until the evaluation and prediction stages.
the other stages are less bug affecting respectively exposing at most of the pps.
further data preparation introduces more pps than exposed.
this difference is more severe in the other two earlier pipeline stages i.e.
environment setting and model building.
about of the pps are introduced in the earlier four pipeline stages about of which are exposed in the later four pipeline stages.
the other way around training exposes more pps than introduced.
this difference holds in the other two later pipeline stages i.e.
evaluation and prediction.
nearly of the pps are exposed in the later four pipeline stages.
thus pps should be proactively detected and localized before severe consequences occur so as to reduce time cost and resource consumption.understanding performance problems in deep learning systems esec fse november singapore singapore besides for each pp we measure the distance between its exposing stage and introducing stage which is used as an indicator of the difficulty of pp localization.
intuitively the larger the distance the more difficult to localize a pp from its symptom to root cause.
as shown in fig.
5b .
of the pps are exposed and introduced in the same stage while .
of the pps cannot be exposed in the introducing stage.
specifically .
of the pps are exposed two stages later.
extremely .
of the pps are exposed seven stages later i.e.
they are introduced in the first stage but exposed in the last stage.
hence pp localization is challenging for a considerable amount of pps.
moreover we investigate the symptom distribution of the pps exposed in each stage which is shown in fig.
6a.
this distribution helps pinpoint the potentially useful performance indicators for detecting pps exposed in different stages.
for example time related indicators can be valuable to detect pps exposed in initialization data preparation and prediction because the most common symptom of the pps exposed in these stages is under the category of time .
similarly we report the root cause distribution of the pps introduced in each stage in fig.
6b.
this distribution helps hint the potential technical solutions to localize pps introduced in different stages.
for example the most frequent root cause of the pps introduced in most stages is under the category of api and hence api misuse detection could be developed to localize pps introduced in these stages.
summary.
the most bug prone stages are data preparation environment setting model building and training which introduce nearly of the pps.
the most bug affecting stages are training and data preparation which expose around of the pps.
nearly of the pps cannot be exposed in the introducing stage.
moreover we introduce two new stages that are not covered in the previous stage analysis for general dl bugs and investigate the introducing and exposing stages that are not distinguished in the previous study .
.
approach assessment rq4 to the best of our knowledge there is no pp detection and localization approach for dl systems.
notice that performance analysis approaches in can estimate performance metrics i.e.
time and gpu memory but cannot directly pinpoint pps.
based on their estimation either automated approaches need to be further designed or developer experience need to be relied on to identify pps.
therefore we do not use them.
thus we select and assess the following three typical performance analysis approaches which can be used by developers to improve the performance of dl systems.
tensorflow profiler17 it is built on top of nvidia cuda profiling interface to track the performance of tensorflow models.
it visualizes the time cost and resource consumption of various tensorflow operations in the model finds performance bottlenecks and recommends best practices to improve performance.
differently general python profiling tools e.g.
cprofile and memory profiler can only measure performance metrics but cannot directly pinpoint pps.
therefore we do not use them.
xla accelerated linear algebra it is a domain specific compiler that can accelerate tensorflow models.
each tensorflow operation is executed by a precompiled gpu kernel implementation.
xla can compile the tensorflow graph into a sequence of kernels generated specifically for the given model and fuse the kernels to avoid memory operations between the execution of different kernels to improve the performance .
tensorflow documentation it includes all tensorflow api documentation19and performance guide20where developers can find hints about performance problems and optimization solutions.
generally we assess each technique in two dimensions i whether a technique is applicable to a pp or whether a pp is in the capability scope of a technique and ii whether a technique can solve a pp.
the assessment results on our benchmark see sec.
.
are shown in the last five columns in table .
the first six columns of table show the number of reproduced pps across root causes and symptoms where the number in parentheses is the total number of pps.
they cover all root causes except for mismatched library version and the three hardware relevant root causes.
they cover all high level symptoms but achieve a relatively low coverage of processor relevant symptoms.
as shown in the seventh column of table tensorflow profiler is only applicable to .
pps but is not applicable to the others for two reasons.
first tensorflow profiler requires a tensorflow version of at least .
.
however some pps are reproduced with a lower version.
second tensorflow profiler requires a full training or evaluation process to track the performance which is not always available for the pps in our benchmark.
moreover of these pps tensorflow profiler fails to finish profiling because of out of memory errors for pps and does not raise any warning or raises a false warning for pps.
hence we consider these pps as not solved by tensorflow profiler.
for the remaining pps tensorflow profiler either raises a warning but suggests a fix that achieves a smaller performance improvement than our fixed version in the benchmark or helps detect the pp by reporting the most time consuming operation but fails to raise a warning and suggest a fix.
thus we consider these pps as partially solved by tensorflow profiler as reported in the eighth column of table .
these results demonstrate that tensorflow profiler has limited capability in tackling pps.
as presented in the ninth column of table xla is applicable to .
pps.
there are two reasons that xla is not applicable to the others.
first xla uses just in time jit compilation.
however compilation errors might occur for some pps in our benchmark.
second xla is designed for optimizing the performance of tensorflow models.
thus it is not applicable to pps whose root causes are not related to tensorflow operations or computation graphs.
furthermore of these pps xla only improves the performance for pps but still achieves a smaller performance improvement than our fixed version in the benchmark.
this is reasonable because xla is actually not aware of the pps but optimizes performance by fusing nodes in computation graphs while our fixed version reduces the number of nodes in computation graphs.
hence we consider these pps as partially solved by xla as reported in the tenth column of table .
for the other pps xla does not have any performance improvement because of the small number of nodes in computation graphs.
thus we consider these pps as not solved by xla.
these results indicate that pps in dl systems often cannot be eliminated by the compilation optimization techniques in xla.
november singapore singapore junming cao bihuan chen chao sun longjie hu shuaihong wu and xin peng table benchmark pps across root causes and symptoms and assessment results root causesymptomtotalprofiler xla doc.
time memory processor unknown app.
par.
app.
par.
app.
api not using efficient api not using batch api inefficient api usage model confusion with computation graph inefficient model structure improper model parameter improper hyper parameter library buggy library version mismatched library version data inefficient data transmission inefficient data preprocessing improper data input hardware total as shown in the last column of table tensorflow documentation is only applicable to .
pps.
we consider tensorflow documentation as applicable as long as the documentation mentions the optimization solution of a pp.
there are two main reasons that tensorflow documentation is applicable to a small portion of pps.
the first is that performance characteristics especially non time characteristics are hardly described in api documentation.
the second is that many pps are caused by inefficient usages of multiple apis but api documentation is often focused on individual api usages.
although performance guide covers usages of multiple apis they only cover limited apis such as tf.data .
we consider these pps as solved by tensorflow documentation.
these results show thattensorflow documentation provides limited support for pps.
summary.
efforts like profiling compilation optimization and documentation have been devoted to optimizing the performance of dl systems from different perspectives.
however they provide limited capability in tackling pps potentially due to the lack of a comprehensive understanding of pps in dl systems.
implication application and threat we discuss the implications for developers and researchers demonstrate one application to pp detection and discuss the threats.
.
implications developers.
our study reveals the common symptoms of pps that developers could pay attention to when testing and running their dl systems for detecting potential pps.
our study also identifies the common root causes of pps that can be useful for developers to diagnose debug or fix pps.
our study also captures the most bug prone or bugaffecting stages where developers could focus more efforts on to provide the most benefit for pp introduction avoidance or performance optimization.
furthermore our findings provide some development suggestions.
developers should carefully read the release note andapi documentation of dl libraries to get familiar with the rich set of library apis and their performance characteristics.
in this way pps caused by the most common root cause i.e.
api misuses might be reduced.
developers should also be systematically trained to have a comprehensive understanding of computation graph to build efficient dl models.
in this way pps caused by the second most common root cause i.e.
model construction might be reduced.
researchers.
our findings provide several implications on future research in three directions.
first intelligent techniques for highperformance dl system development are needed.
as developers are often unaware of library apis that are specifically designed for high performance or unaware of the performance characteristics of library apis dl library api recommendation methods should be developed.
to realize performance aware api recommendation a knowledge graph of dl library apis should be constructed based on release note api documentation and stackoverflow discussions with a specific focus on modeling performance characteristics of apis and performance differences across library versions.
to locate and replace inefficient code snippets written from scratch by developers semantic analysis techniques should be developed to determine their semantic similarity to existing library apis.
apart from such intelligent techniques at the code level recommendation techniques should be developed to automatically suggest dl library versions efficient dl models and their parameters and environment configurations.
second pp detection techniques are needed.
half of the symptoms i.e.
increasing time over time program hang out of memory memory leak and not using gpu can be regarded as a credible oracle for detecting pps in dl systems.
therefore proactive monitoring and prediction techniques should be developed to detect pps as early as possible before these severe symptoms occur.
dl systems exhibiting the other symptoms are not guaranteed to contain pps as it is often not clear how much time or resources a dl system should consume to run without a pp.
to solve this performance oracle problem one potential way is to design differential testing techniques to compare the performance of dl systems running with different dl libraries understanding performance problems in deep learning systems esec fse november singapore singapore different dl models or different hardware configurations.
however it may incur too much overhead.
hence another potential way is to design static techniques to model and estimate time cost or resource consumption of dl systems so that performance bottlenecks can be identified in advance before execution.
during our manual analysis we find that tensorflow has some built in mechanism in detecting pps and recommending fixes by throwing a warning message e.g.
warning tensorflow multiprocessing can interact badly with tensorflow causing nondeterministic deadlocks.
for high performance data pipelines tf.data is recommended .
however such warning messages are only raised in of the pps indicating the preliminary support in pp detection due to symptom and root cause diversity.
hence built in mechanisms in dl libraries should be further enhanced to detect pps and recommend fixes.
third pp localization techniques are needed.
our study reveals that the exposing stage of a pp is usually not the introducing stage.
for example the location that throws the error message of an out of memory error is usually not the location of the root cause.
therefore it is often challenging to localize pps.
during our manual analysis we find that developers often use logs as the clue to locate pps.
hence automated log analysis techniques should be developed to smartly insert log statements into dl systems and locate potential pps using log traces.
further as api misuse is the most common root cause of pps mining techniques should be designed to learn frequent api usage sequences and localize potential violations in dl systems.
api usage mining has been widely explored in traditional systems but it is interesting to investigate how they are applicable to pps in dl systems.
from our experience there are three challenges to detect api related pps.
first due to the lack of effective type inference tool in python it is hard to precisely extract api usages from python code.
second as traditional api usage mining is not aware of performance characteristics of apis it is non trivial to automatically determine the performance difference among mined api sequences.
third it is difficult to detect pps caused by not using efficient apis because the inefficient apis that developers use are totally different from efficient apis that should be used.
last but not the least rule based techniques should be developed to detect and localize pps considering the potentially large amount of pps on stackoverflow or github.
the challenge is to automatically derive but not manually specify the rules.
.
application to demonstrate the usefulness of our findings we implement a rulebased static checker named deepperf to detect pps in dl systems.
deepperf is implemented with two static analysis tools ast21and jedi22.
it currently supports three types of pps whose detection rules are manually derived from our empirical study sec.
.
checker repeated node creation.
creating the same nodes repeatedly to a computation graph is one of the common types of pps under the root cause category of confusion with computation graph .
deepperf is designed to detect node creation apis that are called in loops with the same argument values e.g.
the two apis tf.matmul andoptimizer.minimize in fig.
.
actually it is similar to loop invariant computation and code motion licm optimization which pp detection results of deepperf checkerdetected confirmed fixed pp proj.
fp pp proj.
pp proj.
checker checker checker total has been well studied in classic compilers .
however grappler23 the default graph optimizer in tensorflow runtime cannot eliminate this type of pps although it has the loop optimizer.
notice that this type of pp has been reported in .
however to the best of knowledge its detection has not been investigated in prior studies to implement the checker we first extract tensorflow apis that may add computation graph nodes by parsing the tf export decorators in the source code of tensorflow python apis24.
then we manually review these apis to exclude apis that actually do not add nodes e.g.
tf.assign or apis that produce different values given the same inputs e.g.
tf.random.uniform .
finally we obtain apis.
our checker determines whether these apis are called with same argument values among loop iterations.
to this end it tracks variables that are changed among loop iterations including the loop control variable variables that are assigned in the loop body but are defined outside the loop and any variables that depend on them.
it identifies apis called without using changed variables as arguments as pps.
our analysis is inter procedural.
if there are functions called in the loop it passes changed variables to callee functions analyzes changed variables in callee functions and identifies apis called without using changed variables as arguments in callee functions.
checker inefficient order of batch and map.as showed in fig.
calling mapbefore batch is not efficient and hence batch is suggested to be called before mapto reduce the number of times the mapped function is called.
to detect such api misuse of batch and map our checker first identifies tf.dataset object and then analyzes the call sites to check whether batch is called after map.
checker disabled parallelism of mapand interleave .as listed in fig.
calling mapwithout setting its num parallel calls argument disables parallelism.
it also holds for interleave .
to detect such api misuse of mapandinterleave our checker identifies tf.dataset object and analyzes the call sites to check whether map andinterleave are called without setting num parallel calls .
evaluation on our benchmark.
in our pp benchmark four two and two pps belong to the pp types targeted by the three checkers.
three two and two of them were successfully detected by the three checkers.
the only one false negative of checker is caused by the incomplete type inference in jedi.
as reported in sec.
.
tensorflow profiler is not applicable to these eight pps.
xla is applicable to four one and one of them but fails to solve them.
tensorflow documentation is applicable to zero two and two of them by only hinting the solution in api documentation or performance guide.
evaluation on github projects.
we used pygithub25to crawl github repositories that used tensorflow and python and november singapore singapore junming cao bihuan chen chao sun longjie hu shuaihong wu and xin peng had at least stars and ran deepperf on these repositories.
we reported detected pps as issues to developers and also manually reviewed and verified all the detected pps.
as tensorflow profiler and xla are dynamic analysis tools it is difficult for us to properly configure and execute github repositories.
tensorflow documentation only provides guidance but is not a tool.
thus we did not compare our checkers with them in this large scale evaluation.
the results are shown in table where the statistics about detected confirmed and fixed pps are reported for each checker.
specifically checker detected pps in projects.
it detected false positives i.e.
the fourth column in table .
the reason is that we use lightweight heuristics to decide loop invariants based on ast and jedi but do not use heavyweight data control flow analysis for the scalability of our checker.
pps in projects have been confirmed by developers and of them in projects have been fixed.
checker detected pps in projects with no false positive.
pps in projects have been confirmed by developers but none of them has been fixed.
the reason is that the fix requires extra effort in vectorizing the mapped function e.g.
the batch parser function in fig.
which is non trivial.
in that sense automated vectorization is required in tensorflow like auto vectorization in llvm26.
checker detected pps in projects with no false positive.
pps in projects have been confirmed by developers while of them in projects have already been fixed.
the projects that have confirmed fixed our detected pps include popular ones like keras tensorflow agents tensorflow hub andtensorforce .
besides we randomly sampled pps from the and fixed pps for check andchecker respectively and measured the execution time of the buggy and fixed version.
on average the execution time was improved by .
and .
after fixing pps respectively.
summary.
pp is a widespread problem in dl systems and rulebased pp detection is promising.
the three checkers in deepperf detected pps in projects with false positives.
pps in projects have been confirmed by developers while of them in projects have been fixed by developers.
.
threats we discuss the threats to our empirical study pp benchmark and detection approach.
our study investigates pps in dl systems written with tensorflow andkeras .
thus it is not clear whether our findings can generalize to dl systems developed with other dl libraries likepytorch .
we believe it deserves a separate study to investigate differences across dl libraries.
further our study analyzes pps from stackoverflow posts.
however github is another valuable source of pps.
it is interesting to further explore pps from github to strength our findings which in fact requires large manual efforts as we spent six person months to analyze pps.
our pp detection results on github projects also indicate the potential applicability of our findings.
moreover our study involves manual analysis on pps which may incur biases.
to reduce them two of the authors separately analyzed pps and a third author was involved to resolve disagreements.
our benchmark consists of pps whose size to be honest is not very large.
however considering the large human efforts involved in constructing the benchmark we believe it is acceptable.
we are still enlarging our benchmark via reproducing those nonsampled pps from the pps and collecting pps from github.
our rule based static checker deepperf currently only supports three types of pps.
here deepperf is not designed to cover all type of pps but to demonstrate the potential of rule based pp detection as well as the usefulness of our findings.
we plan to manually enrich the detection rules in deepperf to support more pp types.
in the long run we hope to automatically learn the detection rules.
related work we discuss the closely related work in understanding and analyzing deep learning bugs and performance problems.
.
deep learning bugs the recent success in applying deep learning techniques to a variety of domains has gained increasing interest in understanding characteristics of bugs in deep learning systems.
zhang et al.
collected bugs in deep learning systems developed in tensorflow from stackoverflow posts and github commits.
they analyzed the symptoms and root causes of these bugs and explored the challenges and strategies in bug detection and localization.
islam et al.
and humbatova et al.
expanded the scope of zhang et al.
s study to include more deep learning libraries.
islam et al.
analyzed types root causes impacts and pipeline stages of bugs in deep learning systems written in caffe keras tensorflow theano andtorch while humbatova et al.
constructed a taxonomy of bugs in deep learning systems that use tensorflow keras andpytorch based on manual analysis of bugs and interviews with developers.
in their follow up work islam et al.
analyzed bug fix patterns.
kim et al.
built a benchmark of bugs from deep learning systems.
differently jia et al.
explored the symptoms root causes and locations of bugs in the tensorflow library.
apart from the studies that are focused on a general scope of bugs in deep learning systems several recent studies have targeted more specific bugs.
zhang et al.
studied failures of deep learning jobs that are running on a remote shared platform in microsoft.
chen et al.
investigated faults related to the deployment of deep learning models to mobile devices.
zhang et al.
summarized five common training problems in deep learning systems and developed a tool to automatically detect and repair training problems.
wan et al.
studied api misuses when deep learning systems use cloud ai services summarized eight misuse patterns and developed static checkers to automatically detect some of the misuse patterns.
huang et al.
explored dependency bugs across the dl stack.
some of these studies reveal some partial characteristics of performance problems in deep learning systems.
for example zhang et al.
and islam et al.
respectively recognized low efficiency and hang as a symptom of deep learning bugs.
zhang et al.
identified gpu out of memory as a failure category of deep learning jobs.
chen et al.
recognized memory and speed issues as two types of faults in the inference stage of deployment process.
wan et al.
derived four performance related api misuse patterns of cloud ai services.
despite these efforts there still lacks a comprehensive study to understand characteristics of performance problems in deep learning systems and thus our study aims to bridge this knowledge gap and raise the awareness of performance problems in dl systems.understanding performance problems in deep learning systems esec fse november singapore singapore besides some studies have explored general problems and challenges in developing and deploying deep learning systems.
for example guo et al.
measured the accuracy and performance differences across four deep learning libraries.
zhang et al.
identified seven kinds of frequently asked deep learning questions in stackoverflow and analyzed their resolution difficulty and root causes.
han et al.
explored the topics that developers discuss when developing deep learning systems.
chen et al.
built a taxonomy of challenges in deploying deep learning systems to different platforms through manual analysis of stackoverflow posts.
pham et al.
measured accuracy variance in training deep learning systems.
cummaudo et al.
studied pain points that developers face when using cloud services of computer vision by mining stackoverflow posts.
although these studies are not designed for deep learning bugs they shed light on debugging and bug detection in deep learning systems.
specifically guo et al.
reported performance differences in terms of time cost and memory consumption when trained deep learning models are migrated or quantized to different mobile devices and web browsers and called for performance optimization and testing techniques.
zhang et al.
summarized performance as a category of frequently asked deep learning questions in stackoverflow and recognized that performance questions are the most difficult to answer.
our study is inspired by these studies to systematically characterize performance problems in deep learning systems.
moreover some advances have been made to detect deep learning bugs.
for example zhang et al.
developed a static analysis approach to detect numerical bugs in neural architectures based on abstract interpretation.
lagouvardos et al.
proposed a static analysis to detect shape incompatibility errors in tensorflow programs while verma and su proposed a dynamic abstract interpreter to catch such errors.
wardat et al.
developed a dynamic analysis approach to locate faults in deep neural networks.
in addition great efforts have been devoted to testing deep learning systems e.g.
and deep learning libraries e.g.
for quality assurance.
zhang et al.
presented a comprehensive survey of work in this direction.
however little attention has been received to detecting and testing performance problems in deep learning systems and our study sheds light on this area.
.
performance problems many empirical studies have characterized performance problems from different perspectives e.g.
root causes discovery diagnosis fixing and reporting for desktop or server applications highly configurable systems mobile applications database backed web applications and javascript systems .
they shed light on potential directions on performance analysis e.g.
detection profiling and testing .
our study is the first to understand performance problems in deep learning systems which differs from traditional systems on the programming paradigm.
advances e.g.
have been made to identify general performance problems with dynamic profiles from production runs.
a large body of work has designed pattern based methods to detect specific performance problems e.g.
reusable cacheable data e.g.
inefficient redundant loops e.g.
and inefficient collections e.g.
.
besides a lot of techniques have been proposed for performance testing i.e.
generating test cases totrigger worst case performance e.g.
and find performance problems e.g.
.
another line of work is performance profiling technique to identify hot paths e.g.
and fit a performance model to the input size e.g.
.
these performance analysis approaches are designed for traditional systems and cannot be directly applied to deep learning systems.
recently some performance analysis approaches have been proposed for deep learning systems.
for example qi et al.
modeled and estimated time cost of training deep neural networks while gao et al.
estimated gpu memory consumption.
such estimation techniques are useful to find potential performance problems in advance.
liu et al.
measured the performance of training deep learning models on mobile devices while ma et al.
compared time cost of javascript based deep learning libraries when running deep learning tasks in browsers.
these studies empirically demonstrate the performance differences.
to reduce memory usage of deep neural networks rhu et al.
developed a dynamic memory manager to virtualize memory usage while wang et al.
proposed a dynamic gpu memory scheduler.
to make deep learning models efficient han et al.
used pruning and quantization to compress models yan et al.
used a performance model to estimate the time of distributed model training and find the optimal distributed configuration and menghani presented a survey in this area.
these approaches are system level performance optimization techniques while deepperf is at the source code level.
despite these efforts the characteristics of performance problems in deep learning systems are still unclear and our study fills this gap.
conclusions we present the first comprehensive study to characterize pps in dl systems written in tensorflow andkeras and build the first benchmark of pps in dl systems to assess existing approaches in tackling them.
further we develop a static checker deepperf to detect three types of pps and detect many new pps in github projects.
data availablity statement all the study data and source code of deepperf are available at to foster future research.