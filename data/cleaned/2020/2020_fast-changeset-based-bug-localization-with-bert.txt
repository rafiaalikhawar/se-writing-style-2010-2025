fast changeset based bug localization with bert agnieszka ciborowska virginia commonwealth university department of computer science richmond va usa ciborowskaa vcu.edukostadin damevski virginia commonwealth university department of computer science richmond va usa kdamevski vcu.edu abstract automaticallylocalizingsoftwarebugstothechangesetsthatinduced themhas the potential toimprove software developerefficiency and to positively affect software quality.
to facilitate this automation a bug report has to be effectively matched with source codechanges evenwhenasignificantlexicalgapexistsbetween natural language used to describe the bug and identifier namingpractices used by developers.
to bridge this gap we need techniquesthatareabletocapturesoftwareengineering specificand project specificsemanticsinordertodetectrelatednessbetween the two types of documents that goes beyond exact term matching.
popular transformer based deep learning architectures such as bert excel at leveraging contextual information hence appear to be a suitable candidate for the task.
however bert like models arecomputationallyexpensive whichprecludesthemfrombeing used in an environment where response time is important.
inthispaper wedescribehowbertcanbemadefastenoughto be applicable to changeset based bug localization.
we also explore several design decisions in using bert for this purpose including howbesttoencodechangesetsandhowtomatchbugreportstoindividual changes for improved accuracy.
we compare the accuracy and performance of our model to a non contextual baseline i.e.
vector space model and bert based architectures previously used insoftwareengineering.ourevaluationresultsdemonstrateadvan tagesinusingtheproposedbertmodelcomparedtothebaselines especially for bug reports that lack any hints about related code elements.
keywords bug localization changesets information retrieval bert acm reference format agnieszkaciborowskaandkostadindamevski.
.fastchangeset based buglocalizationwithbert.in 44thinternationalconferenceonsoftware engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction two of the most prevalent tools used today by software engineers arerepositoriestostoreprojectfiles e.g.
git andbugtrackersto .
this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
monitorbug fixingactivity e.g.
jira bugzilla .automaticallylinkingabugreportinabugtrackerandrelatedsoftware artifacts from a repository is one of the long standing goals in the software engineering research community due to its poten tial to improve practice by reducing the time developers spend examining code when addressing a newly reported bug i.e.
bug localization .however despitenumerousefforts theaccuracy of bug localization approaches is not yet high enough for wide spread use especially as it applies to different software projects thatvaryinbugreportandcodestyle .inexaminingthetrends frominterviewsconductedwithalargecohortofsoftwaredevelopers from industry and open source software zou et al.
report thatdevelopersdonottrustbuglocalizationtoolsduetotheirinabilitytoadapttodifferenttypesofbugreports specificallynoting that existing techniques only work on the most simple cases with straightforwardtextualsimilaritybetweenthebugreportandcode base .
more work is needed to improve the retrieval quality of bug localization techniques.
atthesametime asindustryisincreasinglyattemptingtouse bug localization to aid developers in their daily work specificrequirements of the problem for modern use are coming to the forefront .onekeycharacteristicfoundbeneficialinmodern software projects is bug inducing changeset or commit level retrieval.abug inducingchangesetisonewherethebugwasinitially introduced into the repository.
retrieving such changesets leads to faster bug repair as they contain related parts of the code that were changed together which makes fixing the bug easier.
however retrievingbug inducingchangesetswith highaccuracy is more challenging than retrieving buggy source code elements due to the potentially large number of commits in the corpus.
in recent years numerous popular natural language processing tasks e.g.
question answering machine translation have all observedimprovedperformancewhenusingneuralnetworkarchitecturesbasedontransformers.thesetransformer basedmodelsare typicallyappliedviatransferlearning byfirstpre trainingthemona very large corpus and then fine tuning on a much smaller datasettowardsthespecifictasktheyaretobeusedfor.transformer based models pre trained on large software engineering corpora e.g.
stackoverflow arenowbecomingavailable withthepotential to improve software engineering tasks like bug localization.
in thispaper weuse thebert bidirectional encoderrepresentations from transformers transformer based architecture which is a highly popular model introduced by devlin et al.
.
buglocalizationisusuallyframedasaninformationretrieval ir task where a document i.e.
a software artifact is retrieved fromacorpus basedonaquery i.e.
thebugreporttext .ameasure ofsemanticrelatednessbetweenthebugreportandthesoftware ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa ciborowska and damevski artifact is necessary to rank the results retrieved from the corpus.
given the fact that transformer based models consist of many neural layers and require heavy computation for each sentence measuringrelatednessbetweenthequeryandthecorpusquickly becomes expensive.
this paper applies bert to the problem of changeset based bug localizationwiththe goalofimprovedretrievalquality especially on bug reports where straightforward textual similarity would not suffice.wedescribeanarchitectureforirthatleveragesbertwithoutcompromisingretrieval speedandresp onsetime.inaddition we examine a number of design decisions that can be beneficial in leveragingbert likemodelsforbuglocalization includinghow best to encode changesets and their unique structure.
our experimental results indicate that the proposed approach improvesuponpopularbuglocalizationtechniquesby e.g.
increasingtheretrievalaccuracybetween5.
and20.
forbugreports with no or a limited number of localization hints.
we note that usingentirechangesetsasinputgranularitysignificantlyhinders the models performance while leveraging more fine grained input data suchashunks resultsinthehighestretrievalquality.wealso observethatthesizeofsearchspace i.e.
thenumberofchangesets in a project significantly impacts the retrieval delay of different bert based models though less in the case of the proposed model.
the main contributions of this paper are approach that applies bert to the bug localizationproblem specifically localizingbug inducingchangesets that is more accurate than the state of the art improvement over other recent bert based architecturesproposed towards changeset retrieval showing significantadvantageswithrespec t to retrieval speed evaluation and recommendations for key design choicesin applying bert to changesets i.e.
code change encoding data granularity .
significance of contribution.
the bert based technique proposed in this paper enables semantic retrieval of software artifacts specifically changesets forbuglocalizationthatgoesbeyond and can complement the exact term matching in the current popularstate of the arttechniques e.g.
.relativetoasimilar recentbert basedtechnique weofferanapproachthatimproves retrieval speed significantly in a way that supports realworld use while also enhancing retrieval quality.
problem description in this section we list and discuss the specific constraints of the buglocalizationproblemthatweaimtoaddress whicharebased on a recent survey of industry practitioners and the problem re quirements observed at a large software enterprise .
our focusisabuglocalizationtechniquethat focuses onretrieving changesets aims to capture semantics and can be applied to bug reports that do not share terms with the relevant parts of the code base and3 quicklyretrievesresultsforanewlycreatedbugreport.
.
localizing changesets .
over the years a large body of researchhasbeendedicatedtolocatingsourcecodefiles orclasses relevanttoabugreport .however recentstudies have pointed out that bug localization at the level of source code files still requires significant effort by software developersinordertolocaterelevantcodewithinlargefiles .adjusting for this finding researchers shifted their efforts towards more fine grained code elements such as file segments and methods which introduce new sets of challenges such asdifficultyinselectingoptimalsegmentsizeandlargemethods thatstillrequireefforttoexamine.morerecently therehasbeen agrowinginterestinchangesetretrieval forbuglocalizationbecausechangesetshaveseveralunique propertiesthat make them convenient to developers aiming to fix a bug.
first they inherently capture lines of code that are related to each other withinthecontextofamodification.second whenlocatingchangesets we can retrieve not only the modified portion of the code but identifyasoftwaredeveloperthatcommittedthemodificationin the first place therefore easing the bug triaging process.
finally changesetsallowforstraightforwardcontext awaredivisioninto asetofhunks i.e.
asetofchangesinoneareaofthefile.hunksare usually convenient to read for developers and allow for easy detection of changes with no semantic value e.g.
changes only in white spaces .
.
leveraging semantics of input documents .
as software evolves rapidly and is actively maintained by multiple developers different portions of the code base become affected by distinctiveidentifiernamingpatternsandconventions whichexacerbate the already existing semantic gap between bug reports andrelatedcodeelements posingasignificantchallengetotraditional ir systems based solely on token similarity .
surveys of practitioners have also indicated that bug reports that explicitlymention the names of classes or methods relevant to the bug fix donotrequireautomatedbuglocalization whileassistinginbug reports with large semantic gaps with the code base is likely more valuable to developers.
for instance one surveyed developer in the study by zou et al.
stated the following about current bug localization itseemsthatexistingtechniquesmainlymakeuseof the textual similarity between bug reports and source code files toperform bug localization.
however i encountered many bugs that haveverylittlesimilaritybetweentheirbugreportsandcodefiles.i wonder what kind of bugs such techniques can localize?
maybe only simple bugs?
.
to bridge this gap researchers have recently proposedtousedeeplearningmodelscapableofbuildingsemanticallyrichdocumentrepresentations .transformerbasedmodels andbertinparticular arecurrentlyoneofthemost excitingdeeplearningtechniquesachievingbroadimprovements acrossavarietyoftext basedtasks.themainstrengthofbert like models is in building a token representation based on bidirectional contextualinformationencodedintheprecedingandsucceeding tokens which leads to richer semantics that is more likely to detectrelatedpairsofbugreportsandchangesetsthatdonotshare terms.priorgenerationsofwordembeddings e.g.
word2vec and glove which have been frequently applied on software engineering tasks do not use word context at inference time i.e.
each token maps to a vector regardless of the surrounding text.
.
fast retrieval in a large search space .
retrieving buginducing changesets requires computing similarity between a bug report of interest and all changesets committed to a repository up tothepresentpointintime.giventhatmodernsoftwareevolves rapidly resulting in large source code repositories with numerous 947fast changeset based bug localization with bert icse may pittsburgh pa usa bertclassification layer aggregation layerencoded feature vector bug report changesetrelevant not relevant a single bert bertclassification layer aggregation layerconcatenated encoded feature vectorrelevant not relevant bug report changeset b siamese bert bert bug report changesetrelevant not relevant linear layermaxsim maxsim maxsim c fbl bert figure bert based architectures for changesets retrieval commits itisimpracticaltocomputepair wisesimilarity due to the large search space.
this is especially the case if computingthesimilaritymeasureitselfisexpensive.thoughdeeplearning models provide state of the art accuracy they typically require more computationalresources thantoken based techniques which emphasizestheneedforabuglocalizationtechniquetolimitthe search space in order to improve performance without compromising accuracy.
approach in order to address the above problem constraints in this paper we investigate the use of a bert model towards bug localization with changesetsasaprimarydatagranularity whichisalsopreferredby practitioners .
.wespecificallyselectedbertasitisthe state of the artinsemanticsmodelingandextractingcontextual information .
.finally toensurethatourapproachisapplicable tolarge industryscalerepositories .
weintroducefastbug localization bert fbl bert which reduces the search space suchthatonlypromisingcandidatechangesetsareconsideredfor neuralre rankingwithbert.inaddition fbl bertencodesabug report and a changeset separately allowing to compute changeset representationsofflineandreducethecomputationaleffortperbug report at retrieval time.
replication package is available at .
.
bert for bug localization thearchitectureofbertconsistsofmultiplelayersoftransformerencoders whichareanabstractionaimedatmodelingsequential data that utilizes self attention the notion of attention is to weight specific terms in the sequence differently i.e.
encoding a stronger relationship from each term in the sequence to the remaining most semantically relevant terms.
as pointed out by mills et al.
retrieval techniques for bug localization can be significantly improved with intelligent query construction i.e.
by carefully choosing which parts of the bug report to use for comparison.
therefore leveragingamodelthatusesattentiontoemphasizecertainwordrelationships has the potential to significantly improve upon prior state of the artbug localization techniques.
usingabertmodelforbuglocalization orothersimilarpurposes involvesthreeessentialsteps pre trainingthemodelwith alargecorpusofgeneralsoftwareengineering relateddata finetuningthebertmodelforbuglocalization andfinally afterbert hasbeencompletelytrained retrievingrelevantbug inducing changesets for a newly reported bug.
during pre training bert uses massive corpora of relevant text to build a language model for a specific domain e.g.
software development.
given that this step requires a significant amountof data and computational resources a common choice is to reuseapre trainedbertmodel whenavailable.inthefinetuning step bert updates the general data representation with respect to a specific downstream task e.g.
bug localization given a much smaller task specificdataset.moreprecisely finetuningabert model occurs by adding an additional layer e.g.
a classification layer tothepre trainedbertmodel.thistask specificlayertakes theoutputofbertasinputandrepresentsthepartofthemodel thatisprimarilytrainedduringfinetuning thoughbert sinternal weights are also updated in the process.
in most scenarios fine tuning can be completed faster and with much less computational resourcesthanpre training.sinceourgoalislocatingbug inducing changesets anaturalchoiceforatask specificdatasetconsistsof bug reports and their inducing changesets.
a key design choice at this stage is how to connect bert with the additional task specific neuralnetworklayer.givenaninputdocument bertencodeseach word in the document with a vector i.e.
for each input document the output of the bert model is an embedding matrix of size d byvlen where d representsthenumberofwordsinthedocument andvlenthelengthofabertvector typically vlen .themost common approach when retrieving bert encoded documents is to aggregate the embedding matrix across words through average or summation whichproducesasinglevectorasoutput.usingsuchan aggregaterepresentationofadocumentallowsforfasterprocessing andeasiercomparisonbetweenpairsofdocuments.however as 948icse may pittsburgh pa usa ciborowska and damevski bert bug reportlinear layermaxsim changesetmodel training offline indexing faiss indexretrieval bug reportmaxsim changesets1...ntop n m most similarranking bug reportfaiss indexfbl bertfbl bert changesets1...mfbl bert fbl bert figure fbl bert for changeset based bug localization pipeline.
pointed by sachdev et al.
this simple aggregation strategy leads to a dissipative data representation that has the potential tonegativelyaffectretrievalperformance.inthenextsection we describe an alternative strategy that takes advantage of the full matrix to encode input data.
in the simplest changeset retrieval scenario presented in fig.
1a each newly arriving bug report is concatenated with every changesetintheprojecthistory.subsequently theyareprocessedbybert producing an embedding matrix which is transformed to a vector by an aggregation layer.
finally the vector is passed into a classification layer that produces a relevancy score between a bug report andachangeset.changesetsareorderedbasedontheirscoresto produce a ranked result set.
this type of bert architecture for informationretrievalisoftenreferredtoassinglebert .in analternativeretrievalarchitecture calledsiamesebert anddepictedinfig.1b thebugreportandthechangesetareprocessedseparately firstthroughbertandthenthroughanaggregation layer.
as a result bug reports and changesets are transformed into independent vectors that are subsequently concate natedandfedintotheclassificationlayertoproducearelevancescore.
the advantage of siamese bert over single bert is that siamesebertenablespre computingchangesetrepresentations offlinesincechangesetsarenotrequiredtobeconcatenatedwith a bug report for retrieval.
h owever siamese bert still requires comparingabugreporttoeachchangeset whichincurssignificant retrieval delay in the case of large number of changesets.
.
fast bug localization bert thefbl bertarchitecture basedoncolbertbykhattabetal.
eschewsaggregationoftheembeddingmatrix andinsteadbuilds a relevance score by leveraging the whole matrix resulting in a morecomplete finegrainedcomparison.morespecifically abug reportbrand a changeset care separately processed by bert creatingembeddingmatrices ebrandec respectively.tocomputethe relevancescorebetween ebrandec foreachwordembeddingin thebugreport vbr ebr wefindthemaximumcosinesimilarity acrosswordembeddingsofthechangeset vc ec andcombinethe maximumcosinesimilaritiesviasummationasillustratedinfig.1c.
as a result the model learns how to associate words from a bug report with tokens in a changeset taking into account the context inwhichtheyappear.toaccountforthetwodifferenttypesofdata weprocess i.e.
bugreportsandchangesets wemodifycolbert by increasing the numbers of bert encoder layers taken to thelinearlayer.morespecifically whilecolbertusestheoutputof thelastbertencoder wetaketheoutputofthelast4encoders asrecommendedby .thismodificationisdictatedbypriorstudies observingthatthatdifferentlayersofbertencodedifferentgranularity of semantic information .
note that the linear layer infbl bertisnotequivalenttotheaggregationlayerdiscussed before butisusedtoreducethesizeofwordembeddingsproduced by bert retaining all word embeddings in a compressed form for faster downstream processing.
there are several benefits that make the fbl bert architecture particularlyapplicabletoourproblem.first themodelpurposely avoids joint document encoding as in single bert delaying interactionbetweenabugreportandachangesettofacilitateoff line encodingofchangesets.moreover byusingcomputationallycheap yet efficient maximum similarity summation as a scoring operator instead of a more complex strategy such as the classification layer insiamesebert theprocessingtimeforaqueryisreduced.finally given that the relevance score computation is isolated and relies solelyonmaximumsimilarity itispossibletoutilizeefficientvector similarityalgorithmstoreducethesearchspaceofall mchangesets by identifying top nchangesets n m that are similar to a new bug report and subsequently re rank only the top nsubset.
toclarifyhowfbl bertoperatesforchangeset basedbuglocalization consider the pipeline depicted in fig.
.
first as shown inthemodeltrainingsectionoffig.
thefbl bertmodelisfine tuned on a project specific dataset consisting of bug reports andbug inducing changesets.
in the next step offline indexing all changesets in the project repository are encoded via fbl bert and storedinanindexsupportingefficientvector similaritysearch.for thispurpose weuseanivfpq invertedfilewithproductquantization index implementedinthefaisslibrary .theivfpq indexusesthek meansalgorithmtopartitiontheembeddingspace intop e.g.
p partitions and subsequently assigns each word embedding to its nearest cluster.
to facilitate efficient search whenaqueryisissued thequeryisfirstcomparedagainstthepartitions centroidstolocatethenearestpartitions andthenthesearch continues to the instance level only within those.
note that the faiss index contains wordembeddings across allchangesets.
after completionofthisstep theretrievalsystemisreadytobedeployed.
when a new bug report arrives it is first encoded via fbl bert producinganembeddingmatrix.next foreachwordembeddingin the embedding matrix we query the faiss index to identify the n prime most similar embeddings across all changesets embeddings stored 949fast changeset based bug localization with bert icse may pittsburgh pa usa in the faiss index.
since among n primemost similar embeddings some may point to the same changeset in the end we obtain a total of n unique candidate changesets.
finally we use fbl bert to re rank the candidate changesets and produce the final ranking.
.
changesets encoding strategies software evolution over time is recorded in a repository as a timeorderedsequenceofchangesets.eachchangesetconsistsofalog message providing a short rationale explaining the goal of the modification and a set of source code changes.
depending on the version control system and diffalgorithm used in the software project the representation of source code changes can vary.
in this paper wefocusontheformatthatistheoutputofthe git diff command inwhichaddedlinesofcodeareannotatedwith r emovedlineswith andallmodifiedlinesaresurroundedby3lines ofcontextual unchangedlines.whilethereexistmoreadvanced tree based code differencing algorithms e.g.
gumtreediff providing detailed code change information to a machine learning modelmayaffectthemodelnegatively henceweoptforatextbasedapproach.changesetscanencapsulatecodechangesacross one or multiple source code files and modifications to each file can be divided into hunks groups of modified added or removed lines surrounded by unchanged context lines.
given this specific formatting weexplorehowbesttoutilizechangesets propertiesto construct bert input from two perspectives encoding characteristics of code modifications such asadditions or removals and levels of granularity in a changeset.
inputprovidedtobertmodelsisrequiredtofollowcertainrules.
first adocument e.g.
achangesetorabugreport needstobetokenized and each token replaced by its unique token id.
pre trained bertmodelssupplytheirownberttokenizers thatareoptimized towardsthecorpusonwhichthemodelispre trained.berttokenizers are trained using the wordpiece algorithm .
the main advantage of bert tokenizers is in avoiding out of vocabulary wordsbydividingunknownwordstotheirlargestsubwordspresent in the vocabulary which is likely to be beneficial in our setting assoftwareprojectscanhaveveryspecificvocabulariesunlikely tobe observedelsewhere .secondly bertuses apre defined set of special tokens.
in general due to how bert is trained more detailsin themodelrequiresthateachtokensequencestarts with special classification token and ends with separator token whileotherspecialtokens suchaspadding are usedifandwhennecessary.specialtokenscanconveyinformation about the structure of data allowing bert to differentiate between partsoftheinput henceweexplorehowspecialtokenscanbebestutilizedtoencodechangesets.tothisend weproposethefollowing encoding strategies depicted in fig.
.
d a changeset is considered a single document that is feed into the model.
to inform the model that a changeset sequence begins wedefineandpre appendthespecialtoken atthebeginning of the code sequence.
since this strategy does not utilize specific characteristics of a code change it serves as a baseline to compare against other strategies.
arc in this encoding a changeset is split into lines and the lines are subsequently grouped based on whether they are added removed or provide context as indicated by their initial character bert tokenizer font reset d encoding arc encoding arc l encodinggit diff 68f73a31d3bd23bb9be3de8de4cfa69258483b46 b org eclipse swt widgets composite.java void updatefont font oldfont font newfont boolean updatefont font oldfont font newfont control children getchildren for int i i children.length i control control children b org eclipse swt widgets control.java void updatefont font oldfont font newfont font font getfont if font.equals oldfont setfont newfont boolean updatefont font oldfont font newfont boolean samefont getfont .equals oldfont if !samefont setfont newfont return !samefont void updatelayout boolean resize boolean all bug summary font reset to default after screen saver description all editors and views using a styledtext widget have the font reset to default after coming back from my screen saver.
.
this breakpoint gets hit when i return from the screen saver styledtext control .updatefont font font line font line void updatebooleanall boolean update same font void update new font control childrenbooleanall boolean update new font void update new font control childrenchildreni void update new font boolean update same font void updatebooleanall figure changeset encoding strategies.
foradded forremoved andanemptyspaceforcontextlines.the lines in each group are concatenated to create a sequence to which wepre appendaspecialtoken forthesequenceofaddedlines for the sequence of removed lines and for the sequence of contextlines.finally allthesequencesareconcatenatedtogether to create an input for the model.
by grouping different parts of 950icse may pittsburgh pa usa ciborowska and damevski changesets based on their characteristics we aim to investigate whetheranyparticulartypeofmodificationismorebeneficialthan the other.
with the arc strategy the model is given an opportunity to learn how to combine information of different types and ifnecessary decidetodisregardaportionofitifitpoorlyaffects performance.
arcl similarly as in arc a changeset is divided into lines however arc lencoding does not group the lines.
instead it preserves theorderingoflineswithinachangeset suchthatspecialtokens or arepre appendedwherevertypeofmodification changes.
while this strategy results in more accurate data representation comparedtoarc arc lisalsomorechallengingforthe model since the special tokens occur multiple times and in several places.
given that a bug report and a changeset are encoded separately the model has to differentiate between these two types of documents.tothisend whenencodingabugreport wedefineaspecial token that is pre appended to the query i.e.
the bug report.
another dimension in choosing how to best encode changesets is related to their granularity i.e.
using entire changesets or separating a changeset to a file or hunk level.
leveraging hunks as the primary data dimension in an ir model brings several advantages.
first bugshavebeenobservedtobetypicallycausedbysmallpieces of code thus the inherent fine granularity of hunks makes themlesssusceptibletonoisewhencomparedtowholesourcecode files .
second dividing changesetsinto hunks alleviatesissues caused by tangled commits .
given the fact that hunks are typically small and concentrate on an enclosed portion of the code bert is not affected by long range token dependencies which is a problem typically affecting source code .
finally shorter input documents are less likely to exceed the maximum sequence length acceptedbybert whilelongerdocumentshavetobetruncated which may negatively affect the results.
howev er despite easily accessiblesmallerdatagranularitywithinachangeset todate most oftheeffortsarefocusedonleveragingentirechangesets .
experimental evaluation .
research questions rq1 howeffectiveisfbl bertwhencomparedto state of the art techniques based on the vsm and related bert based architectures?
the main opportunity in using fbl bert is in incorporating additionalcontextandsemanticswhenretrievingbug inducingchangesets which should provide improvements in accuracy over the state of the art especiallyforbugreportsthatprovidehighlevel bug descriptions and lack explicit localization hints.
researchers have identified that a non trivial amount of bug reports alreadycontain localization hints i.e.
they mention the class or method namesrelevanttofixingthebug andsomerecentapproachesfor bug localization argue that only bug reports that lack extensive localizationhintsshouldbeconsideredinevaluation .wefollow the methodology proposed by kochhar et al.
to categorize bug reports into groups based on the completeness of localiza tion hints they provide and evaluate the performance for each bug reportgroup separately.
wealso investigate how theruntime performanceoffbl bert whichutilizesfinegrainedmatching compares to other bert based architectures that rely on embed ding aggregation and perform retrieval across the entire searchspace.
as baselines we use locus a state of the art approach based on vsm that locates bug inducing changesets and tbert single and tbert siamese approaches that utilize aggregated bert based representations that have recently been proposed for software engineering.
rq2 which changeset encoding strategy is the most profitable?
are there advantages to using hunks changeset files or entire changesets as the primary data dimension?
inthisrq wefirstinvestigatewhetherencodinginformationabout the type of modification in each line of a changeset can increase the performance of the fbl bert model.
we evaluate two alternativestoencodechangesetssemantics arc arc l andabaseline approach d which disregards change related information.
second we investigate how granularity of the input data affects the model performanceandwhatarethebenefitsandchallengesofleveraging changesets changeset files or hunks in our model.
to answer this rq we fine tune fbl bert separately for each of the encoding strategies and with each input data granularity resulting in evaluation configurations per software project measuring the model s performance in retrieving relevant changesets.
.
dataset and baselines to answer the rqs we leverage the dataset of bugs and their inducing changesets collected and manually validated by wen et al.
manuallyvalidateddatasetsremovetheerrorthatcanbe introducedbytheszzalgorithmthatmapsthebugfixingtothe inducing commit .
this dataset includes software projects namely aspectj jdt pde swt tomcat and zxing descriptive statistics are presented in table .
to create a training set for each project weselectedthefirsthalfofproject spairsofbugreportsand bug inducingchangesets orderedbybugopeningdate asatrainingset andlefttheremaininghalfasatestset.foreachpairinthe training sets we also create a negative sample by randomly choosingacodechangewhichdoesnotbelongtotheinducingchangeset essentially forming triplets of bug report bug inducing changeset notbug inducingchangeset.weexperimentedwithchoosing negative samples by selecting a syntactically similar changesetthat was not bug inducing but we did not observe a significantchange in retrieval accuracy.
as this type of generating negative samplesincurredsubstantialcomputationalcosttogather weopted to use random sampling.
finally for each project we obtained a balancedtrainingsetwithequalnumberofpositiveandnegative examples.
note that although training sets do not include all available code changes during bug localization the model performs retrieval across allcode changes available for a specific project as explainedinsection3.
.tostudytheimpactofdifferentchangeset data granularity on the bert based models we created a separate datasetforeachtypeofgranularity i.e.
changesets changeset files and hunks.
to this end for changeset file and hunk granularity wedividethebug inducingchangesettofile orhunk levelcode changes such that one bug report creates multiple pairs with files or hunks from its respective inducing changeset.
we compare the performance of the proposed model with locus whichisanunsupervisedmodelthatutilizeshunk level 951fast changeset based bug localization with bert icse may pittsburgh pa usa table projects in evaluation dataset.
bugs changesets changeset files hunks aspectj jdt pde swt tomcat zxing granularity and the vsm to locate relevant changesets based on the maximum similarity score obtained between a bug report ahunk and a log message.
note that fbl bert does not use log messages as our goal is to explore mapping from natural language inabugreporttocodechanges.whilewellwrittenlogmessages canhaveapositiveimpactontheresultsbyboostingthescoresforsomechangesets notallrelevantcodechangesareaccompaniedby logsofgoodquality .asasecondsetofbaselines weemploytbertarchitecturesforsoftwareartifactsretrievalrecently proposedbylinetal.
.outofthethreearchitecturesinvestigated by lin et al.
we selected tbert single and tbert siamese asourbaselines rejectingtbert twin sinceitsperformancein terms of accuracy and time was significantly surpassed by the two others.ingeneral bothofthesearchitecturesarefairlysimilarto thosepresentedinfig.1withanexceptionofusingmoreadvanced embedding aggregation operators .
.
metrics toevaluatetheperformanceofthemodel weemployasetmetrics commonly used to evaluate performance of ir systems.
mean reciprocal rank mrr quantifies the ability of a model to locate the first relevant changeset to a bug report.
the metric is calculatedasanaverageofreciprocalranksacross bbugreports whileareciprocalrankforabugreport biisequaltoaninverted rank of the first relevant changeset in the ranking mrr b b summationdisplay.
i 1strank bi.
mean average precision mapmeasureshowwellamodelcan locate all changesets relevant to a bug report.
map is calculated asthemeanofaverageprecisionvalues avgp forbbugreports while average precision for a bug report bi avgp bi is computed based on the positions of all relevant changesets in the ranking map b b summationdisplay.
i avgp bi.
precision k p kevaluateshowmanyofthetop kchangeset inarankingarerelevanttoabugreport.thevalueofp kisequal tothenumberofrelevantchangesets relbi locatedinthetop k position in the ranking averaged across bbug reports p n b b summationdisplay.
i relbi k. .
experiment setup the experiments were conducted on a server with dual core .2ghz intel xeon and utilized nvidia tesla v100 with 32gbtable mean reciprocal rank mrr of changeset based bl techniques for different types of bug reports.
bug report type technique granularity blnl n 151blpl n 75blfl n 105blnl pl n 226all brs n locus hunks .
.
.
.
.
tbert changesets .
.
.
.
.
single change.
files .
.
.
.
.
hunks .
.
.
.
.
tbert changesets .
.
.
.
.
siamese change.
files .
.
.
.
.
hunks .
.
.
.
.
fbl bert changesets .
.
.
.
.
change.
files .
.
.
.
.
hunks .
.
.
.
.
rammemoryrunningoncudaversion10.
.toimplementour model weused pytorchv.
.
.
huggingfacelibraryv.
.
.
and faiss v. .
.
with gpu support.
since pre training is a computa tionally expensive task and requires a huge dataset we decidedto use an available pre trained bert model bertoverflow .
bertoverflowistrainedonstackoverflowdata henceitcontainsa mixture of code snippets and natural language descriptions which islogicalforthebuglocalizationtaskthatoperatesonbothcode andnaturallanguage.we finetunedourbertmodelandtbert baselinesfor4epochswithbatchesofsize16andalearningrate of 3e .
based on the average number of tokens in bug reports hunks changeset files and changesets across the evaluation projects we set the maximum length limit to and 512respectively.allinputdocumentsaretruncatedorpaddedto theirrespectivelengthlimit.forthefaissindex wesetthenumber of partitions to and retrieved a total of changesets forre ranking with fbl bert .
in the case of locus we set the modelparametersto 5and 2 .
indicatedbytheauthors to provide the highest performance.
results .
rq1 retrieval performance retrieval accuracy.
table contrasts the retrieval performance ofthefbl bertmodelagainstthebaselineapproachesforthree different typesof bug reports not localized partiallylocalized or fullylocalized.
ifa bugreport hasno mentionsof relevantclasses itisclassifiedasnotlocalized br nl whensomeoftherelevant classes appear in the report the bug is categorized as partially localized br pl andifallrelevantclassnamesareprovided the bug report is fully localized br fl .
note that in the case of fbl bert we use the results of the model trained with arc l encodingsince onaverage itprovidesthebestperformanceacross the evaluation projects as shown in section .
.
fbl bertoutperformslocusforbr nlandbrplby5.
and .
respectively while in the case of br fl locus surpasses our approach by .
.
given that locus relies on more direct term matching between a bug report and a changeset it makes intuitive sense that such a model performs best when localization hints are present in a bug report and struggles in their absence as indicatedbylowermrrvaluesforbr nlandbrpl .ontheotherhand 952icse may pittsburgh pa usa ciborowska and damevski fbl bert utilizes higher level association between bug reports andbug introducingchangesets whichcanresultinexactmatches getting less emphasis.
interestingly the highest improvement in re trieval accuracyis observed for br plindicating thatthe model can effectivelyretrievechangesetsbasedonpartialcluesbyassociating them with patterns learned from historical data.
theperformanceofbothtbertmodelsandfbl bertimproves when the models are trained and evaluated on hunks or changesetfiles.
compared to leveraging changesets across all bug reports fbl bertimprovesbetween23.
.
whiletheretrievalaccuracyoftbert singleandtbert siameseincreasesby16 and .
.
respectively.
while this results indicate that leveragingfinegraineddataaffectsretrievalperformancepositively itis important to note that the poor performance observed for changesetscanbepartiallyattributedtotheinputsizelimitofthebert model i.e.
tokens which is more often exceed by changesets than hunks or changeset files.
more specifically in our dataset truncation affects about of hunks and of changeset files compared to of changesets.
in general fbl bert outperforms tbert single and tbertsiameseby4.
and7.
respectivelyacrossalltypesofbugreports.
comparing the results of fbl bert trained on hunks to tbert modelstrainedonchangeset files giventhatchangeset filesprovide onaverage thebest performance fortbert models wenote varyingdifferenceinretrievalaccuracydependingonthebugreport type.
in the case of br nl fbl bert improves mrr score by only about over tbert models.
for br pl fbl bert improves by and .
over tbert single and tbert siamese while for brflthe improvement is equal to .
and .
respectively.
the larger gapin retrieval accuracy for br pland brflbetween fblbertandtbertmodelsindicatestheimportanceoftoken level embeddingmatching i.e.
whiletbertusesaggregatedembedding torepresentandcomparedocuments thetoken levelembedding matching performed by fbl bert allows this model to better recognizethekeycodenamespresentedinthebugreport which in turn translates to higher retrieval accuracy.retrieval time.
one of the key desirable characteristics of fblbert is to perform efficient retrieval across a large corpus.
this would allow it to leverage fine grained data such as changesetsfiles or hunks which were observed to provide the best retrievalaccuracy while maintaining reasonable retrieval delay.
in fig.
we compare the average retrieval time per bug report with respect to the increasing number of documents in the search space i.e.
changesets changesets filesandhunks.ingeneral fbl bertretrieves relevant documents faster than both tbert models with theretrievaltimegapincreasingasthesearchspacegrows.more specifically tbert single is the slowest model and requires about 50s to perform retrieval over a small number of documents e.g.
zxing andnearly1000s !
foralargeproject e.g.
jdt .tbertsiamese is significantly faster than tbert single and up to the searchspaceofabout15kdocuments itperformson pairwithfblbert.
however after that point retrieval time for tbert siamese rises steadily to reach about 70s for the largest search space while in the case of fbl bert the retrieval time is still just above 1s.
by comparingtheperformanceoffbl bertagainsttbertmodels it becomes evident that plain bert based models can quickly hit a retrievaldelaywallwhichmakesthemimpracticaltouse.onthe020k 40k 60k 80k 100k 120k 160k10 number of documentstime aspectj jdt pde swt tomcat zxing figure4 averageretrievaltimeperabugreportwithdifferentsizesofsearchspace tbert single tbert siamese fbl bert .
otherhand fbl bertscalesupwithrespecttothesearchspace size allowing to leverage fine grained data to increase retrieval accuracy without sacrificing model responsiveness.
note that the observed speed improvement is the result of both fbl bertandfaiss.morespecifically thetrainingobjectiveoffbl bert i.e.
finding most similar embedding vectors enables usingvectorsimilaritysearch e.g.
faiss .asaconsequence faiss can be used to retrieve the kbest candidates k n wherenis documents withsimilarword levelembeddingrepresentations that arethen re rankedby fbl bert.by re ranking only kdocuments thesearchspacebecomessignificantlyreduced hencedecreasing the retrieval time.
on the other hand typical bert based pipelines e.g.
tbert concatenate bug reports and changesets anduseneuralnetworklayerstoestimatearelevancyscore.this approachprecludespruningthesearchspaceviafaiss therefore duringretrievalabugreporthastobecomparedtoall ndocuments which in turn increases retrieval delay.
error analysis.
togainmoreinsightintofactorsthatnegatively affecttheretrievalaccuracyoffbl bert wemanuallyanalyzed the bug reports for which the model struggles the most.
morespecifically we selected all bug reports where the bug inducing hunk was ranked or worse by fbl bert.
this resulted in 20bug reports brnl brpl brfl that the authors independently analyzed contrasting the retrieved hunks to the true bug inducing hunks in order to devise a set of common issues causinglowretrievalaccuracy.theauthorsalsoexaminedthemost similarterms andtheirweights forboththeretrievedandgoldset hunks focusing specifically on the sources of largest differences between the two.
finally the authors discussed their independent observationsandagreedonthreecommonerrorcategories stack trace code snippets comments and code tokens splitting where a single bug report can belong to more than one error category.
we discuss each of these in turn.
in11outof20bugreports thedifficultytoretrievethecorrect hunk was caused by the presence of a code snippet or a stack trace in the bug report.
since code snippets and stack traces typicallyconsists of multiple class names or code tokens they have a potential to introduce noise through unrelated code names which in turn can lead the model astray .
for out of bug reports 953fast changeset based bug localization with bert icse may pittsburgh pa usa table retrieval performance for different configurations of fbl bert.
bugs mrr map p p p mrr map p p p mrr map p p p changesets d arc arcl aspectj .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jdt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pde .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
swt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tomcat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zxing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
all projects .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
changeset files d arc arcl aspectj .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jdt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pde .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
swt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tomcat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zxing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
all projects .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hunks d arc arcl aspectj .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jdt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pde .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
swt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tomcat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zxing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
all projects .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
we noted that the model was misguided by source code comments presentinthetop 1retrievedhunk.sincesourcecodecomments are formulated in natural language a highly contextual model like berttendstoemphasizetheirsimilaritywiththebugreportasitis also expressed in natural language.
for both of the above error categories webelieve thatthe wholesaleremoval ofthe problematic text i.e commentsfromcodeandcodesnippetsandstacktraces from bug reports would negatively affect the model as it removes bothrelevantandirrelevantinformation.hence researchersshould explorestrategiestotreatthisdataseparately perhapsbyencodingtheircontentwithinbertwithspecialtokensakintothearcand arclstrategies we discuss in this paper.
finally for5ofthebugreports fbl bertfailedduetospurious matches in code tokens that were split into sub tokens during preprocessing.oneofthepreviouslyobservedstrengthsofbert isinusingthewordpiecealgorithmtoavoidtheout of vocabularyproblembysplittingunseentokensintothelargestsub tokensthat arepartofthebertvocabulary .sincesourcecodeidentifier namesaretypicallyproject specificwords theydonotoccurinthe pre trained vocabulary hence they are often split by wordpiece e.g.
managerservlet manager servlet .thesub tokenscan thenspuriouslymatchotherterms includingsub tokensfromother split identifiers but not the whole unsplit term.
researchers in the biomedical domain recognized the same issue affecting medical terms and proposed domain specific bert adaptations .
.
rq2 changeset encoding strategy table3showsretrievalperformanceoffbl berttrainedandevaluatedwithdifferentchangesetencodingstrategiesandinputdatagranularities.
for eachproject the three bestperforming configurationsarehighlighted suchthatdarkgreenmarksaconfiguration with the highest retrieval performance while green and yellow correspond to the second and third best configurations.
overall wenoticethatusingentirechangesetsasthegranularityofinput results in by far the worst performance across all of the investigatedconfigurationsforallevaluationprojects.wecanattribute this result to truncation of changesets due to input length limitation of the bert model and tangled changes within a single changeset which are likely to affect the model by introducing noiseviaunrelatedcodemodifications.ontheotherhand while the model based on hunks or changeset files is not free of these problems the finer data granularity allows it to partially overcome them.
for instance in case of tangled changes dividing the entirechangesetintohunksorchangeset filescreatesmultiplenew data points which limits the noise introduced by instances that arepoorlyrelatedtothebug.thedifferenceinretrievalaccuracy acrossall themetricsbetweenusing hunksandchangeset filesas the input data is minor and differs from to .
per project.
thisresultsisindicativeof theobservationthatleveraginghunks andchangeset filesperform similarlyandareboth resilienttothe problems affecting changesets.
examining the results for different changest encoding strategies weobservethatarc lperformsuniversallybestacrosshunks andchangeset files.interestingly atthelevelofchangeset files the baselineencodingd whichdoesnotencodemodificationtype doessurprisinglywellandoutperformsarcencoding.weattributethisresulttothespecificsof arcencoding whichgroupslinesbasedon the performed modification hence in the case of larger documents the grouping may affect the semantics of the documents.
on the 954icse may pittsburgh pa usa ciborowska and damevski other hand arc encoding for hunks is less likely to be susceptible to that problem since hunks are typically much shorter.
analyzing theresultsfordifferentprojects weobservethatarc lperforms best for aspectj jdt and tomcat with an improvement in mrr scores of .
.
and .
over their second best configurations respectively while arc is the most beneficial strategy for the pde project.
in the case of swt we observe the highest retrieval accuracy with arc l while zxing performs best with d encoding however both of these observations are likely negligible given the lowdifferencebetweenarc landotherencodingsforswt and therelativelyfewerbugreportsinthezxingproject.overall we conclude thatleveraging changesets semantics via encodingmodification with either arc and arc lincreases retrieval accuracy overthedconfigurationwhichdoesnotprovidethemodelwith additional information about the change.
however based on these results the difference between arc and arc lis not significant enough to clearly indicate which strategy is superior on average.
.
threats to validity theconclusionsofthispapersufferfromseveralthreatstovalidity.
a key threat to the internal validity of our study are the specific parameterchoicesweusedtobuildourfbl bertmodel.amitigating factor is that all parameters were either studied by us or were reported in other prior reputable papers as recommended oroptimal .anotherthreatisourautomatedseparationof bug reports based on localization hints into not localized partially localized and fully localized which may result in mistaken categorization even though we used a well known and frequently followed procedure .
leveraging changesets for bug localization poses another threat due to possible noise that can be introduced by szz which could result in poor quality mapping between bug reports and bug inducing changesets.
however the dataset was validated manually andthereforesuchmistakes iftheystillexist should notsignificantlyaffectourconclusions.errorsduetotangledchanges are still possible in the dataset as such changes are difficult to remove manually.
we believe tangled commitsto have affected our final presented results as discussed in rq2 however sincetangledcommitsareapartofsoftwaredevelopmentremoving them completely may arguably result in unrealistic evaluation.
athreattoexternalvalidity whichconcernstheabilitytogeneralize our evaluation results is that we applied the bug localization technique only on a limited number of bugs collected from a se lection of popular open source java projects.
a mitigating factoris that the projects have a variety of purposes and developmentstylesandthebenchmarkweusedhasalsobeenappliedtoprior changeset basedbuglocalizationstudies .anotherthreat to external validity is in the chosen evaluation metrics which may not directly gauge user satisfaction with our bug localization technique impactingthevalidityofthereportedresults.thethreat is mitigated by the fact that the selected metrics are well known and widely accepted as best available to measure and compare the performance of ir techniques.
related work bug localization has generated significant research interest overthe years.
in this section first we survey related code elementbasedbuglocalizationtechniques followedbyapproachestowards bug inducingchangesetretrieval.finally wereviewmethodsfor encoding changesets characteristics.
.
code element based bug localization buglocalizationtechniquespredominantlyutilizeinformationretrieval where the bug report text is used to formulate a query that is matched to a corpus of code elements i.e.
classes or methods.
to compute similarity between bug reports and source code the vector space model vsm is often used as one of the simplest and effective information retrieval algorithms which is leveraged bymany bug localization techniques.
for instance buglocator combines two rankings one produced by similarity between the bugreportandcodeelementsandanotherbasedonsimilarityofthebugreporttopriorfixedbugreports.bluir usescodeandbug report structure to create groups of terms and computes similarity betweendifferentgroupsseparately whileamalgam creates anensembleconsistingofbuglocator bluirandadefectpredictor leveraging the development history of a project.
brtracer focusesonanalyzingandprioritizingstacktraceswhentheyareincluded in bug reports.
kochhar et al.
were among the first toreport that evaluation of bug localization was biased by explicitlocalization hints in a significant subset of the included bug re ports .
vsm based techniques are likely to perform well on suchbugreports thoughlocalizingthemmaynotbeasusefulto developers .
mills et al.
refute the idea that vsm based bug localizationaresignificantlyaidedbyhints andnotethatvsmcan performwellforbuglocalizationifmoreattentionispaidtohow the query is constructed from the bug report text .
however theirfindingsdonotprecludeadditionalaccuracyimprovements by using more complex semantic models such as bert.
morerecently softwareengineeringresearchershavebeeninterestedintheapplyingdeeplearningtechniquestowardsbuglocalization.forinstance tranp cnn isarecenttechniquethat combines cross project transfer learning and convolutional neuralnetworkstoachievestate of the artperformanceonfile level bug localization.
cooba improves on tranp cnn by combining a sharedencodertocapturecross projectwithper projectfeatures and using adversarial training to ensure that the per project information remains unaffected by noise .
lam et al.
s technique dnnloc combines a deep neural network with the vsm in order to be effective across different types of similarity .
while we also leverage a deep learning model bert is significantly different fromthesepriortechniques.recentworkinbuglocalizationalso includes reports on the value of retrieving changesets instead of source code elements .
.
changeset based bug localization theearliestworkonchangeset basedbuglocalizationislocus whichisbasedonvsmmatchingofbugreportstohunks.toadjust for localization hints locus adapts its similarity scores based on theproportionofcodeelementmentionsinabugreport.bhagwan etal.
introducedorca atoolthatusesaprovenancegraphto 955fast changeset based bug localization with bert icse may pittsburgh pa usa identify commits leading to faulty builds.
changelocator uses historical data on software crashes to build a model identifying relevantchangesetsbasedoncollectiononcrashreports.although this approach allows to retrieve changesets it requires sufficient amount of historical data to train the model and a stack trace as aninput.oneofthebenefitsofvsmisthatitisanunsupervised approach henceatrainingcorpusofbugreportsandtheirinducing commits is not required.
however as vsm fundamentally requires at least partial token overlap while it ignores the context in which tokensappearindocuments allbuglocalizationtechniquebased on it have a limited accuracy ceiling .
recently researchers have also shifted their attention to deep learningmodelsforchangeset basedlocalization.forinstance muralietal.
proposedbug2commit anunsupervisedmodelleveraging multiple dimension of data associated with bug reports and commits suchas metrics stacktracesor commitmetadata.they observedthatusingembeddingscanleadtoimprovementinmodelaccuracy when compared to bm25.
lin et al.
studied the tradeoffsbetweendifferentbertarchitecturesforthepurposeofchangeset retrieval and observed the accuracy of siamese architecture is on pair with single bert architecture while being significantly faster.
however thespeedand interactivity of these models is not on par with the bert technique described in this paper.
.
changeset representation building a semantically rich representations of changesets is rel evant to other software engineering applications beyond bug localization i.e.
just in time defect prediction recommendation of a code reviewer for a patch tangled change prediction.
approaches that define novel changeset embeddings vector representations ofchangeset including cc2vec and commit2vec leverage the difference between added and removed lines of code among otherchangesetcharacteristics.corleyetal.
studiedhowincludingdifferenttypesoflinesfromachangesetaffectstheperformance oflatentdirichletallocation basedfeaturelocation observingthat including context additions and log messages but excluding removedlines achievesthebestperformance.however thesestudies didnotutilizeatransferlearningtechnique likebert whichrequires compatibility with a pre trained model and also prior work did not extensively explore hunks as a primary data dimension.
conclusion thispaperpresentsanapproachforautomaticallyretrievingbuginducingchangesetsforanewlyreportedbug.theapproachuses thepopularbertmodeltomoreaccuratelymatchthesemantics in the bug report text to the inducing changeset.
more specifically we describe the fbl bert model based on the prior work by khattabetal.
whichspeed suptheretrievalofresultswhile performing fine grained matching across all embeddings in the two documents.
the results show an improvement in retrieval accuracyforbugreportsthatlacklocalizationhintsorhaveonly partial hints.
we also evaluate different approaches for utilizing changesetsinbert likemodels producingrecommendationson the input data granularity and the use of special tokens for the purpose of capturing changeset semantics.