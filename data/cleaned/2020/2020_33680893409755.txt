makingsymbolic execution promisingbylearningaggressive state pruning strategy sooyoung cha korea university republicof korea sooyoungcha korea.ac.krhakjoooh korea university republicof korea hakjoo oh korea.ac.kr abstract we present homi a new technique to enhance symbolic execution bymaintainingonlyasmallnumberofpromisingstates.inpractice symbolic executiontypicallymaintainsasmany statesaspossible in a fear of losing important states.
in this paper however we show that only a tiny subset of the states plays a significant role inincreasingcodecoverageorreachingbugpoints.basedonthis observation homiaimstominimizethetotalnumberofstateswhile keepingpromising statesduringsymbolicexecution.weidentify promisingstatesbyalearningalgorithmthatcontinuouslyupdates theprobabilisticpruningstrategybasedondataaccumulatedduring thetestingprocess.experimentalresultsshowthat homigreatly increases code coverage and the ability to find bugs of klee on open sourcec programs.
ccs concepts software and its engineering software testing and debugging.
keywords dynamicsymbolic execution onlinelearning acmreference format sooyoungchaandhakjoooh.
.makingsymbolicexecutionpromising bylearningaggressivestate pruningstrategy.in proceedingsofthe28th acmjointeuropeansoftwareengineeringconferenceandsymposiumonthe foundationsofsoftwareengineering esec fse november8 13 virtualevent usa.
acm newyork ny usa 12pages.
.
introduction symbolic execution is an effective software testing method to increase code coverage and find subtle bugs.
the key idea of this method is to systematically explore program s diverse pathsbysubstitutingprograminputswithsymboliconestoexecute theprogramsymbolically.atahigh level symbolicexecutioniteratively selects executes and forks a state while maintaining a set of corresponding author permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 8 13 virtual event usa associationfor computing machinery.
acm isbn ... .
during itstesting process.
in particular it forksthe state into one or two separate states according to the feasibility of branch conditionsencountered during the symbolicexecution.
thanks to the systematicprocess symbolicexecutionhasbeen actively used in a variety of applications operation systems smartphone apps neuralnetworks andsmart contracts .
however performing symbolic execution on real world programsinevitablyfacestheinfamousstate explosionproblemthat exponentiallyincreasesthenumberofstatestobemaintained leading to significant increases of memory usage.
hence in practice symbolicexecutor e.g.
klee takesasinputthememorybudget topreventunexpectedmemoryusage andmaintainsasmanystates aspossible withinthememorybudgettoreducetheriskoflosing importantstatesduringtesting.thisreasonablebehaviorcauses the symbolic executor to suffer from two practical problems.
first preservingasmanystates aspossible increases thetotalnumber ofcandidate states whichmakesit difficultforsymbolic executor to decide proper states in a sense of increasing code coverage or finding bugs.
second since the accumulated states easily exceed agivenmemorybudget numerousstatesarerandomlyprunedto reducethememoryusage.aswedemonstrateinsection when performingklee with thedefault memory budget 2gb on c open source programs the number of states to maintain is tens of thousands and the number of blindly pruned states ranges from tensofthousandsto hundredsof thousandsonaverage.
to resolve this state explosion problem we aim to minimize the totalnumberofstatesbuttokeep promising statesduringsymbolic execution.ofthepreservedstates weobservedthatthereexista very few promising states to effectively increase the code coverage or to reach the bug points thus symbolic execution becomes more effectiveandefficientifweonlymaintainthosesmallnumberof promisingstatesinasenseofresolvingthestate explosionproblem.
toachieveourgoal thetechnicalchallengesweneedtoaddressare toestimatehowpromisingeachstateisand todeterminehow manystates we needto prune.although diverse approachesexist withthegoalofreducingthesearchspaceofsymbolicexecution their goals are not to maintain a small number of promisingstates theexistingapproachesaimtoidentifyandprune onlytheredundantstatesthatmeetthepredefinedcriteriafromthe totalones.forinstance post conditionedsymbolicexecution is to prune only the states having the same path suffixes as previously explored states and jaffar et al .
discard program pathsguaranteedto be unreachable to bugpoints.
we presenta newtechniqueto adaptivelymaintainonly asmall number of promising states during symbolic execution via online learning.
to achieve our goal we introduce two key ideas aprobabilistic pruning strategy and a learning algorithm.
first we esec fse november8 13 virtualevent usa sooyoungchaandhakjoo oh define the probabilistic pruning strategy that contains both continuousanddiscreteprobabilitydistributions.weusetheformer distribution to score how promising each state is and the latter one todecide how manystates arepruned.
that is we reduce the problemofsolvingthetwotechnicalchallengesintotheproblem oflearningbothprobabilisticdistributions.second wepresenta learning algorithm that continuously updates the two probabilistic distributions online based on data accumulated during symbolic execution.
experimental results show that symbolic execution with our techniquesignificantly improvesbranchcoveragewhilemaintaining a relatively small number of states compared to the general symbolic execution on open source c programs.
we implemented our technique in a tool homi on top of klee and evaluated it on c programs 61kloc .
symbolic execution with homisucceedsin coveringmore branchesandfindingmore realbugsthan conventional symbolic execution on benchmarks.
forinstance our technique is able to generate the bug triggering inputs that causeabnormalterminationandsegmentationfaultin grep .
andcombine .
.
respectively while conventional symbolic executionfailedto doso.
contributions .our contributionsare as follows we present a new technique to maintain only promising states by continuously learning the probabilistic pruning strategyonlineduringsymbolic execution.
wedemonstratethe effectivenessof homion9open source cprogramsbycomparingsymbolicexecutionwithvs.withouthomi.
we make our tool homi anddata publiclyavailable.
preliminaries inthissection wedescribeageneralalgorithmandlimitationof symbolicexecution andexplainourobservationtopresentthegoal ofthis paper.
symbolicexecution.
themainideaofsymbolicexecution istosystematicallyexploreprogram sdiversepathsbyreplacing program inputs with symbolic ones to execute a program symbolically.
algorithm 1presents a generic algorithm for symbolic execution except for a few change line that stems from our main approach.
generally symbolic execution maintains a set s of program states until the time budget expires where a single state consists of a tuple instr store .
each element of a tuple respectivelydenotesthenextinstructiontobeexecuted instr a symbolic store store which maps the program variables into symbolic values and a path condition which is a conjunction of branchconditionsevaluatedsymbolicallyinthestate.thesymbolic executiongeneratestest casesbyiterativelyselecting executing andupdatingthe states in sduringits testingprocess.
therunprocedureinalgorithm 1takesaprogram pundertest and the time budget n as input and returns a set tof test cases generatedwithinthetimebudget.atline2 thealgorithminitializes a setsas an initial state instr0 store0 true whereinstr0is the veryfirstinstructionexecutedintheprogram p store0istheinitial 1homi symbolic execution input program p time budget n andthe probabilisticdata p .
output asetoftest cases t procedure run p n p s instr0 store0 true initial states t initial test cases repeat sp prunem s m r mis memory and ris ratio.
sp sp prune s p t prune states s s sp for each spdo generatetest cases t t model instr store select s choose astate s s instr store instr store execute instr store ifinstr if theninstr1elseinstr2 then ifsat thens s instr1 store ifsat thens s instr2 store else ifinstr haltthen t t model generatetest cases untilbudgetnexpires or s for each sdo generatetest cases t t model returnt mapping information and is set totrue.
for instance suppose that there existsasmall program pundertest as follows void main int x int y if x printf good if x y assert bad withthisprogramasaninput instr0issettothefirstinstructionof theprogram if x store0is and istrue.at line the algorithm initializes a set tof test cases to an empty set.
at line the prunemfunction decides a set of states to be pruned thatis itrandomlyselectsasubset spofswiththesizeof s r e.g.
r .
when s thesizeof s exceedsthegivenmemorycapacity m. otherwise it returns an empty set i.e.
sp .
at line the algorithmupdatestheset swiththedifferencesetbetween sand sp.foreverystateinthepruningset sp thealgorithmgenerates atest case twhichisamodelofthepath condition inthestate instr store at line8 .
after the test case generation the selectfunction namely a searchheuristic choosesasinglestate instr store from the set sbased on its own selection criteria line .
with the selected state the executefunction executes the instruction instr and returns the updated state instr store .
at line if the instruction instr is an if else statement algorithm 1first checks the feasibility of the path conditions corresponding to both two branches.
ifthe path conditionof the ifstatement is satisfiable thealgorithmaddsthenewstate instr1 store intothe set s line .
likewise the algorithm adds the newstate intotheset sifthepath conditionoftheelsestatementissatisfiable line .
when both sides of the branch are feasible the algorithm forks the single state into two states where this forking process causesthestate explosionprobleminsymbolicexecution.onthe otherhand if instr isthehaltstatement thealgorithmgenerates 148makingsymbolicexecutionpromising by learning aggressive state pruning strategy esec fse november8 13 virtualevent usa table the number of states and pruned states on c opensource programs time budget 5h memory budget 2gb gawk grep vdir ginstall trueprint states s 37k 41k 43k 60k 49k pruned states 34k 112k 115k 587k 155k atest case tandaddsapairofpath conditionandatest case t to the set tof test cases.
for simplicity we have omitted the caseswhen instr istheotherinstructionssuch asload store and call instructions.
algorithm 1repeats this process until the time budgetnexpires or the set sbecomes an empty set.
when the loop ends at line the algorithm generates test cases using the path conditions of all remaining states in the set s which have not yet reached the halt statement.
lastly the algorithm returns the generatedtest cases tas an output.
limitation.
the general symbolicexecution attempts to maintainasmanystatesaspossiblewithinthememorybudgettoreduce the loss of critical states during testing.
this behavior however significantlydegradestheperformanceofsymbolicexecutionapplied to real world programs as the number of states in both sand spgrows.
the greater the number of states in the set s the harder it is for the selectfunction tochoose meaningful states whichare likelytoincreasethecodecoverageortoreachthebuggylocations.
furthermore theincreasesinthesizeofthesetofprunedstates sp mayleadtothelossofpromisingstatesastheyareforciblypruned from the set sofcandidate states.
table1shows the average number of candidate states s and pruned total states when performing klee a popular symbolic execution tool on open source c programs for hours with the defaultmemorycapacity 2gb.overall thesizeofthecandidateset is tens of thousands and the number of pruned states ranges from tens of thousands to hundreds of thousands.
for instance when performingkleeongrep the selectfunctionshouldchooseastate from about candidate states on average for each iteration at line the prunemfunction blindly prunes about112 states duetoexceedingthememorybudgeteventhoughthepromising states mayexist among the prunedones.
goal.thegoalofthispaperistomaintainonlypromisingstates via aggressive state pruning during symbolic execution.
in our work we define the promising states as having the potential to effectively increase branch coverage when they are further explored and observethat thereare a veryfew promisingstates amongthe total candidate states.
hence if we succeed in performing symbolic execution while keeping them only we are able to maximize code coverage and to find many bugs.
that is the prunefunction in algorithm 1enables the symbolic execution to maintain the minimized set sof candidate states while preserving the promisingstates andpreventssituationswherethecandidatestatesare blindlyprunedduetomemoryoverrun.toachievethisgoal the technical challenges we mustaddressare as follows howpromisingeachstate is?
howmanystates dowe needto prune?inthispaper weaddressthechallengesviatheprobabilisticpruning strategylearnedonlineduringsymbolic execution.
ourtechnique in this section we describe our technique homi in detail.
section3.1defines the probabilistic pruning strategy prune used in algorithm .
section .2describes our symbolic execution algorithm algorithm withtheonlinelearningtechniqueforthe probabilisticpruning strategy.
.
probabilisticpruning strategy the pruning function prune in algorithm 1decides the set sp of pruned states based on the probabilistic data p. this function takesasinputtheset sofallstates theprobabilisticdata p andthe timecycle t.forevery tseconds thepruningstrategyselectsthe setspof unpromising states in two steps sampling and pruning.
in the experiments we set the hyper parameter tto seconds basedonourobservationthattheshortpruningcycle e.g.
is generally more effective than the large one e.g.
when testing real world benchmarks.
sampling.
the first step sampling is to obtain the two important values from the probabilistic data p wherepconsists of a tuple f pstgy pratio .fdenotes a set of nfeatures to represent each state in the set sas ann dimensional boolean vector.
pstgy is the distribution of an n dimensional vector to calculate how promising eachstate is and pratiois thedistribution of theratio r todecidethenumberofstatestobe pruned.for simplicity weassumethattheparametervector andratio raregivenbysampling stepfromthelearneddistribution pstgyandpratio respectively.we explainhowwe obtain thesetwovaluesinsection .
.3and3.
.
.
pruning.
the second step is to select the states to be pruned by using the two sampled values andr and the set fof features inp.
we define the probabilistic pruning strategy as the following prunefunction prune s p t argmin sp s sp s r summationdisplay.
s spscore s if f nequal otherwise wherethefunctionreturnsanemptysetwhentheset fisempty.
if not the function scores each state in the set s and returns the setspofthekstates withthe lowestscores in s e.g.
k s r .
to estimate how promising a state is we first transform each stateintoafeaturevector.eachfeaturedenotesabooleanpredicate that checks whether the path condition of the state scontains a specific branch condition .
for instance a feature describes whether the path condition of the state sinvolves the branch condition.
if true the feature feat s is otherwise it is .
formally the i th feature isdefinedas feati s braceleftbigg1 if i s otherwise where it takes a single state sas input and returns or .
using the setfofnfeaturesinthegivenprobabilisticdata p wecanconvert astate intoan n dimensional boolean vector as follows feat s feat1 s feat2 s ... featn s .
149esec fse november8 13 virtualevent usa sooyoungchaandhakjoo oh algorithm2 our approach input aprogram p time budget n output thesetoftest cases t procedure homi p n t d initialize twosample spaces stimeandsratio n sample fromu stime p u n u sratio repeat t run p n p for each t t do d d t b b branches t goodd extract d newf fgenenator goodd pstgy pratio n pgenerator goodd newf p newf pstgy pratio t t t untilbudgetnexpires returnt thefeaturesareautomaticallygeneratedonlinebythedataaccumulatedduringsymbolicexecution.weexplainhowthesefeatures are obtainedinsection .
.
.
aftertransformingeachstateintheset sintoafeaturevector wecalculatethescoreofeachstate susingtheinnerproductofthe feature vector feat s andthe sampled n dimensional vector as score s feat s .
for example when nis andfeat s can be .
.
.
and respectively where the output of score s is .
.
the featurevector denotesthatthepath conditionofthestate s only contains the branch condition corresponding to 1st feature.
in the vector .
.
.
thei th value represents the importanceofthe i th feature.
finally the prunefunctionreturnstheset spofthe s rstates with the lowest scores in the set sas output where the pruning ratioris obtained from the learned distribution pratio.
we remark that the selection for the set spcan be done efficiently after calculating the score of each state in the total set s it ranks the states accordingtotheirscores andthenpicksthebottom kstates where kis s r. note that we reduce the problem of solving the two technical problems discussed in section 2into the problem of learning probabilisticdistributions pstgyandpratio.
.2homi the key point of our approach algorithm is to continuously update the features and the two probabilistic distributions pstgyand pratio via online learning during symbolic execution.
except for theprobabilisticdata p theinputandoutputofouralgorithmare thesameastheonesofalgorithm .unlikethealgorithm 1which performsthe runprocedureonlyoncewithinthetimebudget n ouralgorithmperformsthe runprocedurentimesbydividing n intonsmallerbudgets n .thisisbecauseouralgorithmterminates numerousstatesearlythroughtheprobabilisticpruningstrategy in therunprocedure thereby our algorithm performs the run proceduremultipletimeswiththeupdateddata ptorecovertheterminated promising states.
note that we can also perform algorithm1in the same way.
however without our state pruning strategy weexperimentallyobservedthatitusuallyperformsbetter to run algorithm 1once for a long time period than to do multiple times for ashort time period section .
.
weexplaintheworkflowofhowalgorithm 2worksindetail.at line2 algorithm 2initializestheset toftest casesandaccumulated datadtoanemptyset respectively.atline3 thealgorithminitializes two sample spaces stimeandsratio the former stimedenotes the sample space for the time budget n to run the runprocedure atline7 andthelatter sratio representsthesamplespaceforthe pruning ratio used in the probabilistic pruning strategy prune.
stimeandsratioare definedas stime sratio where the two hyperparameters maxand itv are to define the discrete space of itvequal intervals with maxas the maximum timebudget.likewise thediscretespace sratioisdefinedequallyby thetwohyperparameters maxand itv.forinstance if stimeand sratioare and their discrete spacesare as follows stime sratio in the experiments for the space stime we set maxand itvto seconds and .
we respectively set maxand itvto .
and for thespace sratio thatis ourstrategyprunes20 or40 or60 of totalstates for aggressive state pruning.
at line the algorithm samples the initial time budget n from the uniform distribution u stime .
it initializes the probabilistic datapconsisting of a triplet f pstgy pratio at line initially thesetfoffeaturesisanemptysetandeachoftwoprobabilistic distributions pstgyandpratio is a uniform distribution.
as the setfinpissettoanemptyset thealgorithmperformsthe run procedure without any state pruning on the first iteration of the loopatlines6 .ouralgorithmrepeatsthefollowingtwomain processes until the time budget nexpires performing symbolic executionwiththeprobabilisticpruningstrategybasedonthedata p line7 and2 updatingthedata p line13 .onthefirstiteration of the loop homiperforms the runprocedure algorithm with the time budget n and initial probabilistic data p and returns the sett of generated test cases at line .
after the algorithm calculatestheset bofbranchescoveredbyeachtest case tinthe sett we accumulate the tuple t b inthe set d line8 .
.
.
collecting promising data.
at line we run the extract functiontoextractthemost promising butminimalsetofdata goodd fromtheset dofaccumulateddata.conceptually theset gooddis the smallest subset of dwhere the unions of bingoodd is the same with the set of branches covered by all the test cases in d.toformally definethe set goodd we firstcalculate d as d argmax d d uniondisplay.
b d b .
where the notation argmax returns the set d of all arguments thatmaximizetheobjective.theset d isthesetofallsubsetsof dwhichcollectivelymaximizethesetofcoveredbranches.then the setgooddisdefinedas goodd argmin d d d 150makingsymbolicexecutionpromising by learning aggressive state pruning strategy esec fse november8 13 virtualevent usa where the notation argmin returns one of the arguments that minimizetheobjective.inpractice calculatingtheset gooddcorresponds to solving the set cover problem the well known np complete problem.
in this paper we obtain the minimal set gooddby applying the greedy algorithm which iteratively selects theelementhavingthelargestnumberofuncoveredbranchesat eachstage.
.
.
generatingfeatures.
atline11 homigenerates nfeatures to transform each state into a feature vector in the probabilistic pruning function prune.
intuitively a feature is a core branch conditionthatcontributestodeterminingthevalueofatestcase thateffectivelyincreasesbranchcoverage.togeneratethefeatures weusethecorebranchconditionsinthepath condition correspondingtoeachpromisingtest caseintheset goodd.wedefine acorebranchcondition asaconditionthatcanbeexpressedin the predefinedlanguage las follows cond cond cond cond cond cond lv n lv wherethelanguageissmallyetsufficienttorepresenttheminimum branchconditionsthatarenecessarytodirectlydeterminethevalue of each test case.
an l value lv denotes a symbolic value or the value of i th index of an array .
a condition cond consistsofabooleanconditiontoexpressthatthel valueequalsto aconstantvalue n. acore branchcondition isasingle condor a conjunction disjunction of cond.
to generate a set of features from the promising data goodd we first collect the set pcof all path conditionsfrom gooddas follows pc goodd second we collect the set newfof new features by extracting core branchconditions inthe set pcas newf l pc that is we extract only the conditions that can be expressed in thelanguage lamongthebranchconditionsofeach intheset pc.
for instance suppose that the set pccontains two sets of pathconditions as follows pc nequal3 .
where the two branch conditions and inpc canbeexpressedinthelanguage l e.g.
lv n whiletheremaining conditions and nequal3 cannotbe.hence wecandefine the setnewfoffeatures from pcas follows newf where the two features in the set newfare the minimal conditions todetermineamodelofeachpath condition forinstance themodel of the first path condition inpcis equals to theoneoftheminimalcondition .inshort theset newf of generated features at line represents the key evidences of the minimal test cases that contribute to maximizing branch coverage until the currentstate.
.
.
learningdistribution.
atline12 thefunction pgenerator learns the probabilities of two values n dimensional weight vector andthepruningratio r andreturnsatuple pstgy pratio n .the first elementpstgydenotes the probability for the weight vector thatscoreshowpromisingeachstateis andthesecond pratio is the probability for the pruning ratio rthat determines the number ofstatestobepruned.thetimebudget n isthenewlyallocated timebudgetforthe runprocedureonthenextiterationoftheloop.
the probabilitypstgyconsists of ndistributionsas pstgy p1 p2 p n. wherepidenotestheprobabilityoftheweightvalue icorrespondingtothe i thfeatureintheset newf.todefinethe i thdistributionpi we first collect the set of promising test cases goodtfrom gooddas follows goodt t t goodd .
whenever each test case tis generated during symbolic execution ouralgorithmadditionallymaintainsaquadrupleofinformation usedto generateeachtest case tas follows t f r n wherefisthesetoffeatures istheweightvector risthepruning ratio and n is the time budget.
using this additional information we collect the set goodfof the features which are used at least once when generating the promising test case in the set goodtas follows goodf uniondisplay.
f goodtf.
that is the set goodfcontains the features that contribute to generatingeffectivetest casesintermsofcodecoverage.finally we can definethe i th distributionpias pi braceleftbiggn wi wi if new i goodf u otherwise where new idenotes the i th feature in the set newfthat hasbeen generatedat line11 inalgorithm .
ifthei thnewfeature new i belongstotheset goodfofpromising features we learn the probability piwhich is the truncated normal distribution with median wi standard deviation wi minimum value and maximum value .
we define the set wi as wi k new i k 1 n goodt .
intuitively widenotesthesetofweightvaluescorrespondingto thei th new feature new ithat has already been used for each testcase ingoodt.
given the set w the median wi and standard deviation wi are calculatedas w summationdisplay.
w ww w w radicaltp radicalvertex radicalbt summationtext.
w w w w w .
ontheotherhand ifthe i thnewfeature new i doesnotbelongto goodf we fix the probability pito a uniform distribution between and since there is no accumulated data corresponding to the i th new feature for learning.
in this way we learn the probability pstgythat consists of the ndistributions from p1topnbased on the mostpromisingdata goodd.
151esec fse november8 13 virtualevent usa sooyoungchaandhakjoo oh after the learning process of the probability pstgy we calculate theprobabilitypratioofthe givenpruning ratio r which is one of the valuesinthe predefineddiscrete space sratioas follows pratio x r r goodt r r goodt theintuitionisthatthemorethepruningratio risusedtogenerate promising test cases goodt the higher the probability of the ratio.
lastly we sample the new budget n based on the following probabilityptime ptime x n n goodt n n goodt the intuitionisthe same as the probability pratioabove.
.
.
sampling values.
we describe how to sample the weight vector and ratio r from the two learned distributions pstgy andpratio inthefirst sampling stepoftheprobabilisticpruning strategy.first wesampletheweightvector byusingoneofthe three sampling methods exploitation reverse exploitation and exploration.
the first two methods are to exploit the learned distributionpstgyas it is or reversely.
the last method is to explore purelyrandom weightvector.
exploitation.
we sample the new weight vector from the learneddistribution pstgyitself as sampleexploit p1 p2 p n 1 2 n where the i th weight value iis sampled from the i th probability piin .ourexpectationisthatthenewweightvector statistically similar to the promising weight vectors in the set gooddwill likely increasethe codecoverageonthe nextiterationoftheloop.
reverseexploitation.
wesamplethenewweightvector rby exploitingthelearneddistribution pstgyreversely.wefirstgenerate thesetof100real numbers u bysamplingtheuniformdistribution between and1as u r1 r2 ... r100 ri u .
the samplingmethodtakestheprobability pstgyandtheset uas inputandreturns the newweightvector ras samplereverse p1 p2 p n u 1 r 2 r n r we assume that the i th weight value iis sampled from the probabilitypidefined in .
then the i th reverse weight value iris calculatedas follows i r argmax u u u i where thereverse value irinuis thefarthest one fromthevalue isampledfromthedistribution pi.hence irrepresentsthevalue thatis themostunlikelytobe sampledinthelearneddistribution pstgy.weexpectthatthisweightvectorwouldleadthesymbolic execution to explore the branches uncovered in previous iterations.table benchmarkprograms programs loc ofbranches gawk .
.
grep .
combine .
.
trueprint .
ginstall .
ptx .
vdir .
pr .
dd .
exploration.
for the last method we generate a weight vector by sampling from the uniform distribution u n where thei th value iis a random real number between and .
in the experiments toaccumulateenoughdata dforlearningthedistributionpstgy algorithm 2repeatstheloop usingonlytheexploration methodmtimes e.g.
m .
after enough data is collected we set the same probabilitiesfor the three sampling methods.
finally wesamplethepruningratio rbasedontheprobability in whensamplingtheweightvector byexploitationorreverse exploitation.
otherwise when sampling the weight vector by exploration the pruning ratio is randomly sampled in the uniform distributionu sratio .
likewise we obtain the next testing budget n on the same basis as sampling the pruning ratio that is we samplethebudgetfromtheuniformdistribution u stime forthe explorationcaseonly.if not we sample the one in .
astheloopatlines6 15inalgorithm 2iterates ourtechnique homi isableto makesmarter decisions on howto representeach state f how promising each state is pstgy and how many states are pruned pratio .
experiments in this section we experimentally evaluate our approach homi to answer the following research questions effectiveness how effectively does homiimprove branch coverage?
how many branches and bugs are reachable by homionly?
section .
the number ofstates how many states does homimaintainduringtestingcomparedtogeneralsymbolicexecution?
section4.
comparisonwith naive approach how well does homi algorithm performcomparedtosymbolicexecutionwith random state pruning?
section .
we implemented our approach in a tool homi on top of klee apubliclyavailablesymbolicexecutiontoolfortestingcprograms.
weconductedallexperimentsonalinuxmachineequippedwith twointelxeonprocessorse5 2630and192gbram whereithasa totalof16 cores and32 threads.
.
settings benchmarks.
we used gnu open source c programs for evaluation.table 2showsthetotalnumberoflinesandbranchesfor eachbenchmark wherethelargestbenchmark gawk hasabout12k 152makingsymbolicexecutionpromising by learning aggressive state pruning strategy esec fse november8 13 virtualevent usa branches.thelastfivebenchmarksintable 2areamongthelarger programsingnucoreutils .
.toconstructourbenchmarksuite we usedtwocriteria the benchmarkshave been widely usedin priorworkondynamicsymbolicexecution and2 theyarerelativelylargerandmorechallengingthanthose often usedinexisting work onklee.
baselines.
we compared our approach with the general symbolic execution algorithm without state pruning but with different search heuristics.
specifically we used the following searchheuristics cpicount callpathinstructioncount covnew mindistance minimal distance to uncovered instrcount instruction count querycost randompath depth randomstate androundrobin the last heuristic is the default heuristic of klee thatuses covnewandrandompath inaroundrobinfashion.all theseheuristicsareimplementedinklee .notethatwedeliberately used search heuristics instead of using only the default heuristic.thisisbecause asdemonstratedinsection .
theperformance of the general symbolic execution varies greatly depending onboth the subjectprogram andsearchheuristic.
weapplied homiontopofthebestsearchheuristicthatachieves the highest branch coverage for each program.
for instance we appliedhomiontopofthe mindistance heuristicfor gawkwhile applyinghomiontopofthe cpicount heuristicfor grep.notethat homiandsearchheuristicsare orthogonal andtheyare naturally combined since homiworks regardless of search heuristics in a generic symbolic execution algorithm algorithm homidecides whichstatestopruneatline6whilesearchheuristicsdetermine whichstates to explore further at line10.
other settings.
for all evaluations we maintained the same experimental environments symbolic arguments time budget and memorycapacity.first weusedthesymbolicargumentsusedin e.g.
sym args sym args sym files sym stdin sym stdout .
second we used the same memory capacity 2gb where it is the default setting of klee.
lastly we allocated hours toboththebaselines algorithm andourtechnique algorithm for all benchmarks as time budget.
we repeated all experiments five times andreportedthe averageresults.
.
effectiveness we evaluate the effectiveness of our approach homi from two perspectives branchcoverageandbug findingcapability.insummary homiisabletosignificantlyincreasebranchcoverageand exclusively find bug triggering inputs compared to the general symbolic execution.
.
.
branch coverage.
for each benchmark in table we report the average number of total covered branches figure and exclusively covered branches byhomiand top search heuristics respectively.
as both the general symbolic execution algorithm andhomi algorithm return as output the set of test cases we plotted the number of branches covered by all preceding test cases to depict the coverage graph in figure .
in particular whenthetimebudget 5h expired were executedthe binary of the program with each test case in the set tsequentially wherethe sequence denotesthetimeeachtest casewascreated.wecalculatedthecumulativenumberofcoveredbranchescorrespondingtothecreationtimeofthetest case weused gcov one ofthemostpopulartoolsformeasuringcodecoverage.aswementionedinsection .
homiperformsthegeneralsymbolicexecution withoutstate pruningonthefirstiterationoftheloop.hence to demonstrate the benefits of state pruning only we have plotted the accumulated number of covered branches after the time for the first iteration of algorithm 2elapsed.
in our experiments we first performthegeneralsymbolicexecutionfor800seconds recordthe calculated number of covered branches on the graph and then run algorithm 1fortheremaining timeperiod e.g.
5h 800seconds .
in other words the graphs in the figure 1can clearly demonstrate thecomparisonoftheperformanceofsymbolicexecutionwithand withoutstate pruning technique.
figure1demonstratesthe average numberof branchescovered bythesearchheuristicsovertimein9benchmarks.weusedatotal of heuristics for each benchmark consisting of the top five of the nineoriginalsearchheuristicsandourtechniqueappliedtothebest one among the five.
the experimental results show that the search heuristicwith homisucceedsinachievingthehighestbranchcoverage for all benchmarks.
in particular for the two largest programs gawkandgrep hominotably increases the number of covered branches compared to the bestheuristic without it.for instance in gawk thesearchheuristicwith homi mindistance homi covered about2 884brancheswhile mindistance heuristicitselfmanaged to cover about branches only.
likewise in grep when the time budget 5h expired the best heuristic cpicount with and withouthomicoveredabout2 851and2 505branches respectively.
moreover as shown in a benchmark trueprint the rate for the coverageincreaseovertimeoftheheuristicequippedwith homi was noticeably higher than the ones of other five search heuristics.
inthetwobenchmarks combineandvdir applying homitothe bestheuristichas successfully coveredabout100more branches.
figure1showsthatinmostprograms thenumberofbranches covered byeach top 5search heuristics risessharply at theend of the timebudget this interestingfact is observedbecause we have reportedthebranchcoveragecoveredovertime.thisphenomenon occurssincethealgorithm algorithm generatesthetest cases at lines for each state that has not yet reached the halt statement after the time budget expires and the generated test cases contribute to increasing the total number of branch coverage a lot.
thisimplicitlyshowsthatthegeneralsymbolicexecutionfailsto preferentially explore such promisingstates more.
note that we applied homiwith the best search heuristic just becauseitismorechallengingtoimprovetheperformanceofthe heuristic that has already achieved high code coverage.
in fact homiperformswellregardlessofthesearchheuristic.forinstance applying homievenwiththe6thsearchheuristic cpicount on gawkinfigure 1cancovermorebranchesthanapplying cpicount withouthomi cpicount homicovered2 356branchesonaverage whilecpicount covered2 only.
.
.
exclusively covered branches.
table3shows the number of exclusively covered branches achieved by each technique.
in table3 thei th best heuristic on each benchmark corresponds to the one in figure for instance the best besth and second best heuristic 2ndh on combine arerandompath andcpicount 153esec fse november8 13 virtualevent usa sooyoungchaandhakjoo oh of covered branches gawk time s mindistance homi mindistancequerycost instrcount covnew cpicount of covered branches grep time s cpicount homi cpicountquerycost covnew instrcount randomstate of covered branches combine time s randompath homi randompath cpicount covnewquerycost roundrobin of covered branches vdir time s covnew homi covnew roundrobin querycost mindistance randomstate of covered branches ptx time s covnew homi covnew cpicount querycost randomstate roundrobin of covered branches trueprint time s mindistance homi mindistance randompath roundrobinquerycost covnew of covered branches ginstall time s cpicount homi cpicount roundrobin randompath depth covnew of covered branches pr time s covnew homi covnewquerycost roundrobin randomstate cpicount of covered branches dd time s cpicount homi cpicount roundrobin randompath depth randomstate figure the average branch coverage achieved by top heuristics and homion benchmarks table the average number of branches exclusively covered by top heuristics and homion benchmarks besth homibesth 2ndh 3rdh 4thh 5thh gawk grep combine vdir ptx trueprint ginstall pr dd total respectively.thenumberachievedbyourtechnique besth homi denotesthenumberofbranchesthatourtechniquecoversbutall theremainingtop 5heuristicsfailtocover.theresultsshowthatourtechnique besth homi ishighlyeffectiveinincreasingthe number of exclusively covered branches.
in total the best heuristic withhomiwas able to cover branches while the best heuristic withouthomicovered only branches in summary the former exclusively covered .
times more branches than the latter.
for instance inthelargestprogram gawk homisignificantlyenhanced the performance of the best heuristic mindistance by about .
times.
likewise our technique cpicount homi ongrepexclusivelycovered208branches butthe best heuristic alone managed to exclusivelycover branches.
in addition the number of exclusive branches covered by our technique besth homi is even greater than the sum of the numbers of branches exclusively covered by each of top heuristics wheretheformeris813andthelatteris619 i.e.
.
the number of branches that one of the top heuristics can cover but our technique besth homi fails is .
in other words the branchcoverageachievedbyapplying homitothebest heuristic onlyfor5hoursisalmostthesameastheoneachievedbyapplying 154makingsymbolicexecutionpromising by learning aggressive state pruning strategy esec fse november8 13 virtualevent usa table comparisonofbug finding ability oftop heuristics with vs.without homi.
benchmarks crash types bug triggering inputs error locations besth homibesth 2ndh homi2ndh gawk .14abnormal termination nostalgi line inmain.c abnormal termination compat m r line 526in libc stdlib stdlib.c grep .
abnormal termination n w n line in src dfa.c combine .
.0segmentationfault field line 385in src field.c segmentationfault fi r.o1 r line 633in src df options.c each of top heuristics for hours hours in total .
despite the obviousadvantagesof homi itisstillnotoptimalsinceitalsofailed to cover branches achieved by top search heuristics.
from this observation selective decision on applying homiwould be an interestingfuture work.
.
.
bug finding capability.
in table4 we compared the bugfinding capability of two best heuristics both with and without homi respectively forthethreelargestbenchmarks gawk grep andcombine.
in summary homifound a total of five reproducible bugsonthethreebenchmarks.inparticular thethreebugswere only detectable by homiwhile the general symbolic execution failedto find thesebugs.
table4showsthebenchmark thecrash type thebug triggering input generated by homi the error location and success or failure ofbug findingforeachtechniqueinorder.inparticular wemarked eachtechniqueas success whenthetechniquesucceededin findingthebugatleastonceduringfiveiterationsofthetimebudget 5h .onthecontrary wheneachtechniquetotallyfailedtofindthe bugduringthetimeperiod 5h 5times wemarkeditas failure .theresultsshowthatourtechniquewasabletogenerateatotal offourdistinctbug triggeringinputsin gawkandcombine butthe best heuristic without homionly generated the two inputs.
we confirmedthatthefirstbug triggeringinput nostalgi found ingawkisreproducibleinthelatestversion gawk .
.
.oneinterestingpointisthatthediscoveredbugsaredifferentwhenapplying homito the bestandthe secondbestheuristics the bestheuristic withhomicausedacrash abnormal termination on combinewhile thesecondbestonecausedasegmentationfaulton grep.thatis we expect that applying homito diverse new search heuristics willallowmore bug detection.
.
the numberofcandidate states foreachbenchmarkintable wecomparethenumberofstates that our technique and the general symbolic execution maintain during the testing period.
figure 2shows the average number of states that can be selected by each technique for every second moreprecisely theaveragenumberdenotesthesetsizeofstates s atline10inalgorithm .theresultsshowthatourtechnique besth homi maintains a relatively small number of states for most of the time period on all benchmarks compared to the general symbolic execution.
when performing the general symbolic execution without the state pruning at first which is the first iteration of the loop in algorithm our technique also faced thestate explosionproblem.afterthefirstiteration however ourshas successfullymaintainedasmallnumberofstates.forinstance in gawk ourtechnique mindistance homi keptabout1 897states persecondonaveragewhilethe mindistance heuristicmaintained about states.
in other words our technique succeeded in achieving the highest branch coverage in figure 1while maintaining .
times fewer states than the general symbolic execution.
ingrep cpicount homiandcpicount retained2 030and41 states on average respectively.
in vdir even after the first iteration of the loop in algorithm our technique sometimes faced the state explosion problem.
we confirmed that this problem occurs becausethenumberofstatesgrowsexponentiallyfasterthanthe number prunedbyour state pruning strategy.
forthegeneralsymbolicexecutionwithoutourtechnique we observed that keeping the fewer number of states is not directly related to improving the branch coverage.
for instance in ptx roundrobin heuristicmaintainsabout4 660statesduringthesymbolicexecution whichisalmostthesamenumberofstatesmaintainedbyourtechnique covnew homi .however figure 1shows thatroundrobin covered about fewer branches on average thancovnew ofwhichthenumberofstatesisabout60 053during symbolicexecution.inanotherbenchmark gawk thenumbersof candidate states maintained by cpicount andmindistance are almostthesame where theformeris andthelatter is .
however the difference in the number of covered branches by the twotechniquesisapproximatelyabout350branches.thatis the key answer for increasing code coverage is not to blindly maintain the number of states but to smartly keep only the promising states.
although homisuccessfullymaintainssuchpromisingstateson ourbenchmarksuite wewerenotabletoprovidehigh levelinsights intowhythosestatesarepromising.inourapproach wedetermine how promising a state is based on its corresponding feature vector andweightvector.however asourlearningalgorithmrepresents each state as low level features that check whether it contains core branch conditions it was difficult to decode the learning outcomes anddescribe the intuitionbehindpromisingstates.
.
comparisonwith naiveapproaches we evaluate the efficacy of homi algorithm by comparing it withtwonaivemethods.thefirstnaivemethodistoreplacethe probabilistic pruning strategy prune in algorithm 1with the random pruning strategy randomprune and then to perform 155esec fse november8 13 virtualevent usa sooyoungchaandhakjoo oh time s of states gawk mindistance homi mindistancequerycost instrcount covnew cpicount time s of states grep cpicount homi cpicount querycost covnew instrcount randomstate time s of states combine randompath homi randompath cpicount covnewquerycost roundrobin time s of states vdir covnew homi covnew roundrobin querycost mindistance randomstate time s of states ptx covnew homi covnew cpicount querycost randomstate roundrobin time s of states trueprint mindistance homi mindistance randompath roundrobinquerycost covnew time s of states ginstall cpicount homi cpicount roundrobin randompath depth covnew time s of states pr covnew homi covnewquerycost roundrobin randomstate cpicount time s of states dd cpicount homi cpicount roundrobin randompath depth randomstate figure the average numberofstatesforeachtechniqueto selecton benchmarks algorithm 2withoutonlinelearning line10 as randomprune s r s sp sp s sp s r whereris sampledfromthe uniformdistribution u sratio defined in section .
.
the second naive method is to perform the general symbolic execution algorithm multiple times by dividing the total budget 5h into smaller budgets n wheren is sampled fromu stime definedinsection .
.ingrep wecomparedbranch coverage achieved by ours cpicount homi the best heuristic cpicount the first naive approach cpicount randomprune andthe secondone cpicount respectively.
figure3shows that homi algorithm is essential to effectively improve branch coverage.
for example ours covered at least more branches than the second best method cpicount randomprune .the secondandthethirdbestmethods cpicount randomprune andcpicount achieved nearly identical branch coveragewhentimebudgetexpired.forthegeneralsymbolicexecution algorithm withoutstate pruning itismuchbetterto of covered branches grep time s cpicount homi cpicount randomprune cpicount cpicount figure comparisonwith twonaive approacheson grep performsymbolicexecutionforalongtimethantoperformsymbolicexecutionseveraltimeswithsmallbudget theformerandthe latter wasableto cover and2 branches respectively.
156makingsymbolicexecutionpromising by learning aggressive state pruning strategy esec fse november8 13 virtualevent usa .
threatsto validity we manually tuned the several hyper parameters t max and itv.
to determine each value we ran homiwith a few different values e.g.
onthreebenchmarks andchoseanappropriate one achieving the highest coverage during the experiments.
then weappliedthesamevaluefortheremainingsixbenchmarks.however thetunedvaluesmaynotbesuitableforlargeropen sourcec programs e.g.
loc 100k .
inevaluation weusedboththedefaultsmtsolverofklee stp andthedefaultmemorybudget 2gb .however theperformanceof homimayvaryfordifferent smt solvers and memory budgets.
we used c open source programsextensivelyusedinpreviousworks .
butthesemaynot be representative.
related work inthissection wediscussexistingworksthatarecloselyrelatedto our goaland approach respectively.at a high level our goalis to prunethesearchspaceofsymbolicexecution andourapproachbelongstothetechniquesthatcombinesymbolic executionwithmachine learning .
reducingsearchspaceofsymbolicexecution.
homiisdifferent from and orthogonal to the existing techniques .
these techniques aim to conservatively prune redundant states based onsome predefined criteria.
on the other hand homiaims toaggressively prune the states based on adaptivecriterialearnedonlineduringsymbolicexecution.theread writeset rwset analysis aims to prune program paths that will execute thesamebasicblocksaspreviouslyexploredpaths.likewise the goalofpost conditionedsymbolicexecution istodiscard the states having the same path suffixes as previously explored states during testing.
jaffar et al.
aims to subsume the paths guaranteed to be unreachable to the annotated assertions in the program.
chopper presents a novel technique to perform symbolicexecutionwhilesafelyexcludingtheirrelevantfunctionsin theprogramwhicharenotthetargetsofuserstotest.notethatour tool homi can further enhance symbolic execution by combining thesetechniques that safely prune redundantpaths.
combining symbolic execution with learning.
ourwork aligns with thisline of researchthat employsmachinelearning to boost symbolic execution .
paradyse presents a new approachto automatically generate searchheuristics of symbolic execution via offline learning.
chameleon is a novel symbolicexecutionthatadaptivelyswitchessearchheuristicsfor better performance via online learning.
mlb uses machine learning to effectively handle the complex path conditions that involve externalfunction callsor floatingpointarithmeticinsymbolic execution.
leo is a machine learning based approach to boost symbolic execution by transforming the program under test into an easy to analyze program while preserving its semantics.
contest aimstolearnusefultemplatesthatreducetheinput space of the program under test by selectively generating symbolic variablesduringtesting.ontheotherhand weusealearningalgorithm to aggressively prune unpromising states online during symbolic execution.
conclusion we present a new approach homiwith the goal of maintaining promising states only via aggressive state pruning.
the key idea is to continuously learn the probabilistic pruning strategy based on the cumulative data during the testing period.
experimental results on open source cprojects showthat symbolic execution withhomiisabletonotablyincreasebranchcoverageandfindreal bugs while keeping a relatively small set of states.
we believe that minimizingcandidate states insymbolicexecutionwillemerge as anewsolution against the state explosionproblem.