adaptive performance anomaly detection for online service systems via pattern sketching zhuangbin chen the chinese university of hong kong hong kong chinajinyang liu the chinese university of hong kong hong kong chinayuxin su school of software engineering sun yat sen university zhuhai china hongyu zhang the university of newcastle nsw australiaxiao ling yongqiang yang huawei cloud bu beijing chinamichael r. lyu the chinese university of hong kong hong kong china abstract to ensure the performance of online service systems their status is closely monitored with various software and system metrics.
performance anomalies represent the performance degradation issues e.g.
slow response of the service systems.
when performing anomaly detection over the metrics existing methods often lack the merit of interpretability which is vital for engineers and analysts to take remediation actions.
moreover they are unable to effectively accommodate the ever changing services in an online fashion.
to address these limitations in this paper we propose adsketch an interpretable and adaptive performance anomaly detection approach based on pattern sketching.
adsketch achieves interpretability by identifying groups of anomalous metric patterns which represent particular types of performance issues.
the underlying issues can then be immediately recognized if similar patterns emerge again.
in addition an adaptive learning algorithm is designed to embrace unprecedented patterns induced by service updates or user behavior changes.
the proposed approach is evaluated with public data as well as industrial data collected from a representative online service system in huawei cloud.
the experimental results show that adsketch outperforms state of the art approaches by a significant margin and demonstrate the effectiveness of the online algorithm in new pattern discovery.
furthermore our approach has been successfully deployed in industrial practice.
ccs concepts computer systems organization cloud computing reliability maintainability and maintenance.
keywords cloud computing performance anomaly detection online learning corresponding author suyx35 mail.sysu.edu.cn .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn .
.
.
.
reference format zhuangbin chen jinyang liu yuxin su hongyu zhang xiao ling yongqiang yang and michael r. lyu.
.
adaptive performance anomaly detection for online service systems via pattern sketching.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa 12pages.
introduction with the emergence of cloud computing many traditional software systems have been migrated to cloud computing platforms as online services.
similar to conventional shrink wrapped software the performance of online service systems is an important quality attribute.
as online services need to serve millions of customers worldwide a short period of performance degradation could lead to economic loss and user dissatisfaction.
therefore proactive and even adaptive system troubleshooting has become the core competence of online service providers.
enterprises that have promoted the automation of system troubleshooting have already received real gains in reliability efficiency and agility .
in industrial scenarios online service systems are closely monitored with various metrics e.g.
the cpu usage of an application service response delay on a basis.
this is because the monitoring metrics often serve as the most direct and fine grained signals that flag the occurrence of service performance issues.
in addition they provide informative clues for engineers to pinpoint the root causes.
however due to the large scale and complexity of online service systems the number of metrics is overwhelming the existing troubleshooting systems .
automated anomaly detection over the metrics which aims to discover the unexpected or rare behaviors of the metric time series is therefore an important means to ensure the reliability and availability of service systems.
although many efforts e.g.
have been devoted to performance anomaly detection most of the existing work does not possess the merit of interpretability.
specifically at each timestamp they calculate a probability indicating the likelihood of performance anomalies.
a threshold is then chosen to convert the probability into a binary label normal vs. anomaly.
however in reality a simple recommendation of the suspicious anomalies might not be of much interest to engineers.
this is because they need to manually investigate the problematic metrics recommended by the model for fault localization.
for large scale online services this process ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhuangbin chen jinyang liu yuxin su hongyu zhang xiao ling yongqiang yang and michael r. lyu is like finding a needle in a haystack.
the problem is compounded by the fact that false alerts are not rare.
moreover many stateof the art methods train models with historical metric data in an offline setting.
as online services continuously undergo feature upgrades and system renewal the patterns of metrics may evolve accordingly i.e.
concept drift .
without adaptability these models are unable to accommodate the ever changing services and user behaviors.
in this paper we propose adsketch a performance anomaly detection approach for online service systems based on pattern sketching which is interpretable and adaptive.
the main idea is to identify discriminative subsequences from metric time series that can represent classes of service performance issues.
this is similar to the problem of shapelet discovery in time series data .
particularly for multiple subsequences that describe the same type of performance issue we take the average of them and regard the result as a metric pattern for the issue.
for example services may be experiencing performance degradation when we observe a level shift down on service throughput or a level shift up on cpu utilization.
the advantages of such metric patterns are twofold.
first the normality of the incoming metric subsequences can be quickly determined through a comparison with the metric patterns.
second by associating the patterns with typical anomaly symptoms we can immediately understand the ongoing performance issues when the metric subsequences exhibit known patterns.
this is similar to failure issue profiling .
in this way adsketch provides a novel mechanism to characterize service performance issues with metric time series.
previous work on failure issue profiling often requires handcrafted features which suffers from limited generalization.
for example brandon et al.
manually defined a set of features collected from metrics logs and anomalies to characterize failures.
pattern sketching with metrics enjoys the advantages of automation and accuracy.
moreover adsketch is able to adaptively embrace new anomalous patterns when detecting anomalies on the fly.
experimental results demonstrate the superiority of our design over the existing state of the art time series anomaly detectors on both public and industrial data.
in particular we have achieved an average f1 score of over .
in production systems.
to sum up this work makes the following major contributions we propose adsketch an interpretable and adaptive approach for service performance anomaly detection.
adsketch offers a way to characterize service performance issues with monitoring metrics.
different from the existing work adsketch is able to provide explanations e.g.
the type of the underlying performance issues for its prediction results and accept new patterns on the fly.
the implementation of adsketch and datasets are publicly available on github1.
we conduct experiments with public data as well as industrial service metric data collected from huawei cloud.
the experimental results demonstrate the effectiveness of adsketch in terms of both anomaly detection and adaptive metric pattern learning.
furthermore our framework has been successfully incorporated into the service performance monitoring system of huawei cloud.
our industrial practice confirms its practical usefulness.
.
.
.
interface throughput .
.
.
request timeout number time0.
.
.
application cpu usagefigure examples of performance anomaly patterns background problem statement .
performance anomaly patterns in online service systems in online service systems a large number of metrics are configured to monitor various aspects of both logical resources e.g.
a virtual machine and physical resources e.g.
a computing server .
cloud systems often possess an abundance of redundant components providing the ability of fault tolerance and self healing e.g.
load balancing availability zones .
consequently the majority of service breakdowns tend to manifest themselves as performance anomalies first instead of fail stop failures .
we observe when performance anomalies of similar types happen their impacts tend to trigger similar reactions symptoms on the metric time series which we refer to as metric patterns.
for example a level shift up on interface throughput may indicate slow service response which could be caused by a load balancing failure a level shift down on it may suggest service unavailability and the culprit could be performance bugs e.g.
memory leak bugs .
similar observations have been made in .
the rationale behind such a phenomenon is twofold.
first the design of the metrics is sophisticated and fine grained each of which is dedicated to monitoring a specific problem e.g.
request timeout high api error rate.
second cloud systems widely employ the microservices architecture where cloud applications employ lightweight container deployment e.g.
cloud native applications serverless computing.
with this architecture each microservice is designated for well defined and modularized jobs e.g.
user login location service.
thus they tend to develop individual and stable patterns which can manifest through their monitoring metrics.
.
metric pattern mining metric patterns i.e.
time series subsequences describing the misbehaving moments of metrics can be leveraged to sketch the performance issues for anomaly detection.
this is essentially profiling the mode of recurrent anomalies.
for example hardware failures often come with a sudden drop in the corresponding metrics and the value remains zero for some time.
if anomalies come into existence they can be immediately identified by matching the established patterns.
such metric patterns can also facilitate problem mitigation.
for example when low service throughput and high cpu usage are detected engineers can scale up the microservice by adding local cores to increase its capacity.
the key challenge is how to automatically discover what anomalous patterns a metric authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive performance anomaly detection for online service systems via pattern sketching icse may pittsburgh pa usa adaptive pattern learninganomaly free metricmetric for anomaly detection metric pattern discoverynew metrics in online scenarios normal patterns abnormal patternsoffline phaseonline phase offline prediction online prediction figure the overall framework of adsketch time series has experienced.
for each identified pattern engineers can label the typical performance issues it often associates with.
in online scenarios if a metric encounters any known anomalous patterns the underlying performance issues can be recommended.
pattern sketching therefore provides a means to accumulate and utilize engineers knowledge.
in real world scenarios the patterns exhibited in metrics are extremely complicated and can have numerous variants in terms of scale length and combination.
particularly we have identified the following challenges for metric pattern discovery which are illustrated in fig.
.
each metric time series records around one week of monitoring data whose anomalies are shown in red.
background noise .
although a large amount of metric time series is generated a significant portion of them is trivial which only records plain system runtime behaviors.
moreover due to the dynamics of online services some metrics may experience concept drift .
for example the application cpu usage in fig.
drops abruptly which could be caused by a role switch e.g.
from a primary node to a backup node or user behavior change.
how to distinguish anomalous patterns from normal ones is non trivial.
pattern variety .
a metric curve can possess multiple distinct patterns simultaneously.
for example in fig.
the interface throughput has two anomaly patterns i.e.
spike up and spike down.
also the patterns can have different scales as indicated by the two spikes in the request timeout number.
we need to consider the context of each metric for pattern extraction.
varying anomaly duration .
different performance issues may vary in duration.
the first two anomalies in the interface throughput constitute such an example.
particularly how long an anomaly lasts is also an important factor that engineers rely on to understand a service s health state.
when characterizing the issues such a fact should be properly considered.
.
problem statement the goal of this work is to detect performance anomalies for modern software systems especially online service systems based on monitoring metrics.
to facilitate issue understanding and problem mitigation we intend to improve the interpretability of the detection results.
to this end we propose to sketch performance issues with metrics based on our observation that similar issues often exhibit alike patterns.
by extracting such anomalous metrictable summary of variables variable meaning tn an anomaly free metric time series ta an input metric time series for anomaly detection t a subsequence of metric time series m the length of the metric subsequence t p the percentile threshold to find deviated subseqs pn the index set of normal metric patterns pa the index set of anomalous metric patterns c the vector of cluster mean vectors sc the vector of cluster sizes rc the vector of cluster radii patterns we can conduct performance anomaly detection by examining whether the incoming metric subsequences match the known patterns.
moreover by associating the extracted metric patterns to specific performance issues we can obtain a quick understanding of the ongoing issues in online scenarios.
additionally as online services are continuously evolving unprecedented metric patterns may emerge.
thus our algorithm should be adaptive to the new patterns.
the problem can be formally defined as follows.
the input of a metric time series can be represented as t rl wherelis the number of observations.
tm i is a consecutive subsequence of tstarting from ti with length m wherei .
the objective of performance anomaly detection is to determine whether or not a given tm iis anomalous i.e.
whether there are performance issues happening from timestamp itoi m .
particularly we also try to explain the type of performance issues associated with tm i. the anomalous subsequences will be used to construct abnormal metric patterns while the benign ones will be regarded as normal patterns.
both the normal and abnormal metric patterns will be updated as the anomaly detection proceeds.
methodology .
overview in online service systems performance anomalies often serve as the early signals for critical failures which should be detected effectively.
however accuracy alone is far from satisfactory as it will be labor intensive to manually investigate the problematic metrics for issue understanding.
adsketch facilitates this process by providing prompt anomaly alerts with explanations.
the overall framework of adsketch is shown in fig.
which consists of two phases namely offline anomaly detection andonline anomaly detection.
in the offline phase adsketch takes as input a pair of metric time series.
one metric time series is anomaly free which serves as the basis to detect anomalies in the other metric if any .
in this process a set of metric patterns will be automatically learned.
a metric pattern is essentially the mean of a set of similar metric subsequences representing similar service behaviors.
the identified metric patterns are divided into two types i.e.
normal and abnormal.
the abnormal patterns often characterize some particular types of performance issues as discussed in sec.
.
.
thus authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhuangbin chen jinyang liu yuxin su hongyu zhang xiao ling yongqiang yang and michael r. lyu anomaly free metric time series!!
metric time series for anomaly detection!
break due to percentile threshold unfulfillmentapply self union and cross union stamp to get the most similar subsequences apply affinity propagationtothe mean of each subgraphthe mean of each clustermetric patterns andis the only abnormal patternapply affinity propagation to get globally similar subsequences and calculate the metric patterns construct a graph gbased on subsequences similarity and find the connected subgraphs isolated subgraphs also the anomaly candidates metric subsequencesthe most similar subsequencea group of similar subsequencessubsequence graph link figure the algorithm of performance anomaly pattern discovery time0.
.
.
interface throughput metric subsequence index0.
.
.
spw distance figure the spw distance of different metric subsequences by investing manual efforts to link them to the corresponding issues a clearer picture of the underlying problems can be easily obtained if similar patterns are encountered again.
in the online phase we leverage the metric patterns built in the offline phase to conduct anomaly detection in online scenarios where metrics arrive in streams.
particularly in production environments unprecedented patterns could appear.
thus we design an adaptive learning algorithm to capture the new patterns continuously.
before formally introducing our algorithms we have summarized the variables involved in table .
.
offline anomaly detection .
.
metric pattern discovery.
the idea for discovering the abnormal patterns follows the basic definition of an anomaly if a metric subsequence deviates significantly from those collected during a service s normal executions it is likely that the subsequence captures some misbehaving moments of the service.
to measure how deviated a metric subsequence is we calculate its distance to other subsequences and search for the smallest distance score.
intuitively metric subsequences which have large scores to others tend to be anomalous.
the function for distance measure is customizable and we adopt euclidean distance in this paper.
given a metric time series with lobservations the number of all possible subsequences is l m wheremis the length of its subsequences.
a na ve solution for calculating the smallest pair wise distance which we refer to as spw distance hereafter would be brute force searching.
however this algorithm owns a quadratictime complexity which is practically infeasible for large time series.
fortunately some novel scalable algorithms have been proposed in the literature to attack such all pairs similaritysearch problems for time series subsequences.
particularly yeh et al.
proposed stamp which has achieved orders of magnitude faster compared to state of the art methods.
for exceptionally large datasets an ultra fast approximate solution is also provided.
an illustrating example is provided in fig.
where we can see the misbehaving metric subsequences have larger spw distances.
in particular the original stamp algorithm adopts z normalization for data preprocessing.
however we found min max normalization yields more meaningful results in our scenario.
for a subsequence tm iin a metric time series t we record the index and distance score of another subsequence having the spw distance to it.
such index and score of all subsequences i.e.
tm i i constitute two vectorsiands.
in particular for tm i its closest subsequence can either come from the same time series i.e.
self union or another time series i.e.
cross union .
in the first case a trivial match region aroundtm iwill be excluded to avoid self matches .
the proposed algorithm for metric pattern discovery is presented in algorithm which is illustrated in fig.
.
algorithm 1takes as input two metric time series i.e.
tnandta tnis anomaly free andtamay contain anomalies to be detected and two hyperparameters i.e.
mandp mis the length of subsequences and p is the percentile threshold to find the deviated subsequences .
as production service systems are mostly running in normal status the anomaly free input is easily obtainable we discuss how we address the violating cases in sec.
.
.
in line of algorithm we apply stamp to tnwith self union i.e.
similar subsequences come fromtn and obtain the index and score vectors innand snn.
in line we search similar subsequences for tafromtn i.e.
cross union and get inaandsna.
intuitively given the fact thattnis anomaly free subsequences in tahaving large spw distances to their closest peers in tnare suspected to be anomalous.
interestingly we later learn that mercer et al.
proposed a similar idea concurrently.
we introduce a percentile threshold i.e.
p on snato find such deviated subsequences.
in particular pis loosely set to avoid missing anomalies i.e.
false negatives.
such a setting will inevitably produce false positives.
we next discuss how we alleviate this issue.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive performance anomaly detection for online service systems via pattern sketching icse may pittsburgh pa usa algorithm performance anomaly pattern discovery input tn ta m andp output two disjoint sets of pnandpa 1inn snn stamp tn tn m 2ina sna stamp tn ta m 3g connectedsubgraphs inn ina sna p 4ni isolatednodes g 5 g graphwisemean g 6c affinitypropagation g 7 c clusterwisemean c 8pn emptyarray pa emptyarray 9foreachidxin size c do c all subsequences in the cluster ifc nithen 11pa appendpawithidx else 13pn appendpnwithidx end 15end algorithm performance anomaly detection input t pa and c output anomaly detection result for t 1dt pairwisedistance t c 2idx minindex dt 3ifidx pathen return true 5else return false 7end a metric pattern is defined as the mean of a group of similar subsequences which represents some typical behaviors of the metric time series.
to mine similar subsequences we propose to leverage their similarity connections.
specifically in line we construct a graphgwhose nodes correspond to the subsequences.
two nodes will be linked if any one of them is deemed as the most similar subsequence to the other as indicated by innandina.
note such a relationship is not mutual i.e.
tm iis the most similar to tm jdoes not necessarily imply the opposite case.
we break the edges whose distance score fails to meet the threshold requirement p. the above operations are depicted in the first part of fig.
.
next we find the connected subgraphs of g each of which is composed of subsequences resembling each other.
particularly there will be some isolated nodes i.e.
subgraphs with a single node which are collected at line .
such deviated subsequences constitute a set of anomaly candidates i.e.
ni.
the second part of fig.
3illustrates this process.
up to this point we have divided the subsequences of tnand tainto different parts each of which is represented as a subgraph.
however each subgraph cannot be directly regarded as a metric pattern because the graph construction criteria can be too strict i.e.
only the most similar pairs are connected so some subgraphsmight still be similar the loosely set percentile threshold pmay flag some normal subsequences as abnormal i.e.
false positives .
to further combine the similar subsequences we apply the affinity propagation algorithm to cluster the mean vector of each subgroup line .
we choose this algorithm because of its superior performance and efficiency and it requires no pre defined cluster number.
as a result similar normal subgraphs can be merged together and abnormal subgraphs have a chance to embrace their normal communities.
thus each cluster will contain all similar subsequences across the two time series inputs and different clusters represent distinct patterns.
the mean of clusters i.e.
c will form the set of metric patterns line .
for each cluster we check whether or not all its members come from the set of anomaly candidatesni line .
if yes the mean of the cluster will be regarded as an abnormal metric pattern and otherwise normal indexed by paandpn respectively.
the third part of fig.
3presents the above operations.
finally all subsequences in the anomalous clusters will be predicted as an anomaly to be the output of this phase.
.
.
metric pattern interpretability.
in this section we expound on how to label the performance issues that each metric pattern represents.
by allowing metric patterns to have semantics the understanding and mitigation of service problems can be greatly accelerated.
given the fact that the duration of different performance issues may vary our fixed length metric patterns may over represent i.e.
the metric pattern is much larger than the issue s duration or under represent i.e.
the metric pattern is only an excerpt of the issue the corresponding issues.
to alleviate the first problem we select a relatively small m which turns out to be aligned with the goal of better performance.
for the second problem we adopt the following strategy to group clusters which are actually describing a common issue.
for each pair of clusters we check whether they have some subsequences that share some parts in common.
all clusters sharing such overlaps together can recover the complete picture of the issue.
thus we regard them as describing an identical issue.
finally for each metric pattern domain engineers will label the type of performance issue that triggers it.
particularly one pattern can have multiple labels simultaneously.
the metric patterns with overlaps will share the same set of performance issue labels.
.
online anomaly detection .
.
anomaly detection on the fly.
based on the metric patterns identified in algorithm we now describe our algorithm algorithm for anomaly detection in online scenarios.
the idea is straightforward given a new metric subsequence twith length m we search for its most similar metric pattern line and check which pattern pool it comes from.
if tis more similar to an abnormal pattern it will be predicted as anomalous otherwise normal line .
in real world systems where monitoring metrics are generated in a stream manner this process is continuously running for all coming subsequences.
when an anomaly is identified we would like to provide more interpretation about it e.g.
what kinds of performance issues have happened.
this is done by simply recommending the issues associated with the most similar metric pattern for all involved metrics.
particularly in algorithm each cluster i.e.
cat line contains all subsequences that are deemed as similar.
the design of our online anomaly detection only requires the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhuangbin chen jinyang liu yuxin su hongyu zhang xiao ling yongqiang yang and michael r. lyu algorithm adaptive pattern learning input t pn pa c sc andrc output updated variables pn pa c sc andrc 1dt pairwisedistance t c 2idx minindex dt 3 g sc t sc 4dw distance c rc 5dt distance t 6d max dt dw 7dn da max rc max rc 8ifidx pathend daelsed dnend 9ifdt dthen addtto the most similar cluster 10 c sc rc sc d ifsc max sc andidxisanewcluster then 12pn appendpnwithidx 13pa removeidxfrompa else 15d max d d dwill be assigned to dn ordaaccordingly end 17else create a new anomalous cluster for t 18pa appendpawith length c 19 g append gwitht 20rc appendrcwith 21sc appendscwith 22end mean vector of each cluster i.e.
c. thus instead of keeping all its members which is storage intensive the clusters can be simply represented by their mean vectors.
note that the offline and online anomaly detection can work collaboratively as a performance anomaly detector without the interpretability component which requires human intervention.
so far the metric patterns for anomaly detection are discovered based on historical data.
however due to the dynamics of online service systems e.g.
software upgrade customer behavior change the metrics may experience concept drift which produces brand new patterns.
thus an adaptive learning mechanism is desirable to help adapt to such unprecedented patterns and update the metric patterns accordingly.
in the next section we will introduce the algorithm to this end called adaptive pattern learning.
.
.
adaptive pattern learning.
the algorithm of adaptive pattern learning is presented in algorithm which automatically updates metric patterns during streaming anomaly detection.
to start with for each cluster we calculate its size and the maximum distance between its mean vector and all members which we refer to as radius denoted asscandrc respectively.
in particular the size and radius of clusters with only a single member are one and zero.
for adaptive pattern learning all clusters can be sufficiently !
!!!
r!
metric subsequencemean vector!!!!
r!
tangent normaltangent!!!!
r!
normal figure the update of the radius of a cluster represented with the following properties c sc andrc.
all subsequences can be discarded.
the main idea is that given a new subsequence t we determine whether it possesses a known metric pattern carried by an existing cluster.
if yes the cluster will absorb tas a new member and update its properties otherwise a brand new anomalous cluster with onlytitself will be created representing an unseen metric pattern.
specifically we first search for the closest pattern of t line .
then we determine whether tshould become a new member to the corresponding cluster by checking if the distance dt is smaller than the largest radius recorded in all clusters i.e.
dt max rc .
if it is the case tshould be considered as an old pattern otherwise it should be expressing a new pattern.
when a cluster accepts a new member line we need to update its mean vector c i.e.
the metric pattern size sc and radiusrc .
for c it can be precisely updated by the equation at line i.e.
.sc can be trivially updated by increasing itself by one.
the update of the radius rc is a bit problematic.
we cannot directly calculate the new radius as the original subsequences are not available.
to address this problem we employ the worst case distance for approximation.
as shown in fig.
the new radius reaches its maximum value when tlies in the inward pointing normal of the tangent space at the member yielding the radius denoted as tr which can be calculated by the equation at line .
we omit the proof which is standard.
two cases are possible.
the first the left subfigure is that trcontinues to be the farthest member from the new mean .
the second the right subfigure is that ttakes the place of trand becomes the farthest one.
therefore besides dw we also compute the distance betweentand i.e.
dt and compare them line .
the bigger one will be the new radius line .
recall we need to check if dt max rc to decide whether or not tshould be taken as a new member.
considering the high imbalance between normal and abnormal clusters we maintain two maximum radii for them denoted asdnandda respectively line .
once a cluster alters its radius we reset the maximum radius of its kind dnordaas determined by line if it is exceeded by d line .
on the other hand if the cluster rejects t we form a new anomalous cluster containing only tby properly setting its properties line .
an issue with this strategy is that false positives will accumulate inpaas the unseen patterns can also be normal.
we alleviate it authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive performance anomaly detection for online service systems via pattern sketching icse may pittsburgh pa usa by setting a threshold to the size of the newly formed anomalous clusters line .
the role of the cluster will be switched from abnormal to normal if its size exceeds the threshold line .
the rationale is that performance anomalies are generally rare events.
a large anomalous cluster would mean the particular type of issue it represents occurs too often.
however a pattern with a large frequency tends to be the metric s normal behavior.
in this paper we simply set the default threshold as the largest size of the anomalous clusters identified in the offline stage i.e.
max sc .
nevertheless more sophisticated strategies can be applied by for example considering the distribution of clusters sizes.
.
time and space complexity .
.
time complexity.
for algorithm the theoretical time complexity of operation stamp is o n2 .
thus line require o l2n ando l2a respectively where lnandlaare the length oftnand ta.
another operation with an interesting time complexity is the affinity propagation algorithm line whose complexity is quadratic in the number of clusters which is often small i.e.
o c .
other operations are of trivial linear time complexity which is also the case for algorithm 2and algorithm .
overall adsketch owns a time complexity of o n2 o l2n l2a c .
fortunately unlike other models such as deep neural networks stamp can be embarrassingly parallelized by distributing its unit operation spw distance calculation to multi core processors .
moreover stamp has an ultra fast approximation to generate results in an anytime fashion.
.
.
space complexity.
as described in sec.
.
.
pattern clusters have a lightweight representation i.e.
c sc andrc.
we also need pnandpato distinguish anomalous patterns from the normal ones.
besides cwhose space complexity is o m c other vectors are ofo c .
therefore the dominant term of space complexity iso m c .
since both mand c are usually small the space overhead of adsketch can be considered trivial.
experiments in this section we evaluate adsketch using both public data and real world metric data collected from the industry.
particularly we aim at answering the following research questions.
rq1 how effective is adsketch s offline anomaly detection?
rq2 how effective is adsketch s online anomaly detection?
rq3 how effective is adsketch s adaptive pattern learning?
the evaluation process of much existing work e.g.
essentially corresponds to the process adopted in rq1 i.e.
the offline anomaly detection phase because the threshold they select for anomaly alerting is determined by iterating the full range of its possible values.
the best results achieved during the iteration process are reported.
to fully examine the performance of different methods in online scenarios we fix models data and parameters including the threshold learned in offline mode as if they are deployed in production systems i.e.
rq2.
the online adaptability of adsketch will be evaluated in rq3.
.
experiment setting .
.
dataset.
to evaluate the effectiveness of adsketch in performance anomaly detection we conduct experiments on two publiclytable dataset statistics dataset curves points anomaly ratio yahoo .
aiops18 .
industry .
available datasets.
moreover to confirm its practical significance we collect a production dataset from a large scale online service of huawei cloud.
table 2summarizes the statistics of the datasets.
public dataset .
the public datasets for experiments are yahoo and aiops18 .
particularly we do not conduct online anomaly detection on yahoo due to its limited number of anomalies.
yahoo.
yahoo released by yahoo!
research is a benchmark dataset for time series anomaly detection.
part of the dataset is synthetic which is simulated by algorithmically injecting anomalies and part of the dataset is collected from the real traffic of yahoo services.
the anomalies in the real dataset are manually labeled.
all time series are sampled every hour.
in particular as our goal is detecting performance anomalies for online services we only use the real dataset which reflects the real world service performance issues.
for each time series we select the first data points as the anomaly free input any anomalies are ignored while the remaining part as the input for offline anomaly detection.
aiops18.
aiops18 dataset was released by an international aiops competition held in .
the dataset is composed of multiple metric time series collected from the web services of large scale it companies.
particularly the dataset contains two types of metrics i.e.
service metrics and machine metrics.
the service metrics record the scale and performance of the web services including response time traffic connection errors while the machine metrics reflect the health states of physical machines including cpu usage network throughput.
some metric time series has a sampling interval of one minute while that of others is five minutes.
each metric has a training and a testing time series.
thanks to its large quantity we follow the following procedure to separate the data for adsketch offline and online anomaly detection.
first we extract a small part of the training time series that is anomaly free which often contains thousands of data points.
then we use the remainder of the training time series for offline anomaly detection.
finally the whole testing time series will be employed for online anomaly detection.
we also compare the performance of online anomaly detection with and without the adaptive learning component.
industrial dataset .
to evaluate adsketch in production scenarios we collect various metrics e.g.
application cpu usage interface throughput request timeout number round trip delay from a large scale online service we conceal the name for privacy concern of huawei cloud.
the system under study produces millions of metric time series which contain an abundance of different metric patterns.
the number of metric curves collected is which come from multiple instances of virtual machines authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhuangbin chen jinyang liu yuxin su hongyu zhang xiao ling yongqiang yang and michael r. lyu containers and applications of the selected service system.
for each metric we collect one week of data with a sampling interval of one minute resulting in more than four million data points in total.
the anomalies representing the performance issues of the service are labeled by experienced domain engineers.
from table we can see that the anomaly ratio is very low.
particularly we use the first day as the anomaly free input whose anomalies if any are simply ignored.
the next three days are used for offline anomaly detection.
finally we conduct online anomaly detection on the remaining three days where we also evaluate the adaptability of different approaches to unseen anomaly patterns.
.
.
evaluation metrics.
as anomaly detection is essentially a binary classification problem i.e.
normal and abnormal we employ precision recall and f1 score for evaluation.
they can gauge the performance of an anomaly detection algorithm at a fine grained level.
a satisfactory algorithm should be able to quickly and precisely detect both the occurrence and duration of performance anomalies.
specifically precision measures the percentage of anomalous metric points that are successfully identified as anomalies over all the metric points that are predicted as anomalous precision tp tp fp.
recall calculates the portion of anomalous metric points that are successfully identified by adsketch over all the actual anomalous points recall tp tp fn.
finally the f1 score is the harmonic mean of precision and recall f1score precision recall precision recall.tpis the number of anomalous metric points that are correctly discovered by adsketch fpis the number of normal metric points that are wrongly predicted as an anomaly by adsketch fnis the number of anomalous metric points that adsketch fails to notice.
since there are multiple metrics in each dataset we report their average weighted by the size of each metric time series.
.
.
comparative methods.
the following methods are selected for comparative evaluation of adsketch.
as all baselines have open sourced their code we directly borrow the implementations and follow the procedure of model training and parameter tuning introduced in each method.
lstm .
this method employs long short term memory lstm network to capture the normal behaviors of metrics in a forecasting based manner.
specifically it predicts the next values of a metric based on its past observations.
the predicted values are then compared with the actual values.
anomaly warnings will be raised if the differences exceed the pre defined thresholds.
donut .
donut adopts the variational autoencoder vae framework to properly reconstruct the normal metric subsequences.
the trained model will have a large reconstruction loss when it meets anomalous instances which serves as the signal to alert anomalies.
lstm vae .
similar to donut this work detects anomalies based on metric subsequence reconstruction.
it combines lstm and vae in the model design.
loda .
loda is an online anomaly detector based on the ensemble of a series of one dimensional histograms.
each histogram approximates the probability density of input data projected onto a single projection vector.
loda calculatesthe likelihood of an anomaly based on the joint probability of the projections.
iforest .
isolation forest iforest is composed of a collection of isolation trees which isolates anomalies based on random subsets of the input features.
the height of an input sample averaged over the trees is a measure of its normality.
samples with noticeably shorter heights are likely to be anomalies.
we use metric subsequences as the input samples.
dagmm .
dagmm utilizes a deep autoencoder to generate a low dimensional representation for each input data point which is further fed into a gaussian mixture model to estimate the anomaly score.
sr cnn .
sr cnn first applies spectral residual to highlight the most important regions for seasonal metric data where anomalies often reside.
it then trains a convolutional neural network cnn through synthetic anomalies to detect the real anomalies.
.
experimental results .
.
rq1 the effectiveness of adsketch s offline anomaly detection.
to answer this research question we compare adsketch with the baselines in the offline setting.
the results are shown in table where we can see the average f1 score of adsketch outperforms all baseline methods in all datasets.
in aiops18 and industry the improvement achieved by adsketch is more significant.
in particular the patterns of anomalies in yahoo are relatively simple.
by iterating over all possible values of the anomaly threshold the baselines can find the best setting for the dataset under study.
among them lstm and donut achieve comparable performance compared to that of adsketch i.e.
.
whose average f1 scores are .
and .
respectively.
moreover lstm has the best recall i.e.
.
while the best precision i.e.
.
goes to loda .
dagmm and sr cnn turn out to be the worst methods in this dataset.
in terms of aiops18 and industry datasets we can see adsketch surpasses the baselines by a larger margin.
specifically the average f1 score of adsketch in aiops18 is .
while that of the second best method i.e.
lstm vae is .
.
adsketch also attains the best precision and recall.
in aiops18 the anomaly patterns are much more complicated.
baselines tend to predict more data points as anomalous leading to a lower precision.
different from them adsketch is able to precisely capture them and outperforms other methods.
the situation is similar in industry.
particularly this dataset is collected from online services and many of its metric curves possess more perceivable and regular patterns.
thus all methods perform better in this dataset than in the other two.
the average f1 scores of adsketch and the second best method i.e.
lstm are .
and .
respectively.
in table we can see among all comparative methods lstm and lstm vae have better overall performance which are forecastingbased and reconstruction based methods respectively.
they both try to model the normal patterns of a metric time series and alert anomalies once the metric significantly deviates from the learned patterns.
the difference is that a forecasting based method aims to predict the next metric values and a reconstruction based method tries to encode and regenerate metric subsequences.
we can see authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive performance anomaly detection for online service systems via pattern sketching icse may pittsburgh pa usa table experimental results of offline anomaly detection yahoo aiops18 industry method precision recall f1 score precision recall f1 score precision recall f1 score lstm .
.
.
.
.
.
.
.
.
lstm vae .
.
.
.
.
.
.
.
.
donut .
.
.
.
.
.
.
.
.
loda .
.
.
.
.
.
.
.
.
iforest .
.
.
.
.
.
.
.
.
dagmm .
.
.
.
.
.
.
.
.
sr cnn .
.
.
.
.
.
.
.
.
adsketch .
.
.
.
.
.
.
.
.
table experimental results of online anomaly detection aiops18 industry method prec.
rec.
f1 prec.
rec.
f1 lstm .
.
.
.
.
.
lstm vae .
.
.
.
.
.
donut .
.
.
.
.
.
loda .
.
.
.
.
.
iforest .
.
.
.
.
.
dagmm .
.
.
.
.
.
sr cnn .
.
.
.
.
.
adsketch .
.
.
.
.
.
table experimental results of adaptive pattern learning aiops18 industry method prec.
rec.
f1 prec.
rec.
f1 loda .
.
.
.
.
.
adsketch .
.
.
.
.
.
except for lstm vae in yahoo these two methods attain the best results compared to other baseline counterparts in the other two datasets.
however lstm lacks the ability to explicitly detect anomalies in the level of subsequence.
many anomalies are composed of a collection of anomalous points corresponding to the period of performance issues.
lstm vae does not take into account the relationship among subsequences.
many suspicious subsequences are not necessarily anomalies if they often occur in the history of the service systems.
compared to them adsketch is able to simultaneously learn the subsequence level features and consider the context of metric time series.
.
.
rq2 the effectiveness of adsketch s online anomaly detection.
we also compare adsketch against the selected methods for online anomaly detection.
table 4presents the experimental results.
except for donut in aiops18 all models and algorithms encounter an obvious performance degradation in both datasets.
nevertheless adsketch manages to maintain the best ranking .
in aiops18 and .
in industry which is followed by lstm .
in aiops18 and lstm vae .
in industry .
particularly in aiops18 the average f1 score of different methods drops by .
this observation demonstrates the existence of unprecedented metric patterns in online scenarios.
by relying on the outdated data and parameters e.g.
adsketch s metric patterns and baselines anomaly thresholds learned from the offline stage the methods cannot accommodate them.
in addition by plotting the metric time series we observe the emergence of concept drift on metrics.
this can be caused by software upgrades or the integration of new service components e.g.
virtual machines containers .
in the industrial dataset the evaluation results of the baselines are more promising i.e.
the average f1 score drops by less than .
this is because the anomalies are triggered by real world performance issues.
the issues have a more natural distribution and the collected metrics exhibit relatively stable patterns.
adsketch presents a significant performance degradation.
we found it is because in some cases the two metric time series fed to the offline stage are often both anomaly free.
consequently no abnormal patterns will be learned disabling adsketch to detect anomalies in the online stage.
therefore when designing an anomaly detection algorithm adaptability is indispensable.
.
.
rq3 the effectiveness of adsketch s adaptive pattern learning.this research question looks into the issue of online adaptability.
particularly we only compare adsketch with loda which is the only baseline method with the design of online learning.
similar to rq2 we only conduct experiments with aiops18 and industry datasets.
table 5shows the experimental results where we can see adsketch s adaptive pattern learning indeed brings performance gains.
with more anomalous patterns identified adsketch is able to detect anomalies more accurately i.e.
a better precision .
in aiops18 and .
in industry .
the average f1 score also enjoys some improvements i.e.
.
in aiops18 and .
in industry.
particularly in the industrial case adaptive adsketch achieves a performance of over .
in all evaluation metrics even in some cases without any abnormal patterns learned from the offline stage .
such an achievement indicates its potential to meet the industrial requirements of performance anomaly detection.
on the other hand the online version of loda does not show much performance improvement i.e.
an average f1 score of .
in aiops18 and .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhuangbin chen jinyang liu yuxin su hongyu zhang xiao ling yongqiang yang and michael r. lyu pattern length m .
.
.
.
.
.9avg.
f1 score .
.
.
.
.
.
.
percentile threshold p .
.
.
.
.
.
offline online adaptive online figure parameter sensitivity in industry which even falls behind some methods without the capability of online learning.
.
.
parameter sensitivity.
in adsketch there are only two parameters to tune both in algorithm i.e.
the pattern length mand the percentile threshold pfor identifying deviated metric subsequences.
we evaluate the sensitivity of adsketch to these two parameters by conducting experiments with different settings.
due to space limitations we only show the results of the industry dataset.
the default value of mandpfor the dataset is and .5th respectively.
we fix one parameter and employ a different setting for the other one.
specifically mranges from to and p varies from 97th to .8th.
fig.
6presents the results.
performance degradation is observed in both offline and online stages when the two parameters deviate from their default setting.
the offline stage exhibits a greater sensitivity and thus less anomalous metric patterns are captured.
nevertheless both the online anomaly detection and adaptive pattern learning algorithms achieve stable performance with a smaller set of abnormal patterns.
this further confirms adsketch s capability of new pattern discovery.
industrial practice .
online deployment since october adsketch has been successfully incorporated into the performance anomaly detection system of a large scale online service system in huawei cloud.
the deployment process can be easily done by leveraging the existing data analytics pipeline for example data consumption by apache kafka and online parallel execution by apache flink .
after months of usage adsketch has demonstrated its effectiveness on metric based system troubleshooting.
a lot of positive feedback has been received from on site engineers.
particularly engineers confirmed its superiority in anomaly detection over the current algorithms e.g.
fixed thresholding moving average in operation.
one typical case is multiple benign spikes arriving suddenly and consecutively.
adsketch is able to quickly figure out that such recurrent spikes have happened before which reduces the number of false alerts.
in terms of issue understanding engineers benefited from adsketch by having readily available descriptions about the anomaly symptoms.
therefore we have initialized a project of metric pattern database construction.
adsketch is continuously accumulating anomalous patterns in the database.
moreover engineers also expressed the need for metric pattern auto correlation across different metrics.
this is because multiple anomalies collectively could constitute a .
.
.
application cpu usage .
.
.
interface throughput time0.
.
.
requests per minutefigure case study of adsketch stronger performance issue indicator.
we leave the identification of such correlations to our future work.
.
case study we provide some case studies of adsketch collected from production systems in fig.
where anomalies are indicated by the red lines.
due to space limitations we only showcase three metric time series.
clearly all anomalous metric patterns have been successfully located regardless of shape scale and length.
each metric time series possesses at least two types of anomalous patterns e.g.
level shifts and spikes.
interestingly we found the depression in the second metric can help catch a similar pattern in the third metric demonstrating the feasibility of cross metric pattern sharing.
moreover engineers confirmed that these patterns are typical based on which they can make a good guess about the ongoing issues.
for example the spikes often come from user request surge or network attack the depressions in the second and third metrics often indicate service restart or link flap.
to quantify the interpretability of adsketch we label the recurrent performance issues and employ the learned metric patterns to identify them.
as performance issues may contain uncertainty we allow one pattern to be associated with multiple labels simultaneously sec.
.
.
.
during the evaluation an anomaly interpretation is considered correct if the predicted performance issue appears in the label set.
in our experiments adsketch attains a promising f1 score of .
.
this demonstrates the potentials of adsketch in providing interpretable results to engineers which can greatly accelerate the investigation of service performance issues.
.
threats to validity we have identified the following major threats to validity.
internal threats.
the implementation and parameter selection are two critical internal threats to the validity.
to reduce the implementation threat we directly borrow the codes released by the baseline approaches.
for the proposed approach we employ peer code review i.e.
the authors are invited to carefully check the implementation for mistakes.
in terms of parameter selection we conduct multiple comparative experiments with different parameters for all methods.
we choose the parameter settings empirically based on the best results.
external threats.
the selection of the service system and the baselines are two main external threats to validity.
we choose a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive performance anomaly detection for online service systems via pattern sketching icse may pittsburgh pa usa large scale online service of huawei cloud which produces millions of metrics with diverse patterns.
moreover we detect anomalies by following the basic definition of an anomaly i.e.
the data point that deviates from the majority in a dataset.
thus adsketch is generalizable to other systems.
for baselines we select the representative ones in the literature covering a wide spectrum of techniques.
construct threats.
the main construct threat to validity is that the anomaly free input i.e.
tn to algorithm 1actually contains anomalies.
although anomaly free data are easily obtainable in reality false negatives could happen if the data are contaminated.
we alleviate this issue by applying percentile thresholding to tn.
specifically after obtaining the closest subsequence pairs in tn we break the connection between those having a distance above the percentile threshold.
thus the set of anomaly candidates i.e.
ni becomes larger.
if tnis indeed clean this operation is harmless as the isolated normal metric subsequences can be grouped with other similar ones again if not they will stay isolated and eventually be recognized as anomalies.
we have also conducted experiments on some cases where tncontains anomalies and the results show its effectiveness.
related work performance anomaly detection on time series has been a hot topic.
monitoring metrics used to profile the runtime status of a system are usually denoted as multiple univariate time series.
in the literature anomaly detection methods on time series can be categorized into statistical traditional machine learning and deep learning approaches.
in industry autoregressive moving average model arma remains the most popular statistical method to detect obvious anomalous data points from univariate time series.
to capture complex anomalous patterns ma et al.
summarized several type oriented patterns from the metrics of cloud databases to diagnose the performance degradation in associated online services.
more complex pattern recognition methods utilize machine learning based models.
for example unsupervised clustering methods can be used to detect anomalous points in time series data.
similar to our work pang et al.
proposed a clustering based statistical model called lesinn to detect anomaly patterns from history.
however it is not robust in real industry practices due to complicated parameter tuning.
with the assumption that anomalous data should be in smaller numbers and isolated from a large number of normal observations isolation forest iforest employs multiple binary trees to distinguish anomalies in non linear space.
extreme value theory evt learns the hidden state of a random variable around the tails of its distribution to adaptively enhance the performance of many statistical and machine learning methods.
however evt heavily relies on hyperparameter tuning.
in recent years there has been an explosion of interest in applying neural networks to conduct anomaly detection on time series data.
for example zong et al.
proposed a deep autoencoding gaussian mixture model dagmm to detect anomalous data points from each observed data without considering the temporal dependencies in time series.
to detect complex anomalies in spacecraft monitoring systems lstm ndt leverages long short term memory lstm networks with nonparametric dynamic thresholding to pursue interpretability throughout the systems.
zhao etal.
and lin et al.
also employed lstm to predict performance anomalies in software systems.
inspired by the spectral residual algorithm in other domains ren et al.
proposed srcnn to detect anomalies from seasonal metric data for large scale cloud services which contain the periodic recurrence of fluctuations.
donut designs an unsupervised anomaly detection method based on the variational auto encoder vae framework to detect anomalies from low qualified seasonal metric time series with various patterns.
donut provides a theoretical explanation compared to other deep learning methods.
lstm vae combines lstm networks and the vae framework to reconstruct the probability distribution of observed data in time series.
however lstm vae ignores the temporal dependencies in time series.
omnianomaly learns the normal patterns using a large collection of historical data.
the anomalous patterns are located from the large margin of reconstruction loss to the normal patterns.
however the aforementioned deep learning based methods usually follow an end to end style and play as a black box inside.
due to poor interpretability the detection results cannot provide engineers with actionable suggestions for fault diagnosis.
furthermore all these methods have difficulties handling unseen metric patterns brought by the frequent updates of online services.
conclusion in this paper we propose adsketch a performance anomaly detection approach based on pattern sketching.
by extracting normal and abnormal patterns from metric time series anomalies can be quickly detected through a comparison with the identified patterns.
by associating metric patterns with typical performance issues adsketch can provide interpretable results when any known patterns appear again.
moreover we design an adaptive learning algorithm to help adsketch embrace unprecedented metric patterns during online anomaly detection.
we have conducted experiments on two public datasets and one production dataset collected from a representative online service system of huawei cloud.
for offline anomaly detection where models parameters are still being tuned adsketch has achieved the highest f1 score outperforming the existing methods by a significant margin.
for online anomaly detection where models are fixed adsketch safeguards its best rankings.
finally the adaptive pattern learning brings noticeable performance gains especially in the industrial dataset.
from our industrial practice we have witnessed it shedding light on accurate and interpretable performance anomaly detection which confirms its practical benefits conveyed to huawei cloud.
we believe adsketch is able to assist engineers in service failure understanding and diagnosis.
for future work we will extend our algorithms to multivariate metric time series.
we will also try to provide more detailed information about failures by exploring the correlations among the metric patterns.