arxiv .03220v2 oct 2024fairquant certifying and quantifying fairness of deep neural networks brian hyeongseok kim university of southern california los angeles usajingbo wang purdue university west lafayette usachao wang university of southern california los angeles usa abstract we propose a method for formally certifying and quantifying individual fairness of deep neural networks dnn .
individual fairness guarantees that any two individuals wh o are identical except for a legally protected attribute e.g.
g ender or race receive the same treatment.
while there are existin g techniques that provide such a guarantee they tend to suffe r from lack of scalability or accuracy as the size and input dimensi on of the dnn increase.
our method overcomes this limitation by applying abstraction to a symbolic interval based analys is of the dnn followed by iterative refinement guided by the fairness property.
furthermore our method lifts the symbo lic interval based analysis from conventional qualitative certification toquantitative certification by computing the percentage of individuals whose classification outputs are provably fair instead of merely deciding if the dnn is fair.
we have implemented our method and evaluated it on deep neural networks trained on fo ur popular fairness research datasets.
the experimental resu lts show that our method is not only more accurate than state of the art techniques but also several orders of magnitude faster.
i. i ntroduction the problem of certifying the fairness of machine learning models is more important than ever due to strong interest in applying machine learning to automated decision making in various fields from banking and healthcare to public policy and criminal justice .
since the decisions are socially sensitive it is important to certify that the mach ine learning model indeed treats individuals or groups of indiv iduals fairly.
however this is challenging when the model is a deep neural network dnn with a large number of hidden parameters and complex nonlinear activations.
the challen ge is also exacerbated as the network size and input dimension increase.
in this work we aim to overcome the challenge by leveraging abstract interpretation techniques to certify fairness both qualitatively and quantitatively.
our work focuses on individual fairness which at a high level requires that similar individuals are treated simil arly .
here similar individuals are those who differ only in some legally protected input attribute e.g.
gender or race but agree in the unprotected attributes and being treated similarly means that the dnn generates the same classification output.1let the dnn be a function f x yfrom input domain x to output range y where an individual x xis an input and a class label y yis the output.
assume that each input 1this notion can be understood as causal fairness or depen dency fairness which is a non probabilistic form of counterf actual fairness .certification problem an bracketle tf xj x an bracketri ht p stacks to certify?fair unfair and undecided rates initial partition p x added to s no yes certification subproblem an bracketle tf xj p an bracketri ht abstraction forward analysis refinement backward analysis quantification rate computation undecidedfair unfairnew partitions pl pu added to s fig.
.
fairquant for certifying and quantifying fairness of a dnn model f wherexjis a protected attribute and xis the input domain.
x an bracketle tx1 ... x d an bracketri htis ad dimensional vector and xj where j d is a protected attribute.
we say that the dnn isprovably fair certified for the entire input domain xif f x f x holds for any two individuals x xandx x that differ only in xjbut agree in the unprotected attributes xiwherei ne ationslash j .
conversely the dnn is provably unfair falsified for input domain xiff x ne ationslash f x holds for any two individuals x xandx x that differ only in the protected attribute.
if the dnn is neither certified nor fals ified it remains undecided .
given a dnn f a protected attribute xj and an input domainx aqualitative certification procedure aims to determine whether fisfair unfair orundecided for allx x. qualitative analysis is practically important because if fis provably fair the model may be used as is but iffis provably unfair the model should not be used to make decision for any x x. when the result of qualitative analysis is undecided however there is a need for quantitative analysis to compute the degree of fairness.
for example the degree of fairness m ay be measured by the percentage of individuals in input domain xwhose classification outputs are provably fair.
both qualitative certification and quantitative certification are hard problems for deep neural networks.
while there are many verification tools for deep neural networks existing verifiers such as reluval deeppoly and crown focus on certifying perturbation robustness which is a fundamentally different property and cannot cer tifyindividual fairness.
to the best of our knowledge the only existing technique for certifying individual fairness of a dnn isfairify .
however since it directly analyzes the behavior of a dnn in the concrete domain using the smt solver the computational cost is extremely high as a result fairify can only certify tiny networks.
furthermore it cannot quantif y the degree of fairness.
prior works on quantitative analysi s of fairness focus on either testing which d o not lead to sound certification or statistical parity that concerns another type of fairness group fairness whi ch differs significantly from individual fairness.
to fill the gap we propose the first scalable method forcertifying andquantifying individual fairness of a dnn.
our method named fairquant takes a certification problem consisting of the dnn f protected attribute xj and input domainx as input and returns one of the following three outputs certified fair for all input x x falsified unfair for all input x x or undecided meaning thatfis neither fair nor unfair .
in the third case our method also computes the percentage of inputs inxwhose classification outputs are provably fair.
more specifically our method provides a lower bound of the certified percentage which can guarantee that the dnn meets a certain requirement e.g.
the dnn is individually fair for at least of all inputs in x. as shown in fig.
fairquant iterates through three steps abstraction forward analysis refinement backward anal ysis and quantification rate computation .
assuming that the legally protected attribute xjhas two possible values e.g.
male and female forward analysis tries to prove that for e ach xin the input partition p which is the entire input domain xinitially flipping the value of the protected attribute of xdoes not change the model s output.
this is accomplished by propagating two symbolic input intervals i x pthat are male and i x pthat are female to compute the two corresponding output intervals that are overapproxima ted.
if the classification labels for all xandx are the same our method returns certified fair .
on the other hand if the classification labels for all xandx are different our method returns falsified unfair .
in these two cases of the inputs in the partition pare resolved.
otherwise we perform refinement backward analysis by splittingpinto partitions plandpuand apply forward analysis to each of these new partitions.
since smaller partition s often lead to smaller approximation errors refinement has the potential to increase the number of certified or falsifie d inputs and decrease in the number of undecided inputs.
to ensure that our method terminates quickly we propose two early termination conditions based on the refinement depth of each partition p x. the refinement depth is the number of timesxis partitioned to produce p. there are two predefined thresholds.
once the refinement depth exceeds the higher threshold we classify the partition pasundecided and avoid splitting it further.
but if the refinement depth exceeds the lower threshold without exceeding the higher threshold we use random sampling to try to find a concrete example x pthat violates the fairness property.
if such a counterexamp le is found we classify pasundecided and avoid splitting it further.
otherwise we keep splitting pinto smaller partitions.
we have evaluated our method on a large number of deep neural networks trained using four widely used datase ts for fairness research bank for predicting marketing german for predicting credit risk adult for predicting earning power and compas for predicting recidivism risk .
for comparison we apply fairify since it represents the current state of the art in certifying i ndividual fairness we also apply crown since it is currently the best robustness verifier for deep neural networks.
our resul ts show that crown is not effective in certifying individual fairness.
as for fairify our method fairquant significantly outperforms fairify in terms of both accuracy and speed for all dnn benchmarks.
in fact fairquant often completes certification in seconds whereas fairify often times out after minutes and certifies nothing or only a tiny fraction of the entire input domain.
to summarize this paper makes the following contributions we propose the first scalable method for certifying and quantifying individual fairness of dnns using symbolic interval based analysis techniques.
for forward analysis we propose techniques for more accurately deciding if the dnn is fair unfair for all inputs in an input partition.
for refinement we propose techniques for more effectively deciding how to split the input partition.
for quantification we propose techniques for efficiently computing the percentages of inputs whose outputs can be certified and falsified.
we demonstrate the advantages of our method over the current state of the art on a large number of dnns trained using four popular fairness research datasets.
the remainder of this paper is organized as follows.
first we motivate our work in section ii using examples.
then we present the technical background in section iii.
next we present the high level procedure of our method in section iv followed by detailed algorithms of the abstraction refinem ent and quantification subroutines in sections v vi and vii.
we present the experimental results in section viii review th e related work in section ix and finally give our conclusions in section x. ii.
m otivation in this section we use an example to illustrate the limitations of existing methods.
a. the motivating example fig.
left shows a dnn for making hiring decisions.
it has three input nodes i1 i2andi3 two hidden neurons h1 andh2 and one output node o .
the values of h1andh2 are computed in two steps first the values of i1 i2andi3 are multiplied by the edge weights before they are added up then the result is fed to an activation function.
for instan ce i1 i2 i3h2h1 oi1 i2 i1 .
.
.
.
.
.
.
.0i1 i2 i3h2h1 oi1 i2 i1 .
.
.
.
.
.
.
.
fig.
.
symbolic interval analysis of an example dnn for maki ng hiring decisions the left figure is for female applicants i2 and the right figure is for male applicants where i2 .
except for the protected attributei2 the symbolic intervals of the other attributes are the same .
the activation function may be relu z max z .
the output of the entire network fis based on whether the value ofois above that is positive label is generated if o otherwise negative label is generated.
the dnn takes an input vector xwith three attributes x1 is the interview score of the job applicant x2is the gender for female and for male and x3is the number of years of experience.
furthermore x2is the protected attribute while x1 andx3are unprotected attributes.
in general the input domain may be unbounded e.g.
when some attributes are real value d variables.
however for illustration purposes we assume t hat the input domain is x x x1 x2 andx3 meaning that xhas a total of5 individuals.
consider the individual x an bracketle t5 an bracketri ht meaning that x1 x2 andx3 .
according to the dnn in fig.
the output is the positive label.
after flipping the value of the protected attribute x2from to we have the individual x an bracketle t5 an bracketri ht for which the dnn s output is also the positive label.
since the dnn s output is oblivious to the gender attribute we say that it is fair for this input x. consider another individual x an bracketle t1 an bracketri htwhose genderflipped counterpart is x an bracketle t1 an bracketri ht.
since the dnn produces the negative label as output for both it is still fair for thi s inputx.
to summarize the dnn fmay be fair regardless of whether a particular x xreceives a positive or negative output as long asxreceives the same label as its counterpart x the dnn is considered fair.
in contrast since the individual x an bracketle t1 an bracketri htand its counterpart x an bracketle t1 an bracketri htreceive different outputs from f wherexgets the positive label but x gets the negative label the dnn is not fair for this input x. furthermore this pair x x serves as a counterexample.
b. limitations of prior work one possible solution to the fairness certification problem as defined above would be explicit enumeration of the x x pairs.
for each x x we may flip its protected attribute to generate x and then check if f x f x .
however since the size of the input domain xmay be extremely largeor infinite this method would be prohibitively expensive in practice.
another possible solution is to leverage existing dnn robustness verifiers such as reluval deeppoly and crown .
however since robustness and individual fairness are fundamentally different properties a pplying a robustness verifier would not work well in practice.
the reason is because a robustness verifier takes an individual x and tries to prove that small perturbation of x often defined by x x where is a small constant does not change the output label.
however during fairness certification w e are not given a concrete individual x instead we are supposed to check for all x xandx x wherexj ne ationslash x j. if we force a robustness verifier to take a symbolic input i x x it would try to prove that the dnn produces the same output label for all inputs in x implying that the dnn makes the same decision for all inputs in x .
recall our example network fin fig.
.
while our method can prove that fis fair for an input domain that contains x an bracketle t5 an bracketri htandx an bracketle t5 an bracketri ht both receive a positive outcome as well asx an bracketle t1 an bracketri htandx an bracketle t1 an bracketri ht both receive a negative outcome this cannot be accomplished by a robustness verifi er since it is almost never possible for all individuals in the input domain to have the same outcome .
the only currently available method for qualitatively ce rtifying individual fairness of a dnn is fairify which relies on the smt solver and may return one of the following results sat meaning that there exists a counterexample th at violates the fairness property unsat meaning that there is no counterexample or unknown meaning that the result remains inconclusive .
the main problem of fairify is that it works directly in the concrete domain by precisely encoding the non linear computations inside the dnn as logical formulas and solving these formulas using the smt solver.
since each call to the smt solver is np complete the overall computational cost is high.
although fairify attempts to reduce the computational cost by partitioning input doma in a priori and heuristically pruning logical constraints it does not scale as the network size and input dimension increase.
indeed our experimental evaluation of fairify shows that only tiny networks with 100neurons can be certified.
c. novelty of our method we overcome the aforementioned accuracy and scalability limitations by developing a method that is both scalable and able to quantify the degree of fairness.
first fairquant relies on abstraction to improve efficiency scalability while maintaining the accuracy of symbolic forward analysis.
this increases the chance of quickl y certifying more input regions as fair or falsifying them as unfair and decreases the chance of leaving them as undecide d. specifically we use symbolic interval analysis instead of the smt solver used by fairify .
the advantage is that symbolic interval analysis focuses on the behavior of the dnn in an abstract domain which is inherently more efficient and scalable than analysis in the concrete domain.input domain x partition x x1 partition x x1 .
.
.
.
.
.partition x x1 fair partition x x1 fair fig.
.
iterative refinement tree for the example dnn in fig.
to increase the chance of certifying or falsifying the dnn within an inpu t partition.
second fairquant relies on iterative refinement partitioning of the input domain to improve the accuracy of forward analysis.
instead of creating input partitions a priori it conducts iterative refinement on a need to basis guided by the fairness property to be certified.
this makes it more effecti ve than the static partitioning technique of fairify which divides the input domain into a fixed number of equal chunks even before verifying any of them.
to see why iterative refinement can improve accuracy consider our running example in fig.
.
initially forward analysis is applied to the dnn in the entire input domain x for which the certification result is undecided .
during refinement our method would choose x1 overx3 to split based on its impact on the network s output.
after splitting x1 intox1 andx1 we apply forward analysis to each of these two new partitions.
as shown in fig.
while the partition for x1 remains undecided the partition for x1 is certified as fair.
this partition has pairs of x xandx x where x2 ne ationslash x .
therefore from the input domain xwhich has x x pairs we certify asfair.
next we split theundecided partition x1 intox1 and x1 and apply forward analysis to each of these two new partitions.
while the first new partition remains undecided the second one is certified as fair.
since this partition has six x x pairs it represents of the input domain.
this iterative refinement process continues until one of the following two termination conditions is satisfied eith er there is no more partition to apply forward analysis to or a predetermined time limit e.g.
minutes is reached.
iii.
p reliminaries in this section we review the fairness definitions as well as the basics of neural network verification.
a. fairness definitions letf x ybe a classifier where xis the input domain andyis the output range.
each input x xis a vector in thed dimensional attribute space denoted x an bracketle tx1 ... x d an bracketri ht where1 ... d are vector indices.
each output y yis a class label.
some attributes are legally protected attrib utes e.g.
gender and race while others are unprotected attrib utes.
letpbe the set of vector indices corresponding to protected attributes in x. we say that xjis a protected attribute if and only ifj p.definition individual fairness for a given input given a classifier f an input x x and a protected attribute j p we say that fis individually fair for xif and only if f x f x for anyx xthat differs from xonly in the protected attributexj.
this notion of fairness is local in the sense that it requires the classifier to treat an individual xin a manner that is oblivious to its protected attribute xjofx.
definition individual fairness for the input domain given a classifier f an input domain x and a protected attribute j p we say that fis individually fair for the input domain xif and only if for all x x f x f x holds for any x xthat differs from xonly in the protected attributexj.
this notion of fairness is global since it requires the classifier to treat all x xin a manner that is oblivious to the protected attribute xjofx.
the method of explicit enumeration would be prohibitively expensive since the number of individuals in xmay be astronomically large or infinite.
b. connecting robustness to fairness perturbation robustness which is the most frequently checked property by existing dnn verifiers is closely relat ed to the notion of adversarial examples.
the idea is that if the dnn s classification output were robust then applying a small perturbation to a given input xshould not change the classifier s output for x. definition robustness given a classifier f an input x x and a small constant we say that fis robust against perturbation if and only if f x f x holds for all x x such that x x .
by definition perturbation robustness is a local property defined for a particular input x where the set of inputs defined by x x is not supposed to be large.
while in theory a robustness verifier may be forced to check individual fairne ss by setting to a large value e.g.
to include the entire input domainx it almost never works in practice.
the reason is because by definition such a global robustness property requires that all inputs to have the same classification outp ut returned by the dnn such a classifier fwould be practically useless.
this observation has been confirmed by our experiments using crown a state of the art dnn robustness verifier.
toward this end we have created a merged network that contains two copies of the same network with one input for one protected attribute group e.g.
male and the other input for the other group e.g.
female .
while the verifier finds counterexamples in seconds and thus falsifies fairnes s of the dnn it has the same limitation as fairify it merely declares the dnn as unsafe unfair in our context as soon as it finds a counterexample but does not provide users with any meaningful quantitative information.
in contrast our me thod provides a quantitative framework for certified fairness by reasoning about all individuals in the input domain.iv.
o verview of ourmethod in this section we present the top level procedure of our method detailed algorithms of the subroutines will be presented in subsequent sections.
let the dnn y f x be implemented as a series of affine transformations followed by nonlinear activations where each affine transformation step and its subsequent nonlinea r activation step constitute a hidden layer.
let lbe the total number of hidden layers then f fl fl ...f2 f1 x w1 w2 ... wl .
for each k wkdenotes the affine transformation and fk denotes the nonlinear activation.
more specifically w1consists of the edge weights at layer and x w1 ixiw1 i. furthermore f1is the activation function e.g.
relu x w1 max x w1 .
a. the basic components similar to existing symbolic interval analysis based dnn verifiers our method consists of three basic components forward analysis classification and refineme nt.
a forward analysis the goal of forward analysis is to compute upper and lower bounds of the network s output for all inputs.
it starts by assigning a symbolic interval to each input attribute.
for example in fig.
i1 is symbolic where x1 .
compared to concrete values symbolic values have the advantages of making the analysis faster and more scalable.
they are also sound in tha t they overapproximate the possible concrete values.
b classification in a binary classifier e.g.
the dnn in fig.
for making hiring decisions the output is a singular nodeowhose numerical values needs to be turned to either the positive or the negative class label based on a threshold value say .
for example if o the output label is guaranteed to be positive since o 0always holds and if o the output label is guaranteed to be negative sinceo 0always holds.
however if o the output label remains undecided this is when our method needs to conduct refinement.
c refinement the goal of refinement is to partition the input domain of the dnn to improve the upper and lower symbolic bounds computed by forward analysis.
since approximation error may be introduced when linear bounds are pushed through nonlinear activation functions e.g.
u nless relu is always on or always off by partitioning the input domain we hope to increase the chance that activation functions behave similar to their linear approximations for eac h of the new and smaller input partitions thus reducing the approximation error.
b. the top level procedure algorithm shows the top level procedure whose input consists of the network f the protected attribute xj and the input domain x. together these three parameters define the fairness certification problem denoted an bracketle tf xj x an bracketri ht.
within the top level procedure we first initialize the inpu t partition pasxand push it into the stack s. each input partition is associated with a refinement depth.
since pisalgorithm overview method of fairquant input neural network f protected attribute xj input domain x result rcer rfal rund which are percentages of certified falsified and undecided inputs 1initial partition p xwith refinement depth 2pushpinto an empty stack s initially undecided 3rcer rfal rund 4whilesis not empty and not yet timed out do poppfrom the stack s certify current partition 6result symbolic forward f xj p ifresult undecided then split current partition pl pu backward refinement f p pushplandpuinto the stack s update the percentages 10rcer rfal rund quantify fairness result p x 11returnrcer rfal rund initially the entire input domain x its refinement depth is set to .
subsequently the refinement depth increments ever y timepis bisected to two smaller partitions.
in general the refinement depth of p xis the number of times that xis bisected to reach p. in lines of algorithm we go through each partition stored in the stack s until there is no partition left or a time limit is reached.
for each partition p we first apply symbolic forward analysis line to check if the dnn fis fair for all individuals in p. there are three possible outcomes fair certified meaning that f x f x for allx pand its counterpart x unfair falsified meaning that f x ne ationslash f x for allx pand its counterpart x or undecided .
next if the result is undecided line we apply backward refinement by splitting pinto two disjoint new partitions pl andpu.
by focusing on each of these smaller partitions in a subsequent iteration step we hope to increase the chance of certifying it as fair or falsifying it as unfair .
finally we quantify fairness line by updating the percentages of certified rcer falsified rfal and undecided rund inputs of x. specifically if the previously undecided partitionpis now certified as fair we decrease the undecided raterundby p x and increase the certified rate rcerby the same amount.
on the other hand if pis falsified as unfair we decrease rundby p x and increase the falsified rate rfalby the same amount.
in the next three sections we will present our detailed algo rithms for forward analysis section v backward refinemen t section vi and quantification section vii .
c. the correctness before presenting the detailed algorithms we would like to make two claims about the correctness of our method.
the first claim is about the qualitative result of forward analysis which may be fair unfair or undecided.
theorem when forward analysis declares an input partitionp xasfair the result is guaranteed to be sound in thatf x f x holds for all x pand its counterpartx similarly when forward analysis declares pasunfair the result is guaranteed to be sound in that f x ne ationslash f x holds for allx pand its counterpart x .
the above soundness guarantee is because s ymbolic forward soundly overapproximates the dnn s actual behavior.
that is the upper bound ubis possibly bigger than the actual value and the lower bound lb is possibly smaller than the actual value.
as a result the symbolic interval computed by s ymbolic forward guarantees to include all concrete values.
in the next three sections we shall discus s in more detail how the symbolic interval is used to decide if p isfair unfair orundecided .
when an input partition pisundecided it means that some individuals in pmay be treated fairly whereas others in pmay be treated unfairly.
this brings us to the second claim about thequantitative result of our method represented by the rates rcer rfalandrund.
theorem the certification rate rcercomputed by our method is guaranteed to be a lower bound of the percentage of inputs whose outputs are actually fair.
similarly the fals ified raterfalis a lower bound of the percentage of inputs whose outputs are actually unfair.
in other words when our method generates the percentages offairandunfair inputs it guarantees that they are provable lower bounds of certification and falsification respective ly.
the reason is because s ymbolic forward soundly overapproximates the actual value range.
when the output interval s indicate that the model is fair unfair for all inputs in p it is definitely fair unfair .
thus both rcerandrfalare guaranteed to be lower bounds.
since the sum of the three rates is meaning that rund rcer rfal the undecided rate rundis guaranteed to be an upper bound.
v. s ymbolic forward analysis algorithm shows our forward analysis subroutine which takes the subproblem an bracketle tf xj p an bracketri htas input and returns the certification result as output.
algorithm subroutine s ymbolic forward input neural network f protected attribute xj input partition p result certification result which may be fair unfair undecided 1i p xj andi p xj 2o forward pass f i for x pwithxj 3o forward pass f i for x pwithxj 4if olb o lb oub o ub then 5result fair 6else if olb o ub oub o lb then 7result unfair 8else 9result undecided 10returnresult a. the two steps our forward analysis consists of two steps.
first a standar d symbolic interval based analysis is invoked twice for the0 oo both negative fair o o both positive fair o o oneg o pos unfair o oopos o neg unfair fig.
.
sufficient conditions for deciding fairness based on symbolic output intervals oando and the threshold there are two fair conditions left and two unfair conditions right .
symbolic inputs iandi to compute the corresponding symbolic outputs oando .
second oando are used to decide if the certification result is fair unfair orundecided .
in the first step the symbolic input i p xj is defined as the subset of input partition pwhere all inputs have the protected attribute xjset to .
in contrast i p xj is defined as the subset of pwhere all inputs have xjset to .
the output ois a sound overapproximation of f x for x i whereas the output o is a sound overapproximation off x forx i .
the subroutine f orward pass used to compute oando is similar to any state of the art neural network verifier based on symbolic interval analysis in our implementation we used the algorithm of reluval .
in the second step the two output intervals o ando o lb o ub are used to compute the certification result.
to understand how it works recall that in the concre te domain the numerical value of the dnn s output node is compared against a threshold say to determine if the outp ut label should be positive or negative.
in the symbolic interv al abstract domain the upper and lower bounds of the numerical values are used to determine if the model is fair unfair or undecided.
below are the five scenarios ifolb 0ando lb bothoando have the positive label meaning that fis fair for p. ifoub 0ando ub bothoando have the negative label meaning that fis fair for p. ifolb 0ando ub ois positive but o is negative meaning that fis unfair for p. ifoub 0ando lb ois negative but o is positive meaning that fis unfair for p. otherwise fremains undecided for p. fig.
illustrates the first four scenarios above.
specifical ly the horizontal line segments represent the value intervals ofo ando whose upper lower bounds may be either 0or .
the vertical lines represent the threshold value .
we can extend out method from two protected attribute pa groups e.g.
male and female to more than two pa groups.
for example if the protected attribute xjhas three values we will have three symbolic inputs i i andi and three corresponding symbolic outputs o o ando .
to decide if fis fair or unfair in this multi pa group setting we check if individuals in each pa group receive the same output label and the output labels for the three pa groups are th e same.we can also extend our method from binary classification to multi valued classification.
for example if there are th ree possible output labels we will have o1 o2 ando3as the symbolic intervals for the three values for one pa group i ando o 2ando 3for the other pa group i .
to decide iffis fair or unfair for this multi valued classification we check which output labels are generated for iandi and whether these two output labels oando are the same.
b. the running example for our running example in fig.
consider the initial input partition p x. for ease of understanding we denote the symbolic expressions for a neuron nassin n after the affine transformation and as s n after the relu activation.
furthermore swill be used for i ands will be used for i .
leti p xj 0andi p xj .
after affine transformation in the hidden layer we have sin h1 2x1 .2x3 ands in h1 2x1 .2x3 .
.
if we concretize these symbolic expressions we will have sin h1 and s in h1 .
based on these concrete intervals we know that h1is always active for both iandi .
since the activation function is relu we have s h1 sin h1 and s h1 s in h1 .
for the hidden neuron h2 we have sin h2 .2x1 .4x3ands in h2 .2x1 .4x3 .
whose corresponding concrete bounds are and respectively.
in both cases since h2is nonlinear neither always on nor always off we must approximate the values using linear expressions to obtain s h2 ands h2 .
while we use the sound overapproximation method of wang et al.
other techniques e.g.
may also be used.
after overapproximating the relu behavior of h2 we obtains h2 .128x1 .257x3 .128x1 .257x3 .
ands h2 .178x1 .357x3 .
.178x1 .357x3 .
.
finally we compute sin o .528x1 .017x3 .
.528x1 .017x3 ands in o .578x1 .117x3 .
.578x1 .117x3 .
.
from these symbolic bounds we obtain the concrete bounds of o ando .
since these output intervals are not tight enough to determine the output labels for iandi which are needed to decide if the model is fair or unfair for the partition p the model remains undecided.
to improve the accuracy we need to split pinto smaller input partitions and then apply symbolic forward analysis t o each partition again.
how to split pwill be addressed by the iterative backward refinement method presented in the next section.
vi.
i terative backward refinement the goal of iterative backward refinement is to split the currently undecided input partition pinto smaller partitions so that for each of these smaller partitions symbolic forwa rd analysis will obtain a more accurate result.
algorithm sho ws the pseudo code which takes the network fand the partition pas input and returns two smaller partitions plandpuasoutput.
inside this procedure lines are related to spl itting p and lines are related to early termination conditions.
algorithm subroutine b ackward refinement input neural network f input partition p result smaller partitions plandpu if any 1letcexcount be the total number of counterexamples found 2if p.depth max refinement depth then return null null do not split p 4else if p.depth min sample depth and sampled cex p then cexcount return null null do not split p 7else r r are gradient masks computed for i i 8gi backward pass f r 9gi backward pass f r 10g gi gi best input attribute to bisect 11xi argmaxxig xi ub xi lb xi 12pl p xi 13pu p xi returnpl pu a. early termination conditions in lines of algorithm we check if p.depth exceeds the predefined max refinement depth .
if the answer is yes we avoid splitting pfurther.
for example if max refinement depth it means the current partition p occupies only p x 220of the entire input domain x. by increasing the refinement depth we can decrease the percentage of undecided inputs over x. ifp.depth has not exceeded the maximal refinement depth we check if p.depth exceeds the predefined min sample depth which is set to a value e.g.
smaller than max refinement depth .
when p.depth exceeds this threshold we start searching for counterexamples in pvia random sampling.
inside the random sampling subroutine s ampled cex p shown in line we sample up to concrete inputs in p and check if xand its counterpart x satisfyf x ne ationslash f x .
if this condition is satisfied a counterexample is found but p remains undecided in this case we increment cexcount and stop splitting p. if no counterexample is found we continue splittingpinto smaller partitions.
note that in both early termination cases lines and the partition pwill be marked as undecided since we are not able to decide whether the dnn model is fair or unfair to all individuals inp.
b. splitting input intervals in lines of algorithm we split pinto smaller partitions plandpuby first identifying the input attribute xithat has the largest influence on the output lines and then bisecting its input interval xi .
our method for identifying the input attribute xiis based on maximizing the impact of an input attribute on the network s output.
one way to estimate the impact is taking the product of the gradient g xi and the input range ub xi lb xi .
inthe literature the product is often called the smear value .
unlike existing methods such as wang et al.
howeve r our computation of the smear value is different because we must consider both inputs iandi which may have different gradients.
specifically during forward analysis we store the neuron activation information in two gradient mask matrices denot ed randr wherer is if the j th neuron at i th layer is always active if it is always inactive and if it is unknown.
the neuron activation information is used later to perform backward refinement for this partition p. during refinement we first compute the two gradients gi andgi and then take the average.
our goal is to identify the input attribute that has the largest overall influence on the network s output.
c. the running example consider our running example in fig.
again.
to compute the smear value we start with the output layer s edge weight s which are .
for h1and for h2.
since the relu associated withh1is always on gi h1 andgi h1 are set to the interval .
however since the relu associated with h2is nonlinear as indicated by the gradient mask matrices rand r gi h2 andgi h2 are set to the interval .
then we propagate these gradient intervals backwardly to getgi i1 gi i1 .
.
.
.
andgi i2 gi i2 andgi i3 gi i3 .
next we compute the average g based on which we compute the smear values.
since x1has the smear value of .
.4andx3has the smear value of .
.
we choose to partition pby bisecting the input interval of x1.
this leads to the smaller partitions shown in fig.
.
d. generalization while we only consider relu networks in this paper our refinement technique can be extended to non relu activations.
recall that by definition relu z inactive if z and relu z active if z .
let z be a non relu activation function.
to compute the gradient mask matricesrandr we use thresholds 1 2 to approximate the on off behavior the mask is inactive if z 1and active if z 2. although the approximate on off behavior of non relu activation function z is not the same as the on off behavior of relu z it serves as a practically useful heuristic to rank the input attributes.
furthermore this generalization wi ll not affect the soundness of our method since the gradient masks computed in this manner are only used for picking which input attribute to split first.
vii.
f airness quantification we now present our method for updating the percentages of certified and falsified inputs when the dnn model is found to be fair or unfair for the current input partition p. the pseudo code is shown in algorithm .algorithm subroutine q uantify fairness input certification result input partition p input domain x result percentages rcer rfal rund 1partition size producttext xi p xi negationslash xj ub xi lb xi 2domain size producttext xi x xi negationslash xj ub xi lb xi percentage of inputs in partition p 3rp partition size domain size 4ifresult fair then 5rcer rp 6rund rp 7else ifresult unfair then 8rfal rp 9rund rp 10returnrcer rfal rund there are three cases.
first if the current partition pis found to be fair meaning that all inputs in pare treated fairly we compute the percentage of input domain xcovered by the partition p denoted rp and then add rptorcer the percentage of certified inputs.
second if the current parti tion pis found to be unfair meaning that all inputs in pare treated unfairly we add rptorfal the percentage of falsified inputs.
in both cases we also subtract rpfromrund.
otherwise the current partition premains undecided and the percentages remain unchanged.
consider our running example with input partition pdefined asx1 x2 x3 as shown by the right child of the root node in fig.
.
this partition has a total of individuals and its corresponding i p x2 andi p x2 1contain individuals each.
in contrast the entire input domain xhas individuals or pairs of x and its counterpart x wherex2 ne ationslash x .
for this input partition p o ando are the output intervals.
assuming that the decision threshold is the bounds of oando imply that the dnn model will generate the positive label for both iandi meaning that the dnn model is fair for all individuals in p. since the input partition size is and the input domain size is the rate rp .
after certifying pto be fair we can add to rcer the certification rate and consequently subtract from rund the undecided rate.
while the above computation assumes that population distribution for each feature is uniform and thus the percentag e e.g.
is computed directly from the partition size e. g. and the domain size e.g.
the method can be easily extended to consider a non uniform population distributio n. furthermore note that the method works regardless of wheth er the input attributes have integer or real values.
viii.
e xperiments we have implemented fairquant in a software tool written in c by leveraging the openblas2library for fast matrix multiplication and symbolic representation of the upper an d lower bounds.
our forward analysis follows that of wang et al.
.
for experimental comparison we also run fairify i statistics of the datasets and dnn s used in our experiments .
dataset pa inputs dnn layers neurons accuracy bank age 16bm .
bm .
bm .
bm .
bm .
bm .
bm .
bm .
german age 20gc .
gc .
gc .
gc .
gc .
adult gender 13ac .
ac .
ac .
ac .
ac .
ac .
ac .
ac .
ac .
ac .
ac .
ac .
compas race 6compas .
compas .
compas .
compas .
compas .
compas .
compas .
which is the only currently available tool for dnn individu al fairness certification.
since fairify cannot quantify the degree of fairness we compute the certified falsified undecided r ates based on its reported statistics.
it is worth noting that fairify and our method fairquant have a fundamental difference in falsification.
fairify stops and declares an input partition as sat as soon as it finds a counterexample in that partition thus the number of coun terexamples that it finds is always the same as the number of sat partitions it reports.
however sat partitions are no t necessarily unfair partitions since unfair partitions require all inputs to be counterexamples but a sat partition excludin g the one counterexample still remains undecided .
fairquant checks if an entire partition is unfair .
moreover when the partition is undecided it can minimize the amount ofundecided inputs by only sampling for counterexamples after it reaches a deep enough refinement depth.
this is made possible through our iterative refinement.
for example in a dnn model named gc fairify finds sat partitions together with unsat and unknown partitions .
however none of these sat partitions are unfair partitions.
instead the percentage of falsified inputs is close to being representing counterexamples out of over trillion individuals in the input domain the perc entage of certified inputs is .
unsat partitions out of partitions and the rest remains undecided.
fairquant on the other hand finds counterexamples furthermore it is able to formally certify .
of the inputs as fair.a.
benchmarks table i shows the statistics of the benchmarks including deep neural networks trained on four popular datasets for fairness research.
among the networks came from fairify and the other were trained by ourselves using tensorflow.
all of these networks have a single node in the output layer to determine the binary classification result.
columns show the name of each dataset with its considered protected attribute pa and the number of input attributes.
columns show the name of each dnn model its number of hidden layers number of hidden neurons and classification accuracy.
the accuracy for dnns trained on bank german and adult was provided by the fairify paper.
for the models we trained using compas we have reserved of the data for testing.
all the networks coming directly from fairify onbank german and adult datasets are small where the largest has only hidden neurons.
moreover most of them have only or hidden layers.
thus we additionally trained much larger networks with up to hidden neurons using the compas dataset.
details of the four datasets are given as follows.
bank is a dataset for predicting if a bank client will subscribe to its marketing it consists of samples.
german is a dataset for predicting the credit risk of a person it consi sts of samples.
adult is a dataset for predicting if a person earns more than it consists of sample s. finally compas is a dataset for predicting the risk of recidivism it consists of samples.
we evaluate our method using three legally protected input attributes.
for bank andgerman we use age for adult we use gender and for compas we use race.4these are consistent with fairify and other prior works in the fairness research.
b. experimental setup we ran all experiments on a computer with cpu 4gb memory and ubuntu .
linux operating system.
we set a time limit of minutes for each dnn model.
our experiments were designed to answer three research questions is fairquant more accurate than the current state of theart in certifying individual fairness of a dnn model?
is fairquant more scalable than the current state ofthe art in handling dnn models especially when the network size increases?
is fairquant more effective than the current state ofthe art in providing feedback e.g.
by quantitatively measuring the percentages of certified falsified and undecided inputs?
fairify requires a parameter ms maximum size of an input attribute based on which it creates a fixed number of input partitions prior to certification.
on the dnn models trained forbank german and adult we used the default msvalues 3we used the preprocessed compas data provided by .
4for bank and german we use binarized ageattribute provided by fairify .
for compas we binarize race attribute into white non white as done in .table ii results for fairness certification fairify vis a visfairquant dataset dnnfairify fairquant new time cex cex cer fal und time cex cex cer fal und bankbm 30m .
.
.82s .
.
bm 31m .
.
.23s .
.
bm 31m .
.
.21s .
.
bm 35m .
.
.12s .
.
bm 23m .
.
.03s .
.
bm 12m .
.
.44s .
.
bm 30m .
.
.26s .
.
bm 30m .
.
.99s .
.24germangc 32m .73s .
.
gc 33m .72s .
.
gc 8m .
.
.77s .
.
gc 4m .
.
.29s .
.
gc 30m .24s .
.19adultac 32m .
.
.23s .
.
ac 31m .
.
.04s .
.
ac 32m .12s .
.
ac 36m 8m .
.
ac 33m 4m .
.
ac 33m .
.
.20s .
.
ac 30m .
.
4m .
.
ac 30m .
.
.18s .
.
ac 30m .
.
.50s .
.
ac 32m .
.
.01s .
.
ac 30m .44s .
.
ac 30m .
.
.91s .
.17compascompas 17m .
.
.
.01s .
.
compas 31m .01s .
.
compas 30m .30s .
.
compas 30m .01s .
.
compas t o .24s .
.
compas m o .19s .
.
compas m o .25s .
.
and for fairify to create and partitions respectively.
on the new dnn models trained for compas we set msto a small value of to create partitions forfairify .
this was done to maximize fairify s performance such that it does not choke in verifying each input partiti on.
by default fairify uses seconds as soft timeout for each input partition and uses minutes as hard timeout fo r the entire dnn.
this means that it takes at most seconds to verify a single input partition and if unsolved it just m oves to the next partition until the entire minutes runs out.
to run fairquant we set the parameters min check depth to and max refinement depth to for all dnn models.
we also use minutes as hard timeout but fairquant always finished before the limit.
c. experimental results table ii shows the results of our method fairquant in comparison with fairify5.
columns show the names of the dataset and the dnn model.
columns show the statistics reported by fairify including the time taken whether a counterexample is found cex and the number of counterexamples found cex .
t o or m o in column respectively means that fairify either spent all 30m or ran out of memory in the network pruning step prior to verifying any input partition .
columns show the percentage of certified falsified and undecided inputs cer fal und .
columns show the corresponding results from fairquant .
5the order in which fairify sorts the partitions before running the verification query is random and non deterministic so there may be m inor difference in the reported counterexamples in the original evaluation and ours.bm gc ac compas 10102030runtime m bm gc ac compas 1020406080100undecided fig.
.
comparing the runtime overhead left and accuracy right of fairify in red and fairquant new in blue.
results for rq to answer the first research question rq i.e.
whether our method is more accurate than the current state of the art we need to compare the results sh own in columns for fairify with the results shown in columns for fairquant .
specifically columns and indicate whether the tool is able to find a counterexample or not within the time limit.
while our method fairquant found counterexamples for all dnns fairify found counterexamples for only of the models.
in addition it found counterexamples for only on e of the newly added models.
moreover columns and show that on models where both tools found counterexamples the number of counterexamples found by fairquant is often thousands of times more.
for example the largest number of counterexamples found by fairify is for gc but the large number of counterexamples found by fairquant is for ac .
results for rq to answer the second research question rq i.e.
whether our method is more scalable than th e current state of the art we need to compare the running ti me shown in column for fairify with the running time shown in column for fairquant .
while our method fairquant always finished within the time limit of minutes fairify timed out on compas and ran out of memory on compas and compas .
even on the models where both tools finished the time taken by fairify is significantly longer.
to illustrate the scalability advantage of our method we took a subset of the models for which both fairquant and fairify finished and plot the running time in a bar chart shown on the left side of fig.
.
here the red bars represent the tim e taken by fairify and the blue bars represent the time taken by fairquant .
the results show that fairquant is many orders of magnitude faster and can certify dnn models that are well beyond the reach of fairify .
results for rq to answer the third research question rq i.e.
whether our method is more effective in providi ng feedback to the user we need to compare the results in columns for fairify with the results in columns for fairquant which show the certified falsified and undecided percentages.
since fairify was not designed to quantitatively measure the degree of fairness it did poorl y in almost all cases.
except for a few dnn models for bank gc and compas its certified percentages are either or close to and its undecided percentages are almost .
it means that for the vast majority of individuals in the input domain whether they are treated by the dnn model fairly or not remains undecided.
in contrast the certified percentages reported by our metho d fairquant are significantly higher.
for the models trained using the compas dataset in particular the certified perce ntages are around or above and more importantly the undecided percentages are always .
it means that fairquant has partitioned the input domain in such a way that each partition is either certified as being fair or falsified as be ing unfair.
even on the subset of dnn models where some inputs remain undecided by fairquant the undecided percentages reported by our method are significantly lower than fairify as shown on the right side of fig.
.
d. summary the results show that our method is more accurate and more scalable than the current state of the art technique s for qualitative certification.
in addition our method is able to formally quantify the degree of fairness which is a capability that existing methods do not have.
for some dnn models fairquant still has a significant percentage of inputs left undecided.
this is because we set min check depth and max refinement depth for all benchmarks.
thus as soon as fairquant reaches the refinement depth see the refinement tree shown in fig.
and finds a counterexample in the input partition it will stop refinin g further at that moment all inputs in the partition are trea ted conservatively as undecided.
in general a smaller refinement depth allows fairquant to terminate quickly.
during our experiments fairquant terminated after .29s and .24s for gc and gc respectively compared to the minutes and minutes taken by fairify and yet returned better results.
in fact for gc fairify spent minutes but failed to find any counterexample.
if we increase the refinement depth by increasing the two threshold values offairquant its quantification results will get even better.
ix.
r elated work our method is the first scalable method for certifying and quantifying individual fairness of a deep neural network a nd it outperforms the most closely related prior work fairify .
to the best of our knowledge no other methods can match the accuracy scalability and functionality of our method.
our method differs from existing techniques for verifying individual fairness properties for neural networks.
libra uses abstract domains to perform verification but is limited in scalability due to the expensive pre analysis w here a network of hidden nodes takes several hours even with leveraging multiple cpus.
deepgemini which outperforms libra is built on top of marabou a smtbased neural network verification tool and thus shares the same limitations of fairify .
furthermore it only evaluates on networks with up to around hidden neurons.
other workshave tackled neural network verification of different defini tions of individual fairness.
benussi et al.
and khedr et al.
proposed different methods to certify a definition of global individual fairness proposed in .
ruoss et al.
veri fy a type of a local individual fairness property that is similar to local robustness given an input xand a small constant for perturbation.
this is different from a global perspective we have discussed so far in this paper.
group fairness is yet another type of fairness property which can be verified using probabilistic techniques .
the difference between individual fairness and group fairness is that while individual fairness requires similar individuals to be treated similarly group fairness requires similar demographic groups to be treated similarly.
there are other prior works related to fairness verification of other types of machine learning models but they are not applicable to deep neural networks.
testing techniques can quickly detect fairness violations in machi ne learning models including neural networks but it does not provide formal guarantee that is important for certain applications.
there are also techniq ues for improving fairness of machine learning models which are orthogonal to our met hod that focuses on certifying and quantifying fairness of exis ting dnn models.
at a high level our method is related to the large number of robustness verifiers for deep neural networks based on interval analysis smt solving and mixed integer linear programming .
while these verifiers can decide if a model is robust against adversarial perturbation they cannot directly ce rtify individual fairness as explained earlier in section ii.
ot her neural network verifiers that deal with differential or equivalence verification are also different si nce they evaluate over two networks instead of one network.
x. c onclusion we have presented fairquant a scalable method for certifying and quantifying individual fairness of a deep neura l network over the entire input domain.
it relies on sound abstraction during symbolic forward analysis to improve sc alability and iterative refinement based on backward analysi s to improve accuracy.
in addition to certifying fairness it is able to quantify the degree of fairness by computing the percentage s of inputs whose classification outputs can be certified as fai r or falsified as unfair.
we have evaluated the method on a large number of dnn models trained using four popular fairness research datasets.
the experimental results show that the method significantly outperforms state of the art techni ques in terms of both accuracy and scalability as well as the abilit y to quantify the degree of fairness.