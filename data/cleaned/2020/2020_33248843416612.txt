defect prediction guided search based software testing anjana perera anjana.perera monash.edu faculty of information technology monash university melbourne australiaaldeida aleti aldeida.aleti monash.edu faculty of information technology monash university melbourne australia marcel b hme marcel.boehme acm.org faculty of information technology monash university melbourne australiaburak turhan burak.turhan monash.edu faculty of information technology monash university melbourne australia abstract today mostautomatedtestgenerators suchassearch basedsoftwaretesting sbst techniquesfocusonachievinghighcodecoverage.however highcodecoverageisnotsufficienttomaximise the number of bugs found especially when given a limited testing budget.inthispaper weproposeanautomatedtestgenerationtechnique that isalso guided bythe estimated degree ofdefectiveness ofthesourcecode.partsofthecodethatarelikelytobemoredefectivereceivemoretestingbudgetthanthelessdefectiveparts.to measure the degree of defectiveness we leverage schwa a notable defect prediction technique.
we implement our approach into evosuite a state of the art sbsttoolforjava.ourexperimentsonthedefects4jbenchmark demonstrate the improved efficiency of defect prediction guided test generation and confirm our hypothesis that spending more time budget on likely defective parts increases the number of bugs found in the same time budget.
ccs concepts software and its engineering software testing and debugging search based software engineering.
keywords search based software testing automated test generation defect prediction acm reference format anjana perera aldeida aleti marcel b hme and burak turhan.
.
defectpredictionguidedsearch basedsoftwaretesting.in 35thieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september virtual event australia copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction software testing is a crucial step in improving software quality.
findingeffectivetestcases however isacomplextask whichisbecomingevenmoredifficultwiththeincreasingsizeandcomplexityofsoftwaresystems.automatedsoftwaretestingmakesthislabour intensivetaskeasierforhumansbyautomaticallygeneratingtest cases for a software system.
in particular search based software testing sbst techniques havebeenverysuccessfulinautomatically generating test cases and are widely used not only in academia but also in the industry e.g.
facebook .
sbsttechniquesusesearchmethodssuchasgeneticalgorithms to find high quality test cases for a particular system.
these methods focus on code coverage and research shows that sbst methodsareveryeffectiveatachievinghighcoverage .
they can even cover more code than the manually written test cases .however atestsuitewithhighcodecoveragedoes notnecessarilyimplyeffectivebugdetectionbythetestsuite.indeed previousstudiesshowthatsbstmethodsarenotaseffective in finding real bugs .
even evosuite a state of the art sbst tool could only find on average of the bugs fromthe defects4j dataset which contains bugs from java projects .
ideally sbst techniques should aim at generating test cases that reveal bugs however this is a difficult task since duringthesearchfortestcasesitisnotpossibletoassessifatest case has found a bug e.g.
semantic bugs .
in this paper we aimto enhance sbst techniques by incorporating information fromadefectpredictionalgorithmtoinformthesearchprocessofthe areasinthesoftwaresystemthatarelikelytobedefective.thus the sbst technique while it cannot tell whether the test cases itproducesareindeedfindingbugs itisabletogeneratemoretest casesforthedefectiveareas andasaresult increasesthelikelihood of finding the bugs.
defect prediction algorithms estimate the likelihood that a file class or method function i na softwaresystemisdefective.thesemethodsareveryeffectiveat identifyingthelocationofbugsinsoftware .asaresultof their efficacy defect prediction models are used to help developers focustheir limitedtesting effortoncomponents thatare themost likelytobedefective .inaddition defectpredictionhasbeen used to inform a test case prioritisation strategy g clef of the classesthatarelikelytobebuggy andfounditispromising.our 35th ieee acm international conference on automated software engineering ase workisthefirsttousedefectpredictionforimprovingautomated test case generation.
we introduce defect prediction guided sbst sbst dpg which usesinformationfromadefectpredictortofocusthesearchtowards thedefectiveareasinsoftware ratherthanspendingtheavailable computational resources i.e.
time budget to cover non defective areas.weemploy schwa asthe defectprediction approach whichcalculatesthedefectscoresbasedonthechangehistoryof the java classes.
next a budget allocation algorithm called budget allocationbasedondefectscores bads allocatesthetimebudget foreachclassbasedonthepredictionsgivenbythedefectpredictor.
at a high level it follows the basic and intuitive rule highly likely tobedefectiveclassesgetahighertimebudgetallocatedandless likely to be defective classes get a lower time budget.
finally we usedynamosa astateoftheartsbstalgorithm togenerate test suites for each class in the project by spending the allocated time budgets.
real worldprojectsareusuallyverylargeandtherecanbeeven more than classes in a project.
hence it takes significant amountofresources e.g.
time torunthesetestgenerationtools foreachclassintheproject.atthesametime theavailablecomputational resources are often limited in practice .
therefore it isnecessarytooptimallyutilisetheavailableresources e.g.
time budget togeneratetestsuitesfortheprojectswithmaximalbug detection.
existing sbst approaches allocate the available time budget equally for each class in the project .
usually most classesareclean hencewearguethatthisisasub optimalstrategy.
ourproposedapproachaddressesthisbyallocatingtheavailable time budget to each class in the project based on the class level defect prediction information.
we evaluate how our approach performs in terms of the efficiencyinfindingrealbugscomparedtothestateoftheart.second we examine if our approach finds more unique bugs.
this is particularlyimportanttoinvestigate asitwillrevealifusinginformation fromthedefectpredictioncanhelpsbstrevealbugsthatcannot be found otherwise.
we evaluate sbst dpgon reported real bugsfrom6opensourcejavaprojectsinthedefects4jdataset.our empiricalevaluationdemonstratesthatinaresourceconstrained environment whengivenatighttimebudget sbst dpgissignificantly more efficient than the state of the art sbst with a large effect size.
in particular sbst dpgfinds up to .
more bugs on average compared to the baseline approach.
in addition our approach is also able to expose more unique bugs which cannot be found by the state of the art approach.
insummary thecontributionofthispaperisanovelapproach thatcombinesdefectpredictionandsbsttoimprovethebugdetectioncapabilityofsbstbyfocusingthesearchmoretowardsthe defective areas in software.
in addition we present an empirical evaluationinvolving434realbugsfrom6opensourcejavaprojects which took roughly hours that demonstrates the efficiency of our proposed solution.
finally the source code of our proposed techniqueandthescriptsforpostprocessingtheresultsarepublicly available here related work .
search based software testing search based software testing sbst is an effective strategy for achieving high code coverage .
shamshiri et al.
and almasi et al.
studied the bug detection performance of sbst on opensourceandindustrialsoftwarerespectively.whileevosuite whichweconsiderasthestateoftheartsbsttoolgivenits maturity found more bugs than the other techniques used in their studies overall the results show that the bug detection is still a significantchallengeforsbst.particularly evosuitefoundonlyan average of bugs from the defects4j dataset .
it is clear that usingonlythe100 branchcoveragecriterionwasnotsufficient to search for test cases that can detect the bugs.
in contrast we usedefectpredictioninformationtofocusthesearchtoextensively explore the search space for test cases in defective areas.
gay studiedtheeffectofcombiningcoveragecriteriaonthe bug detection performance of sbst and found that multiple coveragecriteriaoutperformasinglecriterion.however theauthorsdid not recommenda generalstrategy toselect whichcriteria tocombine sincetheirselectionstrategiesalsoproducedmanyineffective combinations.
our work is the first approach that focuses on informingsbstofthedefectiveareastospendmoresearchresources to such areas.
thus we believe our approach will further improve the bug detection capability of the single criterion or combination of criteria.
.
defect prediction previousworkondefectpredictionhaveconsideredawiderangeofmetricssuchascodesize codecomplexity object oriented organisational and change history to predict future defects in a software project.
graves et al.
showed that the number of changes and particularly the recent changes to the code areeffectiveindicatorsoffuturedefects.kimetal.
followedthe observationthatbugsoccurinsoftwarechangehistoryasbursts hence they argue that recent changes to the code and recent faults in the code are likely to introduce bugs in the future.
rahmanetal.
proposedasimpleapproach whichwaseventually implemented by the google engineering team that ordersfilesbythenumberofbugfixcommitsinafile andfoundoutthat its performance is quite similar to the more complex approach fixcache .furthermore theyshowedthatthefilesthathave been recently involved in a bug fix are likely to contain further bugs.patersonetal.
usedanenhancedversionofthisapproach as the defect predictor to inform a test case prioritisation strategy of the classes that are likely to be buggy and found it is promising.
inparticular theyusedschwa whichpredictsdefectsinprogramsbyusingthreemetrics recentchanges recentbugfixes and recent new authors to the code.
.
budget allocation problem search based software test generation tools like evosuite generate test suites for each class in the project separately.
this is doneby running a search method such as genetic algorithm ga foreach class to maximise statement branch and method coverage oracombinationofthethree.oneofthecrucialparametersthat 449hastobe tuned isthetimebudgetfor eachclass whichisusedas a stopping criterion for the ga. allocating a higher time budget allowsthesearchmethodtoextensivelyexplorethesearchspace of possible test inputs thus increasing the probability of finding the optimum.
forsmallprojects itisfeasibletorunautomatedtestgeneration individually for each class in the project.
real world projects however are usually very large e.g.
a modern car has millions of lines of code and thousands of classes and they require a significant amount of resources e.g.
time to run the test generation toolsfor each class in the project.
even in an open source project like apache commons math there are around classes.
in a projectlikethis itwouldtakeatleast13 14hourstorunautomatedtestgenerationwithspendingjustoneminutepereachclass.atthe same time the available computational resources are often limited inpractice .thereforethisraisesthequestion howshouldwe optimallyutilisetheavailablecomputationalresources e.g.
time budget to generate test suites for the whole project with maximal bug detection?
.
previous work on bug detection performance of sbst allocated a fix time budget to test generation for each buggy class.
since the buggy classes are not known prior to running tests in practice all the classes in the project have to be allocated the same timebudget.usually mostclassesarenotbuggy henceweargue thatthisisasub optimalstrategy.ourapproachsolvesthisproblembyallocatingtimebudgettoclassesbasedontheinformationgiven by a defect predictor.
campos et al.
proposed a budget allocation based on the complexityoftheclassesinordertomaximisethebranchcoverage.in particular they used number of branches in a class as a proxy to thecomplexityoftheclass.incontrast thescopeofthisresearch is to maximise the number of bugs detection.
contrary to the previous works that considered test suite generation for a regression testing scenario we focus on generating tests to find bugs not only limited to regressions but also the bugs that are introduced to the system at various times.
defect prediction guided search based software testing defectpredictionguidedsbst sbst dpg seefigure1 usesdefect scores of each class produced by a defect predictor to focus thesearch towards the defective areas of a program.
existing sbst approachesallocatetheavailabletimebudgetequallyforeachclass intheproject .usually mostclassesarenotbuggy hence we argue that this is a sub optimal strategy.
ideally valuable resources should be spent in testing classes that are likely to be buggy henceweemployadefectpredictor knownasschwa to calculate the likelihood that a class in a project is defective.
our approach has three main modules i defect predictor dp ii budgetallocationbasedon defectscores bads andiii searchbased software testing sbst .
.
defect predictor the defect predictor gives a probability of defectiveness defect score for each class in the project.
the vector srepresents this output.inourimplementationofsbst dpg wechoose a thelevelofgranularityofthedefectpredictortobetheclasslevel and b the schwa as the defect predictor module.
paterson et al.
successfully applied schwa as the defect predictoring cleftoinformatestcaseprioritisationstrategyoftheclassesthatarelikelytobebuggy.certainlyotherdefectprediction approaches proposed in the literature e.g.
fixcache change bursts would also be suitable for the task at hand.
a strength of schwa is its simplicity and that it does not require traininga classifier which makes it easy to apply to an industrial setting where training data is not always available like the one we study .
in addition schwa can be considered as an enhancement of a tool implemented by the google engineering team .
schwausesthefollowingthreemeasureswhichhavebeenshown to be effective at producing defect predictions in the literature see section2.
i revisions timestampsof revisions recentchanges are likely to introduce faults ii fixes timestamps of bug fix commits recentbugfixesarelikelytointroducenewfaults and iii authors timestampsofcommitsbynewauthors recentchanges by the new authors are likely to introduce faults .
the schwa tool extracts this information through mining a version control system such as git .
the tool is readily available to use as a python packageatpypi .therefore giventherobustnessofthistool anditsapproach wedecidetouseitasthedefectpredictormodule in our approach.
schwa starts with extracting the three metrics revisions rc fixes fc andauthors ac for all classes c cin the project.
foreach timestamp it calculatesa timeweightedrisk twr using the equation .
twr t i exp 12ti tr the quantity tiis the timestamp normalised between and where0isthenormalisedtimestampoftheoldestcommitunder consideration and is the normalised timestamp of the latest commit.
the number of commits that schwa tracks back in versionhistory of the project n is a configurable parameter and it can take values from one commit to all the commits.
the parameter tr is called the time range and it allows to change the importance given to the older commits.
the time weighted risk formulascoresrecenttimestampshigherthantheolderones see figure .
once schwa calculated the twrs it aggregates these twrs per each metric and calculatesa weightedsum scfor eachclass c c in the project as in equation .
sc wr summationdisplay.
ti rctwr ti wf summationdisplay.
ti fctwr ti wa summationdisplay.
ti actwr ti the sum summationtext.
ti rctwr ti is the total of the time weighted risks of therevisions metric for class c. similarly summationtext.
ti fctwr ti and summationtext.
ti actwr ti arethesumsofthetwrsofthe fixesandauthors metrics for class c c. the quantities wr wf andwaare weights that modify the twr sum of each metric and their sum is equal to .
the weighted sum sc is called the score of class c c. 450project schwa defect predictordefect scorestime budgets evosuite sbst test suitebudget allocation based on defect scores bads figure defect prediction guided sbst overview 1rupdolvhg 7lphvwdps7lph hljkwhg 5lvn figure time weighted risk tr .
finally theschwatoolestimatestheprobability p c ofthata classcis defective as given in equation .
p c exp sc inthispaper werefertothisprobabilityofdefectiveness p c as the defect score of class c c. .
budget allocation based on defect scores budget allocation based on defect scores bads takes the defect scores s p c c c asinputanddecidesonhowtoallocatethe availabletimebudgettoeachclassbasedonthesescores producing a vectortas output.
ideally all the defective classes in the project should get more time budget while non defective classes can be left out from test generation.
however the defect predictor only givesanestimationoftheprobabilityofdefectiveness.therefore badsallocatesmoretimebudgettothehighlylikelytobedefective classesthantothelesslikelytobedefectiveclasses.thiswaywe expectsbsttogethighertimebudgettoextensivelyexplorefor test cases in defective classes rather than in non defective ones.
.
.
exponential time budget allocation based on defect scores.
algorithm1illustratestheproposedtimebudgetallocationalgorithm of bads where sis the set of defect scores of the classes t isthetotaltimebudgetfortheproject tministheminimumtime budgettobeallocatedforeachclass tdpisthetimespentbythe defect predictor module and ea eb andecare parameters of the exponentialfunctionthatdefinetheshapeoftheexponentialcurve.
tis the set of time budgets allocated for the classes.algorithm exponentialtime budget allocationbased on defect scores input the set of all the classes c where n c s s1 s2 ... s n t tmin tdp ea eb ec output t t1 t2 ... t n 1r assign rank s 2r prime normalise rank r 3w prime 4forallci cdo 5w prime i ea eb exp ec r prime i 6w normalise weight w prime 7t 8forallci cdo 9ti wi t n tmin tdp tmin thedefectscoresassignmentinfigure3isagoodexampleof the usual defect score distribution by a defect predictor.
usually thereareonlyafewclasseswhichareactuallybuggy.allocating higher time budgets for these classes would help maximise the bug detection of the test generation tool.
following this observationand the results of our pilot runs we use an exponential function line in algorithm to highly favour the budget allocation for the few highly likely to be defective classes.
moreover there is relatively higher number of classes which are moderately likely to be defective e.g.
.
defect score .
.
it is alsoimportanttoensurethereissufficienttimebudgetallocatedfor these classes.
otherwise neglecting test generation for these classes could negatively affect bug detection of the test generation tool.
we introduce a minimum time budget tmin to all the classes because we want to ensure that every class gets a budget allocated regardless of the defectiveness predicted by the defect predictor.
the exponential function in algorithm together with tminallow an adequate time budget allocation for the moderately likely to be defective classes.
upon receiving the defect scores s bads assigns ranks r for all the classes according to the defect scores.
next the normaliserankfunctionnormalisestheranksintherange wherethe rankofthemostlikelytobedefectiveclassis0andtheleastlikelyto 451defect scorefrequency .
.
.
.
.
.
figure3 distributionofthedefectscoresassignedbyschwa for the classes in chart bug from defects4j.
be defective class is .
then each class gets a weight w prime i assigned based on its normalised rank by the exponential function.
the amount of time budget allocated to class ciis proportional to w prime i. the parameters ea eb andechave to be carefully selected such that the weights are almost equal and significantly small for the lower ranked classes and the difference between the weights ofadjacently ranked classes rapidly increases towards the highlyrankedclasses.thenormalise weightfunctionnormalisesthe weights to the range ensuring the summation is equal to and produces the normalised weights vector w. finally bads allocatestimebudgetforeachclassfromtheremainingavailable time budget t n tmin tdp based on its normalised weight line in algorithm .
.
.
the tier approach.
according to the defect predictor outcome almostalltheclassesintheprojectgetnon zerodefectscoresattachedtothem.thisgivestheimpressionthatalltheseclassescan bedefectivewithatleastaslightprobability.however inreality thisdoesnotholdtrue.foragivenprojectversion thereareonly afewdefectiveclasses.adefectpredictorislikelytopredictthat clean classesare alsodefective witha non zero probability.while the exponential function disfavours the budget allocation for these less likely to be defective classes tminguarantees a minimum time budget allocated to them.
if we decrease tminin order to make the budgetallocationnegligibleforthe likelytobecleanclasses then it would risk a sufficient time budget allocation for the moderately likely to be defective classes.
weproposethe2 tierapproachwhichdividestheprojectinto two tiers following the intuition that only a set of classes are de fective in a project.
bads sorts the classes into two tiers before the weights assignment such that the highly likely to be defective classes are in the first tierand the less likely to be defective classes are in the second tier.
this allowsto further discriminate the less likely to be defective classes and favour the highly likely to bedefective classes by simply allocating only a smaller fraction ofthe total time budget to the second tier and allocating the rest to thefirsttier.section4.
.3providesmoredetailsontheparameter selection of the tier approach.
.
search based software testing weuseevosuite asthesearch basedsoftwaretesting sbst moduleinourdefectpredictionguidedsbstapproach.evosuite is an automated test generation framework that generates junittest suites for java classes.
it was first proposed by fraser and arcuri in since then it has gained growing interest in thesbstcommunity .itseffectivenesshasbeen evaluatedonopensourceandaswellasindustrialsoftwareprojects in terms of the code coverage and bug detection .
furthermore evosuite won out of of the sbst unit testing tool competitions .
to date evosuite is actively maintained and its source code and releases are readilyavailable to use at github and their website .
therefore given the maturity of evosuite we decide to use it as the sbst module in our approach.
morerecently panichellaetal.
developedanewsearchalgorithm dynamosa dynamicmany objectivesortingalgorithm as an extension to evosuite which stands as the current state of the art.
it has been shown to be effective at achieving high branch statement and strong mutation coverage than the previous ver sions of evosuite .
moreover dynamosa was thesearchalgorithmofevosuite whichwontheunittestingtool competitionatsbst2019 .therefore weuse dynamosa as the search algorithm in evosuite.
design of experiments we evaluate our approach in terms of its efficiency in finding bugs andtheeffectivenessinrevealinguniquebugs i.e.
bugsthatcannot befoundbythebenchmarkapproach.ourfirstresearchquestion is rq1 issbst dpgmoreefficientinfindingbugscomparedtothestate of the art?
toanswerthisresearchquestion werunasetofexperimentswhere wecompareourapproachagainstthebaselinemethoddiscussed in section .
.
all methods are employed to generate test casesfor defects4j which is a well studied benchmark of buggy programsdescribedinsection4.
.oncethetestcasesaregenerated we check if they find the bugs in the programs and report the resultsasthemeanandmedianover20runs.tocheckforstatistical significanceofthedifferencesandtheeffectsize weemploytwotailed non parametric mann whitney u test with the significance level .
andvarghaanddelaney s hatwidea12statistic .we also plot the results as boxplots to visualise their distribution.
inaddition toanalysetheeffectivenessoftheproposedapproach we seek to answer the following research question rq2 does sbst dpgfind more unique bugs?
to answer this research question we analyse the results from theexperimentsinmoredetail.whilethefirstresearchquestion focusesontheoverallefficiency inthesecondresearchquestionweaim to understand if sbst dpgis capable of revealing more unique bugs which can not be exposed by the baseline method.
part of the efficiency of our proposed method however could be due to its robustness whichismeasuredbythesuccessrate hencewealso report how often a bug is found over runs.
.
time budget in real world scenario total time budget reserved for test generation for a project depends on how it is used in the industry.
for example aprojecthavinghundredsofclassesandrunningsbst1 2minutesperclasstakesseveralhourstofinishtestgeneration.ifan organisation wants to adaptsbst in their continuous integration ci system thenithastosharetheresourcesandschedules withtheprocessesalreadyinthesystem regressiontesting code quality checks project builds etc.
in such case it is important that sbst uses minimal resources possible such that it does not idle other jobs in the system due to resource limitations.
panichella et al.
showed that dynamosa is capable of converging to the final branch coverage quickly sometimes with a lowertimebudgetlike20seconds.thisisparticularlyimportant since faster test generation allows more frequent runs and thereby itmakessbstsuitabletofitintotheci cdpipeline.therefore we decide that seconds per class is an adequate time budget for test generationand15secondsperclassisatighttimebudgetinausual resource constrainedenvironment.
weconduct experimentsfor cases of total time budgets t nand nseconds.
.
experimental subjects weusethedefects4jdataset asourbenchmark.itcontains 434realbugsfrom6real worldopensourcejavaprojects.weremove4bugsfromtheoriginaldataset sincetheyarenotreproducibleunderjava8 whichisrequiredbyevosuite.theprojects are jfreechart bugs closure compiler bugs apache commons lang 64bugs apachecommons math 106bugs mockito 38bugs andjoda time bugs .for eachbug the defects4j benchmark gives a buggy version and a fixed version of the program.
the difference between these two versions of the program isthe applied patch to fix the bug which indicates the location of the bug.
the defects4j benchmark also provides a high level interface to perform tasks like running the generated tests against the other version ofthe program buggy fixed to check ifthe tests areable to find the bug fixing the flaky test suites etc.
.
defects4j is widely used for research on automated unit test generation automated program repair fault localisation testcaseprioritisation etc.thismakesdefects4jasuitable benchmark for evaluating our approach as it allows us to compare our results to existing work.
.
baseline selection weusethecurrentstateoftheartsbstalgorithm dynamosa with equal time budget allocation sbst nodpg as our baseline for comparison.
previous work on bug detection capability of sbst allocatedanequaltimebudgetforalltheclasses .even though campos et al.
proposed a budget allocation targeting themaximumbranchcoverage wedonotconsiderthisasabaseline in our work as we focus on bug detection instead.
our intended applicationscenarioisgeneratingteststofindbugsnotonlylimited to regressions but also the bugs that are introduced to the system in different times.
hence we consider generating tests to all of the classesintheprojectregardlessofwhethertheyhavebeenchanged ornot.therefore inequalbudgetallocation totaltimebudgetis equally allocated to all the classes in a project.
.
parameter settings there are modules in our approach.
each module has various parameters to be configured and the following subsections outline the parameters and their chosen values in our experiments.
.
.
schwa.
schwa has parameters to be configured wr wf wa tr andn.
we choose the default parameter values used in schwa as follows wr .
wf .
wa .
and tr .
.
our preliminary experiments with n and all commits suggest that n gives most accurate predictions.
.
.
evosuite.
arcuri and fraser showed that parameter tuning of search algorithms is an expensive and long process and the default values give reasonable results when compared to tuned parameters.therefore weusethedefaultparametervaluesusedin evosuite in previous work except for the following parameters.
coveragecriteria weusebranchcoveragesinceitperformsbetter among the other single criteria .
gay found some multiple criteriacombinationstobeeffectiveonbugdetectionthansingle criterion.
however they did not recommend a strategy to combinemultiplecriteriaastheirstrategiesalsoproducedineffective combinations.
therefore we decide to use only single criterion.
assertion strategy as shamshiri et al.
mentioned mutationbasedassertionfilteringcanbecomputationallyexpensiveandlead to timeouts sometimes.
therefore we use all possible assertions as the assertion strategy.
givenacoveragecriterion e.g.
branchcoverage dynamosa exploresthesearchspaceofpossibletestinputsuntilitfindstest casesthatcoverallofthetargets e.g.
branches orthetimeruns out i.e.
timebudget .theseareknownasstoppingcriteria.this way ifthesearchachieves100 coveragebeforethetimeout anyremainingtimebudgetwillbewasted.atthesametime dynamosa aims at generating only one test case to cover each target in the system under test sut since its objective is to maximise the coveragecriteriongiven.thisalsohelpsinminimisingthetestsuite produced.
however when it comes to finding bugs in the sut justcoveringthebugdoesnotnecessarilyimplythattheparticulartestcasecandiscoverthebug.hence wefindthatusing100 coverage as a stopping criterion and aiming at finding only one test case for each target deteriorate the bug detection capability of dynamosa .
therefore inourapproach weconfigure dynamosa togenerate more than one test case for each target in the sut retain all these test cases disable testsuite minimisationand remove100 coveragefromthestoppingcriteria.by testsuitesizeinordertoincreasethebugdetectioncapabilityof sbst.
.
.
bads.
following the results of our pilot runs we use the default threshold of .
to allocate the classes into the two tiers.
in particular thetophalfoftheclasses rankedindescendingorder according to defect scores are allocated in the first tier n1 and the rest are in the second tier n2 .n1andn2are the number of classes in the firstandsecond tiers respectively.
ourpreliminaryresultsalsosuggestthatallocating90 and10 ofthetotaltimebudget t tothefirsttier t1 andthe secondtier t2 sufficiently favours the highly likely to be defective classes whilenotleavingoutthelesslikelytobedefectiveclassesfromtest 453generation.inparticular wechoose t1 n1andt2 n2 secondsat t nandt1 n1andt2 n2secondsat t n.wechoose15and30secondsas tminforthefirsttier tmin1 att nandt nrespectively.
the rationale behind choosingthesevaluesfor tmin1isthatitguaranteestheclassesin thefirsttieratleastgetatimebudgetoftheequalbudgetallocation i.e.
budgetallocationwithoutdefectpredictionguidance .for tmin ofthesecondtier tmin2 weassign3and6secondsat t n andt nbecause we believe t2is not enough to go for an exponential allocation.
theparametersfortheexponentialfunctionareasfollows ea .
eb .
and ec .
.
the rationale behind choosing the parameter values for the exponential function is as follows.
the exponential curve is almost flat and equal to for the values in the xaxis from .
to see figure .
then after x .
it starts increasing towards x .
finally at x the output is equal to .
1rupdolvhg 5dqn hljkw ehiruh 1rupdolvdwlrq figure4 exponentialfunctionofbads.
ea .
eb .
and ec .
.
prototype we implement the defect prediction guided sbst approach in a prototypetoolinordertoempiricallyevaluateit.theprototyped tool is available to download from here .
experimental protocol as we mentioned earlier to answer rq1andrq2 we conduct experiments for t nand nseconds.
in sbst dpg schwa uses current versions of the repositories oftheprojects.foreachbug schwapredictsthedefectivenessof the classes at the commit just before the bug fixing commit.
for each bug in defects4j there is a buggy version and a fixed version of the project.
we take each buggy version of the projects and thengeneratetestsuitesonlyforthebuggyclass es ofthatproject versionusingthetwoapproaches.totaketherandomnessofsbst intoaccount werepeateachtestgenerationrun20times andcarry out statistical tests when necessary.
consequently we have to run a total of approaches buggy classes repetitions timebudgets 880testgenerations.wecollectthegeneratedtestsuitesaftereachtestgenerationrun.next weusethe fixtest suiteinterface in defects4j to remove the flaky tests from each test suite .
then we execute each resulting test suite against the respectivebuggyandfixedversionstocheckifit findsthebugor not using the run bug detection interface.
if the test suite is not compilable or there is at least one failing test case when the test suite is run against the buggy version then it is marked as broken.
if not it will be run against the fixed version.
then if at least one testcasefails thetestsuiteismarkedas fail i.e.
testsuitefinds thebug .ifallthetestcasespass thenthetestsuiteismarkedas pass i.e.
test suite does not find the bug .
results we present the results for each research question following the methoddescribedinsection4.whilethemainaimistoevaluate ifourapproachismoreefficientthanthestateoftheart wealso focus on explaining its strengths and weaknesses.
table mean and median number of bugs found by the approaches against different total time budgets.
mean medianp value hatwidea12 t s sbst dpgsbst nodpg sbst dpgsbst nodpg n151.
.
.
.
.
.
n171.
.
.
.
.
rq1.
is sbst dpgefficient in finding bugs?
as described in section we perform runs for each sbst approachandeachbuggyprogramindefects4jandreporttheresults asboxplotsinfigure5.aswecansee overall ourproposedmethod sbst dpgfindsmorebugsthanthebaselineapproachforboth15 and seconds time budgets.
we also report the means medians and the results from the statisticalanalysisintable1.sbst nodpgfinds133.95bugsonaverage at total time budget of seconds per class.
sbst dpgoutperforms sbst nodpg andfinds151.45bugsonaverage whichisanaverage improvementof17.
.
morebugsthan sbst nodpg.thedifference of the number of bugs found by sbst dpgand sbst nodpg isstatisticallysignificantaccordingtothemann whitneyu test p value .
withalargeeffectsize hatwidea12 .
.thus wecan conclude that sbst dpgis more efficient than sbst nodpg.
attotaltimebudgetof30secondsperclass sbst dpgfindsmore bugsthanthesbst nodpg.accordingtothemann whitneyu test thedifferencebetweensbst dpgandsbst nodpgisnotstatistically significant with a p value of .
.
however the effect size of .
suggeststhatsbst dpgfindsmorebugsthansbst nodpg67 of the time which is significant given how difficult it is to find failing test cases .
in summary defect prediction guided sbst sbst dpg is significantly more efficient than sbst without defect predictionguidance sbst nodpg whentheyaregivena tight time budget in a usual resource constrained scenario.
when there is sufficient time budget sbst dpgis more effective than sbst nodpg67 of the time.
7rwdo 7lph xgjhw vhfrqgv 1xpehu ri xjv rxqg ssurdfk qr figure the number of bugs found by the approaches against different total time budgets 5hodwlyh 5dqnlqj 3rvlwlrq ri wkh odvvhv 1xpehu ri fodvvhv zkhuh d exj zdv irxqg ssurdfk 67qr figure the number of classes where a bug was found by the2approaches groupedbytherelativerankingpositions of the classes in the project at t nseconds to further analysethe differences between the two approaches figure6reports thedistributionofthe numberofclasseswherea bug was found across runs for the approaches grouped by the relativerankingpositionproducedbyschwaattotaltimebudget of seconds per class.
relative ranking position is the normalised rank of the respective class as described in algorithm .table2 summaryofthebugfindingresultsgroupedbythe relative ranking position of the classes in the project at t nseconds.
rank buggy classesavg.
time budgetmean number of classes where a bug was found sbst dpgsbst nodpg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
we observe that when the buggy classes are correctly ranked at the top by schwa and allocated more time by bads the performanceofsbst dpgissignificantlybetterthanthebaselinemethod.
more than half of the buggy classes are ranked in the top of the project by schwa as shown in table and allocated .
seconds of time budget on average by bads.
around of thebuggyclassesarerankedinthe10 oftheprojects.bads employsanexponentialfunctiontolargelyfavourasmallernumber of highly likely to be defective classes and allocates an adequate amount of time to the moderately defective classes.
only of the buggy classes are ranked below the first half of the project.
bads assumes not all classes in a project are defective andfollowsthe2 tierapproachtooptimisethebudgetallocation fortheproject.thus alltheclassesinthe secondtier whichcontains theclassesthatarerankedaslesslikelytobebuggy getaverysmall timebudget 2seconds .unsurprisingly sbst nodpgfoundmore bugsoutofthese59buggyclassesthansbst dpg.thisindicates thatthedefectpredictor saccuracyiskeytothebetterperformanceofsbst dpgandthereispotentialtoimp roveourapproachfurther.
forcompleteness wealsomeasureandpresentthenumberof truepositives falsenegatives andrecallofschwa.basedonthe0.5threshold i.e.
ifthedefectscoreisgreaterthanorequalto0.5thentheclassisbuggyanditisnon buggyifthedefectscoreislessthan .
schwalabels436buggyclassescorrectly truepositives and mislabels75buggyclasses falsenegatives .hence schwaachieves a recall of .
.
the defect predictor i.e.
schwa and bads modules add an overhead to sbst dpg.
while this overhead is accounted in the time budget allocation in sbst dpg we also report the time spent by the defect predictor and bads modules together.
schwa and bads spent .
seconds per class on average standard deviation .4seconds whichtranslatestoa4.
and2.
overheadin15 and seconds per class time budgets respectively.
therefore this showstheoverheadintroducedbyschwaandbadsinsbst dpg is very small and negligible.
455table3 successrateforeachmethodat ntotaltimebudget.bugidsthatwerefoundbyonlyoneapproacharehighlighted with different colours sbst dpgandsbst nodpg.
bugidsbst dpgsbst nodpg lang .
lang .
lang .
lang lang .
.
lang .
lang .
.
lang .
.
lang .
.
lang .
lang .
lang .
.
lang .
.
lang .
.
lang .
.
lang .
.
lang .
lang .
.
lang .
.
lang lang lang .
lang .
lang lang .
.
lang .
lang .
lang .
.
lang lang .
lang .
.
lang .
.
lang .
.
lang .
.
lang lang .
.
lang .
.
lang .
lang lang .
lang .
lang .
.
lang .
lang .
math math .
math .
math math .
.
math 1bugidsbst dpgsbst nodpg math .
.
math .
math .
math math .
math .
.
math math .
.
math .
.
math .
math math .
.
math .
math .
math math .
.
math math .
.
math math .
math .
.
math .
.
math .
.
math .
math math .
math .
.
math .
.
math .
.
math .
.
math .
.
math math math .
math math .
.
math math .
math .
math .
.
math math math math math .
.
math .
.
math .
math .
math .
.
math 1bugidsbst dpgsbst nodpg math .
.
math .
.
math .
math .
math .
math .
math math .
.
math .
math .
.
math math math math .
.
math .
math math math math .
math math .
math .
.
math math .
.
math math .
time time .
time .
.
time .
time time .
time .
time .
time time .
.
time time .
time .
.
time .
time .
.
time .
time .
time .
time .
time .
time .
.
time .
.
chart .
.
chart .
0bugid sbst dpgsbst nodpg chart .
.
chart .
.
chart .
chart .
chart .
.
chart chart chart .
chart .
.
chart .
.
chart chart .
chart chart chart chart .
chart .
.
chart .
.
chart chart chart mockito mockito mockito .
.
mockito closure .
closure .
.
closure .
.
closure .
.
closure .
closure .
.
closure .
.
closure .
.
closure .
.
closure closure .
closure .
closure .
closure .
closure .
closure .
closure closure .
closure .
.
closure .
.
closure .
closure .
closure .
closure .
.
closure .
.3bugid sbst dpgsbst nodpg closure closure .
.
closure .
closure .
closure .
closure .
closure closure .
closure .
closure .
closure .
closure .
.
closure .
closure .
.
closure .
closure .
closure .
.
closure .
closure .
.
closure .
.
closure .
.
closure .
closure .
.
closure .
.
closure .
closure .
.
closure .
closure .
.
closure .
.
closure .
.
closure .
closure .
.
closure .
.
closure .
closure .
.
closure .
closure .
.
closure closure .
.
closure .
.
closure .
.
closure .
closure .
closure .
.
closure .
.
closure .
.
closure .
closure closure .
.
closure .
.
rq2.
does sbst dpgfind more unique bugs?
to investigate how our approach performs against each bug we present an overview of thesuccess rates for each sbst method at total time budget of seconds per class in table .
success rate is theratioofrunswherethebugwasdetected.duetospacelimitation we omit the entries for bugs where none of the approaches were able to find the bug.
we also highlight the bugs that were detectedbyonlyoneapproach.ascanbeseenfromtable3 our approach outperforms the benchmark in terms of the success rates for most of the bugs.
this observation can be confirmed with the summary of the results which we report in table .
what is particularly interesting to observe from the more granular representation of the results in table is the high number of bugs where our approach has success rate which means that sbst dpgfinds the respectivetable summary of the bug finding results at t n. bugs foundunique bugsbugs found in every runbugs found more often sbst dpg sbst nodpg bugsinalltheruns.thisisanindicationoftherobustnessofour approach.
certainbugsarehardertofindthanothers.outofthe20runs for each sbst approach if a bug is only detected by one of the approaches wecallitauniquebug.thereasonwhywepayspecial attention to unique bugs is because they are an indication of the 456ability of the testing technique to discover what cannot be discovered otherwise in the given time budget which is an important strength .sbst dpgfound236bugsaltogether whichis54.
ofthetotalbugs whereassbst nodpgfoundonly215 .
bugs.
sbst dpgfound unique bugs that sbst nodpgcould not find in anyoftheruns.ontheotherhand sbst nodpgfoundonly14such unique bugs.
out of these bugs have buggy classes ranked in thetop10 oftheprojectbyschwa andtheother5bugsin10 of the project.
we observe similar results at total time budget of secondsperclassaswell wheresbst dpgfound32uniquebugs while sbst nodpgwas only able to find unique bugs.
sbst dpgfound bugs more times than sbst nodpg while forsbst nodpg thisisonly47.92outofthese127bugshavebuggy classesrankedinthetop10 oftheprojectandtheother35bugs in of the project.
ifweconsiderabugasfoundonlyifalltherunsbyanapproach findthebug successrate .
thenthenumberofbugsfound bysbst dpgandsbst nodpgbecome84and76.thereare27bugs which only sbst dpgdetected them in all of the runs.
in summary sbst dpgfinds more unique bugs compared to the benchmark approach.
furthermore it finds a large number of bugs more frequently than the baseline.
thus this suggests that the superior performance of sbst dpgis supported by both its capability of finding newbugswhicharenotexposedbythebaselineandthe robustness of the approach.
we pick math and time bugs and investigate the tests generated by the approaches.
figure 7a shows the buggy codesnippet of mathutils class from math .
the ifcondition at line412isplacedtocheckifeither uorviszero.thisisaclassic exampleofabugduetoanintegeroverflow.assumethemethod is called with the following inputs mathutils.
gcd .then the ifconditionatline412isexpectedtobeevaluated tofalsesince both u andv are non zeros.
however the multiplication of uandvcauses an integer overflow tozero andthe ifconditionatline412isevaluatedto true.figure 7bshowsthesamecodesnippetof mathutils classafterthepatch is applied.
to detect this bug a test should not only cover the true branchofthe ifconditionatline412 butalsopassthenon zero arguments uandvsuch that their multiplication causes an integer overflow to zero.
thefitnessfunctionforthe truebranchofthe ifconditionat line412is u v u v andittendstorewardthetestinputs uandvwhosemultiplicationisclosertozeromorethantheones whosemultiplicationisclosertocausinganintegeroverflowtozero.
for anexample suppose we havetwo individuals u v and u v 1241inthecurrentgeneration.thefitnessofthefirst andsecond individuals willbe6 and14997485 .
thus the first individual isconsidered fitterwhencomparedwith the second one while the second one is closer to detect the bug thanthefirstone.thereforeinasituationlikethis wecanincrease the chances of detecting the bug by allowing the search method to extensively explore the search space of possible test inputs and generate more than one test case test inputs for such branches.sbst nodpggenerated30.75testcasesonaveragethatcoverthe truebranchofthe ifconditionatline412 yetitwasnotableto detectthebuginanyoftheruns.schwarankedmath 94buginthe top of the project and bads allocated seconds time budget to the search.
then sbst dpggenerated .
test cases on average thatcoverthesaidbranch.asaresult itwasabletofindthebug in7runsoutof20.allocatingahighertimebudgetincreasesthe likelihoodofdetectingthebugsinceitallowsthesearchmethodtoexplorethesearchspaceextensivelytofindthetestinputsthatcan detect the bug.
411public static int gcd int u intv 412if u v return math.abs u math.abs v ... a buggy code411public static int gcd int u intv 412if u v return math.abs u math.abs v ... b fixed code figure mathutils class from math figure 8a shows the buggy code snippet of datetimezone class fromtime .the foroffsethoursminutes methodtakestwointeger inputs hoursoffset andminutesoffset and returns the datetimezone object for the offset specified by the two inputs.
if the method foroffsethoursminutes is called with the inputs hoursoffset andminutesoffset then it is expected to return adatetimezone object for the offset .
however the ifconditionatline279isevaluatedto trueandthemethodthrows anillegalargumentexception instead.figure8bshowsthesame code snippet after the patch is applied.
to detect this bug a test casehastoexecutethe ifconditionsatlines273and276to false that ishoursoffset 0o rminutesoffset andhoursoffset and then it has to execute the ifcondition at line totruewith aminutesoffset .
moreover there is a newconditionintroducedatline282inthefixedcodetocheckif thehoursoffset is positive when the minutesoffset is negative seefigure8b .thus thisaddsanotherconstrainttothepossible test inputs that can detect the bug which is hoursoffset .
therefore it is evident that it is hard not only to find the right test inputstodetectthebug butalsotofindtestinputstoatleastcover the buggy code.
asitwasthecaseinmath justcoveringthebuggycode true branch of the ifcondition at line is not sufficient to detect thetime 8bug.for anexample testinputs hoursoffset and minutesoffset cover the buggy code however they cannot detect the bug.
therefore the search method needs more resources togeneratemoretestcasesthatcoverthebuggycodesuchthatit eventually finds the right test cases that can detect the bug.
ourinvestigationintothetestsgeneratedbythe2approaches shows that the baseline sbst nodpg covered the buggy code in of the runs.
sbst nodpggenerated .
test cases on average that cover the buggy code and it was ableto detect the bug in runsoutof20.whereas sbst dpgallocated75secondstimebudget tothesearchasschwarankedthebuginthetop10 oftheproject and generated .
test cases on average that cover the buggycode.
as a result it was able to detect the bug in all of the runs success rate .
.
therefore this again confirms the importance 457272public static datetimezone foroffsethoursminutes inthoursoffset int minutesoffset throws illegalargumentexception 273if hoursoffset minutesoffset return datetimezone.utc 276if hoursoffset hoursoffset throw new illegalargumentexception hours out of range hoursoffset 279if minutesoffset minutesoffset throw new illegalargumentexception minutes out of range minutesoffset 282intoffset ... a buggy code 272public static datetimezone foroffsethoursminutes inthoursoffset int minutesoffset throws illegalargumentexception 273if hoursoffset minutesoffset return datetimezone.utc 276if hoursoffset hoursoffset throw new illegalargumentexception hours out of range hoursoffset 279if minutesoffset minutesoffset throw new illegalargumentexception minutes out of range minutesoffset 282if hoursoffset minutesoffset throw new illegalargumentexception positive hours must not have negative minutes minutesoffset 285intoffset ... b fixed code figure datetimezone class from time of focusing the search more into the buggy classes to increase the likelihood of detecting the bug.
threats to validity internalvalidity.
asoutlinedinsection4.
.
weconfigure dynamosato generate more than one test case for each target in the sut retainallthese testcasesanddisabletestsuiteminimisation.
by this we expect to compromise the test suite size in order to maximise the bug detection of sbst.
to investigate the bene fit of configuring dynamosa in this way we also run the same set of experiments using dynamosa with test suite minimisation and equal budget allocation sbst o. we compare its performance against sbst nodpg.
sbst ofinds .
and .
bugs on average attotaltimebudgetof15and30secondsperclass.sbst nodpgoutperformssbst owithanaverageimprovementof48.
.
and .
.
morebugsineachcase whicharestatisticallysignificantaccording tothemann whitneyu test p value .
with a large effect size hatwidea12 .
.
however this huge improvementcomeswithaprice i.e.
sbst nodpgproduceslargetestsuites.
this can be problematic if the developers have to insert the test oraclesmanuallytothegeneratedtests.thus weidentifythisas a potential threatto internal validity and future worksneed to be doneonadaptingappropriatetestsuiteminimisationtechniques to sbst dpg.
toencountertherandomisednatureofgausedin dynamosa weruntheexperimentsfor20timesandcarryoutsoundstatistical tests two tailed non parametric mann whitney u test and vargha and delaney s hatwidea12statistic .the parameter configurations for schwa and bads are either the default values or based on the results of the pilot runs.
we believetheperformanceofsbst dpgcanbefurtherimprovedby fine tuning the parameters of schwa and bads.
we employ an exponential function to allocate time budgets for classes based on the defect scores.
as opposed to an exponentialallocation adirectmapping i.e.
linearbudgetallocation would havebeensimpleandstraight forward.however asdescribedin section .
.
there are only a few number of classes which are actually buggy i.e.
highly likely to be defective and they need to beallocatedmoretimebudgettomaximisethebugdetectionofthe test generation tool.
thus we believe a linear allocation approach is not able to largely favourthese small number of classes like the exponential allocation approach does.
externalvalidity.
weuse434realbugsfromdefects4jdataset that are drawn from open source projects.
these projects maynotrepresentallprogramcharacteristics especiallyinindustrial projects.although defects4jhasbeenwidelyusedintheliterature asabenchmark.futureworkneedstobedoneon applying sbst dpgon other bugs datasets.
evosuite generates junit test suites for java programs.
thus we maynotbeabletogeneralisetheconclusionstootherprogramming languages.
however the concept we introduced in this research is not language dependent and can be applied to other programming languages as well.
conclusion weintroducedefectpredictionguidedsbst sbst dpg thatcombines class level defect prediction and search based software testing to efficiently find bugs in a resource constrained environment.
sbst dpgemploysabudgetallocationalgorithm budgetallocation basedondefectscores bads toallocatetimebudgetsforclassesbasedontheirlikelihoodofdefectiveness.wevalidateourapproach against real bugs from defects4j dataset.
our empirical evaluationdemonstratesthatinaresourceconstrainedenvironment when given a tight time budget sbst dpgis significantly more efficientthanthestateoftheartapproachwithalargeeffectsize.
inparticular sbst dpgfinds13.
morebugsonaveragecompared tothestateoftheartsbstapproachwhentheyaregivenatight time budget of seconds per class.
further analysis of the results finds that the superior performance of sbst dpgis supported by its ability to find more unique bugs which otherwise remain undetected.
we aim to extend our work in the following directions as future work i employ a defect predictor which uses different features to producepredictions ii adaptanappropriatetestsuiteminimisation strategy to address the generation of larger test suites and iii validate sbst dpgagainst other bugs datasets.