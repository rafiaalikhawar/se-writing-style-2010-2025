towards interpreting recurrent neural networks through probabilistic abstraction guoliang dong zhejiang university dgl prc zju.edu.cnjingyi wang zhejiang university wangjyee zju.edu.cnjun sun singapore management university junsun smu.edu.sg yang zhang zhejiang university leor zju.edu.cnxinyu wang zhejiang university wangxinyu zju.edu.cnting dai huawei international pte ltd daiting2 huawei.com jin song dong national university of singapore dongjs comp.nus.edu.sgxingen wang zhejiang university newroot zju.edu.cn abstract neuralnetworksarebecomingapopulartoolforsolvingmanyrealworldproblemssuchasobjectrecognitionandmachinetranslation thanks to its exceptional performance as an end to end solution.
however neuralnetworksarecomplexblack boxmodels which hinders humans from interpreting and consequently trusting them inmakingcriticaldecisions.towardsinterpretingneuralnetworks several approaches have been proposed to extract simple determin isticmodelsfromneuralnetworks.theresultsarenotencouraging e.g.
low accuracy and limited scalability fundamentally due to the limited expressiveness of such simple models.
in this work we propose an approach to extract probabilistic automataforinterpretinganimportantclassofneuralnetworks i.e.
recurrent neural networks.
our work distinguishes itself from existing approaches in two important ways.
one is that probabilityisusedtocompensateforthelossofexpressiveness.thisisinspired by the observation that human reasoning is often probabilistic .
theotheristhatweadaptivelyidentifytherightlevelofabstraction so that a simple model is extracted in a request specific way.
we conduct experiments on several real world datasets using state ofthe artarchitecturesincludinggruandlstm.theresultshows that our approach significantly improves existing approaches in terms of accuracy or scalability.
lastly we demonstrate the usefulness of the extracted models through detecting adversarial texts.
ccs concepts theory of computation abstraction machine learning theory probabilistic computation.
corresponding authors jingyi wang and xinyu wang.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
abstraction interpretation probabilistic automata recurrent neural networks acm reference format guoliang dong jingyi wang jun sun yang zhang xinyu wang ting dai jin song dong and xingen wang.
.
towards interpreting recurrent neuralnetworksthroughprobabilisticabstraction.in 35thieee acminternationalconferenceonautomatedsoftwareengineering ase september virtualevent australia.
acm newyork ny usa 12pages.
introduction neuralnetworkmodelsaregettingpopularduetotheirexceptional performance in solving many real world problems such as selfdriving cars malware detection sentiment analysis and machine translation .
at the same time neural networks are shown to be vulnerable to issues such as adversarial attacks and embedded back doors .
to be able to trust neural networks it is crucial to understand how neural networks make decisions evenbetter toreasonaboutthembeforedeployingthem in safety critical applications.
neural networks are however complex models that work in ablack boxmanner.humaninterpretationofneuralnetworksis often deemed infeasible .
furthermore the complexity also hinders analysis through traditional software analysis techniques suchastestingandverification.recently therehavebeennoticeableeffortsonportingestablishedsoftwaretestingandverificationtechniquestoneuralnetworkmodels.forinstance multipletesting approacheslikedifferentialtesting mutationtesting andconcolictesting havebeenadaptedtotestneuralnetworks.
furthermore several verification techniques based on smt solving abstract interpretation and reachability analysis havealsobeenexploredtoformallyverifyneuralnetworks.however duetothecomplexityofneuralnetworks existingapproaches oftenhavehighcostand oronlyworkforverylimitedclassesof neural networks .
recently an alternative approach has been proposed.
that is ratherthanunderstandingandreasoningaboutneuralnetworks directly researchers aim to extract simpler models from neural 35th ieee acm international conference on automated software engineering ase ase september virtual event australia guoliang dong jingyi wang jun sun yang zhang xinyu wang ting dai jin song dong and xingen wang networks.
ideally the simpler models would accurately approximate the neural networks and be simple enough such that theyare human interpretable.
furthermore such models can be subject to automated system analysis techniques such as model based testing modelchecking andruntimemonitoring .several attempts have been made on one particularly interesting class of neural networks called recurrent neural networks rnn dueto their stateful nature as well as popularity in various domains.
in omlinetal.proposetoencodetheconcretehiddenstates ofrnnintosymbolicrepresentationandthenextractsimpledeterministic models from the symbolic data .
followup approaches have been proposed to extract different models like determinis tic finite automata dfa from rnn .
a recent empirical study shows that such approaches are useful for capturing the structuralinformationofthernnandhencehelpfulformonitoring its decision process.
existingapproaches however haveeitherlimited accuracy in thecaseof wheresimpledeterministicmodelsareextracted orscalability inthecaseof wheremoreexpressivemodelsare extracted .
for instance the extracted models for the real worldsentiment analysis tasks in have about fidelity even on the training data.
this is not surprising since simple models like dfahavelimitedexpressivenesscomparedtoneuralnetworks.forinstance theworkin extractsdeterministictransitionsbetween symbolicencodingofconcretehiddenstatesinrnn whereas rnn learnedfromreal worlddataareintrinsicallyprobabilistic .ifwewere to improve accuracy by extracting more states and transitions not only the models are computationally expensive to extract but also the extracted models become uninterpretable.
towardsextractingaccurateinterpretablemodelsfromrnn we developatechniqueofextractingprobabilisticfinite stateautomata pfa fromrnninthiswork.ourworkdistinguishesitselffrom existing approaches in two important ways.
one is that we extract probabilisticmodelstocompensateforthelimitedexpressivenessofsimple deterministic models compared to that of neural networks .
thisisinspiredby theobservationthathumanreasoningisoften probabilistic i.e.
humans often develop simple understandingofcomplexsystemsbycuttingcorners i.e.
low probabilisticcases .
the other is that we do not attempt to generate a single modelthatapproximatesanrnnmodelasaccuratelyaspossible as it often leads to models with many states and transitions which arehardtoextractorinterpret.instead wegeneratemodelsthat are sufficiently accurate as per user request.
for instance if theuser requires to get a model which achieves accuracy in approximatingthernn theextractedmodelwouldhavefewerstates than a model extracted with accuracy in that.
this is achieved by adaptively identifying the right level of abstraction through clustering.
our approach is based on a novel algorithm which combines state of the art probabilistic learning and abstraction through clustering.
given an rnn model and a training set we first encode theconcrete numericalhiddenstatesof anrnninto asetofclusters.
then we convert the samples in the training set into a set of symbolic traces each of which is the sequence of clusters visitedbythesample.afterwards weapplyaprobabilisticlearning algorithm on the symbolic traces to learn a pfa.
furthermore given a specific requirement we apply clustering in a greedy wayand automatically determine the right number of clusters i.e.
the levelofabstraction whichconsequentlydeterminesthenumber of states in the learned pfa.
that is we optimize to balance the complexity of the learned pfa i.e.
the fewer states the better and its accuracy in approximating the rnn i.e.
the higher the better .
we applied our approach to several rnn models with stateof the art architectures for solving artificial and real world i.e.
sentiment analysis tasks.
the results show that our approach significantlyimprovesexistingapproachesintermsofeitheraccuracy or scalability and is capable of extracting models which accurately approximate the rnn models.
compared to our approach improves the fidelity of the extracted model from below to over on average on the real world datasets.
compared to whichislimitedtosmallartificialdatasets ourapproachhandles large real world datasets effectively.
lastly we demonstrate the usefulnessoftheextractedmodelsthroughanimportantapplication i.e.
detecting adversarial texts that generated to attack rnn.
to the best of our knowledge this is the first systematic approach for detecting adversarial texts.
we organize the rest of the paper as follows.
we provide preliminaries in section .
we present our approach in detail in section and experiment results in section .
we review related work in section and conclude in section .
preliminaries in this section we review relevant background on recurrent neural networks rnn and probabilisticfinite state automata pfa .
recurrentneuralnetwork inthiswork wefocusonstate of the art rnn architectures such as gated recurrent unit gru and long short term memory lstm .
we introduce rnn at a conceptual level rather than introducing details of gru and lstm sinceourapproachappliestornningeneral.theconceptualmodel ofrnnisshowninfigure1 whichtakesavariable lengthsequence angbracketleftx0 x1 xm angbracketrightas input and produces a sequence angbracketlefto0 o1 om angbracketright as output.
in this work we focus on rnn classifiers r x i wherexis the set containing all the possible values of x x is the setoffinitestringsover x andiisafinitesetoflabels classification which only depend on the last output om.
rnnisstateful i.e.
havinga memory ofprevioustimesteps andwhathavebeencalculatedsofarthroughasetofhiddenstates h. at each time step t the hidden state stand the output otare calculated as follows.
st f uxt wst ot argmax isoftmax vst wherefis usually a nonlinear function like tanhorrelu u w andvare trainedparameters and softmax is anormalizing function which outputs a probability distribution.
we remark that gru and lstm networks have the same conceptual model shown in figure except that more complex functions are used to compute the hidden states.
we refer the readers to for details.
probabilistic finite automata existing works on explaining rnn in focus on extracting models in the form of deterministic finite state automata dfa .
500towards interpreting recurrent neural networks through probabilistic abstraction ase september virtual event australia w w w w. .
.
.
.
.. .
.
.
.
.v v v u uu xt xt xt 1ot ot ot figure a conceptual rnn.
definition .
a dfa is a tuple ad angbracketleftx q q0 qf angbracketright where xisanalphabet qisafinitesetofallpossiblestates q x q isalabeledtransitionfunction q0 qisasetofinitialstates and qf qis a set of accepting states.
note that given a sequence of input symbols the dfa qualitatively determines whether it is accepted if the last state is an accepting state or not.
this limits dfa s capability in approximating rnn.
for instance in the context of explaining rnn trained for sentiment analysis a dfa model produces a binary result i.e.
positive or negative given any text whereas rnn often produces a probability of the text being positive or negative.
to address this limitation we instead focus on extracting pfa in this work which associates probabilities with state transitions in the dfa.
definition2.
apfa isatuple a angbracketleftx q q0 qf 0 angbracketright where xisanalphabet qisafinitesetofstates q x q isalabeled probabilistic transition function such that summationtext.
e x si e s j 1for allsi sj q q0 qis a set of initial states 0is the initial probability distribution over q0 andqfis a set of accepting states.
an example pfa extracted using our approach from an rnn model isshowninfigure4 whereacceptingstatesarerepresented usingdouble edgedcircles theinitialstatesareindicatedwithanar rowfromnowhere andeachtransitionislabeledwithaprobability pfollowed by a symbol ein the form of p e. our approach inthissection weintroduceourapproachstep by step.anoverview of the overall workflow is shown in figure .
the inputs are anrnn model and the associated training set.
there are two main parts i.e.
anabstractionpart ontheleft andalearningpart on theright .theabstractionpartabstractstheconcretehiddenstates of a given rnn into an abstract alphabet.
the goal is to systematicallygrouphiddenstatesofrnn intheformofnumericalvectors that exhibit similar behaviors into clusters.
the learning part then takes the abstract alphabet and systematically learns a pfa.
in the following weintroducethestepsindetailandillustratethemusing the following running example.
example3.
.
thesentenceshowninthefirstrowoftable1is a review selected from the rtmr dataset which is a widely used movie review dataset for sentiment analysis.
the first row in table1istheoriginalreview thesecondrowisthecleanedinput afterremovingthestopwords andthethirdrowisthelabel where represents positive i.e.
a positive review .
.
abstraction theobjectiveofthisstepistosystematicallyabstractthehidden statesof thernn.note thatthestatesof thernnare intheform of numerical vectors which have numerous values and are hardtointerpret.theideaisthatmanyofthehiddenstatesrepresent similar behaviors and thus can be grouped together.
there are two existing techniques on abstracting the hidden states of rnn i.e.
intervalpartition andclustering .theformerarbitrarily partitionstherangeofhiddenstatevaluesintomultipleintervals and assumes that those hidden states in the same interval have similarbehaviors whereasthelatterassumesthatnearbyhidden states exhibit similar behaviors.
in this work we choose the latter for two reasons.firstly clustering has been shownto be intuitive and effective in a recent empirical study .
in fact it has been shownthatthehiddenstatesnaturallyformclusters .secondly existing clustering techniques allow us to flexibly control the level of abstraction by controlling the number of clusters.
thegoalofclusteringistogroupthe infinite hiddenstatespace of rnn into a few clusters.
there are many existing clustering algorithms .inthiswork weadoptthek meansalgorithm.the ideaofk meansistoidentifyanassignmentfunction c h k wherehis a set of concrete states and kis a set of clusters.
intuitively c h kmeans that the concrete state his mapped to clusterk.
ideally the hidden states that are assigned to the same cluster should be close to each other in terms of certain distancemetrics e.g.
euclidean distance .
assume that there are kclusters the assignment ccan be found by optimizing the following objective.
c argmin ck summationdisplay.
k 1nk summationdisplay.
h ck h hk whereck h h h c h k isthe setofhiddenstates which areassignedtothecluster k nkisthesizeof ck and hkismean of all the hidden states in ck.
abstracting traces clustering is applied as follows in our work.
we first collect the rnn hidden state vectors of each sample in the trainingset.next wetrainaclusteringfunctionusingthek means algorithm on those vectors.
note that the number of clusters k isaparameterwhichmustbefixedbeforeapplyingthek means algorithm.
we discuss how to set the value for kin section .
.
once we have a clustering function c we obtain the abstract alphabet i.e.
theclusters andconstructasetofabstracttracesbased onthetrainingsetasfollows.wefeedeverysampleinthetraining set into the rnn and obtain the concrete hidden state at each step.
theresultisasequenceofconcretehiddenstates angbracketlefts0 s1 s2 sn angbracketright wheres0is a dummy initial state i.e.
the dummy initial mapping .
next we apply cto each concrete hidden state but the dummy initialstate s0andobtainanabstracttrace angbracketlefts c s1 c s2 c sn angbracketright wheresissymboldenotingthedummyinitialstate.givenasample x we write x to denote the abstract trace obtained as described above.
afterward we further concatenate x with the label of x whichformstheacceptingstatesoftheextractedpfaasexplained later .notethattheabstractalphabetisthus x s k iwhere kis the set of clusters and iis the set of labels.
we denote the above procedure as x r c k whereris the given rnn ckis 501ase september virtual event australia guoliang dong jingyi wang jun sun yang zhang xinyu wang ting dai jin song dong and xingen wang abstraction learn ing trainingclustering g2 g4 g2 g5 g2 g6 g2 g3 g6 g2 g1 g1 g6 g3 g6 g4 g38 g38 g38 g2 g7 g38 g6 g5rnn g1 g2 g3 g2 g10 g1 g2 g3 g2 g9 g1 g2 g3 g2 g8 g1 g2 g3 g2 g7 g1 g2 g3 g2 g14 g1 g2 g3 g2 g13 g1 g2 g3 g2 g12 g1 g2 g3 g2 g11 g1 g2 g3 g2 g7 g8 g1 g2 g3 g2 g7 g7 g1 g2 g3 g2 g7 g6 g1 g2 g3 g2 g15 g3 g4 g7 g8 g5 g6 abstract alphabet g2 g4 g2 g5 g2 g6 g2 g7 training datahidden states g3 g4 g7 g8 g6 g5 g5 g7 g4 g1 g8 g1 g5 g1 g7 g1 g9 g1 g4 g1 g6pfa abstact trace trainin g clustering g2 g4 g2 g5 g2 g6 g2 g3 g6 g2 g1 g1 g6 g3 g6 g4 g38 g38 g38 g2 g7 g38 g6 g5 rnn g1 g2 g3 g2 g10 g1 g2 g3 g2 g9 g1 g2 g3 g2 g8 g1 g2 g3 g2 g7 g1 g2 g3 g2 g14 g1 g2 g3 g2 g13 g1 g2 g3 g2 g12 g1 g2 g3 g2 g11 g1 g2 g3 g2 g7 g8 g1 g2 g3 g2 g7 g7 g1 g2 g3 g2 g7 g6 g1 g2 g3 g2 g15 g3 g4 g7 g8 g5 g6 abstract alphabet g2 g5 g2 g6 g2 g2 g4 g2 g7 training data hidd en states pfa learning figure overall framework table an example input text original review not a film to rival to live but a fine little amuse bouche to keep your appetite whetted cleaned film rival live fine little amuse bouche keep appetite whetted label concrete trace abstract trace s p the clustering function parameterized by k andxis a sample.
applyingtheaboveproceduretoeverysampleinthetrainingset x we obtain a bag of abstract traces denoted as x as input for the next phase of our approach.
example .
.
given example .
let kbe .
the clustering function maps all hidden state vectors to two clusters denoted as and for simplicity.
the abstract alphabet is thus x s p n wherep narelabelsrepresenting positive and negative respectively.
the concrete trace obtained from the example text is shown inthefourthrowoftable1.withthetrainedclusteringfunction ck the abstract trace is shown in the fifth row.
.
learning the task of the learning part is to construct a pfa based on the abstract traces.
ideally the pfa should be simple i.e.
having a smallnumber ofstatesand transitions andshould havethemaximum likelihood of exhibiting the abstract traces i.e.
accurate with respecttothernn .ourapproachisbuiltontopofthe aalergia learning algorithm proposed in .
we choose aalergia as it has been proved to be useful for learning models suitable for system reasoninglikeprobabilisticmodelchecking .thekeyideaof our learning part is to generate a pfa which generalizes the probabilisticdistributionoftheabstracttracesoverthealphabet.note that this is fundamentally different from existing approaches such as which uses user provided partitioned intervals as system states directly.the details of the learning algorithm are shown in algorithm .
the high level idea is as follows.
we first organize the abstract tracesintoatreecalledfrequencyprefixtree fpt whichcanbe considered as a huge model that exhibits exactly the set of abstract traces.
afterwards we repeatedly merge the nodes in the fpt suchthatthenumberofstatesisreducedgradually.notethattwonodes aremergedonlyiftheyexhibitsimilarbehaviors.onceallnodes with similar behaviors are merged we transform the resultant fpt into a pfa.
in the following we present each step in detail.
frequencyprefixtree thefirststepistotransformtheabstracttraces into an fpt.
let x be the set of abstract traces and xbe the abstract alphabet.
let prefix x be the set of all prefixes of any x x .anfptisatuple tree x angbracketleftn e f root angbracketright wheren isprefix x e n nisthesetofedgessuchthat n n prime e ifand onlyifthere exists xsuchthat n n primewhere isthe concatenation operator fis a frequency function which records the total number of occurrences of each prefix in x androotis theemptystring angbracketleft angbracketrightwhichcorrespondstothedummyinitialstate s. for instance given a bag of traces with angbracketlefta a angbracketright angbracketlefta b angbracketright angbracketlefta b a angbracketright angbracketleftb b angbracketright angbracketleftb b a angbracketrightand angbracketleftb b b angbracketright the fpt is shown on the leftoffigure3.weremarkthataleafnodeofthefptrepresents a complete trace which is associated with a label in i whereas an internal node of the fpt is a prefix associated with a certain symbol in k i.e.
a cluster .
note that an fpt can be regarded as a pfa.
that is the nodes in the fpt can be regarded as states in the pfa and we can obtain the one stepprobabilityfromnode nton asp n n f n f n 502towards interpreting recurrent neural networks through probabilistic abstraction ase september virtual event australia algorithm extract x 1organize x into a frequency prefix tree tree x 2letr be the set of nodes in thefinal pfa 3letb root 4whileb do 5select a node bfromb 6letmerged false 7foreachr rdo test the compatibility between bandr ifcompatible then merged true mergebwithr break 13if!mergedthen addbtor 15removebfromband add the children of btob 16let be a probabilistic transition function 17foreachr rdo 18foreach xdo r r f r f r 20 r angbracketleft angbracketright r summationtext.
xp r r 21letq0 angbracketleft angbracketright which only contains the root node 22let 0be the initial distribution which transits to the root node angbracketleft angbracketright with probability 23letqfbe the set of leaf nodes in r 24construct the pfa as angbracketleftx r q0 qf 0 angbracketright.
wheref n is the number of times nappears in prefix x .i n addition the probability that a node transits to itself is p n n summationtext.
xp n n .
however the fpt is not a good model due to itssize.in otherwords althoughthefpt representsthesetofabstractstatesprecisely thereisnogeneralization a.k.a.over fitting .
to construct a concise pfa we generalize the fpt by repeatedly merging the nodes.
intuitively two nodes are merged if and only if they have similar future behaviors which is determined through a compatibility test.
compatibility test two nodes are considered compatible and thus to be merged if they agree on the last letter and their future probability distributions are sufficiently similar .
while the former is easy to check to check the latter we compare the differences between the probability of all paths from the two nodes in the fpt and check if the difference is within a certain bound.
note that the pathprobabilityistheproductoftheone stepprobabilities.thatis theprobabilityofapath angbracketleft 1 2 k angbracketrightfromanode nisdefined asp n p n n 1 p n 1 2 p n 1 k k .
formally thefutureprobabilitydistributionsoftwonodes nand n primeare sufficiently similar if and only if for all path p n n p n prime n prime radicalbig 6 log f n f n radicalbig 6 log f n prime f n prime wherep n n isthepathprobabilityasdefinedaboveand is a constant coefficient.
note that a larger means that more nodes would pass the compatibility test and consequently be merged.
in this work we set to be following the empirical results shown in .
for instance the node marked aaand the node markedabashown in figure form a pair of compatible nodes as theirlastclustersarethesame i.e.
a andtheirfutureprobability distributions are similar i.e.
both with no future paths .
merging nodes in order to systematically identify and merge the nodes in the fpt we maintain two sets of nodes i.e.
a set of red nodesr see line in algorithm which are to be transformed into states in the learned pfa and a set of bluenodesb see line whicharenodespotentiallytobemergedintothoserednodes.
initially ris empty and bonly contains the root node.
next theloopfromline4to15systematicallycheckseverynode inbtoseewhetheritiscompatiblewithanyrednode atline8 .if there is one the blue node is merged to the compatible red node at line .
otherwise the blue node is turned to a red one and added intoratline14.afterthat weaddthechildrenofthebluenode to the blue set at line unless the blue node is a leaf node .
atline11 abluenode bismergedintoarednode risasfollows.
we update the frequency function of both the ancestors and descendant of the red node r. in particular for any of r s ancestor ra we add its frequency f ra byf b and for any of r s descendants rd let dbe the onwards path from r we add the frequency f rd byf b d .inaddition weaddanedgefrom b sparentto r since bis now merged into r .
for example figure illustrates how two compatible nodes are merged.ontheleftistheoriginalfpt wherethenode bbandnode abare assumed to be compatible and thus to be merged.
on the right is the updated tree after merging where the frequency of nodeaband all its ancestors are updated.
pfa construction the loop from line to runs until there are no more blue nodes to be merged.
afterwards we construct thepfa from line to line as follows.
the remaining nodes inthe fpt which are all in the red set now are turned into statesin the pfa.
the transitions between states in the pfa are thenconstructed systematically based on the tree edges from line to line .
take one red node ras an example.
for each x thetransitionprobabilityfrom rtor isdefinedas p r r f r f r line .
the probability of transition to itself is summationtext.
xp r r line20 .thesetofinitialstatesonlycontainsthe rootnode i.e.
theemptytrace angbracketleft angbracketright line21 .theinitialdistribution associates probability with the root node line .
note that the setofacceptingstatesinthepfaareexactlythelabelset isinceall the leaf nodes with the same ending letter sf iwill be merged as onestateastheirfuturedistributionsarethesame line23 .finally we construct the pfa at line .
example .
.
central to the conversion of fpt is to build the probabilistic transition function from line to .
for the sake of simplicity we take the node aand its outgoing transitions in the left fpt in figure as an example to illustrate the conversion.
in this fpt xcontains two symbols andris .
for node a r a a a a f a a f a and a b a b f a b f a .accordingtotheleft fpt f a f a a andf a b is80 50and30respectively.thus 503ase september virtual event australia guoliang dong jingyi wang jun sun yang zhang xinyu wang ting dai jin song dong and xingen wang figure merging nodes algorithm overall x r a 1letkbe2 2whilenot time out do 3obtain the clustering function ckusing k means 4 x x r c k 5a extract x 6let p a x r x a 7if athen returna 9increase kby 10returna wecangetthatthetransitionprobabilityfrom atoaaundersymbol ais .
the transition probability from atoabunder symbol bis .
and the transition probability from ato itself is .
.
model selection recall that we aim to extract a small model that approximates the rnnaccurately i.e.
bymakingthesamedecisiononasmanyinputs as possible .
the size of the extracted model matters for human interpretation as well as potential tool based analysis.
the number ofstatesinthelearnedpfaislargelydeterminedbythenumberof clusters.usually themoreclustersweuse themoreaccuratethe extractedmodelwillbe whichisevaluatedinsection4.
.inthe extreme case if we consider each valuation of the numeric vectors as a cluster we would have a huge pfa which is perfectly accurate but hardly interpretable.
thus we do not attempt to generate a modelthatapproximatesanrnnmodelasaccuratelyaspossible asitoften leadstomodelswith manystates.instead wegenerate models with a user required level of consistency with the rnn.
such consistency can be measured using fidelity .
givenauser requestintheformof generatingamodelwhich hasa90 fidelitycomparedtothernn thequestionisthenhowtodeterminethe right numberofclusters.ouransweristogradually increasethenumberofclustersuntilamodelsatisfyingtheuserrequestisgenerated.ouroverallalgorithmisshowninalgorithm2 wherexisthesetofconcretetraces i.e.
thesequenceofvaluations ofthehiddenfeaturevectorsofthernn generatedbythesamples inthetrainingset risthernnmodel aistherequiredfidelity of the extracted model and is the parameter for compatibility g1 g2 g3 g4 g5 g6 g5 g1 g4 g1 g7 g5 g8 g5 g6 g9 g2 g10 g1 g7 g3 g5 g6 g5 g5 g8 g8 g7 g11 g5 g6 g3 g3 g1 g1 g7 g12 g5 g6 g13 g2 g3 g8 g7 g5 g5 g6 g5 g8 g9 g9 g7 g3 g5 g6 g5 g10 g5 g4 g7 g11 g5 g6 g5 g3 g4 g8 g7 g12 g5 g6 g5 g10 g3 g5 g7 g5 g5 g6 g13 g2 g14 g14 g7 g3 figure example of a learned pfa testing refer to algorithm .
the output is an extracted pfa a which satisfies the user required fidelity.
thealgorithmworksasfollows.wefirstobtaintheclustering functionckusingthek meansalgorithm parameterizedby kat line3.notethat kisinitially2andisincreasedby1eachtime.then we apply the procedure x r c k presented in section .
to obtain a bag of abstract traces x at line .
after that we extract apf aausingalgorithm1atline5.wemeasurethefidelityofthe extractedmodelatline6.ifthefidelitysatisfiestherequirement i.e.
the condition at line is satisfied the extracted model is returned atline8.otherwise weincreasethenumberofclustersby1atline and start over again.
to obtain a label of a given sample xusing the extracted model we first generate the concrete trace of xgiven the rnn i.e.
the sequence of valuations of the hidden feature vectors of the rnn.
next anabstracttraceisextractedusingtheapproachpresentedinsection3.
.notethattheabstracttraceisintheformofasequence of letters each of which represents a cluster except the last one which represents the label.
next we remove the label from the abstracttraceandsimulateitontheextractedpfa i.e.
fromthe initial state of the pfa for each letter in the abstract trace we takethecorrespondingtransitionofthepfa .let sbethelaststatethat is reached via the abstract trace.
we then apply probabilistic model checking techniques to compute the probability of reaching everylabelfrom s.wewrite p x y whereyisalabeltodenotethe above computedprobability forlabel y.the labelwiththelargest probabilityisthengeneratedasthepredicatedlabelbytheextracted pfa.
example3.
.
figure4showsthepfaextractedfromagrumodel trained on rtmr with clusters.
recall that the abstract trace for the sample text shown in example .
is angbracketlefts p angbracketright asdiscussedinexample3.
.simulatingthetrace excludingthe labelp on the pfa shown in figure we end up with state .
applying probabilistic model checking we obtain that the probability of reaching state representing label n is .
whereas the probabilityofreachingstate4 representinglabel p is0.
.thus the prediction is positive .
we remark that the extracted pfa predicts the label of a sample basedonthetraceofthernn notthesampleitself.thisisbecausethepfaismeanttofacilitateinterpretationofthernnratherthan being a predictive model itself.
evaluation our approach has been implemented as a self contained prototype based on pytorch and scikit learn with about lines of code.
504towards interpreting recurrent neural networks through probabilistic abstraction ase september virtual event australia our implementation including the source code the dataset and the trained models is available at .
in the following we evaluate our approach from two aspects.
first weevaluateitseffectivenessintermsofextractingpfa i.e.
is it capable of generating small pfa which accurately approximates the rnn?
second we evaluate its usefulness i.e.
other than being usefulforhumaninterpretation canweusetheextractedpfato solve real world problems?
.
effectiveness to evaluate the effectiveness of our approach we design experiments to evaluate how well the extracted pfa approximate the rnn.
our test subjects are rnn trained on the following datasets.
tomitagrammars isanartificialdatasetcontainingstrings generated using different grammars.
these grammars were previouslyadoptedforresearchonrnnmodelextraction .
they consist of regular languages with different complexityoveralphabet .thedetaileddefinitionsofthe grammarsarelistedintable2.astringislabeledpositive if it is valid according to the grammar.
we apply the same settingasin togenerateatrainingsetandtestsetbased on the grammars.
that is we craft the training set with various lengths for each grammar i.e.
except tomita6whichhasadifferentlengthsetting anduniformly samplestringsoflength1 28asthetestsetforeach grammar.
the ratio between the training set and the test set i s4t o1 .
balancedparentheses bp isanartificialdatasetcontaining strings generated with an alphabet with letters i.e.
lower caselettersplus and .astringinthedatasetis labeledpositiveiftheparenthesesinthestringarebalanced i.e.
each opening parenthesis is eventually followed by a closing parenthesis.
following we generate a set of stringswithalengthof0 30andamaximumdepth oftheparentheses11.furthermore thenumberofpositive andnegativestringsforeachlengthisbalanced.theratio between the training set and the test set is to .
rotten tomatoes movie review rtmr is a movie review dataset collected from rotten tomatoes pages for sentimentanalysis which contains5331positiveand5331negativesentences.theaveragelengthofthisdatasetisabout21.
we take all the samples inthe dataset and divide them into two groups i.e.
as a training set and as a testing set.
imdbisawidelyusedbenchmarkforsentimentanalysisclassification.itcontains50 000moviereviewswhichareequally splitintoatrainingsetandatestset.intotal thereare25k positive reviews and 25k negative reviews.
the dataset is well collected and processed in a way that makes sure the reviews are as independent as possible.
since the samples in the dataset are much longer than those in rtmr and thesize of the dataset is much bigger to reduce the experiment time wetakethosesampleswithalengthlessthan51.we alsokeep80 oftheselecteddataasthetrainingsetand20 as the test set.table tomita grammars grammar definition tomita1 tomita2 tomita3the complement of tomita4 words not containing tomita5the number of and the number of areboth even numbers for each string tomita6the difference between the number of and the number of is a multiple of tomita7 table size of the datasets task dataset training set test set artificial datasettomita1 tomita2 tomita3 tomita4 tomita5 tomita6 tomita7 bp real world datasetrtmr imdb table performance of target models datasetlstm gru training set test set training set test set rtmr .
.
.
.
imdb .
.
.
.
bp .
.
.
tomita1 .
.
.
.
tomita2 .
.
.
.
tomita3 .
.
.
tomita4 .
tomita5 .
.
.
.
tomita6 .
.
.
.
tomita7 .
.
.
.
table3summarizesthesizeofalldatasets.wetrainrnnmodels to classify the strings in each dataset.
we adopt two popular types of rnns i.e.
lstm and gru.
we set the dimensions of hidden states and the number of hidden layers for the two rnns as and respectively as in .
when training the models we use one hot encoding to encode each character of the artificial dataset and use word2vec to transform each word of the real world dataset into a dimensions numerical vector.
table shows the performance of the trained models all of which is similar to the state of the art performance.
in total we have models.
weapplyourapproachtoall20modelstoextractmodels.we evaluatewhetherthemodelspreciselyapproximatethernnsusingtwomeasurements i.e.
accuracyandfidelity.theformermeasures thepercentageofthesamplesinthetestsetforwhichtheextracted 505ase september virtual event australia guoliang dong jingyi wang jun sun yang zhang xinyu wang ting dai jin song dong and xingen wang g28 g22 g31 g28 g22 g32 g28 g22 g33 g28 g22 g34 g28 g22 g35 g28 g22 g36 g29 g30 g31 g33 g35 g29 g28 g1 g11 g11 g20 g17 g9 g11 g21 g5 g20 g15 g10 g12 g17 g16 g13 g11 g14 g20 g18 g19 g12 g17 g18 g6 g8 g4 g6 g23 g3 g7 g8 g4 g24 g34 g34 g22 g31 g33 g39 g25 g3 g2 g1 g2 g3 g29 g29 g23 g32 g29 g23 g33 g29 g23 g34 g29 g23 g35 g29 g23 g36 g29 g23 g37 g30 g31 g32 g34 g36 g30 g29 g1 g12 g12 g21 g18 g10 g12 g22 g7 g21 g16 g11 g13 g18 g17 g14 g12 g15 g21 g19 g20 g13 g18 g19 g4 g6 g3 g2 g24 g5 g8 g9 g6 g25 g37 g29 g23 g37 g33 g40 g26 g3 g2 g1 g2 g5 g30 g28 g22 g31 g28 g22 g32 g28 g22 g33 g28 g22 g34 g28 g22 g35 g28 g22 g36 g29 g30 g31 g33 g35 g29 g28 g1 g10 g10 g20 g17 g8 g10 g21 g5 g20 g15 g9 g11 g17 g16 g12 g10 g14 g20 g18 g19 g11 g17 g18 g7 g16 g15 g13 g19 g8 g23 g3 g6 g7 g4 g24 g36 g28 g22 g36 g30 g39 g25 g3 g2 g1 g2 g3 g29 g28 g22 g31 g28 g22 g32 g28 g22 g33 g28 g22 g34 g28 g22 g35 g28 g22 g36 g29 g30 g31 g33 g35 g29 g28 g1 g11 g11 g20 g17 g9 g11 g21 g5 g20 g15 g10 g12 g17 g16 g13 g11 g14 g20 g18 g19 g12 g17 g18 g2 g6 g23 g3 g7 g8 g4 g24 g29 g28 g28 g39 g25 g3 g2 g1 g2 g3 g29 g29 g23 g32 g29 g23 g33 g29 g23 g34 g29 g23 g35 g29 g23 g36 g29 g23 g37 g30 g31 g32 g34 g36 g30 g29 g1 g12 g12 g21 g18 g10 g12 g22 g6 g21 g16 g11 g13 g18 g17 g14 g12 g15 g21 g19 g20 g13 g18 g19 g3 g2 g1 g2 g4 g30 g7 g8 g5 g7 g24 g3 g7 g9 g25 g35 g35 g23 g32 g34 g40 g26 g30 g24 g33 g30 g24 g34 g30 g24 g35 g30 g24 g36 g30 g24 g37 g30 g24 g38 g31 g32 g33 g35 g37 g31 g30 g1 g13 g13 g22 g19 g11 g13 g23 g8 g22 g17 g12 g14 g19 g18 g15 g13 g16 g22 g20 g21 g14 g19 g20 g5 g7 g3 g2 g25 g4 g9 g10 g26 g37 g33 g24 g32 g33 g41 g27 g3 g2 g1 g2 g6 g31 g29 g23 g32 g29 g23 g33 g29 g23 g34 g29 g23 g35 g29 g23 g36 g29 g23 g37 g30 g31 g32 g34 g36 g30 g29 g1 g11 g11 g21 g18 g9 g11 g22 g5 g21 g16 g10 g12 g18 g17 g13 g11 g15 g21 g19 g20 g12 g18 g19 g7 g17 g16 g14 g20 g9 g24 g3 g6 g8 g25 g37 g30 g23 g29 g36 g40 g26 g3 g2 g1 g2 g4 g30 g28 g22 g31 g28 g22 g32 g28 g22 g33 g28 g22 g34 g28 g22 g35 g28 g22 g36 g29 g30 g31 g33 g35 g29 g28 g1 g11 g11 g20 g17 g9 g11 g21 g5 g20 g15 g10 g12 g17 g16 g13 g11 g14 g20 g18 g19 g12 g17 g18 g2 g6 g23 g3 g7 g8 g24 g36 g36 g22 g36 g39 g25 g3 g2 g1 g2 g4 g29 figure the accuracy of our approach vs. bl1 g27 g23 g30 g27 g23 g31 g27 g23 g32 g27 g23 g33 g27 g23 g34 g27 g23 g35 g28 g29 g30 g32 g34 g28 g27 g2 g14 g11 g12 g15 g14 g20 g22 g5 g21 g16 g9 g12 g18 g17 g13 g10 g15 g21 g19 g20 g12 g18 g19 g6 g8 g4 g6 g24 g3 g7 g8 g4 g3 g2 g1 g1 g3 g28 g28 g24 g31 g28 g24 g32 g28 g24 g33 g28 g24 g34 g28 g24 g35 g28 g24 g36 g29 g30 g31 g33 g35 g29 g28 g3 g15 g12 g13 g16 g15 g21 g23 g7 g22 g17 g10 g13 g19 g18 g14 g11 g16 g22 g20 g21 g13 g19 g20 g4 g6 g2 g1 g25 g5 g8 g9 g6 g3 g2 g1 g1 g5 g29 g27 g23 g30 g27 g23 g31 g27 g23 g32 g27 g23 g33 g27 g23 g34 g27 g23 g35 g28 g29 g30 g32 g34 g28 g27 g2 g14 g11 g12 g15 g14 g20 g22 g5 g21 g16 g9 g12 g18 g17 g13 g10 g15 g21 g19 g20 g12 g18 g19 g7 g17 g16 g14 g20 g8 g24 g3 g6 g7 g4 g3 g2 g1 g1 g3 g28 g27 g23 g30 g27 g23 g31 g27 g23 g32 g27 g23 g33 g27 g23 g34 g27 g23 g35 g28 g29 g30 g32 g34 g28 g27 g2 g14 g11 g12 g15 g14 g20 g22 g5 g21 g16 g9 g12 g18 g17 g13 g10 g15 g21 g19 g20 g12 g18 g19 g1 g6 g24 g3 g7 g8 g4 g3 g2 g1 g1 g3 g28 g28 g24 g31 g28 g24 g32 g28 g24 g33 g28 g24 g34 g28 g24 g35 g28 g24 g36 g29 g30 g31 g33 g35 g29 g28 g2 g15 g12 g13 g16 g15 g21 g23 g6 g22 g17 g10 g13 g19 g18 g14 g11 g16 g22 g20 g21 g13 g19 g20 g3 g2 g1 g1 g4 g29 g7 g8 g5 g7 g25 g3 g7 g9 g29 g25 g32 g29 g25 g33 g29 g25 g34 g29 g25 g35 g29 g25 g36 g29 g25 g37 g30 g31 g32 g34 g36 g30 g29 g3 g16 g13 g14 g17 g16 g22 g24 g8 g23 g18 g11 g14 g20 g19 g15 g12 g17 g23 g21 g22 g14 g20 g21 g5 g7 g2 g1 g26 g4 g9 g10 g3 g2 g1 g1 g6 g30 g28 g24 g31 g28 g24 g32 g28 g24 g33 g28 g24 g34 g28 g24 g35 g28 g24 g36 g29 g30 g31 g33 g35 g29 g28 g2 g15 g12 g13 g16 g15 g21 g23 g5 g22 g17 g10 g13 g19 g18 g14 g11 g16 g22 g20 g21 g13 g19 g20 g7 g18 g17 g15 g21 g9 g25 g3 g6 g8 g3 g2 g1 g1 g4 g29 g27 g23 g30 g27 g23 g31 g27 g23 g32 g27 g23 g33 g27 g23 g34 g27 g23 g35 g28 g29 g30 g32 g34 g28 g27 g2 g14 g11 g12 g15 g14 g20 g22 g5 g21 g16 g9 g12 g18 g17 g13 g10 g15 g21 g19 g20 g12 g18 g19 g1 g6 g24 g3 g7 g8 g3 g2 g1 g1 g4 g28 figure the fidelity of our approach vs. bl1 model generatethe correctlabel.
thelatter measureshow consistent the extracted model and the rnn model are which is defined as follows.
fidelity summationtext.
x tsign a x r x t whereaistheextractedmodel risthernn tisthetestset xis any sample in the test case and sign y is a sign function which equals if yholds and otherwise.
in the following we discuss the experiment results via a comparison with existing approaches.
there are three approaches which we can potentially compare to.
firstbaseline thefirstoneistheapproachproposedin referred toasbl1 whichextractsdfafromrnn.theysimilarlyreduce the hidden state space through clustering and then regard each cluster as a state of the learned automaton.
next they map the hiddenstate traceof eachinput intoan abstracttrace.finally the transitionsbetweenstatesintheabstractstatetracesthatoccurthe most frequent are taken as transitions in the learned automaton.
forasystematiccomparisonwithbl1 wevarythenumberof clusters used for abstracting the hidden states for both approaches.
figure shows the accuracy of extracted models using bl1 andour approach respectively.
notice that the results on the tomita grammars are the average of the grammars for the sake of space.
itcanbeobservedthatthemodelsextractedwithourapproachare significantly more accurate than those generated by bl1 across all 20models.whileourapproachconsistentlyachievesanaccuracyof70 tonearly90 bl1 saccuracyrangesfrom50 toslightlyabove .this isexpected asthe modelsthat bl1 extractsonly containtransitionswithmaximumfrequencywhileourapproach is able to preserve all transitions through a probability distribution.
furthermore it can be observed that in most cases the models extractedusingourapproachhaveaperformancesimilartothatof the original rnn models i.e.
most of the extracted models have anaccuracywithin10 differencewiththeoriginalmodels.this suggests that our approach is capable of extracting precise models which have similar performance with the rnn.
in terms of fidelity as shown in figure it can be observed that ourapproachissignificantlybetterthanbl1aswell.thefidelity ofthemodelsextractedusingourapproachrangesfrom82 bp lstm to over whereas that of the models extracted using bl1rangesfrom52 bp lstm toabout65 only.specifically thefidelitycomparisonfortomitagrammars bp rtmrandimdbare vs. vs. vs. and vs. respectively.
thedifferencesaremorenoticeableforreal worldcomplextasks like imdb.
one possible reason is that the real world datasets are complicated and the idea underling bl1 does not apply in the realworldsetting.incomparison ourprobabilisticabstractionapproach is capable of taking into consideration the probability distribution amongtheabstractstatesandthusextractaccuratemodels.note thatourapproachextractsmodelswithhighfidelity i.e.
mostof the models have fidelity larger than which shows that our extracted models often precisely approximate the rnns.
506towards interpreting recurrent neural networks through probabilistic abstraction ase september virtual event australia table accuracy comparison between our approach and bl2 modelmethod tomita1 tomita2 tomita3 tomita4 tomita5 tomita6 tomita7 bprtmr idmb lstmours .
.
.
.
.
.
.
.
.
.
bl2 .
.
.
.
.997timeout timeout gruours .
.
.
.
.
.
.
.
.
.
bl2 .
.
.
.
.997timeout timeout table fidelity comparison between our approach and bl2 modelmethod tomita1 tomita2 tomita3 tomita4 tomita5 tomita6 tomita7 bprtmr idmb lstmours .
.
.
.
.
.
.
.
.
bl2 .
.
.
.
.997timeout timeout gruours .
.
.
.
.
.
.
.
.
bl2 .
.
.
.
.996timeout timeout second baseline the second approach we compare to is the one in referredtoasbl2 whichappliesthel algorithm to extract a dfa from rnn.
it first builds a dfa based on an observation table and then buildsan abstract dfafrom the rnn withan intervalpartitionfunction i.e.
aheuristic basedabstraction .after that itcheckstheequivalencebetweenthetwodfasandrefines oneofthemifaconflictoccurs.thealgorithmrepeatstheabove procedure until the two dfas are equivalent and returns the dfa.
we remark that checking equivalence of two dfas is expensive and impractical when the alphabet is large which is the case for real world tasks.
since there is no clustering in bl2 we apply algorithm with a fidelityrequirementof a .99andatimeoutof400seconds.note that the same timeout is set for bl2 which is also the one adopted in .theresultsintermsofaccuracyareshownintable5.first it can be observed that bl2 fails to work on either the rtmr or imdb dataset.
this is expected as the alphabet of these datasets is the vocabulary of the training set which is for rtmr and for imdb.
they are thus way beyond the capability of bl2.
second ourapproachachievescompetitiveresultswith bl2onthe two artificial datasets i.e.
on average bl2 has an accuracy that is .
more than our approach.
the results of comparing fidelity areshownintable6.wecanobservethatourapproachachieves high fidelity i.e.
.
on average with the rnn model on all datasetandbl2failstoreporttheresultsonbothrtmrandimdb forthesamereason.onthetwoartificialdatasets bl2hasafidelity that is .
more than our approach on average.
thirdbaseline thethirdapproachistherecentapproachreported in .
it learns a probabilistic model for approximating rnn throughanextendedversionofthel algorithm.whileithasimpressiveperformanceontaskswithasmallalphabet likeinthecase of the two artificial datasets the authors admittedly report that their approach does not apply when the alphabet is large like in the case of the two real world datasets .
this is confirmed in ourexperimentsaswell i.e.
theirimplementationfailedtowork oneitherrtmr orimdb.weomitadetailedcomparison dueto its limited applicability and the fact that it is not implemented for classification tasks which makes it hard to compare to.based on the above experiment results we thus conclude that our approach is able to extract accurate models from rnn and is capable of handling real world rnn models.
.
level of abstraction our approach allows users to specify a target fidelity and aims to extract a model based on the right level of abstraction.
this is donethroughcontrollingthenumberofclusters whichdetermines the size of the alphabet and consequently the size of the extracted models.algorithm2isdesignedbasedonthehypothesisthatthe more clusters we use the more fine grained the abstraction is and thus the more accurate the extracted model will be.
since the more clustersweuse themorecomplicated i.e.
thelesscomprehensive the extracted model will be it is important tofind a balance.
toevaluatewhetherthishypothesisholdsandunderstandthe relationshipbetweenthenumberofclustersandthefidelity sizeof theextractedmodels weconductthefollowingexperiments.we systematically extract models with clusters ranging from to and evaluate the size and accuracy fidelity of the extracted models.
table summarizes how the size of extracted pfa changes with different numbers of clusters.note that these results are basedon applyingourapproachtothegrumodels.similarresultsareobtained on the ltsm models and are thus omitted.
we observe that asweincreasethenumberofclusters thesizeoftheextractedmodels increases in most of the cases.
however it is not monotonically so.
for instance for the bp dataset the number of states decreases whenthenumberofclustersincreasesfrom6to10.thisisbecause thenumberofstatesisdeterminedjointlybythenumberofclusters andthedegreeofgeneralizationachievedbyalgorithm1.itisthus possible that in some cases the same cluster of hidden feature values behaves differently in different contexts e.g.
the sequence of feature values before reaching the cluster and thus must be differentiated into multiple states in the extracted pfa.
figure shows the relationship between the number of clusters and the accuracy fidelity of the extracted models.
we observe that as weincrease the numberof clusters the accuracy fidelity ofthe extracted models improves overall.
the improvement however mayvaryacrossdifferentmodelsordifferentnumberofclusters.
for some models the improvement is consistent and significant e.g.
inthecaseofthebpdataset forsomemodels theimprovement isconsistentbutminor e.g.
inthecaseoftomitagrammarsand 507ase september virtual event australia guoliang dong jingyi wang jun sun yang zhang xinyu wang ting dai jin song dong and xingen wang g27 g24 g31 g33 g27 g24 g32 g33 g27 g24 g33 g33 g27 g24 g34 g33 g29 g30 g31 g33 g28 g27 g2 g14 g14 g22 g19 g12 g14 g23 g4 g16 g22 g20 g21 g15 g19 g1 g18 g22 g17 g13 g15 g19 g20 g11 g8 g7 g6 g11 g2 g3 g9 g10 g11 g7 g10 g6 g7 g5 g3 g28 g25 g32 g34 g28 g25 g33 g34 g28 g25 g34 g34 g28 g25 g35 g34 g30 g31 g32 g34 g29 g28 g6 g16 g14 g15 g17 g16 g22 g24 g4 g17 g23 g21 g22 g15 g20 g1 g19 g23 g18 g13 g15 g20 g21 g12 g9 g8 g7 g12 g2 g3 g10 g11 g12 g8 g11 g7 g8 g5 g3 figure effects of different number of clusters.
table the model size with different numbers of clusters datasetnumber of cluster tomita bp rtmr imdb rtmr and for some models the accuracy fidelity may drop along the way e.g.
in the case of imdb.
for the last case we suspect that it is due to the fact the rnn model is very complicated and algorithm1failedtoconvergetoaconcise accuratemodelaswe noticethatthenumberofstatesincreasesdramaticallywhenwe increase the number of clusters.
this suggests a future research direction on developing new learning algorithms for probabilistic models that are effective with a large alphabet and complicated probabilistic distribution.
note that existing work such as the one in is limited to models with very small alphabets.
.
usefulness we have shownthat our approach is able to extract models which approximaternnaccurately.thisfacilitatessomedegreeofhuman interpretationandautomaticanalysisofrnn.forinstance given anrnntrainedforsentimentanalysis withtheextractedmodel wecansystematicallycomputetheprobabilityofgeneratinga positive label through manual computation if the model is very small or probabilistic modelchecking otherwise afterprovided witheach word in a sentence.
by monitoring how the probability changes witheachword wecandevelopsomeintuitiveunderstandingon how the sentiment analysis result is derived.
such usefulness is however subjective.inthefollowing wereportanapplicationof the extracted pfa models for adversarial text detection.
givenanrnnmodel adversarialtextsaretextswhicharecrafted specifically to induce mistake i.e.
so that the rnn s classificationresultiswrong .ithasbeenshownin thatadversarialtexts canbesystematicallygeneratedbyapplyingasmallperturbationto a benign text which otherwise is correctly classified by the rnn .
typicalwaysofgeneratingadversarialtextsincludeidentifyingand replacing important words in a sentence with its synonyms o r applyingneuralmachinetranslationtwice e.g.
fromenglishto french and then back to the given sentence.
detecting adversarial textsishighlynontrivialandtothebestofourknowledge there have not been systematic methods proposed for that.
in the following we show that the pfa models extracted using our method can be used to detect adversarial texts effectively.
the intuition is that given a benign text our pfa would associate a much higher probability with its label predicted by the rnn than otherlabels andgivenanadversarialtext theprobabilityassociated with each label would not be very different.
this intuition is partly basedonthefactthattheseadversarialtextsaretypicallygenerated by perturbing a benign text just enough to across the classification boundary.
based on this intuition we design the following metric todetectadversarialtexts.givenatext x whichcouldbebenign or adversarial let t x p x y p x y whereyisthelabelpredictedbythernn p x y istheprobability ofreaching label ybasedon ourextractedpfa whichis obtained as explained in section .
using probabilistic model checking and p x y denotes the summed probability of reaching labels other thany.
we then distinguish adversarial texts from benign ones using a threshold on t x i.e.
a text is considered as adversarial if it has at x smaller than the threshold.
weevaluatetheeffectivenessoftheabovemethodforadversarial text detection on the two real world datesets.
concretely for each benigntextinthetestsetofrtmrandimdbdatasets wegenerate anadversarialtextusingtextbugger .wethenrandomly select 1000benign textsand correspondingadversarial texts tocomposeatestsetforourdetectionmethod.wecalculate t x y for all the texts in the test set and report the auc area undercurve score to measure the effectiveness of our detection methodsinceaucavertsthesupposedsubjectivitywhenselecting the threshold for a classifier and measures how true positive rateandfalsepositiveratetradeoff.tofurtherstudytheeffectof havingadifferentnumberofclusters weapplythemethodwith pfa extracted with different number of clusters.
the results are summarized in table where we vary the number of clusters from to .
we observe that our method effectively detects adversarial texts i.e.
achievinganaverageaucof0.85and0.93forrtmrand imdb respectively.
we do also notice that the auc varies with the numberofclustersinawaywithnoclearcorrespondencewiththe pfa s accuracy fidelity which we will investigate in the future.
the above study suggests that our model extraction approach notonlyoffersawayofsheddingsomelightonhowrnnworks but alsopotentially opens thedoor for applyingsoftware analysis techniques like model based testing model checking runtime monitoring and verification to real world rnn models.
related work wereview relatedworksin thissection.from abroader point of view this workis relevantto theexplanation ofmachine learning 508towards interpreting recurrent neural networks through probabilistic abstraction ase september virtual event australia table auc of adversarial sample detection.
dataset modelnumber of clusters rtmrlstm0.
.
.
.
.
gru0.
.
.
.
.
imdblstm0.
.
.
.
.
gru0.
.
.
.
.
models which can be categorized into local and global explanationsin general.intuitively local explanationtriesto explainwhy thetargetmachinelearningmodelmakesadecisiononacertain input.
one example is the shap like system which uses a linear function to mimic the complex models e.g.
convolutional neural network cnn when producing a certain output on an input.globalexplanation however aimstounderstandtheinternal decision process by using a more interpretable model to mimic the behaviors of the original model on anyinputs.
one example is the work in .
our work takes a global explanation perspective.
this work is related to work on rnn rule extraction.
rule extractionfromrnnistheprocessofconstructingdifferentcomputa tionalmodelswhichmimicthernn .thisworkisespecially relatedtotheworkthatextractsadeterministicfiniteautomaton dfa fromrnn.theseapproachesusuallyrelyonencodingthe hidden states intosymbolic representationsusing techniqueslike clustering orintervalpartitioning .ourworkisdifferentby learningaprobabilisticfiniteautomaton pfa fromthesymbolic data.
there is also some recent work aiming to extract a weighted automaton wa or discrete time markov chain .
however neither of them provide generalization to capture the temporal dependency over the symbolic representations.
our work encodes the concrete states in a similar way but then uses probabilistic abstraction to extract a probabilistic model.
thestudyoflearningpfaisabranchofgrammarinference whichhasbeeninvestigatedunderdifferentsettingsusingmethods like state merging or identifying the longest dependent memory .
recently researchers have proposed to learn pfa for system analysis tasks like model checking or runtime monitoring .thisworkfollowsastate merginglearningparadigmto learn a pfa from the symbolic data extracted from rnn.
conclusion in this work we propose to extract probabilistic finite automata fromstate of the artrecurrentneuralnetworktotrace mimicits behaviorsforanalysisbyprobabilisticabstraction.ourapproach isbasedonsymbolicencodingofrnnhiddenstatevectorsanda probabilistic learning algorithm which tries to recover the prob ability distribution of the symbolic data.
the experiment resultson real world sentiment analysis tasks show that our approach significantlyimprovesthequalityorscalability ofstate of the art modelextractionworks.ourapproachprovidesonepromisingwaytobridgethegapforapplyingavarietyofsoftware systemanalysis techniques to real world neural networks.