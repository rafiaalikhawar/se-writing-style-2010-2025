unleashing the power of compiler intermediate representation to enhance neural program embeddings zongjie li pingchuan ma huaijin wang shuai wang the hong kong university of science and technology hong kong sar zligo pmaab hwangdz shuaiw cse.ust.hkqiyi tang sen nie shi wu tencent security keen lab china dodgetang snie shiwu tencent.com abstract neuralprogramembeddingshavedemonstratedconsiderablepromise in a range of program analysis tasks including clone identification program repair code completion and program synthesis.
however most existing methods generate neural program embeddings di rectly from the program source codes by learning from features such as tokens abstract syntax trees and control flow graphs.
thispapertakesafreshlookathowtoimproveprogramembeddings by leveraging compiler intermediate representation ir .
we firstdemonstratesimpleyethighlyeffectivemethodsforenhancingembeddingqualitybytrainingembeddingmodelsalongsidesource code and llvm ir generated by defaultoptimization levels e.g.
o2 .wethenintroduceirgen aframeworkbasedongeneticalgorithms ga toidentify near optimalsequencesofoptimization flags that can significantly improve embedding quality.
we use irgen to find optimal sequences of llvm optimization flags by performing ga on source code datasets.
we then extend a popularcodeembeddingmodel codecmr byaddinganewobjectivebasedontripletlosstoenableajointlearningoversourcecodeandllvmir.webenchmarkthequalityofembeddingusingarepresentativedownstreamapplication codeclonedetection.when codecmrwas trained with source code and llvm irs optimized by findings of irgen the embedding quality was significantly improved outperformingthestate of the artmodel codebert which wastrainedonlywithsourcecode.ouraugmented codecmralso outperformed codecmr trained over source code and ir optimized withdefaultoptimizationlevels.weinvestigatethepropertiesof optimization flags that increase embedding quality demonstrate irgen sgeneralizationinboostingotherembeddingmodels and establishirgen suseinsettingswithextremelylimitedtraining data.ourresearchandfindingsdemonstratethatastraightforward additiontomodernneuralcodeembeddingmodelscanprovidea highly effective enhancement.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
reference format zongjie li pingchuan ma huaijin wang shuai wang and qiyi tang sen nie shiwu.
.unleashingthepowerofcompilerintermediaterepresentationtoenhanceneuralprogramembeddings.in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm newyork ny usa 13pages.
introduction recentdevelopments indeep neuralnetworks dnns havedelivered advancements in computer vision cv and natural language processing nlp applications.
we have noticed lately an increase ininterestinusingdnnstosolveavarietyofsoftwareengineering se problems including software repair program synthesis reverseengineering malwareanalysis and programanalysis .similartohowdnnsunderstanddiscretenaturallanguagetext nearlyallneuralseapplicationsrequire computing numeric and continuous representations over software whicharereferredtoasprogramembeddingsorembeddingvectors.
the common procedure for generating code embeddings is to processaprogram ssourcecodedirectly extractingtokensequences statements or abstract syntax trees asts to learn program representations .althoughsomepreliminaryapproaches have attempted to extract semantics level code signatures such approachesarelimitedbyuseofsemanticfeaturesthataretoocoarsegrained low code coverage due to dynamic analysis or limited scalability .
to date learning from code syntactic andstructuralinformationhasremainedthedominantapproach inthisfield andaspreviousworkhasargued the use of features at this relatively shallow level is likely to degrade learning quality and produce embeddings with low robustness.
for some cv and nlp tasks data augmentation has been proposed as a tool to improve the quality of learned embedding representations .theseapproachestypicallyincreasetheamount of training data by adding slightly modified copies of already existingdata orby creatingnew piecesofsynthetic datafrom existing data.
thus embedding models can be trained on larger numbers of data samples resulting in higher quality embedding representations.
previous research has shown the value of data augmentation approaches in increasing embedding quality .
thiswork investigatesusingcompiler intermediaterepresentations ir toaugmentcodeembedding.moderncompilersinclude numerous optimization flags that can seamlessly convert a piece of source code into a range of semantically identical but syntactically distinct ir codes.
from a comprehensive standpoint we argue that ourtechnique canboostprogram embeddingontwo fundamental levels.first the translation of a single piece of source code into ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zongjie li pingchuan ma huaijin wang shuai wang and qiyi tang sen nie shi wu severalvariantsofircodewiththesamefunctionalitysignificantly increases the diversity of available training data.
as previously noted such augmented data can commonly improve the quality of learned embeddings.
second although programs with the same functionalitymayappearsyntacticallydistinctassourcecode they arelikelytobecomemoresimilarafterpruningandrearrangement by optimizations.
this alleviates the difficulties imposed by syntax changes as the optimizations regulate syntactic characteristics.
webeginbyillustratingthatusingdefaultcompileroptimization levels such as o2of llvm can produce ir code that significantly improves the embedding quality of a popular embedding model codecmr andoutperformsthestate of the art sota model codebert trained on source code alone.
however despite the promising potential in this misuse of compiler optimizations the high number of available optimization flags and the consequently large search space present a challenge for identifying wellperforming optimizationsequencesto augmentembeddingmodels.
we propose irgen a framework that uses genetic algorithms ga tosearchfor near optimaloptimizationsequencesforgeneration of ir code to augment program embedding models.
compiler optimizationflagsaretypicallycombinedtogeneratemachineinstructions with high speed or small size.
in contrast irgen targets optimizationsequences generatingircodethatis structurallysimilartotheinputsourcecode.thispreventsover simplificationofthe ir code which is undesirable in our task since overly simplified ir oftenbecomes less expressive.
additionally tomaximizelearning efficiency we limit overuse of out of vocabulary oov terms our definition of oov follows ncc see sec.
.
.
wepresentasimpleyetunifiedextension throughtripletloss to enable embedding models to learn from source code and llvm ir.forevaluation weusedirgentoanalyzeirsgeneratedfrom the poj and gcj datasets which include a total of 880c c programs.after143to963cpuhoursofsearch we use a desktop computer to run irgen irgen was able to form optimization sequences with high fitness scores from the optimizationflagsavailableinthex86llvmframework ver.
.
.
.
to evaluate the quality of embedding we setup a representative downstreamtask codeclonedetection.when codecmr wastrained with ir code generated by the identified optimization sequences embedding quality in terms of code clone detection accuracy significantly improved by an average of .
peaking at .
outperformingthesotamodel codebert trainedwithonlysource code for12.
or codecmrjointlytrainedwithsourcecodeandir emitted by default optimizations for .
.
we also demonstrate that irgen is general to augment other neural embedding models and show that irgen can almost doublethe quality of learned embeddings in situations with limited data e.g.
of training dataavailable .
we characterize optimization flags selected by irgenand summarize our findings.
this work can help users take use ofcompileroptimization an out of the box amplifier toimprove embedding quality.
in summary our contributions are as follows we advocate the use of compiler optimizations for software embedding augmentation.
deliberately optimized ir code can principally improve the quality of learned program em beddings by extending model training datasets and normalizing syntactic features with modest cost.
we build irgen a practical tool that uses ga algorithmsto iteratively form near optimal optimization sequences.
additionally wepresentasimpleyetgeneralextensionover modern code embedding models to enable joint learning over source code and ir.
our evaluation demonstrates highly promising results with our augmented model significantly outperforming sota models.wefurtherdemonstratethegeneralizationof irgenanditsmeritinaugmentingverylimitedtrainingdata.irgen is released at .
preliminary neural code embedding as in fig.
converts discrete source code to numerical and continuous embedding vectors with the end goal of facilitating a variety of learning based program analysis.
we introduce program representations in sec.
.
.
we examine alternativemodeldesignsinsec.
.2andtheconceptofdataaugmentation for neural software embedding in sec.
.
.
figure common neural program embedding pipeline.
.
input representation code can be expressed as text and processed using existing nlp models.
however it would be costly and likely ineffective because programming languages usually contain a wealth of explicit and sophisticatedstructuralinformationthatisdifficultfornlpmodels to comprehend .
therefore modern code embedding models often learn programembeddings using code structural representationswhich areinformative.for instance theabstract syntaxtree ast isusedtorepresentcodefragmentsforprogramembeddings.
once a code snippet s ast has been generated there are several methodsforextractingdiscretesymbols e.g.
astnodes foruse in the subsequent learning process.
for example code2vec andcode2seq extractacollectionofpathsfromasttoform embeddings as discussed below.
control flow graphs cfg are also used to form input representation especially when analyzing assembly code.
two repre sentative tools asm2vec and binaryai construct cfgs overassemblycodeandcombinebasicblock levelembeddingsinto the program s overall embedding.
recentresearch has explored the use of hybrid representations that incorporate datafrom different layers.
for instance ncc extracts a so called contextualflowgraphfirst whichsubsumesinformationfromboth control flow graph and data flow graph.
.
neural model learning procedure nlpmodelsaregenerallydesignedtoprocessinfinitesequences oftokens whereassoftwareisstructured.hence theneuralcode authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
unleashing the power of compiler intermediate representation to enhance neural program embeddings icse may pittsburgh pa usa embedding learning process can be divided into two broad categories decomposingprogram structural representations e.g.
ast or cfg into one or multiple token sequences that are then processed by nlp models and attempting to initiate an endto end procedurefordirectlylearningstructuralrepresentations using advanced neural models like graph neural networks gnn .
codebert is a large scale sota code embedding model that primarily learns from token level software representations.
it is inspiredbybert afamousbidirectionalnaturallanguageembeddingmodel.
codebert constructslearningobjectivesusingboth maskedlanguagemodeling mlm andreplacementtokendetection.usingtheseobjectives itistrainedtopredicttokensthathave beenrandomlymaskedoutoftheinputsuntilsaturationaccuracyisreached.anothermodel asm2vec usesmlms particularlyan extendedpv dmmodel toembedx86instructionsatthetoken level.
token sequences can be extracted from tree or graph representations.forexample code2vec and code2seq break astintopaths transformpathstoembeddingsusinglstm and finally aggregate path level embeddings to produce the ast s embedding.
the structure based traversal method converts asts into structured sequences.
tbcnn great and binaryai leverage advanced models such asgnns todirectlyprocess programstructuralrepresentations.
binaryai for example uses standard gnns to propagate and aggregate basic block embeddings into cfg embeddings.besidescfgs neuralmodels cancreatestructureswith richer information.
ncc forms a contextual flow graph with control and data flow information.
each node in the contextual flow graph contains a list of llvm ir statements which nccthen transforms into vectors.
it further uses a gnn to aggregate thenode embeddings into an embedding of the entire program.
as with ncc misimbeginsbyconstructinganovelcontext awaresemanticstructure cass fromcollectionsofprogramsyntax and structure levelproperties.itthenconvertscassintoembedding vectors using gnns.
it outperforms prior ast based embedding tools including code2vec andcode2seq .
.
data augmentation images can be rotated while retaining their meaning e.g.
viaaffine transformations .
similarly we can replace words in natural language sentences with their synonyms which should notimpairlinguisticsemantics.dataaugmentationleveragesthese observations to create transformation rules that can enlarge model training data.
itisworthnotingaconventionaltechnique namelyfeatureengineering cangenerallyhelpdatascienceandmachinelearning tasks.featureengineeringfacilitatestoeliminateredundantdata that can reduce overfitting and increase accuracy.
nevertheless in theeraofdeeplearning itgraduallybecomeslessdesirabletomanually pickusefulfeatures giventhatweneedtofrequentlydeal with high dimensional data like image text video and software.
howtopickusefulfeaturesisoftenobscurewhenlearningfrom thosecomplexhigh dimensionaldata.infact ithasbeendemonstrated that data augmentation generally and notably improves deeplearning modelperformance androbustness andit hasbeen frequently employed as a routine technique to enhance moderntable map scores of codecmr on poj for different input setup.
setup map setup map source .
source llvm ir o2 .
source llvm ir o0 .
source llvm ir o3 .
source llvm ir o1 .
source llvm ir os .
source llvm ir optimized by optimization sequences found by irgen .
deep learning models in a variety of domains .
standard data augmentation approaches however are notdirectly applicable to enhance program embeddings.
augmentingneural program embeddings is challenging and under explored.due to the synthetic and semantic constraints of programming languages arbitrary augmentation can easily break a well formed program.thispaperexploresbringingdataaugmentationtosource code.
in particular we advocate employing compiler optimizations to turn a same piece of source code into semantically identical but syntacticallydiverseircode.notethatwedonotneedto reinventing the wheel to develop extra semantics preserving source code transformations .instead wedemonstratehow amaturecompilercanfacilitateeffectivedataaugmentationsimplybyexploiting optimizations developed over decades by compiler engineers.
motivation thellvmcompilerarchitecturesupportshundredsofoptimization passes each of which mutates the compiler ir in a unique way.tomakecompileroptimizationmoreaccessibletousers the llvmframeworkoffersseveraloptimizationbundlesthatauser can specify for compilation for example o2 o3 and os.
the first two bundles combine optimization passes for fast code execution whereas osaimstogeneratethesmallestexecutablepossible.
ourpreliminarystudyshowsthatbyincorporatingoptimizedir codeintoembeddinglearning theembeddingqualitycanbesubstantiallyenhanced.thissectiondescribesourpreliminaryfinding which serves as an impetus for the subsequently explored research.
learningoversourcecode.
weusepoj acommonly used dataset containing c programs written for tasks.
thisdatasetissplitintothreeprogramsets onefortraining onefor validation andonefortesting seesec.
.wetrained codecmr onepopularcodeembeddingtool onthetrainingsplit andthen performmulti labelclassificationonthetestingsplit.
codecmr generatescodeembeddingsbyfirstconvertingsourcecodetoacharacter sequence and then computing character level embeddings.
the embeddingsarefedtoastackofpyramidconvolutionalneuralnetwork dpcnn in which an average pooling layer constructs the program s embedding.
dpcnn has been shown as powerful at embedding programs.
in our evaluation on poj we observe promisingaccuracyintermsofmap score asshownin source of table1.
map is a commonly used metrics in this field and a highermapscoreindicatesagreaterqualityofcodeembeddings.
as willbe shownin evaluation thisresult iscomparable to those obtained using the sota model codebert.
findings.
despite the decent results we find that codecmr fails togroupquiteanumberofpoj 104programswhobelongtothe same class.
the poj c code and corresponding llvm ir are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zongjie li pingchuan ma huaijin wang shuai wang and qiyi tang sen nie shi wu toolengthytofitinthepaper wepresentsomeveryreadablecases at and summarize our key observations below.
c c programsimplementingthesamefunctionalitycanexhibit distinct syntactic appearance.
for instance at we present a case where two programs p1andp2 are implementing the same daysbetweendates task.wefindthat p1usesone switchstatement whereas p2uses a sequence of ifstatements.
further p1 uses many local variables to encode days in each month while thoseinformationin p2arehardcodedinconstants.thisway p1 andp2 differ from both control and data flow perspectives.
nevertheless wefindthatthellvmircodecompiledfromthese two programs are much closer in both control and data flows.
let l1andl2 see be two llvm ir programs compiled from p1and p2andoptimizedwithoptimizationlevel o3.wefindthat l1andl2 preservesmostofthestructureofthesourcecode.morecrucially l1 andl2both use a llvm ir switchstatement to encode the control structures.
data usage is also regulated where both local variables inp1and the constants in p2become integers hardcoded in ir statements.theinducedirprograms l1andl2are visually very similar revealingthetruesemantics levelequivalenceof p1andp2.
wethussuspectthat codecmrisindeedhamperedbytooflexible code representationin c programs.in other words it is shownas demanding to explore extracting more robust features from c c code to enhance the learning quality.
learning over code structure or semantics.
as previously stated codecmr learnsonthecharacter token sequence.thisindicates that codecmr is less resilient against changes at the syntactic level.
graph level embeddings might be more robust to token level changes given their reliance on the rich structural information containedintheprogram.nonetheless inreal lifecodesamples manychangescanalsooccuratthegraphlevel andasshownin sec.
representativegraph levelembeddingmodelsalsoperform poorly on diverse and large scale datasets such as poj .
some readers may wonder if learning directly from code semantics such as input output behaviors captured by dynamic analy sis is possible.
while dynamic analysis can precisely describe code behaviors on the covered paths it suffers from low coverage.
symbolic execution se is used to include a greater amount of program activity in applications such as code similar ity analysis .
nonetheless se is inherently inefficient where trade offs are made to launch se over real world software .
learning over ir code.
this paper advocatesusing compiler ir toextendmodeltraindatasetandenhancecodeembeddingmodels.
however we do notsuggest learning solely from ir for two reasons.
first compiler optimizations such as static single assignment ssa result in llvm ir codes that typically have ten times as manylinesofcode loc asthecorrespondingcsourcecode.this providesasignificantimpedimenttotrainingembeddingmodels.
in our preliminary study we find that training embedding models using llvm ir code alone resulted in significantly inferior performanceacrossmultipletasksanddatasets.second whenoutputting ir code the llvm compiler prunes or obfuscates certain source code features such as string and variable names.
note that variable names and constants are generally crucial to improving embed dingquality.similarly inllvmircode callsites particularlyto standard libraries like glibc areoften modified.for example the callsite statement in set int row row.insert x would beconverted to a complex function name with additional prefixes.
notably weshouldavoidtweakingwithordisablingcertain annoying optimization passes for example the ssa pass as many optimization flags assume the existence of other flags.
learningoversourcecodeandircode.
weextended codecmr toprocessircodeemittedby clang.weaugmentedthefrontend ofcodecmrto process llvm irs.
we also extended the learning objectives by requiring codecmr to minimize the distance between thesourcecodeandcorrespondingirusingtripletloss seesec.
.
.
we compiled each test program in the poj training set into llvmirtotrainthe codecmr andthenbenchmarkthemapscore using the same setting.
asseenintable usingllvmirinthelearningprocesssignificantlyimprovedembe ddingperformance.forinstance whencompilingthesourcecodeintollvmirwithnegligibleoptimization o0 thejointlearningenhancedthemapscorebyapproximately .
note that jointly training over source code and ir o0 has already outperformed the sota model codebert .
.
more importantly it is seen that compiler optimizations can notably improve codecmr s performance.
we observe that as compared with o0 using optimization levels o2 o3 and osproduces map scores greater than .
we regard the above findings as encouraging and intuitive they demonstrate the possibility and benefit of learning jointly fromsource code and ir code which are more regulated rather than fromsourcecodealone.inevaluation sec.
.
wediscussoptimizationflagsfurtherwithcasestudiestorevealthattheycaneffectively regulatecodegenerationpatterns removesuperfluouscodefragments and generate more consistent ir code in the presence ofsyntactic or structural changes in the source code.
we therefore summarize the key findings of this early investigation as follows launchingajointtrainingusingbothprogramsourcecodeand corresponding ir can notably improve the embedding quality.
limitation of standard optimization levels.
despite the encouraging results we note that these default optimization levelsare selected by compiler engineers with differentfocus e.g.
producing smallest possible executable or fast execution.
however we explore a different angle where optimizations are misused to generatellvm ircode to augment programneuron embedding.
in that regard it is possible to suspect the inadequacy of utiliz ing simply the default optimization levels.
for instance certain cpu flags in osand o3are aggressive in shrinking ir code e.g.
aggressive instcombine which mightnotbepropersinceembeddingmodelsgenerallyprefermore expressive inputs.inevalua tion wefindthataggressiveflagssuchas aggressive instcombine arenotpickedbyirgen.wealsofindthatoptimizationflagsshould be adaptive to source code of different complexity whereas default optimization levels are fixed.
sec.
.3compares optimization flags selected by irgen when analyzing different datasets.
we introduce irgen an automated framework to determine near optimalsequencesofoptimizationflagsforeachparticular dataset.
to compare with standard optimization levels the finalrow of table 1presents the improved performance of codecmr authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
unleashing the power of compiler intermediate representation to enhance neural program embeddings icse may pittsburgh pa usa when using irgen selected optimization flags.
these results show that ir code optimized using irgen formed optimization sequence significantly improved the accuracy.
figure the workflow of irgen.
design of irgen fig.2depicts irgen s workflow.
given a dataset of c source code wefirstformavalidationdataset pwherefitnessscoresarecomputed from see sec.
.
.
our observation shows that the size of p does not need to be large otherwise the ga procedure becomes slow .
for the current implementation we randomly select programs from the training dataset poj or gcj see details in sec.
o f codecmr.
irgen initializesthe firstgeneration ofoptimization sequences sec.
.
and then launches the ga based search to iteratively modify and select sequences giving high fitness scores sec.
.
.
the ga process is repeated ntimes until termination nis currently .aftertermination weselectthe ksequenceswiththe top khighest fitness scores across the entire ga search.
using these optimization sequences each piece of c c code in the trainingdatacanbecompiledinto ksyntacticallydistinctpiecesof llvm ir code.
the resultingaugmented code dataset can be used toempowerneuralembeddingmodels byincorporatingtripletloss as an extra learning objective sec.
.
.
application scope.
this research mainly focuses on the llvm compiler framework given its popularity.
llvm provides a total of optimization flags that are applicable on x86 platforms.
irgen traverses the entire search space to identify sequences expected to improveembeddingquality.irgen siterativemethodis orthogonal to the llvm framework and can therefore be extended to support othercompilers suchas gcc tomanipulateitsgimpleir without incurring additional technical difficulties.
thecurrentimplementationof irgenprimarilyenhancesc c codeembedding.c c programscanbe compiledintollvmir and optimized accordingly.
moreover most security related embeddingdownstreamapplications e.g.
cvesearch concern c c programs.
nevertheless we clarify that code embedding has its wide application on other languages such as java python.
itisworthnotingthatirgenreliesonarichsetofcompileroptimizationstogeneratediverseircode.java pythoncompilers interpreters provide fewer optimization passes and they leave many optimizationsatruntime.asaresult thesearchspaceforirgento explore would be much smaller.
we also have a concern on the expressivenessofjava pythonbytecodeincomparisonwithllvmir.
theirbytecodeseemsverysuccinct potentiallyunderminingthe sota embeddingmodels.overall weleaveit asonefuture work to explore extending irgen to enhance java python embedding.irgen s present implementation does not consider the order of optimizations in a sequence.
we also assume each flag can onlybe used once.
this enables a realistic and efficient design whenga is used similar design decisions are also made in relevant works .
taking orders or repeated flags into account would notably enlarge the search space and enhance the complexity of irgen.wereservethepossibilityofusingmetaheuristicalgorithms withpotentiallygreatercapacity suchasdeepreinforcementlearning for future work.
see sec.
7for further discussion.
design focus.
irgen s ga based pipeline was inspired by literatures in search based software engineering particularly us ing ga for code testing debugging maintenance and hardening .sec.8furtherreviewsexistingstudies.our evaluation will show that the ga method when combined withour well designed fitness function is sufficiently good at selecting well performingoptimization sequences.further enhancement may incorporate other learning based techniques see sec.
.
.
genetic representation followingcommongapractice werepresenteachoptimization sequenceasaone dimensionalvector v f1 f2 ... fl wherel isthetotalnumberofoptimizationflagsofferedbyllvmforx86 platforms.each fiisabinarynumber denotingwhetherthe corresponding flag ci is enabled or not on sequence i. as standard setup we initialize minstances of vector v by randomly setting elementsinavector vas1.theserandomlyinitializedsets referred to as a population in ga terminology provide a starting point to launch generations of evolution.
here mis set as .
.
modification and selection ateachgeneration t weemploytwostandardgeneticoperators crossover and mutation to manipulate all vectors in the population.giventwo parent vectors v1andv2 twooffspringsare generated using k point crossover kcross points are randomly selected on v1andv2 and the content marked by each pair of cross points is swapped between them.
here kis set as and the chance of each potential crossover is set as .
.
we also use flip bit mutation another common method to diversify vectors inthe population.
we randomly mutate of bits in vector v. after these mutations thepopulationsizeremainsunchanged 20vectors butsomevectorsaremodified.eachvectorisassessedusingthefitnessfunctiondefinedinsec.
.
.allmutatedandunmutatedvectorsare thenpassedintoastandardroulettewheelselection rws mod ule wherethechanceforselectingavectorisproportionaltoits fitness score.
this way a vector with a higher fitness score is more likely to be selected into the next generation.
the rws procedure is repeated times to prepare vectors for generation t .
.
fitness function given a vector v denoting a sequence of optimization flags fitness function fyields a fitness score as an estimation of v s merit.
specifically for each v we compile every program pin the validation dataset pusing optimizations specified in vto produce ir programs l l. forac program pandits compiledir l wefirst compute the following fitness score authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zongjie li pingchuan ma huaijin wang shuai wang and qiyi tang sen nie shi wu fitness scorep l simg unk rate0 unk ratel wheresimgdenotes the graph level similarity between the landp.
thevalueof unk rate0denotesthenumber of oovcasesfound in ir code l0when compiling pwith o0 i.e.
the baseline and unk ratelstands for oov cases found in l. then fis acquired by averaging the above fitness score for all programs p p. graph similarity.
the graph similarity metric quantifies the similaritybetweentheoriginalsourcecodeandthecompiledircodeat the cfg level.
this provides a high level assessment of the created ir code s quality.
more importantly this condition prevents excessive reduction of the code by the compiler optimizations ensuring that the ir code reasonably preserves the original source code s structure level features.
wetentativelyassessedthreegraphsimilaritycomputationmethods kernel methods graph embedding and tree edit distance.
graph embedding methods often require to fine tune alargenumberofhyper parameterswhichisgenerallychallenging.
we also find that tree edit distance algorithms had limited capacity to process the very complicated cfgs created for our test cases.
irgen s present implementation therefore uses a classic and widely used kernel method shortest path kernels to quantify thestructuraldistancesbetweensourcecode panditsoptimized ir codel.
overall kernel methods including shortest path kernels denote a set of methods originated from statistical learning theory tosupportpatternanalysis .kernelmethodsareshowntobe effective in various tasks such as classification and regression.
for our scenario we feed the employed kernel function with a pair of cfgderivedfrom sourcecode pandthecorrespondingir l and the kernel function returns a score simg.
oovratio.
inembeddinglearning oovsrepresenttokensthat arerarelyobservedandarenotpartofthetypicaltokenvocabulary.
weclarifythatwefollow ncc todefinevocabulary.particularly our vocabulary denotes a bag of ir statements and therefore ir code is represented by a list of statement embeddings.
accordingly oov in our context denotes a new statement never occurring inthebaselinevocabulary.suchnewstatementscorrespondtoa specialembeddingnotedas inourimplementation degrading the learning quality.
a high oov is discouraged in the fitness function.
that is we leverage the oov ratio to punish an optimization sequence if it results in an ir code with too many oov cases.
to this end a baseline is first computed recording the oov encountered in ir code generated by compiling pwith o0.
then given optimization sequence v we count the oov cases identified in its optimized ir codel and compute the relative oov ratio.
we clarify thatit ispossible to avoidtoken level oovissue by leveragingsub tokenizationtechniqueslikebpe .giventhat said inthecurrentsetting anirstatementisrepresentedbyasingleembeddingvector whereasbperepresentsastatementbymultiple vectors ofsub tokens.
theextra overhead dueto multiple vectors isseenasacceptableforsourcecodebutunaffordableforircode which is orders of magnitude longer.
in fact our preliminary study explored using bpe we report that bpe would result in and longer vectors on our test datasets poj and gcj .
.
learning multiple irs using triplet loss insteadofkeepingsinglesequencewiththehighestfitnessscore irgenretainsthetop ksequencesfromeachgeneration asranked by their fitness scores.
we find that it is beneficial to perform augmentation with multiple llvm ir codes generated by the top k optimization sequences see results in sec.
.
given the ga procedure these top ksequences will evidently share some overlapping optimization flags.
however we find that when a source program iscompiledinto kllvmirprogramsusingthesetop ksequences thesekir programs are still distinct see cases at although they share regulated code structures that are correlated with thereference source code.
hence we anticipate that the augmented datasetwillbediverse whichhasbeengenerallyshowntobeuseful in enhancing embedding learning quality .kdenotes a hyper parameterof irgen.webenchmarktheaccuracychanges in terms of different kin sec.
.
figure learning from ir code with triplet loss.
fig.3depicts an efficient and general extension over program embeddingmodelstosubsumemultipleircode.asexpected we first extend a code embedding model mto process llvm ir.
then we employ a popular learning objective namely triplet loss asthelossfunctionof m.thetriplet whichconsistsofapositive sample anegativesample andananchor isusedastheinputfor tripletloss.ananchorisalsoapositivesample whichisinitially closertosomenegativesamplesthanitistosomepositivesamples.
theanchor positivepairsarepulledcloserduringtraining whereas theanchor negativepairsarepushedapart.inoursetting apositivesamplerepresentsaprogram p anchorrepresentsircodeproduced fromp andnegativesamplesrepresentotherunrelatedsourcecode.
notethat misnotnecessarily codecmr.othernon trivialsource code embedding models can serve min this pipeline see our evaluation on generalization in sec.
.
.
further while we adopt fig.
to enhance m we clarify that there may exist other augmentation pipelines.
we provide proposals of other pipelines at for information of interested audiences.
implementation irgeniswrittenprimarilyinpythonwithabout9klinesofcode.
thisprimarilyincludesourgapipeline sec.
andextensionof codecmr seebelow .irgenisbasedonllvmversion11.
.
.
wealsotentativelytestedllvmversion7.
whichworkssmoothly.
irgenisbuiltinafullyautomatedand out of the box manner.
usersonlyneedtoconfigureirgenwiththepathoftheirllvm toolchain.
we release irgen and data e.g.
augmented models authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
unleashing the power of compiler intermediate representation to enhance neural program embeddings icse may pittsburgh pa usa at .ourresultscanbereproducedusingourreleasedartifacts.
we pledge to keep irgen up to date to support future study.
!
!
!
!
!
!
!
!
!
!
figure the main structure of codecmr.
enhancing codecmr.as stated in sec.
irgen is currently implemented with codecmr which is a sota code embedding modelthathasbeenthoroughlytestedonreal worldc c programs.wefindthatitscodeisstraightforwardtouseandofhigh quality.weemphasizethatirgenisorthogonaltotheparticular codeembeddingmodelsused.weassessthegeneralizationof irgen using another embedding tool ncc which directly computes embeddingsfromllvmir seesec.
.
.weextendedtheofficial version of codecmr to jointly compute embeddings using c source code and llvm ir code.
we also implement a c c parser based on treesitter and a llvm ir parser extended from ncc as we need to compare distance of c c and ir code using kernel methods.fig.
4depictsthemainnetworkstructureofourextended codecmr.codecmrisavariantofdpcnn whichhasbeenshown to efficiently represent long range associations in text.
as shown in fig.
the key building block a word level convolutional neural network cnn can be duplicated until the model is sufficiently deeptocaptureglobalinputtextrepresentations.giventhat ir programs are typically lengthy and contain extensive global information codecmrexhibits promising accuracy.
table augmentation using kcollections of optimized ir.
k 1k 2k 3k 4k 5k 6k map .
.
.
.
.
.
.
tuningk.recallingthatirgengenerates kcollectionsofircodes by returning the top ksequences we now compare the effect of k onlearningquality.weranourexperimentsusing codecmr trained on poj and measured the map accuracy for different kin table2.
overall although increasing kcan continuously extend thetrainingdata thelearningaccuracyreacheditspeakvaluewhen k .
we interpret these results to confirm another important and intuitive observation aligned with data augmentation on natural language or image models involving multiple diverse ir code collections in training datasets augments the learning quality of code embedding.we provide samples on to illustrate how source code can be compiledinto kpiecesofdiverseircode.inourevaluation sec.
wechose k 6asthedefaultoption.
however w eclarifythatthe best value of k as a hyper parameter can be influenced by both the specific dataset and the neural embedding model.
we therefore recommend users to tune kfor their specific learning task.
evaluation our evaluation aims to answer the following research questions rq1 how does codecmr after enhanced by irgen perform in comparison to other relevant works on code clone detection?
rq2 how accurate is the genetic algorithm ga adopted by irgen?rq3 what are the most important optimization flags and their characteristics?doesthe optimalsequenceofflagschange ondifferent datasets?
rq4 what is the generalization of irgen with respect to other models and different learning algorithms?
rq5 canirgenstillachievepromisingaugmentationwhenonlylimited source code samples are available?
before reporting the evaluation results we first discuss the evaluation setup as follows.
dataset.
weusedthepoj andgcj datasetsforour evaluations.
table 3reports the summary statistics of these two datasets.asmentionedinsec.
thepoj 104datasetcontains44 c c programs that implement entry level programming assignments for different tasks e.g.
merge sort and two sum .
thegoogle code jam gcj is an international programming compe tition that has been run by google since .
the gcj dataset contains the source code from solutions to gcj programming challenges.
the gcj dataset is commonly used and contains 901c c programs.
compared to poj the gcj files are longer and morenumerous.wefindthatgcjfilescontaincomplexusageof cmacros.asdescribedlaterintheevaluation wefoundthatthe morelengthygcjcodeanditsrelativelycomplexcodestructures had notable impacts on the optimization sequences selected by the gaprocedure.forbothdatasets weusedthedefaultsettingtosplitthemfortrainingandtesting.wedidnotusetheirvalidationsplits.
foreachdataset weusedirgentoselectthetop koptimization sequencesretainedbythegaprocess.wethencompiledeachc source code in the training datasets into kpieces of llvm ir code to extend the training datasets.
ourmethodrequiresthat thetrainingcodesbe compilable.w e indeedexploredsomeotherdatasetssuchasdevign .however wefoundthatmanyofitscasescannotbecompiled.fixingthese issues would have required considerable manual effort.
another practicalconcernis cost assoonreportedin cost training codecmr on gcj already takes over hours on tesla v100 gpu cards.
consideringlargerdatasetsisoutofscopeforthisresearchproject.
table statistics of the dataset used in evaluation.
split gcj poj classes in training data programs in training data classes in test data programs in test data programs with macro average lines of c code .
.
average lines of llvm ir code .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zongjie li pingchuan ma huaijin wang shuai wang and qiyi tang sen nie shi wu table4 accuracyofall augmented models.foreachmetrics wemark best models whentrainingwithcsourcecode.we also mark the best models when training with c code and llvm ir code optimized following different schemes.
irgen denotes training codecmrusing source code and six collections of llvm ir optimized by sequences formed by irgen.
methodgcj poj map r ap map r ap code2vec .
.
.
.
.
.
.
.
.
.
.
.
code2seq .
.
.
.
.
.
.
.
.
.
.
.
ncc .
.
.
.
.
.
.
.
.
.
.
.
ncc w o inst2vec .
.
.
.
.
.
.
.
.
.
.
.
aroma dot .
.
.
.
aroma cos .
.
.
.
codecmr .
.
.
.
.
.
.
.
.
.
.
.
misim gnn .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
codecmr o0 .
.
.
.
.
.
.
.
.
.
.
.
codecmr o1 .
.
.
.
.
.
.
.
.
.
.
.
codecmr o2 .
.
.
.
.
.
.
.
.
.
.
.
codecmr o3 .
.
.
.
.
.
.
.
.
.
.
.
codecmr os .
.
.
.
.
.
.
.
.
.
.
.
codecmr demix .
.
.
.
.
.
.
.
.
.
.
.
irgen .
.
.
.
.
.
.
.
.
.
.
.
baselinemodels.
tocomparewith codecmraugmentedbyirgen we configure seven embedding models including codebert code2vec code2seq ncc aroma codecmr and misim .codecmr was introduced in sec.
.codebert ncc and misimwere introduced in sec.
.
.
nccis a unique and extensible code embedding framework that learns directly from llvm ir code.
as expected ncccan be augmented with llvm ir optimized by irgen see sec.
.
.
when using ncc weassessedtwovariants nccandncc w o inst2vec .
the latter model omits the standard ins2vec model for ir statement level embedding and instead uses a joint learning approachtosimultaneouslycomputeandfine tunethestatementand graph levelembeddings.for misim weleverageditsprovidedvariant referred to as misim gnn that leverages gnns in the learning pipeline and has been shown to outperform other misimvariants.
aromawasreleasedbyfaceboo ktofacilitatehigh speedquery matching from a database of millions of code samples.
aromadoes notperform neural embedding but instead contains a set of conventional code matching techniques pruning clustering etc.
.
we selected aromaforcomparisonbecauseitisasotaproductiontool that also features code clustering and similarity analysis.
hence aromaandneuralembeddingtoolscanbecomparedonanequivalent basis demonstrating the strength of sota neural embed ding tools particularly after augmentation using irgen.
the of ficial codebase of aromaprovides two variants aroma dot and aroma cos. we benchmarked both variants.
cost.ourlearningandtestingforgawereconductedonadesktop machine with two intel core tm i7 cpu and 16gb ram.
the machine was running ubuntu .
.
irgen takes averaged .
and .
cpu hours to finish all iterations of ga procedureforpoj 104andgcj respectively.despitethehighcpu hours we clarify that the wall clock time can be largely reduced viaparallelism.weexploredtore runthegaprocedureona64core cpu server.
we report that it takes about wall clock hoursfor poj and about wall clock hours for gcj.
setting this parallelismchangesabout60locinirgen seeourcodebaseat .
when needed it is also possible to optimize ga with subsampling for extremely large datasets.
training embedding models are usually very costly.
we employ agpuserverfortrainingforallinvolvedmodels.theserverhas twointel r xeon r platinum8255ccpusoperatingat .50ghz gb of memory and nvidia tesla v100 gpu each with32gb ram.
the learning rate is .
and the repeat number ofresidual blocks is other settings of our extended codecmrare the same with the standard codecmr setting.
in total over epochs took approximately .
and .
hours for poj and gcj respectively.
.
accuracy of irgen we first answer rq1using table .
for neural embedding models welauncheachexperimentsforthreetimesandreporttheaverage aswellastheminimumandmaximumscoresinparentheses.table4reportstheevaluationresultsofbaselinemodelsinlines3 .
in accordance with our research motivation sec.
we also report results using codecmr augmented with ir code optimized by standard optimization levels o0 o1 o2 o3 os .
codecmr demix representstraining codecmrbyusingsourcecodeandfivesetsof ir compiled by all five default optimization levels.
the last row in sec.3reports the performance metrics for codecmr augmented by sixcollections ofllvm irsoptimizedusing sequencesgenerated by irgen.
for both the poj and gcj datasets in addition to map w eusedap asthemetric.bothmetricsarecommonly usedinrelevantresearchtoassessperformanceofembeddingmod els.
ap stands for average precision a method combines recall and precision for ranked retrieval results.
for both metrics a higher score indicates better performance.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
unleashing the power of compiler intermediate representation to enhance neural program embeddings icse may pittsburgh pa usa our results show that modern embedding models including codebert codecmr and misim gnn can largely outperform conventional code matching tools suchas aroma dot andaroma cos .
whenlearningovercsourcecode wefoundthat codebert wasthe bestperformingmodelforthepoj 104dataset whereas misim gnn delivered the best performance for the gcj dataset.
in contrast codecmrperformedlesswellthaneitherofthesotamodelsacross all of the evaluated metrics.
code2vec andcode2seq shows relatively lower accuracy compared with others.
since we run each evaluationthreetimes wefindthattheiraccuracyscoresareunstable.suchobservationisalsoconsistentlyreportedinprevious works .
nevertheless even the peak accuracy scores of them are still much lower than that of the sota models.
whenlearningfromiroptimizedusingstandardoptimization levels codecmroutperformed sota model map scores by more than10 onthegcjdataset.evaluationofthisformof codecmr training on the poj dataset showed consistently promising enhancementrelativetothesotamodelsinmostcases.also comparing with augmenting codecmr with one collection of optimized ir code the codecmr demix setting shows slightly better performance particularly for the poj setting.
this also reveals the strength of training with multiple diverse sets of ir code.
we found that codecmr when augmented by findings of irgen thelastrowoftable constantlyandnotablyoutperformedall the other settings.
we interpret the evaluation results as highly encouraging showingthatirgencangeneratehigh qualityllvm ir code that enhances codecmr to significantly outperform the sotamodels codebert andmisim gnn onallmetrics.again we note that irgen is not limited to enhance codecmr we present evaluation of enhancing nccin sec.
.
.
figure5 smoothedfitnessscoreincreasesover800iterations.
.
fitness function rq2assessestheefficiencyofourfitnessfunction.fig.
5reports thefitnessscoreincreasesfromall800irgeniterationsacrosseach ga campaign.
the test cases despite their diverse functionality manifested encouraging and consistent trends during optimization searching.
the fitness scores kept increasing and were seen to reach saturation performance after around to iterations.
weinterpretthatundertheguidanceofourfitnessfunction irgen can find well performing sequences for both datasets.figure ordered contributions of each optimization flag.
.
potency of optimization flags this section answers rq3by measuring the potency of selected optimizationflags selectedflagsarefullylisted at .we report thatforthepoj 104dataset thetop 1sequence shavingthehighest fitnessscorecontains49flags.tomeasuretheircontribution we first train codecmrusing c source code and llvm ir optimized usingsequence sandrecordthebaselineaccuracyas acc.then we iteratively discard one optimization flag ffromsand measure the augmentationeffectivenessof usingtheremainingsequencewith flags.
the accuracy drop reveals the contribution of flag f. fig.6orders the contribution of each flag in s. overall we interpret that no dominating optimization flags are found in this evaluation.
in other words we interpret that all these flags manifestreasonablecontributiontothemodelaugmentation andthe top flags contributes in total .
.
we thus make our first important observation w.r.t.
rq3 instead of identifying one or few dominating flags that significantlycontributetoenhancingcodeembedding it is rather the formed sequence of optimization flags that is important.
thisevaluationshowsthatasequenceofflagsworkstogetherto producehigh qualityir insteadofoneorafew franchiseplayers that can largely outperform other flags.
in other words the ga processconductedbyirgenis criticaltothisresearch becauseit offersageneralwaytoconstructsuchasequencewithmodestcost.
wenowconsiderthecharacteristicsofthetenhighestpotency flags.
we put these flags into three categories as follows simplify an ir statement.
optimization flags including dce early cse reassociate bdceand loop deletion simplify ir statementsvia various dataflow or controlflow analysis methods.
for instance early cse regulates ir statements by eliminating common subexpression eliminations and reassociate reassociatescommutativeexpressionstosupportbetterconstant propagation.
in all these optimization can make two syntactically distinct pieces of source code more similar in ir.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zongjie li pingchuan ma huaijin wang shuai wang and qiyi tang sen nie shi wu makeirstatementsequencesclosertosourcecode.
flags including mem2reg instcombine and dse cansimplifythecompiledircode makingit moresimilartothesourcecode.forinstance mem2reg promotes memory