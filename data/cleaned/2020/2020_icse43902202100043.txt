autotrainer an automatic dnn training problem detection and repair system xiaoyu zhang juan zhaiy shiqing may chao shen school of cyber science and engineering xi an jiaotong university xi an china yrutgers university united states email zxy0927 stu.xjtu.edu.cn juan.zhai shiqing.ma rutgers.edu chaoshen xjtu.edu.cn abstract with machine learning models especially deep neural network dnn models becoming an integral part of the new intelligent software new tools to support their engineering process are in high demand.
existing dnn debugging tools are either post training which wastes a lot of time training a buggy model and requires expertises or limited on collecting training logs without analyzing the problem not even fixing them.
in this paper we propose a utotrainer a dnn training monitoring and automatic repairing tool which supports detecting and autorepairing five commonly seen training problems.
during training it periodically checks the training status and detects potential problems.
once a problem is found a utotrainer tries to fix it by using built in state of the art solutions.
it supports various model structures and input data types such as convolutional neural networks cnns for image and recurrent neural networks rnns for texts.
our evaluation on datasets models show that a utotrainer can effectively detect all potential problems with detection rate and no false positives.
among all models with problems it can fix .
of them increasing the accuracy by .
on average.
index terms software engineering software tools deep learning training i. i ntroduction in the new software engineering se .
era software is developed with an intelligent component which is usually powered by machine learning ml techniques.
recent advances in deep learning dl have already made it possible for end users to benefit from the intelligence of software.
for example google has deployed new dl based nlp techniques to help improve its search results .
facebook launched shops which bring more businesses online during the covid pandemic and made it possible to search for clothes using over tens of thousands of image attributes.
all of these are enabled by software created in se .
.
with this new trend of se .
developing the dl component represented by deep neural network dnn models becomes an integral part of the whole process.
dnn models and other dl methods just like other programs also have bugs and its own vulnerabilities which brings many new challenges of se research on debugging and repairing dnn model and its development process.
new tools that can help the development of intelligent components will greatly help developers especially for the ones who are new to these techniques.
there are already some efforts trying to study this problem .
for example mode proposed as a dnn debugging technique identifies faulty neurons that lead to undesirablebehaviors and selects additional training samples to correct these neurons behaviors to improve model accuracy.
we refer to such techniques as post training techniques which focuses on fixing model problems whose training has been completed.
however many existing tools are not automatic and require expertises which makes them difficult to use for developers new to this field.
more importantly we observe that many dnn problems have been exposed in the training process and posttraining techniques have a delay in detecting such problems.
as such a lot of resources are wasted in training a problematic model which can be saved if we can detect the problem early in training.
thus a runtime monitoring and detecting technique is highly needed.
existing dnn training frameworks have provided limited support for training monitoring and detection.
tensorboard known as the default debugger of tensorflow is a toolkit which can record various values and provide the visualization during the training process.
for example it can track and visualize metrics like loss and demonstrate histograms of weights as they change over time.
there are some other similar tools such as visdom tensorwatch and manifold .
just like traditional debuggers e.g.
microsoft visual studio debugger which allow programmers to track variable values and operations as well as monitoring the changes of computing resources these tools can facilitate developers in inspecting and understanding the model training status.
however they lack the capability of analyzing the collected data and provide meaningful fixes which makes them less useful.
through our analysis we found that a training problem happens or not is random even for the same training script a training problem happens randomly during the training whole.
to be more specific because of the randomness in training e.g.
initialization of weight values training data sequences running the same training scripts may get different results.
as such a training problem may happen in some cases but not in other cases.
and similarly a training problem may happen in any training iteration if it happens .
we have provided real cases in iii.
considering the fact that many dnn training tasks may take days or even months it is infeasible for developers to watch the numbers or curves all the time to manually detect potential problems which may occur at any time.
unfortunately although these runtime tools can collect and exhibit data they are incapable of analyzing the data to diagnose problems let alone leveraging solutions to alleviate ieee acm 43rd international conference on software engineering icse .
ieee these training problems.
to address the aforementioned limitations a tool that relieves developers from manually monitoring the training procedure is needed.
in addition the tool will have to automatically analyze data diagnose and resolve problems during training so as to increase productivity and efficiency for developers as well as improve the reliability of intelligent software systems.
in this paper we propose a utotrainer a dynamic approach that detects and repairs potential dnn training problems.
the training problems that a utotrainer focuses on are vanishing gradient exploding gradient dying relu oscillating loss and slow convergence and a utotrainer is capable of handling various model structures including convolutional neural networks cnns and recurrent neural networks rnns .
and these can be easily extended as long as a problem definition is provided.
given a model with its training configuration e.g.
hyper parameter optimizers a utotrainer will start training the model and record relevant data like loss values.
during the monitoring a utotrainer conducts regular analysis to recognize potential training problems.
if a problem is detected autotrainer will try to fix it with built in solutions.
these solutions are constructed based on the state of the art work which have been demonstrated to work well in solving the corresponding problems .
during the repair retraining procedure if another problem is detected a utotrainer will regard the old problem as resolved and attempt to repair the new problem.
if no more problems are detected it means all the problems have been addressed by a utotrainer and the trained model with its configuration is delivered to the user.
if autotrainer fails to solve this problem it will notify the user with complete training log.
our contributions are we summarize and formalize definitions for the symptoms of common training problems.
we propose the first automatic approach to detect and repair different training problems during model training.
we develop a prototype a utotrainer based on the proposed idea and evaluate it with public datasets and models.
the evaluation results demonstrate that autotrainer can effectively detect all problems for models and repair problems of them with a ratio of .
.
on average the test accuracy can be improved from .
to .
.5x higher .
our implementation collected datasets configurations and problem solutions are publicly available at .
threat to validity.
we have tried our best to obtain as many models as possible.
a utotrainer is currently evaluated on datasets and models which may still be limited.
similarly there are many configurable parameters used in autotrainer and even though our experiments show that they are good enough to achieve high detection and repair results this may not hold when the number of models is significantly larger.
to mitigate these threats all the original and repaired training scripts model architecture and training configuration details implementation including dependencies and evaluation data e.g.
training logs are publicly available at for reproduction.
ii.
b ackground a. dnn model training a dnn model is a parameterized function f x7!y wherex2xis anm dimensional input i.e.
x2rm and y2yis the corresponding output label.
it usually composes of several connected layers.
formally an n layered dnn can be written as f l1 l2 ln wherelrepresent a layer.
each lcan be expressed as a function whose output is fl l fl bl where landbare the weight and bias values of layer l. is known as the activation function ii c .
the input layer l1takes raw inputs and passes them on to the subsequent layer.
hidden layers extract the features of the input and the output layer lnis trained to predict the output based on the extracted features.
the links between consecutive layers are represented using a set of matrices.
the numerical values in such matrices are referred to as weight parameters.
given a large set of input output pairs xi yi training a dnn model is to update all weight parameters to minimize the differences between a predicted result f x and the corresponding ground truth label y. such differences are measured by a loss function l f x y .
thus training a dnn essentially is to minimize the value of l. specifically training a dnn model consists of the following phrases.
the first step is initialization which initializes the weight matrices.
then starting from the input layer the forward propagation step uses existing weight values to predict output labels for the training samples and calculates the value oflbased on predicted output and ground truth labels.
afterwards the backward propagation step tweaks the weight values from the output layer all the way back to the input layer trying to minimize the difference using an optimization method which is usually a gradient descent algorithm or its variants.
the forward propagation and backward propagation steps will be repeated until the difference converges to a minimum value meaning reaching the stopping criteria or has reached the maximal number of training iterations allowed.
b. gradient descent in dnn model training a loss function evaluates the prediction ability of a dnn model and a smaller value of the loss function means a better model.
thus the training goal is to obtain weight values which result in a minimum loss value.
gradient descent algorithm and its variants are commonly used to solve this optimization problem.
it works by tweaking the weights in the opposite direction to the gradient of the loss function.
specifically each weight has an update proportional to the partial derivative of the loss function with respect to the current weight.
the gradients are usually calculated by auto differentiation ad techniques leveraging the chain rule.
as such computing the gradient for a weight has an effect of multiplying many numbers from subsequent layers .
normally a neural network is designed to have many layers to improve its capacity.
increasing the number of layers can 360enable a neural network to train on a large scale training dataset and efficiently learn more complex mapping functions from inputs to outputs.
however the addition of layers can have negative impacts on training.
the common problems are vanishing gradient andexploding gradient .
problem vanishing gradient problem .in backward propagation when the gradient is computed by multiplying many small number the gradient can be vanishingly small especially for layers that are close to the input layer.
consequently the weights can hardly be changed and the loss function can end up with a very large value meaning the trained model would have a low accuracy.
such a problem is referred to as vanishing gradient vg .
symptoms of vg.
the gradient decreases exponentially from layer to layer and is close to zero in the layers close to the input layer and the training accuracy remain low.
problem exploding gradient problem .in contrast to vg the gradient can grow exponentially as it is propagated backwards.
this also leads to nan or unexpected large values which results in bad model accuracy.
such a problem is referred to as exploding gradient eg .
symptoms of eg.
the gradient increases exponentially from output layer to input layer during backward propagation and can become large or even nan value in the layers close to the input layer and the training accuracy is low.
c. activation function intuitively each neuron in a dnn can be regarded as one special feature to differentiate between the given inputs.
given a set of inputs each neuron computes the weighted sum and then adds a bias to the sum.
after that an activation function takes the computed sum as input and produces an output for the neuron.
specifically the activation function determines how much the input is relevant for the following stage guiding the neural network to leverage important features and suppress irrelevant features.
relu rectified linear unit is a widely used activation function in a neural network which outputs the same value if the input is positive and outputs zero if the input is non positive i.e.
relu x maxfx 0g .
existing work has demonstrated its excellent training effect.
it effectively improves the sparsity of the model achieving better training convergence and accuracy.
however using relu has its own limitations among which dying relu is the most common and serious one.
problem dying relu .when a relu neuron receives a non positive input it will output zero making the neuron inactive.
in such cases the neuron is very likely to remain inactive forever since a gradient based optimization algorithm will not tweak the weights for an inactive neuron.
consequently such neurons cannot be leveraged to distinguish between the inputs and ground truth and if there are many such neurons we may end up with a large part of the neural network contributingnothing to the prediction task.
this is known as the dying relu dr problem.
symptoms of dying relu.
when training a dnn with relu as the activation function the gradients of a large percentage of the neurons are zero and the training accuracy is low.
d. convergence the training goal is to reduce the loss value converge to a minimum.
to determine the point of convergence there are usually two conditions.
one is that the training time has reached the maximal allowed iteration defined by the user .
and the other one is that the training accuracy has reached desired values.
in some training cases we may end up with a set of low accuracy models even after the maximal number of training iterations and they are usually caused by two problems oscillating loss and slow convergence.
problem oscillating loss .it is inevitable for the loss value to go up and down during the training procedure.
but if there are large changes without decreasing trend the training may not converge in a very long time which should be enough for training the model.
we refer to such a problem as oscillating loss ol .
symptoms of ol.
the training accuracy keeps fluctuating in a large range for a long time.
problem slow convergence .the loss value has a high value and decrease so slow that no significant accuracy improvement has been made and it may end up with low accuracy even when the maximal number of training iteration is finished.
we refer to such a problem slow convergence sc .
symptoms of sc.
the training accuracy holds a low value for a long time even though the loss is decreasing slowly.
iii.
i dentifying dnn p roblems during training as far as we know there is no existing tool that can help users identify the aforementioned dnn problems during training.
tensorflow provides a tensorboard debugger tool to help users inspect program variables e.g.
loss value and inserting assertions.
pytorch also allows users to do the same thing by using pytorch hooks .
however it requires expertises to perform the required analysis and patching to solve this problem.
while many of these problems are common problems in dnn training their symptoms and solutions have been studied and analyzed.
in this paper we propose autotrainer a dnn training tool that can automatically monitor dnn internal values i.e.
neuron activations and gradients loss values and training accuracy values during the training procedure and inspect possible problems.
if a problem is identified a utotrainer will try to automatically fix it.
autotrainer is designed to be a training time monitoring and fixing tool because of the following training problem occurrence is highly random.
when training a model using the same configuration and training dataset for multiple times whether a problem occurs or not cases avg acc .
dying relu not happened cases avg acc .
repaired acc .
dying relu happened a occurrence cases avg acc .
cases avg acc .
cases avg acc .
cases avg acc .
no case avg acc .
repaired acc .
b time of occurrence figure .
problems occurrence and time of occurrence are random in a training procedure is random.
this is because there are many random values used in dnn training.
for example the weight values are usually initialized with random values.
in some cases one problem will occur because of these random values while it will not happen in some other cases.
we train a dnn model with layers parameters on the mnist handwritten digit dataset training and testing samples .
in this model we also use relu as the activation function adam as our optimizer and set our learning rate to be .
and the maximal number of epoch to see .
we train the model for times.
figure a shows the distribution of the appearance of the dying relu problem.
we can see that the dying relu problem occurs in of the training processes but not in the remaining which demonstrates that whether a problem occurs or not is random.
the average training accuracy when the dr problem happens is only .
while the value reaches .
when there is no dr problem.
with a utotrainer we are able to detect all these dr problems and fix them improving the accuracy to .
.
the time when a training problem occurs is random.
similar to the randomness of problem occurrence challenge the time when the problem actually happens is also random during training.
we use a model which has the oscillating loss problem as an example.
it is a dnn model with layers and uses relu as activation function adam as optimizer and we set the learning rate to be .
and the maximal number of iterations is .
details of the training scripts is also available in our repository .
the distribution of the stages epoch number when the problem occurs is shown in figure b .
in of the cases the oscillating loss problem is not triggered.
in half cases the problem is detected in the first epochs and the percentages of detecting the problem in other stages are separately and .
it demonstrates at which stage a particular problem occurs is random.
since our system enforces real time surveillance it is able to perform timely detection and repair.
for this model autotrainer can detect the problem at early stage i.e.
before epochs out of in the wide majority of cases i.e.
more than of the cases where the problem occurs .
after the detection our system attempts to resolve the problem by leveraging four solutions i.e.
substituting initializer increasing batch size decreasing learning rate and substituting optimizer.
see iv .
based on our experiments the problem is successfully alleviated in all cases and improves the accuracy to .
.
in contrast existing post training methods do not collect real time data making them unable to detect problems during the training.
problem detection automatic repair new solution no more solution signal problem recognizer problem problem report training monitor repaired failed solution scheduler recorded data model welltrained model figure .
overarching design of a utotrainer iv.
s ystem design figure gives the overarching design of our system which consists of the problem detection module left and the automatic repair module right .
the whole system starts by training a model with an initial training configuration and using the problem recognizer to monitor the training.
when a problem is detected the system will launch the automatic repair module trying to retrain the model with new settings until the training can finish without any problem repaired or a detected problem cannot be solved failed .
notice that if there exist several problems our system will attempt to solve the detected problems one by one in the order of exposure.
autotrainer takes a training configurations i.e.
the original training scripts including model architecture loss function optimizer etc.
and user preferences as inputs.
the user preferences are configurable parameters for autotrainer which includes preferred repair solutions and so on.
a utotrainer has a set of default values for them and user can replace them.
details will be presented in iv b .
the problem detector monitors training information like loss value.
during this the problem recognizer is triggered on a timely basis to analyze the recorded data to recognize symptoms and determine whether a training problem exists.
if a problem is detected the automatic repair module will be leveraged to address it.
otherwise the training monitor will output the trained model with its training configurations to the user.
for each problem a utotrainer has a few built in solutions to fix them.
however one solution may or may not work.
if the detected problem is the same as before if any it means the applied solution cannot solve the problem for this particular case.
hence the solution scheduler will retrieve the next one apply it and restart training.
if a new problem is detected the solution generator will select the corresponding solutions to it.
the order of solutions can be reorganized by users.
if none of the solutions can fix the problem the solution scheduler will report a failed case with the whole training log.
a. training monitor the training monitor starts a training procedure and records data which is used to recognize symptoms and retrain the model when a problem is detected.
the recorded data includes model definition including layers and their configurations e.g.
kernel sizes in convolutional layers .
optimization method definition and its parameters.
training accuracy and loss values.
calculated gradients for each neuron.
hyper parameters and other necessary variables used in training such as the batch size and learning rate.
note that the data of each training procedure will be recorded separately and can be queried by the user.
b. problem recognizer the problem recognizer regularly conducts analysis on the recorded data to recognize training problems.
the symptoms leveraged to detect problems are formalized and shown in table i .
the first column lists the training problems and the second column specifies the symptoms involving gradient and training accuracy.
if the depicted condition is met our system regards the corresponding symptom as observed.
the last column presents the built in solutions in a utotrainer .
vg.
we formalize the symptom of the vg problem as two conditions.
firstly there has not been a trained model whose accuracy is good enough to terminate the training max acc .
this check is by default enabled and checked by all existing dnn training platforms already.
if there is such a model the training should have terminated.
secondly in the recent 1training iterations the gradient has been drop from layer to layer in the backward propagation and the gradient becomes to be very small smaller than a threshold value .
to measure the change and value of gradients we use thel2 norm which is borrowed from existing literature in the ai research community .
eg.
the definition of eg symptoms are very similar to that of vg except that the gradient is growing from layer to layer in backward propagation or it has already become nan values in some layer meaning that it cannot propagate back to the input layer already .
dr. dying relu means that there has been a set of neurons whose gradients have been 0in the recent a few iterations and this set is large forms a large portion of the whole dnn more than a threshold value while the accuracy of the neuron net work is still low.
ol.
intuitively the symptom of an ol problem is that there has been a lot of oscillating loss values from the start till now.
to measure if there are oscillating loss values we first extract two lists of loss values aandbrepresenting the maximum optimal and minimal optimum loss values in time order respectively.
then we calculate the degree of oscillation by computing the differences of a consecutive pair of elements inaandb.
if the oscillation is larger than we think this is a significant oscillation and if such oscillation happens very frequently we think there is an ol problem.
sc.
by definition sc means the accuracy of trained models is growing slowly.
to check this problem a utotrainer checks the training accuracy change for the past iterations.
if the change has been small it indicates that the traininghas been trapped into a local optimal point and the training process has failed to improve it.
based on this a utotrainer determines that the sc problem happens.
c. solution scheduler the main role of the solution scheduler is to pick one solution to fix the problem and restart the training procedure.
for the same problem it will try each possible solution one by one based on the default order if users do not specify preferred orders.
if one solution can fix the problem the scheduler will not be triggered by the same problem.
otherwise it will try a new solution.
and if none of these solutions can fix it autotrainer fails to resolve this problem and will report this to the user to determine what to do next.
d. existing solutions there has been some study on how to solve training problems.
unfortunately there is no silver bullet and one solution cannot be guaranteed to work for all cases.
for each problem autotrainer collects a few possible solutions which have been shown to be effective in prior study and uses them to fix detected training problems and these solutions include s1 adding batch normalization layers.
batch normalization is a method used to normalize the neuron values of a layer by re centering and re scaling them.
this helps remove the unexpected gradient and neuron activation values.
specifically the normalization will squeeze the values into a specific range and as such small gradient updates will not diminish or explode during the backward propagation meaning that the vanishing and exploding gradient problem can be alleviated .
in addition such value range enforcement reduces the possibility of getting an inactive neuron and help resolve the dying relu problem.
regarding to the problem of where to add batch normalization layers authors of this method has performed analysis and demonstrated that adding batch normalization before activation function layers gives the best result.
we follow this guidance and implemented our solution.
s2 substituting activation functions.
as aforementioned relu is a commonly adopted activation function.
the gradient of relu activation is when the input is greater than meaning the gradient will remain the same without decreasing or increasing dramatically if used with the proper optimizer and learning rate .
hence substituting the current activation function with relu and its variants e.g.
selu leakyrelu can mitigate both the vanishing gradient problem and the exploding gradient problem.
s3 adding gradient clipping.
gradient clipping clips gradient values that exceed a specified range which essentially limits the update of a weight value to a limited region.
unlike batch normalization and other normalization methods this method clips the gradient values based on a threshold.
by removing obviously large gradient values it can be used to alleviate the exploding gradient problem .
bengio et al.
and many others have studied and evaluated the concrete values to use in gradient clipping and 363table i problem symptoms and repair solution candidates training problem symptom solution vanishing gradient gradient 8i2 gi l2 gi l3 gi ln gi ln 1v gi l2 2s1 adding batch normalization layers s2 substituting activation functions accuracy max acc exploding gradient gradient 8i2 gi l2 gi l3 gi ln gi ln 3w9j2n gi j nans1 adding batch normalization layers s2 substituting activation functions s3 adding gradient clip accuracy max acc dying relu gradient 8i2 jfj2nj gi j 0gj jnj s1 adding batch normalization layers s2 substituting activation functions accuracy max acc s4 substituting initializer oscillating loss accuracy jfi2 ja b gj k s4 substituting initializer s5 adjusting batch sizes s6 adjusting learning rate s7 substituting optimizer slow convergence accuracy 8i2 jacc acc j s4 substituting initializer s6 adjusting learning rate s7 substituting optimizer 1gb a the gradient of layer ain iterationb 2n the number of layers of a dnn 3n all neurons of a dnn 4k the current training iteration thresholds for iterations6 thresholds for gradients the training accuracy threshold the threshold for the percentage of neurons with gradients the threshold for accuracy difference10 the threshold for the difference of maximum and minimum optimal the threshold for the percentage of times of large loss fluctuation12acc accuracy arry for each iteration 13max the maximum function 14a b arraies of maximum minimum optimal recommend to clip the gradient of each layer to which is adopted by a utotrainer .
s4 substituting initializers.
initializers set a starting point for the optimizers during model training.
thus inappropriate initialization can cause disasters in training deep neural networks.
xavier initialization was proposed solve the oscillating loss and slow convergence problem by initiating the weight values to a proper range.
lu et al.
have shown that the popular initialization schemes like he initialization suffers from the dying relu problem.
in iii we also showed a case where the initialization values can affect the model training.
thus we also try to substitute the used initializers when a model encounters the dying relu slow convergence or oscillating loss problems.
s5 adjusting batch sizes.
batch size is the number of training samples used in one iteration to estimate the error gradient which is an important hyper parameter.
a too large batch size might make the loss value trap into a poor local minimum leading to low accuracy while a too small batch size might make the loss bounce around a lot leading to oscillating loss .
in practice increasing the batch size appropriately has the potential to improve the stability of the training and solve the oscillating loss problem.
setting the proper batch size is not easy for dnn training and various researchers have performed analysis and evaluation and this .
based on our study lecun et al.
and bengio et al.
suggest using as the starting point.
following this we try to initialize the batch size to be .
if the accuracy is too low we try to descries the batch size by a factor of until it reaches .
if the ol occurs we will increase the batch size also by doubling until it reaches according to masters et al.
.
s6 adjusting learning rates.
learning rate is a hyperparameter that determines the amount of change to the modelin each update i.e.
each backward propagation .
if the learning rate is too large the weights are likely to have fluctuating update and the loss value will oscillate and even increase over training epochs .
given this decreasing the learning rate can be helpful to tackle the oscillating loss problem.
generally a small learning rate makes it possible for the model to learn more optimal or even globally optimal weights with the risk of taking a very long time to finish the training.
at one extreme the training may never converge to a low loss value even after the maximal number of training epochs.
as such the slow convergence problem might be resolved if the learning rate is increased .
the values we set for learning rates depend on different optimizers they use.
we follow the suggestions made by their original authors e.g.
adam and existing empirical evidence .
specifically we choose .
for sgd based optimizers and .
for adam and other adaptive optimizers.
if the slow convergence problem still exists a utotrainer increases it times and if the oscillating loss problem still exists a utotrainer decreases it by a factor of .
s7 substituting optimizers.
optimizers are algorithms used to update weights to reduce the loss value.
an optimizer can have different performance in different scenarios.
practically a substitution of an optimizer can help address various training problems.
stochastic gradient descent sgd is a variant of the basic gradient decent algorithm which computes an estimated gradient on a randomly selected small subset of data samples instead of computing an actual gradient on the entire dataset.
based on the rationale the weights are updated more frequently in sgd which can speed up the convergence.
however the high variance in weights may result in fluctuations in the loss value.
momentum is a method introduced to speed up sgd and dampen loss oscillations.
it works by adding a fraction of the update in the past time step to the current update.
364usually the value of momentum is set as .
or a similar value.
the value .
means the weights will update based on of the previous gradient and of the new gradient.
such a mechanism achieves a faster convergence and fewer oscillations compared with sgd.
however if the momentum is too much we may swing back and forth near the local minimum without hitting the minimum.
adaptive moment estimation adam is a widely adopted optimizer that uses momentum and adaptive learning rates.
the adaptive learning rate allows us to start with large learning rate and finish with small learning rate.
as the learning rate is decreasing we will take smaller and smaller steps which can prevent us from missing the local minimum and accelerate the convergence.
in a nutshell we can use algorithms with momentum like sgd momentum or adam to alleviate the oscillating loss problem and randomly use a different optimizer to alleviate the slow convergence problem.
v. e valuation the prototype of a utotrainer is implemented on top of keras .
.
and tensorflow .
.
.
in the evaluation we aim to answer the following research questions rq1 how effective is a utotrainer in detecting and fixing training problems?
rq2 how efficient is a utotrainer in detecting and fixing training problems?
rq3 what is the impact of different configurable parameters in a utotrainer ?
a. setup we performed our experiments on six popular datasets circle blob mnist cifar imdb and reuters .
circle and blob are two datasets from sklearn for classification tasks.
mnist is a gray scale image dataset used for handwritten digits recognition.
cifar10 is a colored image dataset used for object recognition.
imdb is a movie review dataset for sentiment analysis.
reuters is a newswire dataset for document classification.
in total we collected models and their training scripts with various dnn models structures cnn rnn and fully connected layers only for these six datasets.
among them of them have training problems and the rest are normal models which have been confirmed by model authors.
most models are collected from reported buggy models on github stackoverflow existing papers and personal blogs and some of them are gathered from machine learning experts within our organization.
the training scripts of these models and all experiment results are all public at our repository .
if not specified all experiments in this section are conducted on a server with intel r xeon e5 .1ghz core processors gb of ram and a nvidia titan v gpu running ubuntu .
as the operating system.b.
effectiveness of autotrainer experiment design to evaluate the effectiveness of autotrainer we run our collected model training scripts to test the effectiveness of a utotrainer .
due to the randomness in performing these experiments we run the training multiple times to ensure that the problem have been exposed and collect the experiment results of such cases.
to measure the effectiveness of a utotrainer we start two parallel trainings for the same model.
to reduce the effects of randomness we enforce them to share the usage of the same random number including the initialization weights.
they also share the same set of training hyper parameters and optimization methods.
during training we collected training logs including gradient loss values etc.
to help us verify the whole process.
at the end we manually verify them and confirm them with the details provided by buggy model providers i.e.
online posts and machine learning experts .
results for all buggy trainings we detect training problems as some models have more than one.
table ii demonstrates partial results.
the first column lists the six datasets.
the second column shows the model status and the corresponding number of models.
repaired status indicates a model is successfully repaired if any target problem is detected and failed indicates that the problem still exists even after autotrainer has tried all built in solutions.
lastly we use the normal status to denote models without training problems.
the third column lists model identification numbers and the fourth column shows the number of detected problems of each model.
the following columns denote the accuracy the training time and the memory consumption efficiency results see v c .
column original shows the information for the original model training without a utotrainer while column at short for a utotrainer shows that of the repaired models.
column ratio gives the ratio between the values of a repaired model and the corresponding original model.
column improve shows the absolute accuracy improvement that our system achieves.
the cells in purple separately correspond to the models with the highest accuracy improvement maximum training overhead and maximum memory overhead while the cells in grey separately correspond to the ones with least accuracy improvement minimum training overhead and the minimum memory overhead.
all detailed experiment results can be found at our repository .
notice that the same problem may get repaired using different solutions and after a utotrainer repairs the model its accuracy may not be improved.
to evaluate the repair effects we also calculated the number of problems that are fixed by individual solutions and the change ranges in accuracy.
the results are separately shown in table iii table iv and figure .
intable iii the first column shows the datasets and the following columns present the problem and corresponding solutions.
the solutions are from left to right ordered by their default priority used for repairing in a utotrainer .
each number in the top half of the table denotes the number of problems that are repaired successfully by the corresponding 365table ii overall results of a utotrainer accuracy train time average memorydataset status model problemoriginal at improve ratio original s at s ratio original mb at mb ratio .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.98repaired ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
failed ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.00blob normal ave .
.
.
.
.
.
.
n.a .
.
n.a .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.00repaired ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.99circle normal ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.79repaired ave .
.
.
.
.
.
.
.
.
.
.
failed .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.00cifar normal ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.94repaired ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.92mnist normal ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.96repaired ave .
.
.
.
.
.
.
.
.
.
failed .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.02reuters normal ave .
.
.
.
.
.
.
n.a .
.
n.a .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.90repaired ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
n.a n.a .
n.a .
.
.
.
.
.
n.a n.a .
n.a .
.
.
.
.
.04failed ave .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.95imdb normal ave .
.
.
.
.
.
.
normal ave .
.
.
.
.
.
.
repaired ave .
.
.
.
.
.
.
.
.
.
.
failed ave .
.
.
.
.
.
.
.
.
.
366table iii the problem repaired results datasetvg eg dr sc oltotals2 s1 s2 s1 s3 s2 s1 s4 s7 s6 s4 s7 s6 s5 s4 blob circle cifar mnist reuters imdb total repaired failed total table iv the accuracy improvement of problems repaired avg improve datasetvg eg dr sc ol vg eg dr sc ol blob .
.
.
.
.
circle .
.
.
.
.
cifar .
.
.
.
.
mnist .
.
.
.
.
reuters .
.
.
.
imdb .
.
.
.
total average .
.
.
.
.
solution i.e.
column name .
the bottom half summaries the number of problems of different status.
the pie chart in figure demonstrates the distrubution of change ranges in accuracy with corresponding numbers of models.
analysis.
the experiment results demonstrate the effectiveness of our system.
firstly a utotrainer can effectively detect the defined training problems with a success rate on all model trainings and none of the normal trainings are mis classified as problematic.
secondly a utotrainer can effectively repair the buggy training procedures.
after successful repairing it can improve the accuracy by .
with the maximum improvement as .
.
overall a utotrainer achieves around accuracy improvement which is .
times that of training without a utotrainer .
notice that when eg happens it may result in nan values in the model leading to nan output results for all inputs.
in such cases we do not measure the prediction accuracy and directly report them as n.a.
in table ii from table iii we observe that a utotrainer is able to respectively repair .
.
and of vg eg dr sc and ol in buggy trainings.
and table iv shows the detailed average accuracy improvement of a utotrainer on different problems which is .
.
.
.
and .
of vg eg dr sc and ol in repairing respectively.
table iii andtable iv demonstrate that a utotrainer is capable of handling different types of datasets model problems and model architectures.
regarding sc and ol we find that only two solutions can effectively address all the problems we encountered in our evaluation.
figure shows that model accuracy increase distrubution.
specifically over .
of the models get an increase of and over has an increase between and .
it demonstrates that a utotrainer has the advantage of effectively increasing accuracy by repairing training problems.
we also notice that there are models out of all models .
.
.
.
models models models models .
figure .
accuracy change after model training repair.
q0 runtime overheadq t2 t1 experiment results a circle dataset q0 runtime overheadq t2 t1 experiment results b mnist dataset figure .
runtime overhead vs. problem check frequency.
whose accuracy has slight reduction after repaired.
for the worst case the model accuracy decreases from .
to .
.
we manually analyzed them and found that it is mainly because they have other problems that are not covered by a utotrainer such as floating point bugs.
how to detect and repair such problems will be one of our future directions.
c. efficiency of autotrainer experiment design and results to evaluate the efficiency of a utotrainer we run all model trainings with and without a utotrainer enabled.
during training we collected the time used to train the model and the memory usage for both the original training and a utotrainer .
notice that experiments for run time overhead and memory overhead are conducted individually to avoid influencing each other.
the experiments are conducted times and the overhead is calculated as the average of these runs.
the results are shown intable ii .
results and analysis are presented below.
runtime overhead analysis for normal training the runtime overhead is purely from problem checker which is about .
from table ii we observe that the runtime overhead on smaller datasets is usually larger e.g.
blob vs. almost for mnist .
this is because the total time for training on small datasets is relatively short making the runtime overhead ratio larger.
for buggy trainings a utotrainer takes .
more training time on average.
we performed a deeper analysis to understand the overhead of individual components and found that retraining takes over and the rest two parts i.e.
problem checker and repair takes less than .
it means that a utotrainer only costs little time total overhead in automatically searching a suitable solution for the problems which requires lots of manual operations and time consuming in existing strategies.
as discussed in iv to repair a problem it may try several times which leads autotrainer training several models.
checking frequency v.s.
runtime overhead.
more frequent problem checking causes higher runtime overhead.
suppose that one training iteration and one checking separately take t1 andt2time then the overhead of a utotrainer is roughly 367table v check frequency vs. delay in problem detection problemdetection delay q q q q q q vg .
.
.
.
.
.
eg .
.
.
.
.
.
dy .
.
.
.
.
.
ol .
.
.
.
.
.
sc .
.
.
.
.
.
q t2 t1 whereqis the checking frequency.
figure presents the corelations between checking frequency and runtime overhead on circle and mnist.
the x axis is the number of iterations between two checks q and y axis is the runtime overhead.
the solid line represents the collected data and the dashed curve is the theoretical results i.e.
q t2 t1 .
as we can see shapes of experiment data conform to our theoretical analysis.
by comparing the two figures we observe the smaller dataset has the higher runtime overhead which is consistent with our data in table ii .
by default a utotrainer checks the problem every iterations which causes less than overhead even for small datasets like circle.
lower frequency checking can reduce the overhead but may cause longer delay in detection.
table v shows the effects of different check frequencies.
each row represents one problem type and each column denotes a different check frequency.
numbers in cells show the delayed iterations between the occurrence of the problem and the detection of the problem.
considering vg if a utotrainer performs the checking every iterations it needs extra iterations to detect it compared with checking problems every the other iteration.
in a nutshell a lower checking frequency may result in the delay in problem detection which further leads to wasting time on a buggy training.
memory overhead analysis.
autotrainer has very limited memory overhead to since a utotrainer reuses data which has been collected.
to detect problems autotrainer requires the current gradient values and historical loss and training accuracy values.
the gradient information is stored as part of the tensor data for training purpose used in backward propagation and the historical loss and training accuracy values are automatically collected by all major frameworks.
the overhead caused by a utotrainer are mostly due to program variables which are negligible compared with neuron and gradient values and even the overhead introduced by the memory profiling tool itself.
d. effects of configurable parameters autotrainer leverages configurable parameters to determine if a problem is happening or not.
we classify them into three categories and evaluated each of them in this section.
type a type a parameters include 2and 3intable i which are used to determine the time window used to detect the occurrence of vg eg and dr problems.
for these problems the same symptoms will also be observed in the rest iterations once they happen in one iteration.
we comfirmed this with models and runs on each model.
this is also supported by others .
thus we set 2and 3to .type b a utotrainer has only one type b parameter the expected accuracy threshold which is a training task dependent parameter.
if not we will adopt the value which is used to determine if the training should be early stopped and it is provided by keras and tensorflow.
type c parameters in this category include and 3for vg and eg for dr and for ol and for sc defined intable i .
the values of these parameters determine whether autotrainer can successfully capture the real problem or not.
to measure their effects on a utotrainer for each problem we use different values or value pairs if a problem involves multiple such parameters to investigate how they can affect the detection effectiveness.
all experiments are performed on models and they are repeated times.
figure reports the final averaged results.
vg the values of 1and 2affect the detection results of vg.
if 1or 2is too large it will introduce a lot of false positives i.e.
normal trainings are identified as vg .
if they are too small it will reduce the detection accuracy i.e.
true positives .
figure a demonstrates how precision and recall are changed as the parameter values change.
it confirms the aforementioned analysis and suggests the default values in a utotrainer i.e.
1e 1e can achieve high precision and recall.
eg figure b presents the relationship between precision recall and the value of .
larger 3implies that eg becomes more obvious but it also means many eg cases which are less serious will be missed.
hence precision gets higher but recall becomes very low.
luckily when 3is between and a utotrainer can get precision and recall simultaneously.
by default a utotrainer sets 3to .
dr the detection results of dr is highly affected by the value of see figure c .
with larger autotrainer is able to remove obvious false positive fp cases i.e.
detected as dr but is not dr increasing the precision but also may ignore not so serious dr problem causing low recall.
when is set in range autotrainer achieves the best result in precision and recall .
by default is set as in a utotrainer .
ol detecting ol requires two parameters and and its relationships with precision and recall in detection are shown infigure d .
it is consistent with our intuition that larger and values will lead to higher precision and lower recall.
in autotrainer the default values for and are0 03and which results in precision and recall.
sc the only threshold in detecting sc is .
very small results in very low precision and recall.
on one hand such low accuracy change are not common during training hence low recall and on the other hand many of them with such low accuracy change are due to randomly initialized weights hence low precision .
with no so large values the detection precision and recall can grow sharply.
however larger values may result in the ignorance of many buggy training cases leading to low precision.
figure e shows such a change .
.0011e 051e .
.00011e 061e .
.
.
.
.
.0precision recall 1 2 a vanishing gradient .
.
.
.
.
.
precision recall b exploding gradient .
.
.
.
.
.
precision recall c dying relu .
.
.
.
.
.
.
.
.
.0precision recall d oscillating loss .
.
.
.
.
.
.
.
.
.
precision recall e slow convergence figure .
autotrainer detection precision and recall vs. configurable parameters.
curve.
based on our sampling to achieve the best in precision and recall should be from .
to .
.
in a utotrainer we set the default value of as .
.
vi.
r elated work machine learning techniques are widely adopted in various se tasks .
a utotrainer can facilitate software engineering researchers in repairing their buggy dnn models automatically.
it is highly related with dnn model debugging and testing and automatic program repair.
dnn model debugging and testing.
in addition to what we have discussed in i there are some other efforts devoted on debugging dnn models .
ribeiro et al.
produced adversarial examples as training data to debug natural language processing models.
others cleaned up training data that are wrongly labeled to debug rnn models.
lamp utilizes gradient information as data provenance to help debug graph based machine learning algorithms.
ma et al.
proposed differential analysis on inputs to fix model overfitting and under fitting problems.
trader analyzed how problematic word embeddings affect the model accuracy by comparing the model execution traces of correctly classified samples and incorrectly classified samples.
a great number of testing methods have been proposed to test machine learning models such as fuzzing symbolic execution runtime validation fairness testing etc.
deepxplore introduced the neuron coverage metric to measures the percentage of activated neurons or a given test suite and dnn model and generates new test inputs that can maximize the metric to test dl systems.
many others extended the coverage concept and proposed to use them on many different scenarios.
model testing has also been leveraged for many other domains such as image classification automatic speech recognition text classification and machine translation .
recently yan et al.
have studied many coverage criteria and measured their correlations with model quality i.e.
model robustness against adversarial attacks and empirical results show that existing criteria can not faithfully reflect model quality.
automatic program repair.
the aim of automatic program repair is to automatically derive patches to correct bugs in programs which normally includes fault localization patch candidates generation and patch candidates validation.
many different kinds of methods have been employed in automated program repair.
the researchers in proposed search based approaches to generate patches.
there are someother semantics based methods which construct patches using synthesis techniques .
specifications were also utilized to guide the repair process .
more program repair work can be found in the survey .
different from these research efforts our work is to repair dnn models which are not uninterpretable rather than the interpretable code.
automated machine learning automl .
automl focuses on automatically design models for given training tasks.
various kinds of neural architecture search nas algorithms have been design to find efficient models such as bayesian optimization deep reinforcement learning evolutionary algorithms and gradient based methods .
these methods acquire impressive results in their experiments.
additionally open source automl tools such as autosklearn microsoft nni and autokeras also show remarkable model searching results in actual application.
although automl can automatically generate models from the training tasks these models may still face training problems when training.
comparing with automl autotrainer focuses on improving the training process.
it can provide timely monitoring facing the training process and facilitates se researchers in repairing buggy models automatically.
in summary the goal of automl and a utotrainer are different and these two are complementary solutions which can be intergraded with each other.
vii.
c onclusion this paper presents a utotrainer a dnn monitoring and auto repairing system.
it monitors the model training status and automatically fix them once a problem is detected.
by so it can prevent problems from happening at the earliest convention which saves a lot of time and resources.
our evaluation results show that a utotrainer can effectively and efficiently detecting and repairing our targeted five training problems i.e.
vanishing gradient exploding gradient dying relu oscillating loss and slow convergence .
acknowledgement the authors thank the anonymous reviewers for their insightful feedback and constructive comments.
we also thank jiong li for his efforts and feedbacks on this project.
this work is in part supported by the national science foundation of china no.
u1736205 and national key r d program of china under grand no.
2020aaa0107700.
chao shen is the corresponding author.
the views opinions and or findings expressed are only those of the authors.
369references z. lan m. chen s. goodman k. gimpel p. sharma and r. soricut albert a lite bert for self supervised learning of language representations arxiv preprint arxiv .
.
g. cadamuro r. gilad bachrach and x. zhu debugging machine learning models in icml workshop on reliable machine learning in the wild .
a. chakarov a. nori s. rajamani s. sen and d. vijaykeerthy debugging machine learning tasks arxiv preprint arxiv .
.
s. ma y .
liu w. c. lee x. zhang and a. grama mode automated neural network model debugging via state differential analysis and input selection in proceedings of the 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering esec fse .
acm pp.
.
d. man et al.
tensorboard tensorflow s visualization toolkit .
visdom .
tensorwatch .
manifold .
s. ioffe and c. szegedy batch normalization accelerating deep network training by reducing internal covariate shift arxiv preprint arxiv .
.
g. klambauer t. unterthiner a. mayr and s. hochreiter self normalizing neural networks in advances in neural information processing systems pp.
.
l. lu y .
shin y .
su and g. e. karniadakis dying relu and initialization theory and numerical examples arxiv preprint arxiv .
.
d. masters and c. luschi revisiting small batch training for deep neural networks arxiv preprint arxiv .
.
autotrainer repository .
y .
lecun y .
bengio and g. hinton deep learning nature vol.
no.
pp.
.
p. ramachandran b. zoph and q. v .
le searching for activation functions arxiv preprint arxiv .
.
v .
nair and g. e. hinton rectified linear units improve restricted boltzmann machines in icml .
y .
sun x. wang and x. tang deeply learned face representations are sparse selective and robust in proceedings of the ieee conference on computer vision and pattern recognition pp.
.
pytorch documentations .
y .
lecun the mnist database of handwritten digits .
s. hochreiter untersuchungen zu dynamischen neuronalen netzen diploma technische universit t m nchen vol.
no.
.
d. sussillo and l. abbott random walk initialization for training very deep feedforward networks arxiv preprint arxiv .
.
j. miller and m. hardt stable recurrent models arxiv preprint arxiv .
.
i. arnekvist j. f. carvalho d. kragic and j. a. stork the effect of target normalization and momentum on dying relu arxiv preprint arxiv .
.
c. xing d. arpit c. tsirigotis and y .
bengio a walk with sgd arxiv preprint arxiv .
.
convolutional neural networks for visual recognition hub.io neural networks .
n. bjorck c. p. gomes b. selman and k. q. weinberger understanding batch normalization in advances in neural information processing systems pp.
.
a. l. maas a. y .
hannun and a. y .
ng rectifier nonlinearities improve neural network acoustic models in proc.
icml vol.
no.
p. .
i. goodfellow y .
bengio and a. courville deep learning .
mit press .
a. graves generating sequences with recurrent neural networks arxiv preprint arxiv .
.
r. pascanu t. mikolov and y .
bengio on the difficulty of training recurrent neural networks in international conference on machine learning pp.
.
x. glorot and y .
bengio understanding the difficulty of training deep feedforward neural networks in proceedings of the thirteenth international conference on artificial intelligence and statistics pp.
.
k. he x. zhang s. ren and j. sun delving deep into rectifiers surpassing human level performance on imagenet classification in proceedings of the ieee international conference on computer vision pp.
.
how to control the stability of training neural networks with the batch size ty of training neural networks with gradient descent batch size .
d. p. kingma and j. ba adam a method for stochastic optimization arxiv preprint arxiv .
.
y .
bengio practical recommendations for gradient based training of deep architectures in neural networks tricks of the trade .
springer pp.
.
i. sutskever j. martens g. dahl and g. hinton on the importance of initialization and momentum in deep learning in international conference on machine learning pp.
.
s. ruder an overview of gradient descent optimization algorithms arxiv preprint arxiv .
.
n. qian on the momentum term in gradient descent learning algorithms neural networks vol.
no.
pp.
.
keras the python deep learning library .
m. abadi p. barham j. chen z. chen a. davis j. dean m. devin s. ghemawat g. irving m. isard et al.
tensorflow a system for large scale machine learning in 12thfusenixgsymposium on operating systems design and implementation fosdig16 pp.
.
sklearn make circles dataset nerated sklearn.datasets.make circles.html .
sklearn make blobs dataset rated sklearn.datasets.make blobs.html .
cifar datasets .
imdb datasets .
reuters dataset tions reuters21578 .
scikit learn machine learning in python .
d. di nucci f. palomba d. a. tamburri a. serebrenik and a. de lucia detecting code smells using machine learning techniques are we there yet?
in2018 ieee 25th international conference on software analysis evolution and reengineering saner .
ieee pp.
.
d. alrajeh and a. russo logic based learning theory and application in machine learning for dynamic software analysis potentials and limits .
m. tufano j. pantiuchina c. watson g. bavota and d. poshyvanyk on learning meaningful code changes via neural machine translation arxiv preprint arxiv .
.
c. s. p as areanu d. gopinath and h. yu compositional verification for autonomous systems with deep learning components in safe autonomous and intelligent vehicles .
springer pp.
.
r. ben abdessalem s. nejati l. c. briand and t. stifter testing advanced driver assistance systems using multi objective search and neural networks inproceedings of the 31st ieee acm international conference on automated software engineering .
acm pp.
.
x. gu h. zhang and s. kim deep code search in ieee acm 40th international conference on software engineering .
z. liu x. xia a. e. hassan d. lo z. xing and x. wang neural machinetranslation based commit message generation how far are we?
in proceedings of the 33rd acm ieee international conference on automated software engineering .
acm pp.
.
c. chen z. xing and y .
liu by the community for the community a deep learning approach to assist collaborative editing in q a sites proceedings of the acm on human computer interaction .
b. lin f. zampetti g. bavota m. di penta m. lanza and r. oliveto sentiment analysis for software engineering how far can we go?
in proceedings of 40th international conference on software engineering icse .
z. li d. zou s. xu x. ou h. jin s. wang z. deng and y .
zhong vuldeepecker a deep learning based system for vulnerability detection arxiv preprint arxiv .
.
j. henkel s. k. lahiri b. liblit and t. reps code vectors understanding programs through embedded abstracted symbolic traces in proceedings of the 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering .
k. wang r. singh and z. su dynamic neural program embedding for program repair arxiv preprint arxiv .
.
s. bhatia p. kohli and r. singh neuro symbolic program corrector for introductory programming assignments in ieee acm 40th international conference on software engineering icse .
ieee pp.
.
s. iyer i. konstas a. cheung and l. zettlemoyer summarizing source code using a neural attention model in proceedings of the 54th annual meeting of the association for computational linguistics .
s. jiang a. armaly and c. mcmillan automatically generating commit messages from diffs using neural machine translation in proceedings of ieee acm international conference on automated software engineering .
m. t. ribeiro s. singh and c. guestrin semantically equivalent adversarial rules for debugging nlp models in association for computational linguistics acl .
x. zhang x. zhu and s. wright training set debugging using trusted items inthirty second aaai conference on artificial intelligence .
y .
jiang and z. h. zhou editing training data for knn classifiers with neural network ensemble in international symposium on neural networks .
g. tao s. ma y .
liu q. xu and x. zhang trader trace divergence analysis and embedding regulation for debugging recurrent neural networks in ieee acm 42nd international conference on software engineering icse .
ieee .
y .
tian s. ma m. wen y .
liu s. cheung and x. zhang testing deep learning models for image analysis using object relevant metamorphic relations corr vol.
abs .
.
.
available s. ma y .
aafer z. xu w. lee j. zhai y .
liu and x. zhang lamp data provenance for graph based machine learning algorithms through derivative computation in proceedings of the 11th joint meeting on foundations of software engineering esec fse paderborn germany september e. bodden w. sch fer a. van deursen and a. zisman eds.
acm pp.
.
.
available a. odena c. olsson d. andersen and i. goodfellow tensorfuzz debugging neural networks with coverage guided fuzzing in international conference on machine learning pp.
.
j. guo y .
jiang y .
zhao q. chen and j. sun dlfuzz differential fuzzing testing of deep learning systems in proceedings of the 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pp.
.
x. xie l. ma f. juefei xu m. xue h. chen y .
liu j. zhao b. li j. yin and s. see deephunter a coverage guided fuzz testing framework for deep neural networks in proceedings of the 28th acm sigsoft international symposium on software testing and analysis pp.
.
m. wicker x. huang and m. kwiatkowska feature guided black box safety testing of deep neural networks in international conference on tools and algorithms for the construction and analysis of systems .
springer .
j. uesato a. kumar c. szepesvari t. erez a. ruderman k. anderson n. heess p. kohli et al.
rigorous agent evaluation an adversarial approach to uncover catastrophic failures arxiv preprint arxiv .
.
z. q. zhou and l. sun metamorphic testing of driverless cars communications of the acm vol.
no.
pp.
.
s. udeshi p. arora and s. chattopadhyay automated directed fairness testing inproceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
x. gao r. saha m. prasad and r. abhik fuzz testing based data augmentation to improve robustness of deep neural networks in ieee acm 42nd international conference on software engineering icse .
ieee .
a. ramanathan l. l. pullum f. hussain d. chakrabarty and s. k. jha integrating symbolic and statistical methods for testing intelligent systems applications to machine learning and computer vision in design automation test in europe conference exhibition date .
ieee pp.
.
d. gopinath c. s. pasareanu k. wang m. zhang and s. khurshid symbolic execution for attribution and attack synthesis in neural networks in ieee acm 41st international conference on software engineering companion proceedings icse companion .
ieee pp.
.
a. aggarwal p. lohia s. nagar k. dey and d. saha black box fairness testing of machine learning models in proceedings of the 27th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pp.
.
y .
sun m. wu w. ruan x. huang m. kwiatkowska and d. kroening concolic testing for deep neural networks in proceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
a. stocco m. weiss m. calzana and p. tonella misbehaviour prediction for autonomous driving systems in ieee acm 42nd international conference on software engineering icse .
ieee .
h. wang j. xu c. xu x. ma and j. lu dissector input validation for deep learning applications by crossing layer dissection in ieee acm 42nd international conference on software engineering icse .
ieee .
p. zhang j. wang j. sun g. dong x. wang x. wang j. s. dong and d. ting white box fairness testing through adversarial sampling in ieee acm 42nd international conference on software engineering icse .
ieee .
k. pei y .
cao j. yang and s. jana deepxplore automated whitebox testing of deep learning systems in proceedings of the 26th symposium on operating systems principles pp.
.
l. ma f. juefei xu f. zhang j. sun m. xue b. li c. chen t. su l. li y .
liu et al.
deepgauge multi granularity testing criteria for deep learning systems inproceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
y .
tian k. pei s. jana and b. ray deeptest automated testing of deepneural network driven autonomous cars in proceedings of the 40th international conference on software engineering pp.
.
m. zhang y .
zhang l. zhang c. liu and s. khurshid deeproad gan based metamorphic testing and input validation framework for autonomous driving systems in proceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
h. zhou w. li z. kong j. guo y .
zhang l. zhang b. yu and c. liu deepbillboard systematic physical world testing of autonomous driving systems in ieee acm 41st international conference on software engineering icse .
ieee .
s. gerasimou h. f. eniser a. sen and a. cakan importance driven deep learning system testing in ieee acm 42nd international conference on software engineering icse .
ieee .
y .
tian z. zhong v .
ordonez g. kaiser and b. ray testing dnn image classifier for confusion bias errors in ieee acm 42nd international conference on software engineering icse .
ieee .
x. du x. xie y .
li l. ma y .
liu and j. zhao deepstellar model based quantitative analysis of stateful deep learning systems in proceedings of the 27th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pp.
.
p. he c. meister and z. su structure invariant testing for machine translation in2020 ieee acm 42nd international conference on software engineering icse .
ieee .
z. sun j. m. zhang m. harman m. papadakis and l. zhang automatic testing and improvement of machine translation in ieee acm 42nd international conference on software engineering icse .
ieee .
s. yan g. tao x. liu j. zhai s. ma l. xu and x. zhang correlations between deep neural network model coverage criteria and model quality in esec fse 28th acm joint european software engineering conference and symposiumon the foundations of software engineering virtual event usa november p. devanbu m. b. cohen and t. zimmermann eds.
acm pp.
.
.
available w. weimer t. nguyen c. le goues and s. forrest automatically finding patches using genetic programming in proceedings of the 31st international conference on software engineering .
ieee computer society pp.
.
w. weimer s. forrest c. l. goues and t. v .
nguyen automatic program repair with evolutionary computation communications of the acm vol.
no.
pp.
.
c. le goues m. dewey v ogt s. forrest and w. weimer a systematic study of automated program repair fixing out of bugs for each in software engineering icse 34th international conference on .
ieee pp.
.
c. l. goues t. v .
nguyen s. forrest and w. weimer genprog a generic method for automatic software repair ieee transactions on software engineering vol.
no.
pp.
.
a. arcuri evolutionary repair of faulty software applied soft computing vol.
no.
pp.
.
y .
qi x. mao y .
lei z. dai and c. wang the strength of random search on automated program repair in proceedings of the 36th international conference on software engineering .
acm pp.
.
s. jha s. gulwani s. a. seshia and a. tiwari oracle guided componentbased program synthesis in proceedings of the 32rd international conference on software engineering .
acm pp.
.
h. d. t. nguyen d. qi a. roychoudhury and s. chandra semfix program repair via semantic analysis in proceedings of the international conference on software engineering .
ieee press pp.
.
s. mechtaev j. yi and a. roychoudhury directfix looking for simple program repairs in ieee acm ieee international conference on software engineering pp.
.
j. hua m. zhang k. wang and s. khurshid towards practical program repair with on demand candidate generation in proceedings of the 40th international conference on software engineering pp.
.
f. long and m. rinard staged program repair with condition synthesis inproceedings of the 10th joint meeting on foundations of software engineering .
acm pp.
.
b. demsky and m. rinard data structure repair using goal directed reasoning ininternational conference on software engineering .
icse .
proceedings pp.
.
b. demsky m. d. ernst p. j. guo s. mccamant j. h. perkins and m. rinard inference and enforcement of data structure consistency specifications in proceedings of the international symposium on software testing and analysis .
acm pp.
.
d. gopinath m. z. malik and s. khurshid specification based program repair using sat in international conference on tools algorithms for the construction analysis of systems pp.
.
y .
pei c. a. furia m. nordio y .
wei b. meyer and a. zeller automated fixing of programs with contracts ieee transactions on software engineering vol.
no.
pp.
.
l. chen y .
pei and c. a. furia contract based program repair without the contracts in 32nd ieee acm international conference on automated software engineering ase .
ieee pp.
.
l. gazzola d. micucci and l. mariani automatic software repair a survey ieee transactions on software engineering vol.
no.
pp.
.
j. snoek h. larochelle and r. p. adams practical bayesian optimization of machine learning algorithms advances in neural information processing systems vol.
pp.
.
h. jin q. song and x. hu auto keras an efficient neural architecture search system in proceedings of the 25th acm sigkdd international conference on knowledge discovery data mining pp.
.
b. zoph and q. v .
le neural architecture search with reinforcement learning arxiv preprint arxiv .
.
b. baker o. gupta n. naik and r. raskar designing neural network architectures using reinforcement learning arxiv preprint arxiv .
.
z. guo x. zhang h. mu w. heng z. liu y .
wei and j. sun single path oneshot neural architecture search with uniform sampling in european conference on computer vision .
springer pp.
.
e. real s. moore a. selle s. saxena y .
l. suematsu j. tan q. le and a. kurakin large scale evolution of image classifiers arxiv preprint arxiv .
.
r. luo f. tian t. qin e. chen and t. y .
liu neural architecture optimization inadvances in neural information processing systems pp.
.
h. cai l. zhu and s. han proxylessnas direct neural architecture search on target task and hardware arxiv preprint arxiv .
.
auto sklearn .
m. feurer a. klein k. eggensperger j. t. springenberg m. blum and f. hutter auto sklearn efficient and robust automated machine learning in automated machine learning .
springer cham pp.
.
m. feurer k. eggensperger s. falkner m. lindauer and f. hutter auto sklearn .
the next generation arxiv preprint arxiv .
.
microsoft nni .
autokeras .