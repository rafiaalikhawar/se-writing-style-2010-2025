studying the usage of text to text transfer transformer to support code related tasks antonio mastropaolo simone scalabrinoy nathan cooperz david nader palacioz denys poshyvanykz rocco olivetoy gabriele bavota seart software institute universit a della svizzera italiana usi switzerland yuniversity of molise italy zsemeru computer science department william and mary usa abstract deep learning dl techniques are gaining more and more attention in the software engineering community.
they have been used to support several code related tasks such as automatic bug fixing and code comments generation.
recent studies in the natural language processing nlp field have shown that the text to text transfer transformer t5 architecture can achieve state of the art performance for a variety of nlp tasks.
the basic idea behind t5 is to first pre train a model on a large and generic dataset using a self supervised task e.g.
filling masked words in sentences .
once the model is pre trained it is fine tuned on smaller and specialized datasets each one related to a specific task e.g.
language translation sentence classification .
in this paper we empirically investigate how the t5 model performs when pre trained and fine tuned to support code related tasks.
we pre train a t5 model on a dataset composed of natural language english text and source code.
then we fine tune such a model by reusing datasets used in four previous works that used dl techniques to i fix bugs ii inject code mutants iii generate assert statements and iv generate code comments.
we compared the performance of this single model with the results reported in the four original papers proposing dl based solutions for those four tasks.
we show that our t5 model exploiting additional data for the self supervised pre training phase can achieve performance improvements over the four baselines.
index terms empirical software engineering deep learning i. i ntroduction deep learning dl has been used to support a vast variety of code related tasks.
some examples include automatic bug fixing learning generic code changes code migration code summarization pseudo code generation code deobfuscation injection of code mutants automatic generation of assert statements and code completion .
these works customize dl models proposed in the natural language processing nlp field to support the previously listed tasks.
for instance tufano et al.
used an rnn encoder decoder architecture commonly adopted in neural machine translation nmt to learn how to automatically fix bugs in java methods.
the model learned bug fixing patterns by being trained on pairs of buggy and fixed methods mined from software repositories.
this work as the vast majority of the ones previously mentioned e.g.
share one common characteristic they shape the problem at hand as a text to text transformation in which the input and the output of the model are text strings.for example in the work by watson et al.
the input is a string representing a test method without an assert statement and the output is an appropriate assert statement for the given test.
in the approach by haque et al.
the input is composed of strings representing a subroutine to document while the output is a natural language summary documenting the subroutine.
recent years have seen the raise of transfer learning in the field of natural language processing.
the basic idea is to first pre train a model on a large and generic dataset by using a selfsupervised task e.g.
masking tokens in strings and asking the model to guess the masked tokens.
then the trained model is fine tuned on smaller and specialized datasets each one aimed at supporting a specific task.
in this context raffel et al.
proposed the t5 text to text transfer transformer model pre trained on a large natural language corpus and fine tuned to achieve state of the art performance on many tasks all characterized by text to text transformations.
the goal of this work is to empirically investigate the potential of a t5 model when pre trained and fine tuned to support many of the previously listed code related tasks also characterized by text to text transformations.
we started by pre training a t5 model using a large dataset consisting of english sentences and source code components i.e.
methods .
then we fine tune the model using four datasets from previous work with the goal of supporting four code related tasks automatic bug fixing.
we use the dataset by tufano et al.
composed of instances in which the input string is represented by a buggy java method and the output string is the fixed version of the same method.
injection of code mutants.
this dataset is also by tufano et al.
and features instances in which the input output strings are reversed as compared to automatic bug fixing i.e.
the input is a fixed method while the output is its buggy version .
the model must learn how to inject bugs mutants in code instead of fixing bugs.
generation of assert statements in test methods.
we use the dataset by watson et al.
composed of instances in which the input string is a representation of a test method without an assert statement and a focal method it tests i.e.
the main production method tested while the output string encodes an appropriate assert statement for the input test method.
ieee acm 43rd international conference on software engineering icse .
ieee code summarization.
we use the dataset by haque et al.
where input strings are some representations of a java method to summarize an output string is a textual summary.
once the t5 model has been fine tuned on all these tasks we run it on the same test sets used in the four referenced works comparing the achieved results to those reported in the original work.
our results show that the t5 model is able to improve the performance of the original models in all four tasks.
worth noticing is that besides the different architecture of the t5 model the latter can take advantage of a pre training phase in which additional training data is provided as input as compared to the four baselines.
this could explain at least partially the boost of performance that we observed.
also as previously said the additional pre training is done in a selfsupervised way i.e.
by simply masking random tokens in the code text used for pre training making this step relatively cheap to perform and scalable to large code bases that can be easily collected from sources such as github.
in contrast the four baselines exploit a completely supervised training e.g.
in the case of automatic bug fixing the baseline needs pairs of buggy and fixed methods to be trained .
building such a dataset for supervised training has a cost and there are limitations in terms of the amount of data one can mine.
besides the good performance ensured by the t5 having a single model able to support different tasks can benefit technological transfer since it simplifies the implementation and the maintenance of a tool supporting several tasks.
the code and data used in this work are publicly available .
ii.
r elated work dl techniques have been used to support many software engineering tasks.
due to space limitations we discuss only the approaches related to the four tasks we subject to our study with particular attention on those used as baselines.
we also introduce notions needed to understand our experimental design.
a. automatic bug fixing many techniques have been proposed for the automatic fixing of software bugs.
several of them rely on the redundancy assumption claiming that large programs contain the seeds of their own repair.
such an assumption has been verified by at least two independent studies .
in this section we focus on techniques exploiting dl for bug fixing.
mesbah et al.
focus on build time compilation failures by presenting deepdelta an approach using nmt to fix the build.
the input is represented by features characterizing the compilation failure e.g.
kind of error ast path etc.
.
as output deepdelta provides the ast changes needed to fix the error.
in the presented empirical evaluation deepdelta correctly fixed out of compilation errors.
chen et al.
present sequencer a sequence to sequence approach trained on over 35k single line bug fixes.
sequencer takes as input the buggy line together with its abstract buggy context meaning the relevant code lines from the buggy class.the output of the approach is the recommended fix for the buggy line.
the approach tested on a set of bugs was able to automatically fix of them.
similar approaches have been proposed by hata et al.
and tufano et al.
.
the latter is the one we compared our approach with and thus we describe it in more details.
tufano et al.
investigate the performance of an nmtbased approach in the context of automatic bug fixing.
they train an encoder decoder model on a set of bug fix pairs bfps meaning pairs of strings in which the first one input represents a java method that has been subject to a bug fixing activity and the second one target represents the same java method once the bug was fixed.
to build this dataset the authors mined 787k bug fixing commits from github from which they extracted .3m bfps.
after that the code of the bfps is abstracted to make it more suitable for the nmt model i.e.
to reduce the vocabulary of terms used in the source code identifiers and literals .
the abstraction process is depicted in fig.
.
raw source code abstracted code abstracted code with idiomspublic integer getminelement list mylist if mylist.size return listmanager.getfirst mylist return public type 1 method 1 type 2 var 1 if var 1 .
method 2 int 1 return type 3 .
method 3 var 1 return int 1 public type 1 method 1 list var 1 if var 1 .
size return type 2 .
method 3 var 1 return fig.
abstraction process the top part of the figure represents the raw source code to abstract.
the authors use a java lexer and a parser to represent each method as a stream of tokens in which java keywords and punctuation symbols are preserved and the role of each identifier e.g.
whether it represents a variable method etc.
as well as the type of a literal is discerned.
ids are assigned to identifiers and literals by considering their position in the method to abstract the first variable name found will be assigned the id of v ar likewise the second variable name will receive the id of v ar .
this process continues for all identifiers as well as for the literals e.g.
string x int x float x .
the output of this stage is the code reported in the middle of fig.
i.e.
abstracted code .
since some identifiers and literals appear very often in the code e.g.
variables i j literals method names such as size those are treated as idioms and are not abstracted see bottom part of fig.
idioms are in bold .
tufano et al.
consider as idioms the top frequent words in their dataset.
during the abstraction a mapping between the raw and the abstracted tokens is maintained thus allowing to reconstruct the concrete code from the abstract code generated by the model.
337the set of abstracted bfps has been used to train and test the approach.
the authors build two different sets namely bfp small only including methods having a maximum length of tokens for a total of instances and bfp medium including methods up to tokens .
the model was able to correctly predict the patch for the buggy code in and of cases in the bfp small and bfp medium dataset respectively.
while other works have tackled the automatic bug fixing problem the approach by tufano et al.
has been tested on a variety of different bugs rather than on specific types of bugs warnings e.g.
only single line bugs are considered in while compilation failures are addressed in .
thus we picked it as representative dl technique for automatic bug fixing and we use the two datasets by tufano et al.
to fine tune the t5 model for the automatic bugfixing problem comparing the achieved performance with the one reported in the original paper.
b. injection of code mutants brown et al.
were the first to propose a data driven approach for generating code mutants leveraging bug fixes performed in software systems to extract syntactic mutation patterns from the diffs of patches.
tufano et al.
built on this idea by presenting an approach using nmt to inject mutants representative of real bugs.
the idea is similar to the previously described bug fixing paper with however the learning happening in the opposite direction.
indeed given a bug fixing commit the input to the model is in this case the fixed method i.e.
the method obtained after the bugfixing activity while the target is the buggy method before the bug fix .
this allows the model to learn how to inject in a working code a mutant representative of real bugs.
the applied methodology is the same described for the bug fixing work including the abstraction process.
this is to date the only dl based technique for injecting code mutants.
thus we use the dataset exploited by tufano et al.
to fine tune the t5 model for the problem of injecting code mutants comparing the achieved results with the ones reported in the original paper.
specifically we reused their largest dataset referred to as gmident in the paper1 featuring training instances used for hyperparameter tuning evaluation set and used for testing.
on this data the approach by tufano et al.
was able to correctly predict the bug to inject in of cases .
c. generation of assert statements in test methods watson et al.
start from the work by shamshiri et al.
who observed that tools for the automatic generation of test cases such as evosuite randoop and agitar exhibit insufficiencies in the automatically generated assert statements.
1a subset of this dataset named gm ident lit has also been used in the original paper to avoid including in the study bugs requiring the generation of previously unseen literals.
we decided to test the t5 model on the most complex and complete dataset.thus they propose atlas an approach for generating syntactically and semantically correct unit test assert statements using nmt.
to train atlas the authors mined .5m test methods from github with their corresponding assert statement.
for each of those test methods they also identified the focal method meaning the main production code method exercised by the test.
a preprocessing of the dataset has been performed to remove all test methods longer than 1k tokens.
also test methods requiring the synthesis of one or more unknown tokens for generating the appropriate assert statements have been removed.
indeed if the required tokens cannot be found in the vocabulary of the test method they cannot be synthesized when the model attempts to generate the prediction.
finally all duplicates have been removed from the dataset leading to a final set of test assert pairs taps .
each method left in the dataset has then been abstracted using the same approach previously described by tufano et al.
.
however in this case the authors experiment with two datasets one containing raw source code and one abstracted code.
atlas was able to generate asserts identical to the ones written by developers in .
of cases perfectly predicted assert statements when only considering the top prediction and .
when looking at the top in the abstracted dataset while performance is lower on the raw dataset .
for top and .
for top .
this is the only dl based technique proposed in the literature to generate assert statements.
we use the datasets by watson et al.
to fine tune our t5 model for the generation of assert statements problem and compare the achieved performance with the one in the original paper.
d. code summarization code summarization is one of the mainstream methods for automatic documentation of source code.
the proposed summarization techniques fall into two categories extractive and abstractive .
the former create a summary of a code component which includes information extracted from the component being summarized while the latter may include in the generated summaries information that is not present in the code component to document.
dl techniques have been used to support the generation of abstractive summaries.
huet al.
use a deep neural network dnn to automatically generate comments for a given java method.
the authors mine 9k java projects hosted on github to collect pairs ofhmethod commenti where comment is the first sentence of the javadoc linked to the method.
these pairs properly processed are used to train and test the dnn.
the authors assess the effectiveness of their technique by using the bleu score showing the superiority of their approach with respect to the competitive technique presented in .
allamanis et al.
use attention mechanisms in neural networks to suggest a descriptive method name starting from an arbitrary snippet of code.
their approach can name a code snippet exactly as a developer would do in of cases.
338leclair et al.
present a neural model combining the ast source code structure and words from code to generate coherent summaries of java methods.
the approach tested on .1m methods showed its superiority as compared to the previous works by hu et al.
and iyer et al.
.
the approach by haque et al.
is the most recent in the area of dl aided source code summarization and it is an improvement of the work by leclair et al.
.
it still aims at documenting java methods through an encoder decoder architecture but in this case three inputs are provided to the model to generate the summary i the source code of the method as a flattened sequence of tokens representing the method ii its ast representation and iii the file context meaning the code of every other method in the same file.
the authors show that adding the contextual information as one of the inputs substantially improves the bleu score obtained by deep learning techniques.
the dataset used in the evaluation is composed of .1m java methods paired with summaries.
we reuse this dataset for the finetuning of the t5 model for the code summarization problem and compare its performance to the state of the art approach proposed by haque et al.
.
iii.
m ultitask learning for code related tasks the t5 model was introduced by raffel et al.
to support multitask learning in the domain of nlp.
this approach is based on two phases pre training which allows defining a shared knowledge base useful for a large class of sequence tosequence tasks and fine tuning which specializes the model to specific tasks of interest.
in this section we first provide basic information about the t5 model refer to for a detailed explanation of the architecture .
then we explain how we adapted it to the software engineering domain with the goal of supporting the four tasks previously described automatic bugfixing generation of assert statements in test methods code summarization and injection of code mutants.
such a process is depicted in fig.
.
finally we describe the hyperparameter tuning of the model and the adopted decoding strategy.
a. t5 in a nutshell the t5 model is based on the transformer model architecture that allows to handle a variable sized input using stacks of self attention layers instead of rnns or cnns.
when an input sequence is provided it is mapped to a sequence of embeddings that is passed into the encoder.
the encoders are all identical in structure and each one is comprised of two subcomponents a self attention layer followed by a small feed forward network.
layer normalization is applied to the input of each subcomponent while a residual skip connection adds each input of the subcomponent to its output.
dropout is applied within the feed forward network on the skip connection on the attention weights and at the input and output of the entire stack.
the decoders work similarly to the encoders each self attention layer is followed by an additional attention mechanism that attends to the output of the encoder.the output of the final decoder block is fed into a dense layer with a softmax output to produce the output probabilities over the vocabulary.
differently from the generic transformer model the t5 model uses a simplified form of position embeddings where each embedding is a scalar that is added to the corresponding logit used for computing the attention weights.
as pointed out by the authors for efficiency they also share the position embedding parameters across all layers.
the t5 in particular and a transformer model in general offer two main advantages over other state of the art models i it is more efficient than rnns since it allows to compute the output layers in parallel and ii it is able to detect hidden and long ranged dependencies among tokens without assuming that nearest tokens are more related than distant ones.
this last property is particularly relevant in code related tasks since a variable declaration may be distant from its usage.
five different versions of t5 have been proposed small base large billion and billion.
these variants differ in terms of complexity with the smaller model t5 small having 60m parameters against the 11b of the largest one t5 11b .
as acknowledged by the authors even if the accuracy of the most complex variants are higher than the less complex models the training complexity increases with the number of parameters.
considering the available computational resources in our work we decided to use the simplest t5small model.
we expect the results achieved in our study to be a lower bound for the performance of a t5 based model.
nevertheless as reported in section v the t5 small model is still able to outperform state of the art approaches.
the t5 small architecture is characterized by six blocks for encoders and decoders.
the feed forward networks in each block consist of a dense layer with an output dimensionality dff of .
the keyand value matrices of all attention mechanisms have an inner dimensionality d kv of and all attention mechanisms have eight heads.
all the other sublayers and embeddings have a dimensionality d model of .
b. pre training of t5 in the pre training phase we use a self supervised task similar to the one used by raffel et al.
consisting of masking tokens in natural language sentences and asking the model to guess the masked tokens.
differently we did not perform the pre training by only using natural language sentences since all the tasks we target involve source code.
thus we use a dataset composed of both technical natural language i.e.
code comments and source code.
to obtain the dataset for the pre training we start from the codesearchnet dataset which provides 6m functions from open source code.
we only focus on the .5m methods written in java since the four tasks we aim at supporting are all related to java code.
then since for three of the four tasks we support i.e.
automatic bug fixing generation of assert statements and injection of code mutants the authors of the original papers used an abstracted version of source code see section ii we used the src2abs tool by tufano to create an abstracted version of each mined java method.
.67m instancescodesearchnet buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l bfsmall src2abs pre training dataset preparationpre training dataset preparation bfmedium agraw agabt mgident cs 2raw code abstracted code commentspre training t5 small fine tuning datasets preparation4 .42m instancesbuggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l text to text task specific pairs5 fine tuning on tasks mixture t5 pre trained network t5 fine tuned on tasks mixturefig.
overview of the approach used to pre train and fine tune the t5 model.
note that since the tool was run on java methods in isolation i.e.
without providing it the whole code of the projects they belong to src2abs raised a parsing error in 600k of the .5m methods due e.g.
to missing