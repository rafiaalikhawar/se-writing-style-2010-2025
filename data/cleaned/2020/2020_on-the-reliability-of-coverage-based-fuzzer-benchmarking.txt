on the reliability of coverage based fuzzer benchmarking marcel b hme mpi sp germany monash university australial szl szekeres google usajonathan metzman google usa abstract givenaprogramwherenoneofourfuzzersfindsanybugs howdo we know which fuzzer is better?
in practice we often look to code coverageasaproxymeasureoffuzzereffectivenessandconsider the fuzzer which achieves more coverage as the better one.
indeed evaluating10fuzzersfor23hourson24programs we findthatafuzzerthatcoversmorecodealsofindsmorebugs.there isaverystrongcorrelation betweenthecoverageachieved andthe numberofbugsfoundbyafuzzer.hence itmightseemreasonable to compare fuzzers in terms of coverage achieved and from that derive empirical claims about a fuzzer s superiority at finding bugs.
curiously enough however we find no strong agreement on which fuzzer is superior if we compared multiple fuzzers in terms of coverage achieved instead of the number of bugs found.
the fuzzer best at achieving coverage may not be best at finding bugs.
acm reference format marcel b hme l szl szekeres and jonathan metzman.
.
on the reliabilityofcoverage basedfuzzerbenchmarking.in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm newyork ny usa 13pages.
introduction in the recent decade fuzzing has found widespread interest.
in industry wehavelargecontinuousfuzzingplatformsemploying 100k machinesforautomaticbugfinding .inacademia in2020alone almost50fuzzingpaperswerepublishedinthetop conferences for security and software engineering .
imagine wehaveseveralfuzzersavailabletotestourprogram.
hopefully noneofthemfindsanybugs.ifindeedtheydon t we might have someconfidence in the correctness of the program.
thenagain evenaperfectly non functionalfuzzerwouldfindno bugs in our program.
so how do we know which fuzzer has the highest potential of finding bugs?
a widely used proxy measure offuzzereffectivenessisthecodecoveragethatisachieved.after all a fuzzer cannot find bugs in code that it does not cover.
indeed in our experiments we identify a very strong positive correlation between the coverage achieved and the number of bugs found by a fuzzer.
correlation assesses the strength of the associationbetweentworandomvariablesormeasures.weconductour empiricalinvestigationon10fuzzers 24cprograms 20fuzzing campaignsof23hours 13cpuyears .weusethreemeasuresof coverage and two measures of bug finding and our results suggest as the fuzzer covers more code it also discovers more bugs.
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthefirstpage.copyrightsforthird partycomponentsofthisworkmustbehonored.
for all other uses contact the owner author s .
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
fuzzer ranks by avg.
branches coveredfuzzer ranks by avg.
bugs discovered benchmarks a hour fuzzing campaigns .
.
fuzzer ranks by avg.
branches coveredfuzzer ranks by avg.
bugs discovered benchmarks b day fuzzing campaigns .
.
figure scatterplot of the ranks of fuzzers applied to programs for a hour and b hours when ranking each fuzzer in terms of the avg.
number of branches covered xaxis and in terms of the avg.
number of bugs found y axis .
hence it might seem reasonable to conjecture that the fuzzer which is better in terms of code coverage is also better in terms of bug finding but is this really true?
in figure we show the rankingofthesefuzzersacrossallprogramsintermsoftheaverage coverageachievedandtheaveragenumberofbugsfoundineach benchmark.
the ranks are visibly different.
to be sure we also conducted a pair wise comparison between any two fuzzers where the difference in coverage andthe difference in bug finding are statistically significant.
the results are similar.
we identify nostrong agreement on the superiority orranking ofafuzzerwhencomparedintermsofmeancoverageversusmean bug finding.
inter rater agreement assesses the degree to which tworaters here bothtypesofbenchmarking agreeon thesuperiority or ranking of a fuzzer when evaluated on multiple programs.
indeed two measures of the same construct are likely to exhibit a highdegreeofcorrelationbutcanatthesametimedisagreesubstantially .
we evaluate the agreement on fuzzer superiority when comparing any two fuzzers where the differences in terms of coverage andbugfindingarestatisticallysignificant.weevaluate the agreement on fuzzer ranking when comparing all the fuzzers.
concretely our results suggest a moderate agreement.
for fuzzer pairs wherethedifferencesintermsofcoverage andbugfinding is statistically significant the results disagree for to of programs.onlywhenmeasuringtheagreementbetweenbranch coverageandthenumberofbugsfoundandwhenwerequirethe differences to be statistically significant at p .
for coverage andbugfinding dowefindastrongagreement.however statistical significanceat p .0001onlyintermsofcoverageisnotsufficient we again find only weak agreement.
the increase in agreement with statistical significance is notobserved when we measure bug findingusingthetime to error.wealsofindthatthevarianceofthe agreementreducesas moreprogramsareused andthatresultsof 1h campaigns do not strongly agree with results of 23h campaigns.
ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa marcel b hme l szl szekeres and jonathan metzman in summary this paper makes the following contributions we introduce a novel methodology to evaluate proxy measures of fuzzer or test suite effectiveness.
specifically we suggest evaluating agreement instead of correlation and propose a bugbased evaluation without pre determined ground truth.
weprovidethefirstevidenceonthereliabilityofcoverage based benchmarking for the evaluation of fuzzer effectiveness.
we confirm a very strong correlation and a moderate agreement.
weexploreaninterpretationofourresultsfor reachingafault versusexposingabug section6 anddiscussourresultsinthe largercontextoffuzzerbenchmarking wherewemakeconcrete recommendations for future evaluations section .
wepublishalldata theanalysis andthevirtualexperimental infrastructure.we provide preciseinstructions toreproduceand extend our experiments related work codecoveragehaslongbeenusedasaproxymeasureofthebug finding ability of a test suite.
fortunately in practice the most commonsituationisthatthetestsuite detectsnobugs.now ifalltest cases pass how do we assert whether the test suite is effective?
practitionersoftenrelyoncodecoverageinstead .underpinning coverage as a proxy measure is the insight that a test suitecannot find bugs in code that it does not cover.
however recent empirical studies on the correlation between code coverage and bug finding identify different degrees of correlation .
thecode coverage of a test suite or fuzzer can be measured e.g.
asthenumberofprogrambranchesthatareexercisedbythetest suiteorfuzzer respectively.the bugfindingability ofatestsuite orfuzzer canbemeasured e.g.
asthenumberofbugsfoundor thetimeittooktofindthefirstbug.the correlation betweentwo random variables measures the strength of their association and the direction of their relationship.
using artificially injected bugs and developer generated test suites inozemtsevaandholmes findaweakcorrelation between coverage and test suite effectiveness when the size of the test suite iscontrolled for and amoderate tostrongcorrelation iftest suite sizeisignored .however chenetal.
raiseconcernsaboutthe experimental methodology i.e.
the stratification of test set size posingasignificantthreattothevalidityoftheresults.gopinath etal.
identifieda strongcorrelation betweencodecoverageand testsuiteeffectivenessfordeveloper providedtestsuitesandfound the impact of test suite size neglible.
for auto generated test suites the correlation was moderate to strong however the majority of auto generated test suites covered less than of code while the coverage values for developer generated test suites had a much wider spread and they might have been written specifically for detectingthesebugs.gligoricetal.
findaverystrongcorrelation between coverage and bug finding using different measures of correlation.
in contrast to this line of work we use real bugs instead of artificially injected bugs i.e.
mutants .
mutants may or may not be representative of real bugs .
instead of developerprovidedtestsuites ourstudyisconcernedwith testsuites that wereauto generatedbyvariousfuzzers.inourstudy testsuitesize is nota concern either as we explicitlycontrol for themethod by which the test suite i.e.
seed corpus is generated.usingrealbugsandauto generatedtestsuites generatedbyone fuzzer wei et al.
observe that the majority of bugs are found in the last two thirds of the campaign when branch coverage increases only slightly from to .
along this qualitative reasoning they conclude that there is weak correlation between number of faults found and coverage .
kochhar et al.
find a strong correlation between coverage and bug finding for one programanda moderatecorrelation foranother.however chenetal.
raiseconcernsaboutthecorrelationmeasurethatwasusedand note that the association is likely stronger than indicated.
more generally chen et al.
expose several flaws in experimental methodologies of previous work and highlight common pitfalls in the statistical evaluation.
their own experiments indicate a very strong correlation between coverage and bug finding.
inourstudy wecanconfirmaverystrongcorrelation.
however in contrast to all previous work we suggest the use of agreement insteadofcorrelationforempiricalinvestigationsoftestsuiteeffectiveness.the agreement betweentwomeasuresquantifiesthe degree to which both measures would agree on the relative performanceoffuzzers.wedefinetwotypesofagreement agreementonsuperiority whichconcernstwofuzzers andagreementon ranking which concerns more than two fuzzers.
we say that two measures agree on superiority if both measures consider the same fuzzer better performing than the other and the difference is statistically significant.
we say that two measures agree on ranking if both measuresordermorethantwofuzzersaccordingtotheiraverage performance the same way not considering statistical significance.
counterintuitively to the strong correlation result we find that the agreement both on superiority and ranking is moderate.
benchmarkingbugfindingtoolsisdifficult.forstaticanalysis tools dwyer person and elbaum show that even small variations in the tool s configuration can give rise to a very large variation in the tool s bug finding effectiveness.
for fuzzing gavrilov et al.
startfromtheobservationthat bug based metricsareimpracticalbecause thedefinitionof bug isvague and mappingbug revealinginputstobugsrequiresextensivedomainknowledge .infact wewillelaborateonthechallengesofbug basedevaluation insection7.insteadofcountingthenumberofbugs gavrilovet al.
propose to measure the number of changes in program behavior over time that a fuzzer can detect.
tothebestofourknowledge ourworkisthefirsttoevaluate whether coverage based fuzzer benchmarking is reliable does the ranking of two or more fuzzers in terms of coverage agree withtheir ranking in terms of bug finding?
the current guideline on sound fuzzer evaluation suggests that coverage based benchmarking alone may be insufficient referring to the contentious study by inozemtseva and holmes which suggests a weak correlation .
our study provides the first empirical evidence on the reliability of coverage based fuzzer benchmarking.
experimental setup .
research questions our objective is to evaluate the degree to which a coverage based andabug basedbenchmarkingagreeonfuzzerperformance.we aim to answer the following research questions.
1622on the reliability of coverage based fuzzer benchmarking icse may pittsburgh pa usa rq.
correlation.
howstrongistheassociationbetweenthecoverage achieved by a fuzzer and its ability to find bugs?
rq.
agreement.
howstrongistheagreementontheranksor thesuperiorityofthefuzzersincoverage basedversusabugbased benchmarking?
rq.
campaign length.
doestheagreementbetweencoveragebased and bug based benchmarking increase with the length of the fuzzing campaign?
our default is hours .
rq.
campain trials.
does agreement between coverage and bug basedbenchmarkingincreasewiththenumberofcampaignsper fuzzer program ?
ourdefaultis20campaigns per combination .
rq.
extrapolation withinonetypeofbenchmarking howstrong is the agreement on the ranks or superiority of the fuzzers running hour campaigns versus shorter campaigns?
rq.
mitigation of threats to validity.
a how strong is the agreement between two randomized rounds of coveragebased benchmarking?
b how strong is the agreement between different measures of bug finding or between different measuresofcoverage?
c howdoesagreementvaryasthe number of available programs increases?
.
experimental design we evaluate these research questions using a post hoc bug identificationinstead of a pre determined ground truth.
while it requires substantiallymoreeffort theposthocidentificationallowsusto avoid some of the pitfalls of ground truth based benchmarking as discussed in section .1in our design after conducting the fuzzing campaigns we employ a process of automatic and manual dedu plicationtoidentifytheuniquebugsthateachfuzzerdiscovered.
fuzzing campaigns may produce many bug reports some of which actually pertain to the same unique bug.
so we sorted them out.
our experiments generated bug reports too many for ustomanuallydeduplicate.weusedavariantoftheclusterfuzz deduplicationapproachtoautomaticallygroupbugreports.after that we manually deduplicated the automatically deduplicated bugsto get235unique bugs.twoprofessional softwareengineers labeled the bugs to find duplicates.
we note that our experimental design is indeed not very economically.
our fuzzersdid not find a singlebugin30 oftheselectedprogramsdespitesubstantial fuzzingeffort .however itallowsustomitigateanumber of threats to validity of a ground truth based evaluation sec.
.
.
fuzzers and programs benchmark details.
the benchmark programs we used are listed infigure .
many of the programs are popular and wellmaintained open source software libraries that are widely usedtosupport criticalservicesin theinternet.for instance libxml2 is a popular parser library for xml documents phpis the interpreter for websites written in the php programming language andwireshark is a popular network protocol analyzer.
the set of benchmark programs ranges from parser libraries protocol implementations and implementations of compression algorithms allthe way to os service managers interpreters and platforms for 1examples are survivorship bias observer expectancy bias and selection bias.name size harness name branches known bugs libhevc .3k loc hevc dec fuzzer .7k ndpi .0k loc fuzz ndpi reader .9k libhtp .3k loc fuzz htp .2k aspell .1k loc aspell fuzzer .4k grok .6k loc grk decompress fuzzer .0k matio .0k loc matio fuzzer .7k stb .2k loc stbi read fuzzer .7k njs .0k loc njs process script fuzzer .0k zstd .2k loc stream decompress .6k openh264 .1k loc decoder fuzzer .3k libgit2 .9k loc objects fuzzer .2k poppler .6k loc pdf fuzzer .7k libxml2 .1k loc xml reader file fuzzer .8k arrow .4k loc parquet arrow fuzz .9k php .6m loc fuzz execute .5k php .6m loc fuzz parser .8k wireshark .3m loc fuzzshark ip .3k proj4 .2k loc standard fuzzer .2k tpm2 .6k loc execute command fuzzer .8k file .7k loc magic fuzzer .6k muparser .2k loc set eval fuzzer .6k usrsctp .0k loc fuzzer connect .6k libarchive .5k loc libarchive fuzzer .7k systemd .4k loc fuzz varlink .1k total .2m loc .7m figure details about benchmark programs.
in our data analysis we excluded programs below the line because nomore than two of ten fuzzers found a least one bug in at least one campaign.
afl afl aflsmart aflfast fairfuzz eclipser mopt honggfuzz libfuzzer entropic figure the fuzzers available in fuzzbench that we used in our experiments.
in memoryanalytics.
outof these24 benchmark programs there are seven programs containing bugs that could not be found by anyfuzzer four programswherebugswereveryhardtofind andthree programswherenomoremorethantwofuzzerscould find bugs in at least one campaign.
benchmark selection.
our benchmark programs have been randomly selected from programs in oss fuzz that have historically contained a relative high number of bugs.
oss fuzz i sa service that provides fuzzing for open source projects.
integrations are usually performed by project maintainers and or security researcherswhowritefuzztargetsandcompileseedcorporaand dictionariesfortheprojects.thismeansthatourbenchmarkprograms have been prepared for fuzzing by the maintainers and not by us which reduces experimenter bias.
oss fuzz automatically reportseachbugitfindstogetherwiththefirstandlastprogram version in which the bug exists.
most program versions do not contain any bugs which is why a random selection of program versions from oss fuzz would be prohibitively expensive.
most fuzzers would not find any bugs.
hence we use the information fromoss fuzztorandomlychoseourbenchmarkprogramsfrom programversionswhichareknowntohaveanincreasednumberofbugs.similarlytooss fuzz allprogramsareinstrumentedwithaddresssanitizer as oracle to detect bugs.
all fuzzing campaigns are started from an initial seed corpus provided from oss fuzz.
1623icse may pittsburgh pa usa marcel b hme l szl szekeres and jonathan metzman fuzzers.
figure 3shows the list of fuzzers we used.
we chose these fuzzers basedon their importance and ease of use.
entropic libfuzzer honggfuzz afl and afl are widely used in industry while aflsmart aflfast fairfuzz eclipser and mopt afl are important academic works and extensions of afl.
.
variables and measures our objective is to evaluate the degree to which a coverage and a bug based benchmarking agree on fuzzer performance.
we have onemainandtwosupplementarymeasuresofcoverageplustwo main and one supplementary measure of bug finding.
measuresofcoverage.ourmainmeasureofcoverageis branch coverage i.e.
thenumberofbranchesintheprogramthatthefuzzer has exercised until this point in the campaign.
branch coverage captures the control flow in a program subsumes statement coverage is considered to be the most effective proxy measure of bugfinding andistheconventionalmeasureofcoverage to evaluate coverage guided greybox fuzzing .
fuzzbench measures region coverage in minute intervals on a dedicated measurer instance2using clangcompiler flags fprofile instrgenerate and fcoverage mapping and the llvm cov tool .
as supplementary measures of coverage we also analyze the numberofuniquepathsandthenumberofuniqueedgesasmeasured by the afl fuzzer.
the number of unique paths paths continuestobeacommonperformancemeasureforgreyboxfuzzers despiteitsobviousflaws .thenumberofunique edges edges reported as map size by afl based fuzzers is often used as a proxy for branch coverage.
afl maintains a fixed size hashmap containing an entry for every tuple of conditional jumps that are sequentially exercised in the program.
for all measures of coverage wedirectlyevaluatecoverageonthebuggyprogramto avoid the clean program assumption .
measures of bug finding.
our main measures of bug finding arebug coverage i.e.
the number of bugs that the fuzzer has found until a given point in the trial and the time to error i.e.
the length of the fuzzing campaign when the first bug was found.
in order tocountthenumberofbugs bugs ataparticularpointintime we execute all bug revealing inputs and remove all duplicates.
our method of deduplicating bugs is similar to clusterfuzz s. for each crash reported in a trial we take the crash type e.g.
heap bufferoverflow and the top three symbolized stack frames reported byaddresssanitizerorundefinedbehaviorsanitizer.crasheswith the same type and stack frames are considered duplicates and only one of them is counted.
to further improve the quality of the deduplication we manually removed the remaining duplicates.
in ordertomeasurethetime to error tte wereportthelengthof the fuzzing campaign when the first crashing input was generated.
as supplementary measure of bug finding we also count the number of unique crashes crashes i.e.
the number of unique paths thatareexercisedbycrashinginputs.thenumberofunique crashes similar to the number of unique paths is a standard but contentious measure of bug finding.
crashes are flagged as suchby standard code sanitizers such as asan .
for both measures of bug finding we directly evaluate bug finding on programs containing real bugs to mitigate threats to construct validity.
2each fuzzing campaign runs on separate compute instance called runner.spearman s interpretation .
.
neglible correlation .
.
weak correlation0.
.
moderate correlation0.
.
strong correlation0.
.
very strong correlation a taken from schober et al.
cohen s interpretation .
.
no agreement .
.
minimal agreement0.
.
weak agreement0.
.
moderate agreement0.
.
strong agreement0.
.
almost perfect agreement b taken from mchugh .
figure interpretation of spearman s and cohen s .
.
statistical analysis inordertoinvestigatetherelationshipbetweencoverage basedand bug finding based measures of fuzzer performance we compute correlation and agreement.
correlation assesses the strength of the association and the direction of the relationship between two random variables.
we assess the correlation between a measure of coverage and a measure of bug finding using spearman s rank correlation.w eu s e spearman s instead of the more common pearson s correlation aspearson sassumesalinearrelationshipwhileourscatterplots infigure 6indicate an exponential one.
since both variables are continuous represent pairedobservations andtheirrelationshipis monotonic the assumptions for spearman s correlation are met.
the interpretation of spearman s is shown in figure .a.
inter rateragreement assessesthedegreeofagreement between two raters of the same phenomenon.
in our case we measure the agreement between a coverage based measure of fuzzer performance and a bug finding based measure of fuzzer performanceontherankingorsuperiorityofafuzzer.sincecoverageand bugfindingmeasurethesameconstruct i.e.
fuzzerperformance the assumption forassessing agreement is met.
schoberet al.
note that two variables can exhibit a high degree of correlation but can at the same time disagree substantially .
bland and altman suggestthatanytwomeasuresofthesameconstructshould necessarily be strongly correlated but may not strongly agree.
agreement on rank.
in order to benchmark multiple fuzzers simultaneously it might seem reasonable to establish a ranking wherethebestfuzzeraccordingtosomemeasureisrankedhighest cf.fig.
.afuzzer srankingforaprogramandtimestampisbased on the corresponding average for that measure across all twenty trials.wemeasuretheagreementonthecoverage basedandbugfinding based ranks of a fuzzer using spearman s correlation .
agreement on superiority.
unlike a pair wise comparison a rankingdoesnotconsiderthestatisticalsignificanceofthedifferencebetweenanytwofuzzers.hence wealsomeasuretheagreementonthe superiority ofafuzzeroveranotherwhensuperiority is established according toa measure of coverage versus a measure ofbugfinding.usingcohen skappa anddisagreementproportiond we measure agreement on superiority for pairs of fuzzers onlywhere the difference in terms of bothmeasures is statistically significant p .
.
.
for at least of the programs .webelievethereisinsufficientevidenceforfuzzer pairs where differences are statistically significant for the less than of programs.
given a fuzzer pair the coverage and bug based evaluationeach rates whichfuzzerissuperior.wemeasurethe agreementontheseratingsacross atleastthree benchmarkprograms.
we also consider a third method using spearman s .
1624on the reliability of coverage based fuzzer benchmarking icse may pittsburgh pa usa disagreementproportion diseasytointerpret.givenapairof fuzzers where the differences in terms of coverage andbug finding are statistically significant for at least of the programs the disagreement proportion dgives the proportion of programs where both fuzzers are considered superior according to coverage or bug finding respectively d po .
cohen s kappa a standard more robust measure of interrater agreement which also takes into account the possibility of theagreementoccurringbychance .giventhesamepair of fuzzers cohen s kappa is computed as the difference between therelativeobservedagreementonthesuperiorityofafuzzer po andthehypotheticalprobabilityofchanceagreement pedivided by the complement of the probability of chance agreement po pe pe .
cohen s interpretation is shown in figure .b.
spearman srho allowsustouse alldatapointsusinganordinal rather than a binary variable for superiority 1for the superior fuzzer 1for the inferior fuzzer and 0where the difference is not statistically significant according to the given p value.
statisticalsignificance.theevaluatethestatisticalsignificance ofthedifferencebetweentwofuzzers wereportmann whitney utest following the recommendations byarcuri et al.
o nt h e evaluation of randomized algorithms and klees et al.
on the evaluationof fuzzers.
mann whitney uisa nonparametrictest of thenullhypothesisthat forrandomlyselectedvalues xandyfrom two populations the probability of xbeing greater than yis equal to the probability of ybeing greater than x. .
experiment infrastructure we used the fuzzbench fuzzer evaluation platform to conduct ourexperiments.thesystemconsistsofadispatcher the brain of an experiment and workers.
the dispatcher dispatches jobs a to build fuzzers and benchmarks b to start separate worker machines eachofwhichrunsonefuzzingcampaignforone fuzzer program combination c tomeasuretheresultsofthefuzzing campaigns and save results to a central sql database and d to generate reports based on the measurement results.
ameasurement consistsofmeasuringcodecoverageandcrashes.
manycrashinginputsmayrevealessentiallythesamebug.forthis reason we employ a simple deduplication strategy to assign crashing inputs to bugs they reveal cf.
section .
.
for more details on fuzzbench werefertheinterestedreadertothearticlebymetzman et al.
which introduces the fuzzbench infrastructure.
each fuzzing campaign is run inside an ubuntu .
docker container on an n1 standard virtual machine instance running on google cloud.
each instance has virtual cpu core .
gb of ram and gbof disk space available to use.by default we run 20campaignsof23hoursforeach fuzzer program combination.
.
reproducibility thefuzzbenchfuzzerevaluationplatformwasdesignedtofacilitateopenscience rigorousevaluation andreproducibility.
figure5 shows the identifiers of the fuzzbench experiments for this paper.
we also link the exact commit hash i.e.
version of fuzzbench which fixes the exactversions of all fuzzers all benchmark programs and the entire experimental platform that was used for our experiments.eachfuzzbenchreport availableatthelinkbelow describes precisely how our empirical analysis can be reproduced.experiment fuzzbench identifier commit description bug paper 38e344fe runs of hours all fuzzers all subjects crash s db192b60 runs of hours all fuzzers subjects crash s2 db192b60 runs of hours all fuzzers subjects figure reproducibility.
our experiments can be repro duced using the exact same settings and version of our experiment infrastructure.
data availability.
the data used for our evaluation can be downloaded at from the corresponding fuzzbench reports at where the experiment identifier is given in figure .
dataanalysis.wemakeourdataanalysisscriptavailableas jupyter notebook together with all generated tables and images at archival.forlong termarchival wealsopublishthedataand analysis script at the zenodo research artifact archival platform.
experimental results rq1.
correlation weinvestigatewhetherafuzzerthatcoversmorecodeisalsobetter at bug finding.
we ask whether the coverage that a fuzzer achieves is also a good predictor of the number of bugs found.
methodology.3toassessthecorrelationbetweencoverageand bug finding we prepare a scatter plot of the mean branch coverage over the mean number of bugs found across all fuzzing campaigns foreachprogramatanypointintime .forallthreemeasures ofcoverage wealsocomputespearman srankcorrelationbetween themeancoverageachievedandthemeannumberofbugsfound duringtheaveragefuzzingcampaignforeach programxfuzzer combination at any point in time .
results.
figure 6visually depicts the relationship between both coverage and bug finding.
the scatter plots often show an almost straight line suggesting a very strong correlation.
in fact the strength of the association between coverage and bug finding is confirmed in figure .
for all three measures we see an average spearman s rank correlation above .
which we interpret as a very strong correlation cf.figure .
there is a strong correlation between the coverage a fuzzer achieves and the number of bugs it finds in a program.
as a fuzzer covers more code it also finds more bugs.
thescatterplotin figure6alsoprovideshintsastothefunctional relationship.
the linear increase with the log scale y axis seems to suggests an exponential relationship a linear increase in branch coverage yields an exponential increase in the number of bugs found.whilecounterintuitiveatfirst itisnotactuallysurprisingif weconsiderthatmostofthecodehasalreadybeencoveredevenat thestartofthecampaign.eachfuzzingcampaignstartswithaseed corpusthatalreadycoversmuchoftheprogram andwe measure 3theexactprocedurecanbefoundinourdataanalysisscriptwhichwehavemade publically available.
1625icse may pittsburgh pa usa marcel b hme l szl szekeres and jonathan metzman zstdphp parser poppler stb wiresharkndpi njs openh264 php executelibhevc libhtp libxml2 matioarrow aspell grok libgit2 .5k .8k .0k .2k .5k43.6k .0k .4k .8k .0k .0k .0k .0k .1k .2k .3k .4k .5k .0k .0k .0k .0k .
.0k .0k .0k .6k .8k .0k .2k .2k .4k .6k .8k .0k .0k .0k .0k .0k .0k4.0k .0k .0k .4k .5k .5k .5k .0k .0k .0k .0k .2k .4k .
k4.0k .2k .4k .6k .8k .2k .3k .4k .5k .0k .2k .4k .6k .8k .4k .4k .4k .4k0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg.
branches coveredavg.
bugs discovered figure scatter plot of the mean number of bugs found onthelog scale asthemeannumberofcoveredbranches increases in the average fuzzing campaign for a benchmark.
the first data point after the first minute interval when most ofthe shallow brancheshavealreadybeencovered.thiscanbe verifiedbylookingatthestartofthex axisforeachbenchmark.we canseethatthemajorityofbrancheswhicharecoveredin23hours have already been covered in the first minutes.
covering a new branch gets harder over time.
even if coverage is fully saturated and not a single new branch can be covered a fuzzer might still findnewbugs.thisinterpretationagreeswiththeobservationby wei et al.
who found for random testing that the majority of bugs werediscoveredinthelasttwothirdsofthecampaign when branch coverage increased only slightly from to .
in our study there appears to be an exponential relationship between branch coverage and the number of bugs found.
rq2.
agreement coverage versus bug finding a strong correlation between two variables does not necessarily implythattheystronglyagree .weinvestigatethedegree towhichtheresultsofcoverage basedbenchmarkingagreewith the results of bug based benchmarking.
methodology ranking .forevery program fuzzer time stamp combination we have twenty data points trials.
for every measure and for every program time stamp combination we compute the fuzzer ranks by ordering all ten fuzzers accordingto the average measured value across all twenty trials.
for every measureofcoverageandeverymeasureofbugfinding respectively wecomputetheagreementbetweenthecoverage basedandbugbased ranking in terms of spearman s .
branches paths edges arrow0.
.
.
matio0.
.
.
ndpi0.
.
.
njs0.
.
.
openh264 .
.
.
poppler .
.
.
wireshark .
.
.
aspell0.
.
.
grok0.
.
.
libgit20.
.
.
libhevc .
.
.
libhtp0.
.
.
libxml2 .
.
.
php execute .
.
.
php parser .
.
.
stb0.
.
.
zstd0.
.
.
average .
.
.
figure7 averagecorrelation betweencoverageand bugs found for all programs where at least one bug was found.
methodology superiority .
from ten fuzzers we can construct unique pairs of fuzzers.
for each fuzzer pair each program and every measure we determine effect size and statistical significance betweenbothfuzzersintermsofmeanandmedianofthatmeasure across trials of 23h.
for each fuzzer pair if the difference interms of the coverage andin terms of bug finding is statistically significant at p .
.
.
for at least of programs we compute the agreement on superiority for this pair.
results.
figure8.ashowstheagreementonrankingandsuperiorityofafuzzerin23hourscampaigns.intermsof ranking w e observe a moderate agreement between coverage and bug finding.
infigure1themoderateagreementisillustratedbythelargespread.
in terms of superiority for cohen s we observe a weak to moderate agreement for the average pair of fuzzers where the superiority alongbothmeasuresisstatisticallysignificant.acrossallmeasures if we benchmark the average fuzzer pair using a coverage versus bug basedapproach resultsdisagreefor10 to20 ofprograms.
figure15 intheappendixshowsamuchloweragreementifweuse thedifferencein medianinsteadofthemeantoestablishsuperiority.
onlyifthedifferenceintermsofbranchcoverage andthedifferenceintermsofthenumberofbugsfoundisstatisticallysignificant atp .
i.e.
for of fuzzer pairs we observe a strongagreement onthesuperiorityofafuzzer .
.inthis case a coverage based and a bug based evaluation of those eleven fuzzerpairsdisagreesonlyforonebenchmark .
onaverage.
however statisticalsignificanceat p .0001onlyofthedifference incoverageis insufficient weagainonlyobserveaweakagreement seefigure andfigure .d .
the increase in agreement with statistical significance is notobserved when we measure bug finding using the time to error tte .
weobservea moderateagreement betweenacoverage basedand a bug based benchmarking of fuzzer performance.
for fuzzer pairs where thedifferences in terms of coverage andbug finding is statistically significant the results usually disagree for to20 ofprograms.onlyfor branchesversus bugs the agreementonsuperiorityincreasesasthestatisticalsignificance forbothdifferences increases.
4the interpretation of these values can be found in figure .
1626on the reliability of coverage based fuzzer benchmarking icse may pittsburgh pa usa bugs time to error branches edges paths branches edges paths ranking .
.
.
.
.
.
superiority p .
.
.
.
.
.
.
superiority p .
.
.
.
.
.
.
superiority p .
.
.
.
.
.
.
superiority p .
d .
.
.
.
.
.
superiority p .
d .
.
.
.
.
.
superiority p .
d .
.
.
.
.
.
superiority p .
.
.
.
.
.
.
superiority p .
.
.
.
.
.
.
superiority p .
.
.
.
.
.
.
a agreementontherank firstrow andsuperiorityofafuzzerin23hrcampaignsintermsofcohen skappa followingthreerows disagreement proportion middlethreerows andspearman scorrelation lastthreerows .eachcellshowsthemeasureofagreementandsomecells inparenthesis the proportion of fuzzer pairs where the differences are statistically significant at the corresponding p value p .
.
.
.
.
.
.
.
.
campaign lengths in hours agreement on rank spearman bugs tte b agreement on ranksbetween measures of bug finding and branches after a campaign of xhours.
figure agreement on ranking and superiority coverage versus bug finding we also observe that the agreement on superiority is smallest for path coverage versus the number of bugs found particularly forhighsignificancethresholds.pathcoveragehasbeenacommon performance measure for greybox fuzzers despite its obviousflaws .theminimalagreementsuggestsabandoning path coverage as performance measure.
rq3.
agreement over campaign length we investigate whether there is a suitable campaign length where a coverage based and a bug based evaluation maximally agree.
methodology.
to compute the agreement on ranking and superiority of a fuzzer over time we followed the same methodology specified in the discussion for rq2 for every of the time stamps.
results.
figures .b and9show the agreement on ranks and superiorityovertime respectively.intermsof ranking theagreementremainsmoderateovertheentireduration.inthefirstnine hours weobservean increasein agreementbetweenanevaluation based on branch coverage versus one based on the number of bugs.however the agreement on ranks decreases again remaining mod erate overall.
in terms of superiority w ed onot observe an increase inagreement oradecreaseindisagreement overtimeforallthree levelsof statistical significance.
the agreementbetween coverageand bug based benchmarking appears to decrease slightly.
the differences are statistically significant for of fuzzer pairs.
inourstudy wedo notobserveanincreaseinagreement nor a decrease in disagreement over time.
rq4.
agreement over campaign trials we investigate whether there is a suitable number of campaigns per fuzzer program combinationwheretheacoverage based and a bug based evaluation maximally agree.
methodology.
allresults reported aboveare derivedfrom our defaultsetup wherewerun20campaignsoftwentythreehoursfor each fuzzer program combination.inordertoinvestigate the agreement as the number of trials increases we run an additional 40campaignsforasubset5ofthebenchmarkprogramsforatotal 5benchmarkprogramswith60trialsforeachofthe10fuzzers arrow libarchive matio ndpi njs openh264 poppler proj4 tpm2 and wireshark.of60campaignsoftwentythreehoursforeach fuzzer program combination.fromthissetof60trials werandomlysample ntrials without replacement for each combination where n and computeagreementforthosetrialsusingthemethodologyspecified in rq2.
to account for the randomness in the sampling we repeat this experiment times.
results.figure10 showstheagreementonfuzzerrankingasthe numberof trialsincreases.for thefirst20trials in figure10.a we canclearlyseeanincreasingtrend.asthenumberoftrialsincreases theagreementincreasesaswell.however from figure10.b itseems that there is not much benefit in running more than trials as the agreement increases only ever so slightly.
the agreement between coverage based and bug based benchmarking increases as the number of campaigns increases.
however theredoesnotseemtobemuchbenefitinrunningmore than campaigns per fuzzer program combination.
rq5.
agreement with shorter trials weinvestigatethedegreetowhichtheresultsof coverage based or bug based benchmarking using shorter campaigns say hour agree with the results of benchmarking using hour campaigns.
methodology.wemeasure theagreement onthe rankingof a fuzzer when ranked at the end of the campaign versus earlier in the campaign following the methodology we specified for rq3.
results.a sw ec a ns e ei nfigure there is a substantial difference in ranking when we rank fuzzers in a hour campaign versus a hour campaign.
in fact there is only moderate agreementbetweentheresultsofabug basedbenchmarkingat1hour versusthoseofabug basedbenchmarkingat23hours.however as we expect the agreement increases with campaign length.
inthe bottom left of figure .b we can see that minutes before the end of the hour campaign the ranks very strongly agree.
thebenchmarkingresultsforrathershortfuzzingcampaigns may not strongly agree with results of sufficiently long campaigns.
however in our study the benchmarking results for hourcampaignsdoalready verystronglyagree withbenchmarking results of hour campaigns.
1627icse may pittsburgh pa usa marcel b hme l szl szekeres and jonathan metzman a agreement on superiority between measures of bug finding and branches after a campaign ofxhours.
difference significant at p .
.
.
.
.
.
.
.
campaign lengths in hours agreement on superiority cohen bugs tte b agreement on superiority between measures of bug finding and branches after a campaign ofxhours.
difference significant at p .
.
.
.
.
.
.
.
campaign lengths in hours agreement on superiority cohen bugs tte c agreement on superiority between measures of bug finding and branches after a campaign ofxhours.
difference significant at p .
.
.
.
.
.
.
.
campaign lengths in hours agreement on superiority cohen bugs tte d agreementonsuperioritywhenweonlyrequire the difference in coverage but not bug finding to be statistically significant at p .
.
.
.
.
.
.
.
campaign lengths in hours agreement on superiority cohen bugs tte .
.
.
.
.
campaign lengths in hours disagreement on superiority programs bugs tte .
.
.
.
.
campaign lengths in hours disagreement on superiority programs bugs tte .
.
.
.
.
campaign lengths in hours disagreement on superiority programs bugs tte .
.
.
.
.
campaign lengths in hours disagreement on superiority programs bugs tte figure9 agreementonsuperiorityovercampaignlength.weshowagreementwhenevaluatingfuzzerperformancebased onbranchcoverageversusthenumberofbugs solidline andbranchcoveragedversusthetime to error dashedline .the colorshowsthepercentageoffuzzerpairsforwhichthedifferencesarestatisticallysignificantatthecorresponding p value p .
.
.
.
.
.
.
.
.
.
number of campaigns trials agreement on rank spearman bugs tte a agreement over trials for all programs.
.
.
.
.
.
.
number of campaigns trials agreement on ranking spearman b agreement over trials for a subset.
figure agreement as the number of trials increases.
the solid line showstheaverageagreementontherankingofa fuzzer when ranked using branch coverage versus the number of bugs found.
the dashed line shows the average agreementontherankingofafuzzerwhenrankedusingbranch coverage versus the time it takes to find the first bug tte .
rq6.
mitigations of threats to validity we investigate several possible concerns and threats to validity.
a baseline agreement.
a valid concern is that the results of coverage and those of bug based benchmarking may not agree simply because of some randomness in the measurement or broken measures of agreement.
to investigate this concern we check0.
.
.
.
.
campaign length in hours agreement on rank spearman shape bugs edges a agreement on ranks between a campaign of lengthxhoursandoneoflength23hours.forinstance intermsofthenumberofbugsfound the rankofafuzzerafter1hourmoderatelyagrees with the rank of that fuzzer after hours.spearman s p .
spearman s p .91spearman s p .
spearman s p .
.
rank at .
.
hoursrank at hours b scatter plot of fuzzer ranks by branches betweenacampaignoflength .
.
hours facets andacampaignoflength23hours.
here xare the ranks at the given campaign length and yare the ranks at hours.
figure agreement withincoverage or bug based benchmarking as campaign length increases.
thebaselineagreementbetweentworandomroundsofcoveragebased benchmarking.
from the trials per fuzzer program combination generated for rq4 we randomly sample trials without replacement and compute agreement on ranks as specified in rq2.
to account for randomness we repeat this experiment times.
to discharge the concern we expect a high agreement.
aswecanseein figure12.a weobservea verystrongagreement on the rank of a fuzzer between two rounds of coverage based benchmarking for every campaign length.
1628on the reliability of coverage based fuzzer benchmarking icse may pittsburgh pa usa .
.
.
.
.
.
campaign lengths in hours agreement on rank spearman a agreement between two randomized rounds of coverage based benchmarking.
.
.
.
.
.
.
number of programsagreement on rank spearman bugs tte b agreement as the number of benchmark programs increases.
figure investigating threats to validity.
bugs time to error crashes ranking .
.
superiority p .
.
.
superiority p .
.
.
superiority p .
.
.
superiority p .
d .
.
superiority p .
d .
.
superiority p .
d .
.
superiority p .
.
.
superiority p .
.
.
superiority p .
.
.
branches edges paths .
.
.
.
.
.
.
.
d .
.
d .
.
d .
.
.
.
.
.
.
.
figure13 agreementamongmeasuresofbugfinding column bugs and measures of coverage column branches .
b agreementbetweenmeasures.asdiscussedin section3.
wehaveseveralmeasuresofbugfindingandseveral supplementary measures of code coverage.
for a sound empirical analysis we would expect that all measures of bug finding strongly agree andalso thatallmeasures ofcodecoveragestronglyagree along all our measures of agreement.
as we can see in figure there is astrong agreement on superiority and ranking of a fuzzer when comparing fuzzers in terms of time to error versus counting the number of bugs found.
between measures of coverage we identify a strong correlation in most cases as well.
c agreementover programs.despitethis being oneofthe largest empirical studies on the relationship between coverage and bug finding a valid concern might be that the number of benchmark programsis relativelysmall.
toinvestigate thisconcern we randomly chose nprograms without replacement out of the programs where our fuzzers find bugs and we compute agreement according tothe methodologyspecified inrq3 for n .t o accountforrandomness werepeatthisexperiment50times.
figure .b shows the scatter plot for the agreement on the randomly chosenprogramsasthenumber nofprogramsincreases greydots and triangles and the average agreement on fuzzer rank solidand dashed line .
as expected the average agreement is approximatelyconstantasthenumberofprogramsincreases.however the varianceissubstantial rangingbetweenneglibleandverystrong agreementwhenonly n 5benchmarksarechosen.however at n 16benchmarks theagreementrangesonlywithinthemoderate agreement band.repeating thisexperiment bychosing programs withreplacement gives similar results.
a theeffectofrandomnessontherankingwithincoveragebasedbenchmarkingisneglible.
b eventhoughmeasuresof coveragedonotagreewithmeasuresofbugfinding themeasuresagreewithincoverage basedandwithinbug basedbenchmarking respectively.
c the number of benchmark programs hasasubstantialimpactonourresult.however thevariancein agreementisreasonablysmallforourbenchmarksizetosupport our conclusion we would suggest.
we do notrecommend using less than benchmark programs for coverage based fuzzer evaluation.
threats to validity as for any empirical study there are various threats to the validity of our results and conclusions.
one concern is internal validity i.e.
the degree to which our study minimizes systematic error.
for our selection of fuzzers and benchmark programs there is a risk of experimenter bias selec tion bias survivorship bias and confirmation bias.
to minimize experimenter andconfirmationbias fuzzersandprogramswerepreparedbyindependentdevelopers.wepickedprogramsrandomly from the largest publicly available collection of fuzzer harnessesfor open source projects.
each harness was prepared by the correspondingmaintainer.eachfuzzerwasdevelopedandaddedto fuzzbencheitherbythefuzzerdeveloperorthefuzzbenchteam longbeforeourstudystarted.however apossiblecauseof survivorshipandselectionbias isthat tokeepexperimentcostreasonable the benchmark programs were selected from oss fuzz such that a largenumberofbugscanbefound.manyofthosebugswerefound by a subset of the evaluated fuzzers e.g.
afl afl libfuzzer honggfuzz .however ourstudyis notconcernedwithestablishing thestate of the art findingwhichfuzzeristhebest .instead we areinvestigatingthereliabilityofcoverage basedbenchmarking which mitigates most risk of selection and confirmaion bias.
another concern is external validity i.e.
the degree to which ourstudycanbegeneralizedtoandacrossotherprograms fuzzers bugs and measures.
to the best of our knowledge ours is the largest study across all these dimensions.
we chose a large variety of widely used open source c programs from different domains.
giventheresulsinrq6 weareconfidentthatourresultsgeneralizetomanymoreopen sourcecprograms.weconductourevaluation on a large number of actual bugs that these program contained organicallysometimeinthepast.wechosevarious verysuccessful greyboxfuzzers whichare usedatgoogle microsoft othercompaniesandmanyindependentsecurityresearchers .
however there is no guarantee that our results extend to bugs in programs written in other programming languages or fuzzers that arefundamentallydifferentfromgreyboxfuzzers.eventhoughour benchmark programs contain more known bugs than any other bug based benchmark to date the number of bugs however is still lowcomparedtoe.g.
themillionsofbranchesinourbenchmark programs figure .
the sensitivity analysis in rq6 on the impact ofthenumberofprogramschosenfortheevaluationprovidessomeconfidencethatourresultextendstoother similarbugs.therefore it will be useful to replicate this study with other set of subjectprogramswithreal bugsinthem preferablywithanevenlarger and more diverse set of bugs.
1629icse may pittsburgh pa usa marcel b hme l szl szekeres and jonathan metzman athirdconcernis constructvalidity i.e.
thedegreetowhichour study measures what it purports to be measuring.
in this paper we areinterestedin fuzzereffectiveness andoneofthemainquestionswewouldliketoansweriswhethercodecoverageisagood metricforassessingit.wedothisbycomparingcoveragemetricsto bug finding metrics i.e.
two of them number of bugs found and timetofirstbugfound .ourassumptionisthatthesebugbased metricsaretheonesthatreallycapturefuzzereffectiveness.among thesetwowebelievethatnumberofbugsisthemorerobustmetric as it is a more granular give that it considers multiple bug datapoints notjustasingleone.itisstillpossible however thatdueto our limited benchmark program set which contains a limited setofbugs thenumberofbugsthatafuzzerfindsin thissetisan imperfect metric as discussed for the threat of the number of bugs on external validity .
more specifically we measure number of uniquebugsfound where unique doesnot haveanoperational oruniversaldefinition.werelyontheoss fuzzcrashdeduplication algorithm for this which has been successfully field testedover many years.
our results for rq6 where we assess baseline agreementandtheagreementbetweenmeasuresprovidefurther confidenceinconstructvalidity.wedonotmakethecleanprogram assumption sincecoverage basedandbug basedbenchmarking are conducted on the same program version.
finally conclusionvalidity relatestothereliabilityofourmeasurementsandthevalidityofourstatisticaltests.wehaveaddressedtheseissuesbyusingwellestablishedstandardmethodstocompute correlation agreementandstatisticalsignificance.totriangulate weusemultiplemeasures section3.
.wealsocarriedoutvarious sanity checks regarding agreement in section under rq6.
discussion reaching a location versus exposing a bug the underpinning assumption of coverage based benchmarking is thatbugsthatliveincodethatisnotcoveredcanalsonotbeexposed.however we find that the results of coverage based benchmarking maynotreliablyindicatetheresultsofbug basedbenchmarking.
so how is reaching a certain location related to exposing a bug?
inourexperiments weusecodesanitizers todetectbugs.
during compilation a code sanitizer injects assertions into the program binary that failwhen e.g.
a memory safetyissue occurs.
so coveringthoselocationsshouldbeenough right?indeed aszhangand mesbah find that assertion coverage is strongly correlated with test suite effectiveness.
sterlund et al.
demonstrate that a fuzzer that focusses on the coverage of sanitizer instrumentation outperforms existing fuzzers.
now branch coverage subsumes sanitizercoverage .then whydowenotsee astrongagreement between results of coverage based and bug based benchmarking?
if fuzzers were guaranteed to detect the bug when they reached the corresponding code location then evaluating fuzzers based on code coverage would be equivalent to evaluating them based on bugsfound.however simplyreachingagivenbranchorstatement isofteninsufficienttotriggerabug.therootcauseofabugmay not be localized in a single statement but a certain sequence ofstatements may need to be executed throughout the code before the bug is exposed .
on the other hand triggering the bug may be as hard as covering that program branch which reports that thebug has been triggered.
like bugs that cannot be exposed upon coveringabranch thecoverageofthatbranchitselfmayalready require a certain program state.
onehypothesis isthatfaultscouldbeempiricallydistributed inanon uniformmanneracrossthecodebase .asfuturework itwillbeinterestingtoinvestigatethisandotherhypotheses.maybe wecanfindspecificpropertiesordifferencesbetweenthetypical programlocation orbranch andfaultlocationsorerrorconditions more generally.
it would be interesting whether achieving these error conditions versus achieving code coverage require different capabilities from a fuzzer.
yet westillbelievethatcodecoverageisanexcellentmeasurable objectivefunctionforafuzzer.coverageguidancehasbeenthekey to the recent success of greybox fuzzers .
maximizing coverage is the key measurable objective in search based software testing .bugsaresimplytooraretobecomeanexplicitobjective or to provide a reasonable signal during fuzzing.
inourresults weseethatthefuzzerthatisbetterinachieving coverage may still be worse in finding bugs.
the goal of this paper istoinvestigatehowoftenwecanobservethis asymmetry .ifthis happensrarely thatmeansthatfuzzerscanbesoundlyevaluated solelybasedoncodecoverage.ifthishappensoftenontheother hand then it is recommended to use both code coverage and bugs to evaluate fuzzers.
fuzzer benchmarking challenges and recommendations in2020alone almost50fuzzingpaperswerepublishedinthetop conferences for security and software engineering .
to ensure arealisticassessmentof progressinthefield weneed soundmeasures of fuzzer effectiveness.
only if our measures reflect a fuzzer s truebugfindingability canweproperlyevaluatenewtoolsagainst thestate of the art.indeed whileimprovementsmightseemreasonable onlyarigorousevaluationwilltellforsure.forinstance forallsecure thewinningteamatthedarpacybergrandchallenge burned one cpu year every night to assess the previous day s improvements .
nighswander adds that many times obvious changes made things worse and stupid things helped.
stats are vital .
towards this end large benchmarking platforms have beenbuilt e.g.
fuzzbench hasfacilitatedrapidand dramaticadvancesamongthemostsuccessfulfuzzers .however according to a recent survey of researchers and practitioners sound fuzzer benchmarking remains a key open challenge .
in this paper we provide the first empirical evidence that the resultsofacoverage basedevaluationarenotstronglyindicative ofthefuzzers relativebugfindingability.however asweshallsee next a rigorous bug based evaluation is not without perils either.
.
challenges of bug based benchmarking economic considerations.
the most effective fuzzer finds thelargest number of bugs.
to evaluate the effectiveness of a fuzzer in the perfect world we would select a random representative sample of programs where we do not know whether any bugs can be found .
however we would quickly find that bugs are sparsein the typical program and that the cost for experiments with a reasonable statistical power would be prohibitive.
1630on the reliability of coverage based fuzzer benchmarking icse may pittsburgh pa usa syntheticbugs.tomakebug basedbenchmarkingmoreeconomical researchers have proposed to articially inflate the number of bugs in these programs using synthetic bugs .
however itisnofinalconsensusonwhetherthesyntheticbugsare realisitic .
infact as futurework wesuggest to conduct a similar analysis of agreement as proposed in this work between benchmarking based on artificial bugs versus real bugs.
ground truth.
alternatively researchers have been curating realbugs thatwerehistoricallyfoundinprograms .
while this approach is both economical and provides a more representative objective ground truth it is subject to several threats to validity that might not be obvious to the uninformed experimenter.
a evaluatingfuzzersbasedonpreviouslydiscovered bugsintroducesa survivorshipbias fuzzersthatarebetteratfinding previously undiscovered bugs may appear worse than they are.
on theotherhand fuzzersthatcontributedtotheoriginaldiscovery of some of the ground truth bugs may appear better than they are.
b toincreasethenumberofbugsinaprogram andtoreducethe benchmarkingcost curatorsmay front port severaloldbugsinto oneversion.thisintroducesartificial bugmaskingandinteraction effects posing a threat to construct valdity.
c to simplify bug countingandtoprovidethesamebugoracletoallfuzzers curators maymanuallytranslateeachbugintoalocalizedif statement.this introducesan observer expectancybias.forinstance inthiswork the relationship between coverage and bug finding is precisely the subject of our study section ?
overfitting.givenagroundtruthbenchmark researchersmight be enticed to iteratively and unknowingly tune their fuzzer implementationtothebugsinthebenchmark.zelleretal.
identify aparticularlyseverecaseofthisconfirmationbiaswhichinvalidates some empirical evidence in a well cited paper.
they recommendto augment bug based evaluation with a coverage based evaluation during testing executing a location is a necessary condition forfindingabuginthatverylocation.sincewearestillfarfrom reachingsatisfyingresultsincoveringfunctionality improvements in code coverage are important achievements regardless of bugs being found .
.
recommendations forfutureevaluationsoffuzzerperformance basedontheseresults and our experience we make the following recommendations.
in the order of their appearance in the benchmarking process r1ifpossible selectatleast10respresentativeprograms.foreach fuzzer programcombination conductatleast10 better20 campaigns of at least better hours.
increasing these valuesimproves generalityandstatisticalpoweroftheresults.
r2select real worldprograms thatrepresentprogramsthatare typically fuzzed in practice.
select real world bugs that represent the set of bugs which are typically found in programs used in practice.6improving the representativeness of the benchmark increases the external validity of the results.
if experiment cost are a concern authors can prioritize programs that are likely to contain a large number of bugs.
6asfuturework wesuggesttoevaluatetherepresentativenessofsyntheticbugsusing a similar experimental setup as presented in section .r3select asbaselinethe fuzzer that was extended to implement the technical contributions and make sure that the configurations parameters initial seeds dictionaries etc.
are equivalent.forinstance todemonstratetheadvantagesofstructureaware fuzzing we would implement structure aware fuzzingintoastructure un awarefuzzerandcomparetheextendedagainstthebaselinefuzzer.thisimprovesconstruct validity and allows to attribute precisely the observed performanceimprovementstotheproposedtechnicalcontributions.
acomparisontootherfuzzersmaybeconductedoptionallyif the authors wish to establish the new fuzzer as the new stateof the art.
however note that the observed improvements maybelargelyduetodesignandengineeringdifferences e.g.
honggfuzz versus afl .
r4consider using a training set as benchmarks during the fuzzerdevelopmentanda validationset possiblyusingan independent benchmarking platform for the actual empiricalevaluation.thisallowsauthorstoreduceoverfittingand confirmation bias.
r5measure and report both coverage and bug based metrics toprovide a holistic assessment of fuzzer performance.
use classicalmeasuresofcoveragetofacilitate future comparisons across various fuzzers.
do not use fuzzer specific measures such as afl s number of paths .
use the same measurement tooling and procedure across all fuzzers and programs to increaseinternalvalidity.considerusingaposthocbugiden tification section3.
ratherthangroundtruthbugstoreduce threats to internal validity such as survivorship bias.
r6assess and report various non parametric measures of effect sizeandstatisticalsignificance suchasvargha delaney s a12 and mann whitney utest respectively .
this allows to quantifythemagnitudeofthedifferencesandthedegreeto which the differences can be explained due to randomness.
r7discusspotentialthreatstovalidityandyourstrategiestomiti gatetheidentifiedthreats.forinstance discussyourstrategies to mitigate selection survivorship observer expectancy and confirmationbias.ifindicated conductanempiricalevaluation of potential threats to validity.
r8reportallspecificparametersoftheexperimentalsetup including how the programs bugs and initial seed corpus were chosen publishthetools fuzzerandbaseline andthe benchmark programs and bugs to faciliate the reproducibilityoftheresults.publishdata analysis andfigurestofacilitate open access.
upload all artifacts to an open access repository likezenodoforlong termarchival .reproducibilityisthe foundation of sound scientific progress.