a cost efficient approach to building in continuous integration xianhao jin department of computer science virginia tech blacksburg usa xianhao8 vt.edufrancisco servant department of computer science virginia tech blacksburg usa fservant vt.edu abstract continuous integration ci is a widely used practice in modern software engineering.
unfortunately it is also an expensive practice google and mozilla estimate their ci systems in millions of dollars.
in this paper we propose a novel approach for reducing the cost of ci.
the cost of ci lies in the computing power to run builds and its value mostly lies on letting developers find bugs early when their size is still small.
thus we target reducing the number of builds that ci executes by still executing as many failing builds as early as possible.
to achieve this goal we propose smartbuildskip a technique which predicts the first builds in a sequence of build failures and the remaining build failures separately.
smartbuildskip is customizable allowing developers to select different preferred trade offs of saving many builds vs. observing build failures early.
we evaluate the motivating hypothesis of smartbuildskip its prediction power and its cost savings in a realistic scenario.
in its most conservative configuration smartbuildskip saved a median of builds by only incurring a median delay of build in a median of failing builds.
ccs concepts software and its engineering empirical software validation software testing and debugging maintaining software software maintenance tools.
keywords continuous integration build prediction maintenance cost acm reference format xianhao jin and francisco servant.
.
a cost efficient approach to building in continuous integration.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction continuous integration ci is a popular practice in modern software engineering that encourages developers to build and test their software in frequent intervals .
for simplicity and consistency permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
previous studies we refer as build to the full process of building the software and running all the tests when ci is triggered.
while ci is widely recognized as a valuable practice it also incurs a very high cost mostly for the computational resources required to frequently run builds .
overall adopting ci can be very expensive.
google estimates the cost of running its ci system in millions of dollars and mozilla estimates theirs as per month .
for smaller budget companies that have not yet adopted ci this high cost can pose a strong barrier.
in this paper we aim to reduce the high cost of ci while keeping as much of its value as possible.
the cost of ci is commonly defined by the cost of builds and its value is defined by its ability to reveal problems early .
thus we aim to reduce the cost of ci by running fewer builds while running as many failing builds as early as possible .
our goal also responds to the need to run fewer builds that developers frequently express in q a websites which they currently may approach by using ci plug ins to manually skip builds that they deem safe e.g.
changes in readme files.
existing research approaches to save cost in ci include the automatic detection of such non code changes and techniques to make ci builds faster .
in contrast our proposed approach focuses on skipping builds that are predicted to pass in more complicated cases for any kinds of changes that happened between builds.
our approach complements existing techniques and could potentially be applied in combination with them.
we propose smartbuildskip a novel approach to reduce the cost of ci based on automatic build outcome prediction by skipping builds that it predicts will pass and running builds that it predicts will fail.
our strategy is motivated by two hypotheses h1 most builds in ci return a passing result.
we expect that software changes will generally be done carefully making passing builds more common than failing builds.
by this hypothesis skipping passing builds would produce large cost savings.
h2 many failing builds in ci happen consecutively after another build failure.
one of the strongest predicting factors in existing build outcome predictors is the result of the previous build .
also rausch et al.
observed build failures mostly occurring consecutively in a small number of java projects .
by this hypothesis most failing builds could be easily predicted since most follow another build failure.
thus smartbuildskip differentiates between first failures andsubsequent failures following a two phase process.
first smartbuildskip uses a machine learning classifier to predict build outcomes to catch first failures.
after it observes a first failure it then determines that all subsequent builds will fail until it observes a build pass and then changes its operation to predicting again.
this strategy aims to address the limitations of existing build prediction approaches which strongly rely on the outcome of the ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea xianhao jin and francisco servant last build and predict outcome for all builds likely incorrectly predicting some first andsubsequent failures.
lastly we propose smartbuildskip as acustomizable technique in order to help software developers with different costsaving trade off needs e.g.
preferring modest effort savings and low delays in observing build failures or preferring high effort savings with a longer delay to observe build failures.
we performed two empirical studies and two experiments.
first we empirically studied the hypotheses that motivate smartbuildskip.
second we empirically studied the features that predict first failures to inform smartbuildskip s predictor.
third we performed an experiment to evaluate smartbuildskip s ability to predict first and all failures in a dataset of software projects and another one of projects.
fourth we performed another experiment to measure the cost savings that smartbuildskip would produce in our studied datasets.
in our experiments we compared smartbuildskip s performance with the state of the art build prediction technique hw17 .
hw17 makes machine learning predictions for all builds using both historical and contemporary build information.
to the extent of our knowledge hw17 is the build prediction technique that currently provides the highest precision and recall.
smartbuildskip provides two major strengths over hw17 smartbuildskip runs predictions only for first failures and determines that all subsequent builds fail until a pass is observed.
smartbuildskip predicts based only on features describing the current build and the project using no features about the previous build .
we found that this strategy was more effective at predicting both first andsubsequent failures see .
additionally we found that by not relying on the outcome of the previous build smartbuildskip was much more effective in practice.
since the previous build was often skipped and its outcome unknown hw17 was negatively impacted but not smartbuildskip observed in .
the results of our studies support our hypotheses build passes are numerous median of all builds and subsequent failures are also a high proportion of all build failures median .
in our experiments smartbuildskip significantly improved the accuracy of the state of the art build predictor up to median f measure forfirst failures and up to median f measure for all failures.
finally smartbuildskip s predictions resulted in high savings of build effort that could be customized for developers with different preferred trade offs i.e.
faster observation of build failures vs. higher savings in build effort.
in its most conservative configuration smartbuildskip saved a median of all builds by only incurring a median delay of build in a median build failures.
in a more cost saving focused configuration smartbuildskip saved a median of all builds by incurring a build delay for of build failures.
this paper provides the following contributions the conceptual separation of build failures into first and subsequent failures to improve the effectiveness of build prediction models.
two studies of the prevalence of build passes over build failures and of subsequent failures over first failures.
a study of factors that predict first failures.
smartbuildskip a customizable automatic technique to save cost in ci by predicting build outcomes that can beapplicable with or without training data and that improves the prediction effectiveness of the state of the art.
a collection of simple predictors based on factors that predictfirst failures that can be applied as a rule of thumb with no adoption cost.
an evaluation of the extent to which smartbuildskip can save cost in ci while keeping most of its value with the ability of customizing its cost value trade off.
motivating hypotheses we motivate our hypotheses and our proposed approach with an example.
figure depicts an example timeline of builds the ideal timeline in which we would save most effort the timeline produced after applying a state of the art build prediction technique and the timeline produced after applying our approach smartbuildskip.
the example timeline shows a numbered sequence of builds in ci.
we depict passing builds as circles with a p and failing builds as circles with an f. the ideal timeline shows the outcome that an ideal technique would achieve skipping every passing build and building all failing builds.
we depict skipped builds with a dashed empty circle.
this ideal timeline depicts our goal of saving cost in ci by running as few builds as possible while running as many failing builds as possible.
we propose smartbuildskip following two main hypotheses h1 most builds in ci return a passing result.
if this was true our strategy of predicting build outcomes and skipping those expected to pass would provide substantial cost savings since passing builds would be a majority and they would be skipped.
h2 many failing builds in ci happen consecutively after another build failure.
if true if we built an automatic approach that predicted that subsequent builds to a failing build will also fail we would correctly predict a substantial portion of failing builds.
first failures vs.subsequent failures .assuming that our hypothesis h2would be supported we also propose the distinction between first failures the first build failure inside a sequence of build failures and subsequent failures all the remaining consecutive build failures in the sequence.
figure highlights first failures with gray fill.
limitations of existing work.
figure also illustrates the limitations of applying existing build predictors e.g.
to the problem of saving cost in ci by skipping passing builds.
the timeline for existing build predictors uses a diamond to depict the prediction of the outcome of an upcoming build.
if the upcoming build is predicted to pass the technique skips it and transitions to predict for the next build.
we depict this with an arrow leaving the diamond and going into the next diamond e.g.
in build .
if the upcoming build is predicted to fail it is executed.
we depict this with an arrow leaving the diamond and going into the next build.
we posit that existing predictors by not distinguishing first and subsequent failures likely provide limited accuracy for both.
limited prediction of first failures.
we posit that existing predictors will rarely correctly predict first failures because they strongly rely on the status of the previous build for prediction.
first failures are preceded by a build pass by definition.
however we expect that it s more often build passes that are preceded by a build pass.
thus 14a cost efficient approach to building in continuous integration icse may seoul republic of korea p p f p p f p p p p p1 example timeline f p f p pp p f p our approach smartbuildskip existing build predictorsf f ideal timelinef12 f f f p13 f f f f f f f f p figure motivating example timeline.
first failures are highlighted in gray.
in an ideal timeline we would skip all passing builds and run all failing builds.
existing approaches predict outcome for every build.
our approach predicts build outcome if the last build passed.
after observing a failure it continues building until a pass is observed and it goes back to predicting.
after observing a build pass we expect that existing predictors will more likely predict another build pass to follow likely not catching many first failures.
smartbuildskip in turn does not suffer from this limitation since it does not rely on the outcome of the last build for its prediction.
limited prediction of subsequent failures.
since existing techniques predict outcome for all builds even after observing a first failure they may incorrectly predict some subsequent failures to pass.
smartbuildskip in turn will correctly anticipate subsequent failures since it does not make predictions for them.
instead it determines that subsequent builds to a failure will also fail.
our approach smartbuildskip we designed smartbuildskip by following the two hypotheses that we described in .
we also include the timeline produced by smartbuildskip for our motivating example in figure .
smartbuildskip s overall strategy.
smartbuildskip follows a two phase strategy.
in its first phase smartbuildskip predicts the outcome of the next build based on a set of predicting features see .
if the build is predicted to pass it is not executed its cost is saved and smartbuildskip will predict again for the next build.
an example is build in figure .
if the build is predicted to fail smartbuildskip executes it and checks its outcome.
if the actual outcome of the executed build is pass smartbuildskip will predict again for the next build as in build in figure .
if the actual outcome of the executed build is fail smartbuildskip will shift to its second phase as in build in figure .
in its second phase smartbuildskip determines that all subsequent builds will fail and thus executes them until the build passes after which it returns to the first phase as in builds in figure .
the benefit of this two phase strategy is that we expect smartbuildskip to be more successful at identifying both first failures andsubsequent failures by treating them separately.
we expect it to predict first failures better than existing techniques since we train smartbuildskip s predictor using features that specifically predict first failures.
we also expect it to accurately predict most subsequent failures by determining that all builds after a failing one will also fail.
the downside of this approach is that by continuously building after observing a first failure one false positive is guaranteed forevery sequence of failures as in builds and in figure .
however we believe that this downside is smaller than the benefit that smartbuildskip gets from its overall strategy.
besides existing predictors will also likely incur in these false positives because they strongly rely on the last build status which in these cases is a bad predictor.
finally we argue that these first pass builds are valuable for practitioners because they inform them of when they have fixed the problem that caused the build to fail.
smartbuildskip s variants.
we propose two variants of smartbuildskip.
both variants use a random forest classifier to predict builds.
since our focus is to correctly predict failing builds and since we expect ci build output to often be imbalanced smartbuildskip trains with a class weight of in favor of failing builds.
smartbuildskip within this variant is trained in the past builds within the same software project in which it is applied.
it uses the build features that we report in .
smartbuildskip cross this variant is trained in the past builds of different software projects than the one in which it will be applied.
it uses the build features as well as the project features that we report in .
we propose this variant to help with the cold start problem in software projects for which only a few builds have been executed and they would not be enough to provide high quality predictions.
research questions we perform two empirical studies to test our hypotheses and inform the design of smartbuildskip.
then we perform two experiments to evaluate it.
in our studies and experiments we answer the following research questions empirical study evaluating our motivating hypotheses rq1 are passing builds more numerous than failing builds?
rq2 aresubsequent failures numerous?
empirical study characterizing first failures rq3 what features predict first failures ?
experiment smartbuildskip for build prediction rq4 how effective is smartbuildskip predicting first failures ?
rq5 how effective is smartbuildskip predicting all failures ?
experiment smartbuildskip for build effort reduction rq6 how many resources i.e.
builds will smartbuildskip save?
rq7 what is the value trade off for such resource savings?
15icse may seoul republic of korea xianhao jin and francisco servant study subjects.
we perform our study over the travistorrent dataset which includes projects java projects and ruby projects with data for build instances.
we remove toy projects from the data set by studying those that are more than one year old and that have at least builds and at least lines of source code which is a criteria applied in multiple other works .
after this filtering we obtained builds from projects failing builds .
we focused our study on builds with passing or failing result rather than error or canceled since they can be exceptions or may happen during the initialization and get aborted immediately before the real build starts.
besides in travis a single push or pull request can trigger a build with multiple jobs and each job corresponds to a configuration of the building step .
we did a preliminary investigation of these builds and found that these jobs with the same build id normally share the same build result and build duration.
thus as many existing papers have done we considered these jobs as a single build.
we applied lod to remove outliers that have higher or lower than three standard deviations above or below the mean number of the failing ratio.
empirical study evaluating our motivating hypotheses rq1 are passing builds more numerous than failing builds?
we first evaluate our motivating hypotheses to understand if our approach to save build effort in ci is promising.
our first hypothesis posits that passing builds will be numerous and thus skipping them would provide high build effort savings in ci.
research method.
we measured the ratio of passing builds to all builds in each studied project and we show the distribution of such ratios in figure .
result.
for most projects the passing builds represented a very large proportion with a median and a mean of all builds passing.
this result supports our hypothesis that skipping passing builds would strongly save build effort in ci since they generally represented a large portion of the executed builds.
furthermore this result also shows the upper bound for how many builds could be saved given a perfect technique that would correctly predict every single passing build.
rq2 are subsequent failures numerous?
our next hypothesis posits that subsequent failures will be numerous and thus predicting that subsequent builds to a failing build will also fail would correctly predict a substantial portion of failing builds.
research method.
we measured the proportion of subsequent failures toall failures for each project e.g.
in a build history p f f f p the ratio of subsequent failures toall failures is .
we show the distribution of these proportions for all projects in figure .
result.
figure supports the hypothesis that subsequent failures are numerous i.e.
there are many of them.
a high number of projects had a high i.e.
not low ratio of subsequent failures for of projects and for of projects.
thus our approach would correctly predict a high proportion of all build failures since we expect it to correctly predict all subsequent failures.
once it observes a failure it would correctly predict all the subsequent ones.
passing buildsproportion of all builds figure ratio of passing builds to all builds.
passing builds represent a vast portion of all builds.
subsequent failuresproportion of all failures figure ratio of subsequent failures toall build failures.
more than half of all build failures aresubsequent failures.
empirical study characterizing first failures rq3 what features predict first failures ?we found that subsequent failures are numerous and easy to predict.
next we will focus on predicting first failures.
to inform our prediction technique we perform a second empirical study to identify features that characterize them.
research method.
we study two different kinds of features to characterize first failures build features and project features.
as build features we selected all the features included in travistorrent that previous studies found to be correlated with all build failures e.g.
.
our goal was to study whether such features are also correlated with first failures.
then to be able to address the cold start problem we also created four project features that could be used for cross project predictions.
our intuition is that project features would aid the classifier in adapting its trained model across projects of different characteristics since projects using continuous integration are diverse .
to the extent of our knowledge no previous work studied the correlation between all build failures orfirst failures and these project features as defined by us with a single value per project .
we list in table the features that we studied along with a brief description.
build features.
build features will be useful to train our approach with past builds from the same software project.
to identify build features that have a relationship with first failures we first removed subsequent failures from our studied dataset .
then we measured the correlation between the ratio of first failuresto all builds which now only included first failures and passing builds and each studied build feature in each studied project.
for each value of a build feature in a project we measured the ratio 16a cost efficient approach to building in continuous integration icse may seoul republic of korea table features studied for correlation with first failures.
build features feature short description src churn sc the number of changed source lines since the last build.
file churn fc the number of changed source files since the last build.
test churn tc the number of changed test lines since the last build.
num commit nc the number of commits since the last build.
project performance short ps the proportion of passing builds in the recent five builds.
project performance long pl the proportion of passing builds in the whole previous builds.
time frequency tf the time gap hour since the last build.
failure distance fd the number of builds since the last failing build.
week day wd the weekday being monday of the build.
day time dt the time of day of the build.
project features feature short description team size ts the median number of developers over the project s ci usage history.
project size ps the median number of executable production source lines of code in the repository over the project s ci usage history.
project age pa the time duration between the first build and the last build for that project.
test density td the median number of lines in test cases per executable production source lines over the project s ci usage history.
offirst failures to all builds that have that value for that feature in the project.
for continuous features such as src churn we use the pearson correlation coefficient as effect size and its corresponding p value for the significance test.
for categorical variables such as week day we measure effect size using cram r s v and we use pearson sx2for the statistical significance test.
project features.
project features will be useful to train our approach with past builds from other software projects.
when no or few past builds are available for a software project we could use past builds from different software projects to train our predictor.
this situation is known in machine learning as the cold start problem .
in such cases our predictor will use project features to learn how representative past builds from other projects are for the project for which not enough past builds existed.
as we did to study build features we also removed subsequent failures to study project features.
then we measured the correlation across projects between the value of each project feature and the project s ratio of first failures to all builds.
since all features were continuous we applied pearson s correlation coefficient and decided statistical significance for p .
.
results.
we report the results of our correlation analysis for build and project features.
build featuressc .
fc .
tc .
nc .
ps .
pl .
tf .
fd .
correlation coefficient .
.
.
.
.
.
.
.81figure correlation between build features and ratio of first failures.
four build features sc fc tc nc had a statistically significant correlation for more than of projects.
.
.
.
.
.
.
.
.
.
ps .
td .
pa .
ts .316correlation coefficient project features figure correlation between project features and ratio of first failures.
the correlation was statistically significant for three project features ps td pa. build features.
we show in figure the correlation between different build features and the ratio of first failures.
each box in the box plot represents the distribution of correlation coefficients between a feature see table and the ratio of first failures for all the projects for which that feature s correlation was statistically significant p .
.
we report the percentage of projects for which a feature s correlation was statistically significant in its label in the x axis.
we observe that different build features were differently related tofirst failures for example ps project performance short had a median correlation of .
which means that the build was more likely to pass when there are more passing builds in its last five builds and it has a strong correlation.
however this correlation was only statistically significant in .
of projects.
for the design of our technique we will train on the features that had a strong correlation with the ratio of first failures and their results were statistically significant in at least of projects.
four features had these characteristics the numbers of changed lines sc changed files fc changed test lines tc and commits since the last build nc .
a clear implication of these build features being related to first failures is that as changes accumulate in code measured as any of these four build features without a failing build being observed the likelihood of the next build to fail becomes increasingly high.
17icse may seoul republic of korea xianhao jin and francisco servant for the two categorical features wd and dt the results are statistically significant in only .
and .
of all projects and their corresponding mean values of cram r s v are .
and .
.
another interesting observation is that most of the build features that did not show strong statistical correlation with first failures are those that intuitively would be strongly correlated with subsequent failures instead.
that is subsequent failures happen after a particularly short number zero of failing builds fd after a particularly low proportion of passing to failing builds ps pl and probably a particularly short time after another build tf .
intuitively first failures would not particularly have any of these characteristics.
project features.
we use a bar chart to show each project feature and its corresponding correlation coefficient.
the value following the name of each project feature represents its corresponding pvalue.
we found three project features for which first failures were more prevalent i.e.
for which the project feature increased and its difference is statistically significant figure test density td project size ps and project age in ci pa .
these are the features that we will use to design our technique to train across projects.
in simpler words we observed that our studied projects had a larger ratio of first failures when they had larger test cases more lines of code or had been using ci for longer.
this could mean that as software projects mature more bugs affect their builds and or they get better at catching them.
we posit that our observation is likely a combination of both phenomena intuitively larger projects have more points of failure and larger test suites are better at catching problems.
still to understand the underlying causes of our observation in depth further research would be necessary.
experiment evaluating build failure prediction rq4 rq5 how effective is smartbuildskip predicting first failures and all failures ?in our second empirical study we discovered features that predict first failures .
next we use them in smartbuildskip to evaluate it.
we evaluate smartbuildskip in two experiments that complement each other.
first we evaluate its effectiveness for predicting build failures and then we evaluate the cost reduction that its predictions provide in practice .
our first experiment allows us to compare the effectiveness of smartbuildskip with that of existing build prediction techniques e.g.
in the scenario in which they were originally proposed and evaluated a scenario in which the information about previous builds is always known ignoring that it would not be available if a previous build was skipped.
automatic build prediction in such scenario can be useful to give developers more confidence about their code changes e.g.
even if they did not skip builds.
our second experiment allows us to evaluate how much costsmartbuildskip would save in ci in our target scenario a practical scenario in which the outcome of builds that were skipped is unknown.
research method.
we evaluate the prediction effectiveness of smartbuildskip in comparison to the state of the art build predictiontechnique hw17 .
to better understand the benefit of smartbuildskip s two stage design see we separately evaluate predictions for first failures and all failures.
we evaluate both techniques over our dataset described in and we measure their prediction effectiveness using precision recall and f1 score.
we tested our results for statistical significance with a two tailed wilcoxon test and decided statistical significance for p .
.
state of the art build prediction technique hw17.
to provide a point of reference for this evaluation we replicated the state of theart build prediction technique hw17 .
we use the acronym hw17 to refer to it the first letter of the authors last names and its publication date since the authors did not assign it a specific name.
to the extent of our knowledge hw17 is the existing build prediction technique that provided the highest precision and recall.
hw17 predicts build outcomes with a random forest machinelearning algorithm informed by a collection of features about the current build features about the previous build and features generated from analyzing build logs.
in contrast smartbuildskip requires only current build features no previous build features and project features .
only a few features are considered by both hw17 andsmartbuildskip sc fc and tc see their descriptions in .
hassan and wang found these features to be correlated with all failures and we found them to be correlated with first failures see .
our proposed approach smartbuildskip provides two main strengths over hw17 for saving cost in continuous integration smartbuildskip runs predictions only for first failures and determines that all subsequent builds fail until a pass is observed.
hw17 does not make such distinction and runs predictions for all builds.
we posit that smartbuildskip s strategy will be more effective at predicting both first andsubsequent failures which we evaluate in this experiment .
smartbuildskip predicts based only on features describing the current build and the project but it does not rely on features about the previous build.
hw17 like the other existing build prediction approaches does rely on the outcome of the previous build among other features.
we posit that such choice would make hw17 much less effective in realworld usage whenever a previous build is skipped its outcome will be unknown which would negatively impact hw17 but not smartbuildskip which we study in .
predicting first failures vs. all failures.
we evaluate the prediction of first failures andall failures over two different datasets.
for predictingfirst failures we removed subsequent failures from our dataset and evaluated our studied techniques over it.
for predicting all failures we evaluated our studied techniques over the dataset originally used to evaluate hw17 which contains both first and subsequent failures i.e.
all failures.
the paper s authors generously shared this dataset with us and we applied to it the same curation that we described in .
like our dataset hw17 s is also obtained from travistorrent hw17 s dataset is in fact a subset of ours.
hw17 s dataset includes only java projects that use the ant maven or gradle build systems.
in total their dataset contains projects.
this decision strengthens our experiment in two ways.
performing our evaluation for all failures over hw17 s dataset allows us to make a fair comparison between hw17 andsmartbuildskip.
hw17 relies on some pre computed features about the preceding 18a cost efficient approach to building in continuous integration icse may seoul republic of korea predictorssbs within hw17 within sbs cross hw17 crossprecision predictorssbs within hw17 within sbs cross hw17 crossrecall predictorssbs within hw17 within sbs cross hw17 crossf1score figure performance comparison on predicting first failures predictorssbs within hw17 within sbs cross hw17 crossprecision predictorssbs within hw17 within sbs cross hw17 crossrecall predictorssbs within hw17 within sbs cross hw17 crossf1score figure performance comparison on predicting all failures build e.g.
cluster id that are only available in their dataset not in travistorrent.
we decided to use hw17 s dataset so that it could benefit from this pre computed information.
we could still evaluate the prediction of first failures over our larger dataset since hw17 does not benefit from its pre computed features when there are only first failures i.e.
the preceding build to a failing build is always a passing build all of which get the same cluster id value.
cross validation.
we perform fold cross validation also to study the same conditions in which hw17 was evaluated.
thus we randomly divided our dataset into subsets of builds i.e.
folds iteratively using one of them as our test set and the remaining ones as our training set until we have used every fold as test set.
we evaluated the within variations of our studied techniques applying cross validation individually for each software project randomly dividing the set of builds of the same software project into subsets.
we evaluated the cross variations of our studied techniques applying cross validation across software projects randomly dividing the set of projects in our dataset into subsets of projects and using all the builds within a project for testing or training accordingly.
in both cases we report the results of our evaluation metrics per software project.
independent variable technique.
we evaluate four different approaches our proposed approaches and hw17 in their within andcross variants.
smartbuildskip within our proposed approach described in trained in the same software project using the predicting build features that we discovered in .
smartbuildskip cross our proposed approach described in trained in other software projects using the predicting build features and project features that we discovered in .
hw17 within the state of the art build predictor trained in the same software project.
hw17 cross the state of the art build predictor trained in different software projects.dependent variables.
we used three metrics to evaluate our studied techniques precision recall and f1 score.
we calculated the value of these metrics for each studied software project first for the set offirst failures and then for the set of all failures.
we measured precision as the number of correctly predicted build failures divided by the number of builds that the technique predicted as build failures.
we measured recall as the number of correctly predicted build failures divided by the number of actual build failures.
we measured f1 score as the harmonic mean of precision and recall.
results.
we plot the results of this experiment in figure for the prediction of first failures and in figure for the prediction of all failures.
the boxes in these box plots for each dependent variable represent its distribution of values for all the studied projects.
we discuss our observed differences in results in terms of absolute percentage point differences over the median value of each metric across projects.
predicting first failures.
smartbuildskip improved hw17 s median precision by for its within approach and by for its cross approach.
smartbuildskip also improved hw17 s median recall by for its within approach and by for its cross approach.
these differences were statistically significant p .
.
we posit that smartbuildskip cross provided an even higher improvement because its training set was much larger encompassing multiple projects and because build features likely vary little from project to project.
these findings validate our hypothesis in that separately predicting first failures is more effective than training a predictor based on features from all failures.
predicting all failures.
smartbuildskip improved hw17 s median precision by for its within approach and was worse for itscross approach.
it also improved hw17 s median recall by for its within approach and by for its cross approach.
these differences were statistically significant p .
.
we posit that 19icse may seoul republic of korea xianhao jin and francisco servant smartbuildskip s precision and recall are now much higher than hw17 s because it is much better at predicting subsequent failures.
we also observed that both techniques generally improved both their precision and recall.
we believe that this is due to the increase in the number of failing builds in the dataset after adding subsequent failures allowing all techniques to learn them better.
this is particularly acute for smartbuildskip s cross variants which became much more inclined to predict build failures after being trained with much more data across projects which dramatically increased its recall but reduced its precision.
these findings also validate our hypothesis in that choosing to always build after a failure is a highly successful strategy to predict subsequent failures.
experiment evaluating ci cost reduction rq6 rq7 how many resources i.e.
builds will our approaches save?
what is the value trade off for such resource savings?
after finding that smartbuildskip improves the precision and recall of the state of the art build predictor we measure the cost reduction that it would provide in practice.
research method.
we now simulate the more realistic scenario in which the builds that are skipped are not available for training.
we use the same setting as in with one change.
now when a predictor predicts the upcoming build as a pass we skip the build and accumulate the value of the build level features for the next coming build.
we only update the information connected to the last build when the predictor actually decides to build.
in this context we measure four metrics for each evaluated technique how many builds it saves how many failing builds are observed immediately and how many with a delay the delay length of delayed failing builds and a new metric to measure the balance between failing build observation delay and build execution saving.
independent variable technique.
we evaluate the same four predictors as in experiment in addition to a new collection of techniques that we call rule of thumb techniques.
in the spirit of cost saving we propose this additional collection of techniques because of their low adoption cost.
these rule of thumb techniques are based on the individual build features that we observed in .
they simply decide to skip builds when the given feature value is below a certain threshold.
we propose these techniques as a potentially good enough alternative for software teams that do not have the resources to implement and adopt smartbuildskip or for them to use in the time period while they are implementing it.
finally we also include a perfect technique that would skip all passing builds and run all failing builds as a reference for how many builds could be desirably skipped.
independent variable prediction sensitivity.
our simple techniques need a threshold to be applied i.e.
they are defined as predict build failures when the feature value is over x .
in a similar manner smartbuildskip can be also configured for different thresholds of prediction sensitivity.
thus we also evaluate these techniques for multiple thresholds of sensitivity.
only when the possibility predicted by the classifier for the coming build to become a failure is smaller than the threshold we will predict the build as a pass which means the smaller the threshold is the easier we are going to predictbuilds as failing.
finally these varied thresholds and prediction sensitivities will allow us to learn different trade offs that could be achieved in terms of saving cost in ci skipping builds without losing too much value without delaying too many build failures.
we evaluated different thresholds values which meant absolute value for the rule of thumb techniques and predicted likelihood in percentage of the build to fail for smartbuildskip.
studied dataset.
since this experiment is focused on predicting all builds we also use the dataset in which hw17 was originally evaluated .
dependent variables.
we measured four metrics in this evaluation recall failing build delay saved builds and saving efficiency.
recall is the proportion of failing builds that are correctly predicted and executed among all failing builds.
for each failing build that was incorrectly predicted and skipped we also measured its failing build delay as the number of builds that were skipped until the predictor decided to run a build again and then the failure would be observed.
we measured saved builds as the proportion of builds that are skipped among all builds.
finally we measured saving efficiency as the harmonic mean of saved builds and recall to understand their balance.
results.
we plot the results for our experiment in figure .
this figure shows the median value for each metric across studied projects.
for failing build delay it s the median across projects of their median failing build delay.
the y axis is the metric for evaluation and each box contains every project s result.
the x axis has different meanings for different techniques the threshold for rule of thumb techniques e.g.
threshold for src files means that files were changed in that build or the prediction sensitivity in percentage for the predictors.
we make a few observations from our results.
first smartbuildskip within achieves the peak saving efficiency among all techniques for its sensitivity saving of all builds executing of the failing builds immediately and the remaining ones with a median build delay.
if a more conservative approach is sought smartbuildskip within s sensitivity would execute of the failing builds and the remaining ones with a build delay while still saving of all builds.
hw17 achieved the poorest saving efficiency.
as we anticipated in hw17 predicted most builds to pass because it relied too much on the status of the last build.
it saved a large amount of builds but it also executed very few failing builds as a result.
finally our rule of thumb techniques provided acceptable results.
thus a software team looking for a simple mechanism to save effort by skipping builds in ci could simply skip those builds that for example changed more than lines which is the highest saving efficiency for src lines.
in our experiments this threshold saved around builds executing failing builds and the remaining ones with an build delay .
while this trade off may not be the most ideal certainly smartbuildskip provides much better trade offs it has the advantage that it can be adopted by simply informing developers to follow that rule.
finally if more conservative or more risky approaches are preferred figure shows a wide variety of trade offs that could be achieved by different techniques and configurations.
20a cost efficient approach to building in continuous integration icse may seoul republic of korea 50recall perfect hw17 within hw17 cross sbs within sbs cross commits src files src lines test lines 50saved builds perfect hw17 within hw17 cross sbs within sbs cross commits src files src lines test lines 50saving efficiency perfect hw17 within hw17 cross sbs within sbs cross commits src files src lines test lines 50failing build delay perfect hw17 within hw17 cross sbs within sbs cross commits src files src lines test lines figur e cost saved and value kept by evaluated techniques9 discussion diverse cost saving needs.
different developers will have different preferences in the trade off between observing failing builds early and saving build effort.
thus we propose smartbuildskip as a customizable solution with an adjustable prediction sensitivity.
some developers may value observing failing builds early much more than saving cost but still want to save some cost e.g.
developers at large companies that have been using ci for some time and are exploring ways to reduce its cost like facebook microsoft or google .
these developers could configure smartbuildskip in its most conservative sensitivity and save the cost of of their builds while only introducing a build delay in of their build failures.
in contrast other developers may be looking for a way to reduce ci s high cost barrier to adopt it even if it means observing build failures less quickly.
these developers could configure smartbuildskip with a more liberal sensitivity and save the cost of of their builds and still observe failing builds with no delay and the remaining with a build delay .
in this scenario smartbuildskip dramatically lowers the cost of ci for non adopters letting them still get a strong value from it particularly considering that non adopters currently do not benefit from ci at all.
furthermore as developers budgets increase they could also adapt the sensitivity of smartbuildskip over time to build more and observe failures more quickly.
the impact of delayed failing builds.
our approach reduces the cost of ci but it also reduces its value it delays the observation of some build failures.
some existing techniques target developers who cannot afford a single delayed failing build by skipping only tests or commits that are guaranteed to pass i.e.
tests for other modules and non code changes.
in exchange for such guarantee this strategy is limited in how much cost it can save the number of guaranteed pass tests and commits.
our proposed technique targets developers for whom some delay in failure observation is acceptable as do existing techniques based on test selection.
such techniques which introduce failure observation delays are valued and adopted by many large software companies e.g.
google microsoft or facebook .
we argue that for many developers the cost savings provided by smartbuildskip overcome the introduced delay in failure observation particularly for smartbuildskip s most conservative sensitivities which produce a delay of one or two builds.
for context herzig et al.
s approach deployed at microsoft introduced a delay of builds.
ultimately though we believe that different developers would prefer different cost saving trade offs which is why we made smartbuildskip customizable.
other purposes of ci.
the main reason for developers to use ci is to catch bugs earlier but they also use it to have a common build environment make integrations easier enforce a specific workflow simplify testing across multiple platforms be less worried about breaking builds deploy more often and have faster iterations .
most the first four of these purposes are achieved as soon as ci is adopted so we do not expect them to be impacted by introducing a cost saving technique like smartbuildskip.
however the last three purposes and others like safety checking pull requests may 21icse may seoul republic of korea xianhao jin and francisco servant be impacted since they benefit from observing build passes.
this applies to both our and existing techniques that skip tests or builds.
still after adopting a cost saving technique developers remain in control of their build frequency.
they can always build more frequently by making smartbuildskip s prediction sensitivity more conservative or by simply triggering additional builds on top of the ones that smartbuildskip triggers.
furthermore smartbuildskip provides an additional benefit over existing test selection based techniques for purposes that rely on build observations.
test selection techniques may give a false sense of confidence when a build that should have failed instead passes because some of its failing tests were skipped.
when smartbuildskip predicts a build that should have failed as passing it skips it it does not show it as passing which provides more transparency about the unknown status of the build until it eventually fails in a later build.
threats to validity construct validity.
we use metrics as proxies to represent the value early observation of build failures and cost build execution in ci.
however these are metrics that developers have reported as describing the value and cost of ci e.g.
and are metrics that other existing approaches for saving cost in ci have used e.g.
.
herzig et al.
assign specific dollar amounts to each test case that is saved and each failure observation that is delayed.
we avoid using their numbers since they were calculated at microsoft and will probably be different at other companies.
internal validity.
to guard internal validity we carefully tested our evaluation tools on subsets of our dataset while developing them.
our analysis could also be influenced by incorrect information in our analyzed dataset.
for this we selected a popular dataset that has been analyzed in other studies and we filtered outliers and toy projects out of it.
our results may also be affected by flaky tests causing spurious failing builds.
however ci systems are expected to function even in the presence of flaky tests since most companies do not consider it economically viable to remove them e.g.
.
another threat could be the risk of over fitting in our empirical study since we performed it over our complete data set since we aimed to increase the generalizability of our observed correlated features.
to address the over fitting risk we repeated our study on the chronologically earlier half of data for build features and a half of projects for project features through stratified random sampling on number of builds.
the features selected with our original criteria remained the same correlation coefficients sc .
fc .
tc .
nc .
td .
ps .
pa .
.
also our usage of cross validation may result in placing future builds in the training sample.
an alternative approach would have been to use chronological training and testing e.g.
.
however our goal was to compare smartbuildskip with hw17 in the scenario in which it was originally proposed and evaluated i.e.
using cross validation.
nevertheless we believe that smartbuildskip would provide similar precision and recall in a chronological experiment since it uses build features that likely do not vary much over time i.e.
we believe that sc fc tc and nc do not necessarilyvary significantly as projects age.
furthermore smartbuildskip s cross project variant is not affected by this threat since it was trained in different projects than it was tested.
finally we also increase our internal validity by validating the hypothesis that influence our proposed technique via studies and .
external validity.
to increase external validity we selected the popular dataset travis ci which has been analyzed by many other research works.
the projects we chose were all java or ruby projects because there are no projects with other programming languages in the data set.
although these two programming languages are popular different ci habits in other languages may provide slightly different results to the ones in this study.
finally our cost saving technique may not be suitable for software projects that cannot afford a single delay in observing failing builds.
we target projects that can afford some delay in exchange for the cost savings as do other techniques that skip builds e.g.
or tests e.g.
.
related work empirical studies of ci and its cost.
multiple researchers focused on understanding the practice of ci studying both practitionerse.g.
and software repositories .
vasilescu et al.
studied ci as a tool in social coding and later studied its impact on software quality and productivity .
zhao et al.
studied the impact of ci in other development practices like bug fixing and testing .
stahl et al.
and hilton et al.
studied the benefits and costs of using ci and the trade offs between them .
lepannen et al.
similarly studied the costs and benefits of continuous delivery .
felidr et al.
studied the adherence of projects to the original ci rules .
other recent studies focused on the difficulties and pain points of ci.
the high cost of running builds is highlighted by many empirical studies as an important problem in ci which reaches millions of dollars in large companies e.g.
at google and microsoft .
approaches to reduce the cost of ci.
a popular effort to reduce the cost of ci focuses on understanding what causes long build durations e.g.
.
thus most of the approaches that reduce the cost of ci aim at making builds faster by running fewer test cases on each build.
some approaches use historical test failures to decide which tests to run others run tests with a small distance with the code changes or skip those testing unchanged modules .
recently machalica et al.
predicted test case failures using a machine learning classifier .
these techniques are based on the broader field of regression test selection rts e.g.
.
while these techniques focus on making every build cheaper our work addresses the cost of ci differently by reducing the total number of builds that get executed.
a related recent technique saves cost in ci by not building when builds only include non code changes .
our technique predicts build outcomes for any kind of changes code and non code .
thus our work complements existing techniques to reduce cost in ci and could potentially be applied in addition to them.
a related effort for improving ci aims at speeding up its feedback by prioritizing its tasks.
the most common approach in this 22a cost efficient approach to building in continuous integration icse may seoul republic of korea direction is to apply test case prioritization tcp techniques e.g.
so that builds fail faster.
another similar approach achieves faster feedback by prioritizing builds instead of tests .
in contrast our work focuses on saving cost in ci by skipping tasks instead of prioritizing them.
prioritization based techniques increase feedback speed but do not focus on saving cost i.e.
all builds still get executed and all passing tests get executed if no test failure is observed.
finally other complementary efforts to reduce build duration have targeted speeding up the compilation process e.g.
or the initiation of testing machines e.g.
.
characterizing failing builds.
multiple studies investigated the reasons why builds fail.
some studies found that the most common build failures were compilation unit test static analysis and server errors.
paix o et al.
studied the interplay between non functioal requirements and failing builds.
other studies found factors that contribute to build failures architectural dependencies and other more specific factors such as the stakeholder role the type of work item and build or the programming language .
other less obvious factors that could cause build failures are build environment changes or flaky tests .
rausch et al.
also found that build failures tend to occur consecutively which gallaba et al.
describe as persistent build breaks .
these observations inform our hypothesis that subsequent build failures would be numerous and easy to anticipate.
other studies found change characteristics that correlate with failing builds such as number of commits code churn number of changed files build tool and statistics on the last build and the history of the committer .
in our study we separate failing builds into first failures andsubsequent failures.
we found that first failures are predicted by some of the factors that predict all builds line file and test churn and number of commits but also by factors that were not found to correlate with all builds project size age and test density .
finally other studies investigated the characteristics of build failures outside the ci context predicting failing builds.
some works aimed at predicting build outcomes in industrial settings where continuous integration was not yet adopted.
these techniques mostly approached this problem using machine learning classifiers e.g.
measuring social and technical factors and using decision trees applying social network analysis and measuring socio technical factors and using code metrics on incremental decision trees .
in the continuous integration context ni and li predict build outcomes using cascade classifiers measuring statistics about the last build and the committer of the current build.
xie and li use a semi supervised method over change metrics and the last build s outcome.
hassan and wang use a predictor over the last build s status and type.
since all these predictors rely on the outcome of the last build to be known their prediction power may be limited in a cost saving context where the last build means the last build that was executed.
in contrast to these predictors smartbuildskip is not affected by how stale the last build status is since it does not rely on it for its prediction.
conclusions and future work in this article we proposed and evaluated smartbuildskip a novel framework for saving cost in ci by skipping builds that it predicts will pass.
our design of smartbuildskip is based on two main hypothesis that build passes are numerous and that many failing builds happen consecutively.
we studied these hypotheses and found evidence to support them.
thus smartbuildskip works in two phases first it runs a machine learning predictor to decide if a build will pass and skips it or will fail and executes it.
whenever it observes a failing build it determines that all subsequent builds will fail and keeps building until it observes a pass again and starts predicting again.
with this strategy smartbuildskip improved the precision and recall of the state of the art build predictor hw17 and cost savings with various trade offs since we made it customizable to address the needs of diverse populations of developers.
we highlight two specific configurations that we posit will be popular the most conservative one which saves builds and only delays the observation of failing builds by build and a more balanced one that saves of all builds and delays failing builds by builds.
nevertheless smartbuildskip provides many other trade offs that could be desirable in different environments.
smartbuildskip provides a novel strategy that complements existing techniques to cost saving in ci that focus on skipping test cases or builds with non code changes.
in the future we will work on extending smartbuildskip s algorithm with static analysis techniques to predict build failures based on characteristics of the contents of their code changes.
we will also explore adding prediction features based on the historical properties of the changed modules between builds such as their code change history .
currently smartbuildskip benefits from statistical properties of builds.
this future approach would focus on taking advantage of their structural properties.
replication we include a replication package for our paper .