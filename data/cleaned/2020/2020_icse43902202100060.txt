a context based automated approach for method name consistency checking and suggestion yi li department of informatics new jersey institute of technology new jersey usa email yl622 njit.edushaohua wang department of informatics new jersey institute of technology new jersey usa email davidsw njit.edutien n. nguyen computer science department the university of texas at dallas texas usa email tien.n.nguyen utdallas.edu abstract misleading method names in software projects can confuse developers which may lead to software defects and affect code understandability.
in this paper we present d eepname a context based deep learning approach to detect method name inconsistencies and suggest a proper name for a method.
the key departure point is the philosophy of show me your friends i ll tell you who you are .
unlike the state of the art approaches in addition to the method s body we also consider the interactions of the current method under study with the other ones including the caller and callee methods and the sibling methods in the same enclosing class.
the sequences of sub tokens in the program entities names in the contexts are extracted and used as the input for an rnn based encoder decoder to produce the representations for the current method.
we modify that rnn model to integrate the copy mechanism and our newly developed component called the non copy mechanism to emphasize on the possibility of a certain sub token not to be copied to follow the current sub token in the currently generated method name.
we conducted several experiments to evaluate d eepname on large datasets with 14m methods.
for consistency checking deepname improves the state of the art approach by .
.
and .
relatively in recall precision and f score respectively.
for name suggestion d eepname improves relatively over the state of the art approaches in precision .
.
recall .
.
and f score .
.
.
to assess d eepname s usefulness we detected inconsistent methods and suggested new method names in active projects.
among pull requests were merged into the main branch.
in total in cases the team members agree that our suggested method names are more meaningful than the current names.
index terms naturalness of software deep learning entity name suggestion inconsistent method name checking.
i. i ntroduction meaningful and succinct names of program entities play a vital role in code understandability .
misleading names in software libraries can confuse developers and cause them make api misuses leading to serious defects .
during software development the name of a method can become inconsistent with respect to its intended functionality.
the first scenario is that the inconsistency occurs during the coding of a method when a misleading or confusing name is given to the method.
in the second scenario inconsistency occurs during software evolution in which code changes make the method s name and its implementation become inconsistent with one another.
corresponding authorseveral approaches were proposed to detect the inconsistency between the methods names and source code and to suggest an alternative name if such inconsistency occurs.
the approaches follow mainly two directions information retrieval ir and machine learning ml .
the idea of ir approaches is that similar methods should have similar names .
thus they search for the names of methods with similar bodies to suggest for a method with an inconsistent name.
the ir approaches generally follow a searching strategy thus cannot recommend a new method name that is un seen in the training data.
the second direction is machine learning ml .
the ml approaches can overcome the key limitation of the ir direction due to its capability of generating a new name.
while code2vec generates the method s name based on the paths over the ast of its body mnire uses the sub tokens in the program entities names.
other approaches treat the method name suggestion problem as the extreme summarization from the method s body into a short text.
despite their successes the state of the art ml approaches have limitations in dealing with the methods having little content or the entities names that are irrelevant to the functionality.
in this paper in addition to using the body and interface of the method under study we also leverage a philosophy for this naming problem show me your friends i ll tell you who you are .
that is to characterize an entity person in addition to using its his her own properties one can rely on the interactions of that entity person with the surrounding and neighboring entities persons.
for the method name suggestion examining only the content of the current method might be insufficient.
the surrounding and interacting methods of a methodmunder study could include the methods that are called within the body of m callees and the methods that are callingm callers .
the neighboring methods are the ones within the same class with m siblings .
the information from theenclosing class also provides features for such characterization of a method.
the key features from the caller andcallee methods sibling methods and the enclosing class are used in addition to the features from the internal body andinterface of the method mto verify the consistency of the method with regard to its name and to suggest a proper method name.
each of those sources constitutes a context that is helpful ieee acm 43rd international conference on software engineering icse .
ieee for method name consistency checking and recommendation.
some methods have little content but with sufficient contexts and vice versa.
thus all contexts are complementary to one another in name consistency checking and suggestion.
we develop d eepname a context based approach for method name consistency checking and suggestion.
for the methodmunder study it extracts the features from four contexts the internal context the caller and callee contexts sibling context and enclosing context .
for name suggestion only the callee methods are used because the callers of mmight not be written yet the current method does not have a name yet .
we use the name features specifically the sequences of subtokens from the program entities within each context instead of ast or pdg.
it is reported that to infer a method name using sub tokens yields better accuracy than using the ast and pdg of the method .
the insight is that the naturalness of names plays crucial role in method name inference i.e.
method name depends more on entities names than ast or pdg with data control flows .
ast and pdg capture the structure and procedure of the task while the method s name is the summary of the task.
d eepname uses an rnn based encoder decoder to combine all the sequences of sub tokens in the contexts into a sequence of vectors for m. a convolution layer is used on the vector for mto classify the given name to be consistent or not.
to suggest a name for the given method we use our vocabulary to map all the generated vectors into the sub tokens to compose the method name.
we also modify the operations of the aforementioned rnnbased encoder decoder to integrate the copy mechanism and the novel non copy mechanism .
a recent study on the methods names has reported that the high percentage of the sub tokens of a method name appears in a set of the sub tokens from entities names in a method.
due to this the copy mechanism helps emphasize on the possibility of copying certain sub tokens from the contexts to the output i.e the suggested method name.
the non copy mechanism is designed to determine the possibility of a sub token that must not be copied to follow the current sub token in the currently generated method name.
the non copy mechanism complements to the copy one in the way that it pushes down the unlikely candidates with the sub tokens not following a certain token in the resulting ranked list.
thus the likely candidates are pushed up in the list improving suggestion accuracy.
we conducted several experiments to evaluate d eepname in method name consistency checking and in method name recommending on two large datasets used in prior works with 2m and 14m methods .
for inconsistency checking deepname outperformed the state of the art approaches in liuet al.
and mnire by relatively .
and .
in recall .
and .
in precision and .
and .
in f score.
for method name suggestion d eepname improves relatively over the state of the art approaches in both recall .
.
and precision .
.
.
there are .
of the cases suggested by d eepname that exactly match with the correct method names in the oracle and .
of those cases i.e.
.
of total cases do not appear in training data.1private void declaregrouping boltdeclarer boltdeclarer node parent string streamid groupinginfo grouping the old inconsistent method name is declarestream 3if grouping null boltdeclarer.shufflegrouping parent.getcomponentid streamid else grouping.declaregrouping boltdeclarer parent.get componentid streamid grouping.getfields fig.
an example of inconsistent method name this shows that d eepname can learn to suggest the method names rather than retrieving what have been stored.
in total there are .
of the cases in which the names are not previously seen in the training data.
the precision and recall of this set of generated names are .
and .
respectively.
to assess usefulness we made pull requests prs on the suggesting new names for the inconsistent methods as detected by d eepname .
among them prs were actually merged into the main branch and were approved for later merging.
in total there are of the cases that team members agreed that our suggested names are more meaningful than the current names.
this paper makes the following contributions a. representation and tool a novel approach that uses both internal and interaction contexts for method name consistency checking and suggestion.
b. novel technique in d eepname we modify an rnnbased encoder decoder to integrate a newly developed mechanism called non copy mechanism to help our model pushes correct candidates to the top improving top ranked accuracy.
c. empirical results our empirical evaluation shows that d eepname is useful and more accurate than the state ofthe art approaches in real world projects and in a live study all four contexts complement to one another and contribute much to high accuracy.
our replication package is in .
ii.
m otivating examples a. examples figure shows an example in apache storm project having the method declaregrouping with an inconsistent name.
it is used to declare a group information for a stream.
in an earlier version the method was given the name declarestream which was deemed to be confusing and inaccurately reflecting the functionality of this method.
therefore in a later version a developer performed refactorings to rename the method into declaregrouping and at the same time performed code partition.
this example shows a common case in which during the course of software development the name of a method has become confusing and inconsistent with its functionality.
thus an automated tool to detect inconsistent method names is helpful for developers to avoid confusing and mistakes.
when the method name is identified as inconsistent it is also useful to have a tool to recommend a new name for the method.
there are several factors that a tool can leverage to 5751private dimension calculateflowlayout boolean bdochilds ... 3if getparent !
null getparent ... jviewport jviewport viewport jviewport getparent maxwidth viewport.getextentsize .width else if getparent !
null maxwidth getparent .getwidth else maxwidth getwidth ... dimension d m.getpreferredsize ... public dimension xxxxxxxxxxxxxxxx the consistent method name is getpreferredsize 18return calculateflowlayout false fig.
an example of method name suggestion suggest a new name for the method.
first a tool can rely on the body i.e.
the content of the method to suggest its name.
second the types and names of the arguments and return type of the method could also be used to predict the method s name.
the first and second factors are referred to as theinternal content and the interface of the method under study.
these two factors represent the only two key sources of information that the state of the art approaches have been using for method name checking suggestion.
liu et al.
use clone detection on the methods bodies to search for similar methods to suggest similar names.
alon et al.
also rely on the method s content however explore code structures by using embeddings built from the paths over the abstract syntax tree ast of the method under study.
allamanis et al.
and nguyen et al.
also make use of the method s body method interface and class name however break down the names of program entities into sub tokens and then use them to suggest the method name via neural network models or a clustering algorithm in the vector space .
despite their successes the state of the art approaches do not work for the methods with little contents in their bodies.
the method at line in fig.
in tina pos project is named getpreferredsize .
the method contains a single call to calculateflowlayout .
assume that one wants to use an existing name suggestion tool for the body of this method.
however the existing approaches relying on the method s body or interface do not work because none of the tokens of the correct name getpreferredsize appears there the code structure in the body does not help in predicting the method s name.
our tool suggests the correct name getpreferredsize while mnire uses the body to suggest the name getflow .thus simply using the method s body and interface is not sufficient.
b. key ideas show me your friends i ll tell you who you are in addition to the method s body and interface we characterize a method by the surrounding methods that interact with the method under study.
in this problem the surroundingand neighboring methods of a method munder study could include the methods that are called within the body of m let us call them callees the methods that are calling m callers the methods within the same class with m siblings and the program elements declared in the enclosing class of m. for method name consistency checking in addition to the method s body and interface we could use all of those neighboring methods.
for name suggestion we could use callees and siblings since the callers of mmight not be written yet at the time that the current method mis being edited.
in this example while the content of mis short the callee context i.e.
the body of the method calculateflowlayout contains sufficient information for name suggestion.
in fig.
examining the body of the callee method calculateflowlayout we can see that the sub tokens of the consistent method name getpreferredsize appear in the names of the program entities in the callee.
first the sub token getappears at lines and .
second the sub token preferred appears at line .
third the sub token size appears at line and line .
for consistency checking callers and sibling methods can be used since they might be available.
in general the contexts complement to one another and to the internal content of the method contributing to name suggestion.
with the nature of source code the case of little internal content and little contexts of a method is rare.
representation learning from multiple contexts our model learns the representation to integrate names of variables fields method calls from multiple contexts.
in addition to the method s body and interface we call it internal context we also consider the interaction context which includes all the methods interacting with the current method m i.e.
caller methods if available and callee methods.
in fig.
the two sub tokens declare and grouping in the consistent method name declaregrouping can be found at line and line .
we also use the sibling methods in the same class sibling context because they provide the tasks with the same theme.
different contexts might have the sequences of sub tokens with different lengths and nature.
for example in some cases those sequences for callers callees might positively contribute or negatively impact in deriving the method name.
to help our model learn the importance of different contexts in different situations we use a learning scheme for the weights of the contexts.
in fig.
we collect the internal context including the body at line and the method return type dimension at line as one type of context.
we collect the name of the method calculateflowlayout .
also we collect the body and interface of the method calculateflowlayout as the interaction context.
sub token copy and non copy mechanisms it is reported that the high percentage of sub tokens of the method names can be found in the set of the sub tokens of the program entities in the context.
thus we leverage this by using a copy mechanism for recurrent neural network rnn .
this emphasizes on the likelihood of directly copying a sub token from the input into the output position following a sub token.
we also design a non copy mechanism that complements to the copy one.
as copying at a sub token position the non576copy mechanism estimates the likelihood score that a certain sub tokensmust not be copied to follow the current subtokenc.
a higher score implies that the occurrence probability of the sequence consisting of cfollowingsis lower.
thus it helps our model push lower in the resulting ranked list the incorrect candidates with those sub tokens that must not follow certain ones.
the correct candidates thus are pushed higher in the list improving top ranked accuracy .
for example as deriving the sub token at the second position in the method name non copy pushes the candidates declareschool or declarestudent lower in the list since school and student have never been seen to follow declare in the training set.
thus the correct option declaregrouping will be ranked higher.
importantly instead of directly using rnn encoder decoder we modify its operations to integrate our newly developed non copy mechanism with the copy mechanism to help better predict the method name.
the copy andnon copy mechanisms complement each other.
assume that the currently predicted name has two sub tokens a b .copy mechanism suggests c1 c2 c n from the input to likely follow a b .
that is copy mechanism suggests c1is likely to be copied and cn is less.
however non copy mechanism can suggest that c2 from the input must not follow bbecause in training it has never seenb c .
thus with only copy mechanism c2is ranked 2nd however with both we lower the ranking of c2.
iii.
c oncepts and approach overview definition caller and callee methods .
a caller method caller for short is a method calling the current method under study for consistency checking or name suggestion.
a callee method callee for short is a method called within the body of the current method.
definition contexts .
we consider four types of context .
internal context the internal context for the current method contains the content in the body types andnames of thearguments in the interface and the method s return type .
.
interaction context the interaction context for the current method includes the names of the callers of the current method the contents of the callers i.e.
bodies interfaces return types the names of the callees and the contents of the callees bodies interfaces return types .
.
sibling context the sibling context for the current methodmincludes the names of the methods in the same class with m and the contents of sibling methods including the bodies interfaces and return types .
.
enclosing context the enclosing context for the current method includes the name of the enclosing class of the current method and the names of the program entities method calls field accesses variables and constants in the class.
figure shows d eepname s overview.
it is aimed to help in two usage scenarios.
first the project is in the development process and the source code of the current method under consideration and the source code of the callers callees siblings and the enclosing class are available.
d eepname can be used to determine the consistency of its name and then it is used fig.
overview of d eepname s architecture to suggest an alternative name if the current name is deemed inconsistent.
in the second scenario the source code of the current method is written however its caller methods are not available since its name is not there yet.
d eepname can be used to suggest a proper name for that method in this scenario.
there are four main steps in d eepname .
context building.
we collect internal context interaction context sibling context and enclosing context to build context features.
specifically for each type of context we extract the names of the program entities appearing in the context.
we use the camelcase and hungarian convention to break each name into a sequence of sub tokens.
for example calculateflowlayout is broken into calculate flow layout .
we use the sequence of all the sub tokens of the program entities names in the same order in the source code as the feature for that context.
the rationale of using the sequence of sub tokens as feature instead of pdg ast is the naturalness of names developers do not give random names they use names of program entities or method calls fields relevant to the task of the current method.
.
context representation learning.
after extracting as features the sequences of sub tokens for the contexts we need to convert those sequences into vector representations for the models in the later steps.
we use a word embedding model to convert the sequences of sub tokens into the vectors.
we put the vectors for all the sub tokens in the order of the appearances of those sub tokens in the source code.
as the result for each context we have a sequence of representation vectors corresponding to the sub token sequence of that context.
.
context based method representation learning.
deepname relies on all four contexts internal interaction sibling enclosing .
thus we need to produce a representation for method mwith the encoding of all contexts.
from the previous step for a context we have a sequence of representation vectors.
at this step we use an encoder to learn the encoding of the features for a context.
we then use a decoder to combine the encoded information for all the contexts into a representation for m which has the integration of all contexts.
.
consistency checking and method name suggestion.
at the last step d eepname has two separate models for two tasks.
for consistency checking it takes as the input the representation of the method mobtained from the previous step with the existing method name to be checked and feeds to a two channel cnn based model acting as a classifier to classify the method name to be consistent or not.
for method name suggestion each vector in the sequence of method representation vectors is a sub token representation vector.
we use our vocabulary to roll back all the vectors into the subtokens and put them together to generate the method name.
577iv.
c ontext representation learning a. context building our first step is to build the contexts from source code.
for the internal context sibling context and enclosing context we directly extract the names from the program entities the return type and the types in the interface.
the names are broken into sub tokens which are collected into the sequence in the same appearance order in source code.
the sequences of those sub tokens represent the contexts.
for example for the method in fig.
line the internal context is modeled by the sequence of sub tokens calculate flow layout dimension .
the internal context also includes the sub tokens in the return type the types and names of the parameters of the method.
the trivial sub tokens with a single character are removed.
for the interaction context we build a call graph using soot .
we then identify the callers and callees for the current method under study.
the sequences of sub tokens are built in the same manner as in the previous contexts to form the feature for the interaction context.
for consistency checking both callers and callees are considered.
however for method name suggestion we consider only the callee methods if the caller methods do not exist yet.
in fig.
the callee part of the interaction context includes the sub tokens built from parsing the method calculateflowlayout get parent ...view port ...max width dimension get prefer size boolean dimension other callees are processed in the same manner.
all the sequences of sub tokens for all contexts are used as input in the next step.
we denote those sequences as context features .
b. context representation learning to convert the sequences of sub tokens into vectors we consider all the sequences of sub tokens for all the contexts as the sentences of words.
we use glove a word embedding technique to produce the vectors for the collected sub tokens.
we use glove instead of word2vec due to its capability of learning to represent the words from the aggregated global word word co occurrence statistics which captures the relationships between neighboring sub tokens.
to build the vector representation for each context we replace the sub tokens in a sequence for the context feature with the corresponding glove vectors for the sub tokens.
we maintain the same order as the appearance order in the source code.
for example for a context feature sequence fi we have a sequence of vectors vfi in which viis aglove vector for a sub token in the sequence.
because the sequences of vectors might have different lengths we perform zero padding by filling the zero vectors for the sequences whose lengths are less than the maximum length.
this makes the sequences of vectors have the same length.
v. c ontext based method representation from the previous step a context is represented as a sequence of vectors in which each vector represents a sub token.
the goal of this step is to produce a representation for the given method that integrates all of its contexts.
p size p1 p2 p3 ... interac on context internal contextencoderhdecoder ... ...fig.
context based method representation learning a.context feature encoding we first use a rnn based seq2seq encoder to encode the sets of vectors for all contexts from the previous step.
for each context we encode it with a gate recurrent unit gru .
we use multiple grus because different contexts might have different structures and types of information.
multiple grus also help reduce the cross influence between different contexts.
the input for each gru iis the sequence vfiofnvectors representing a context.
each vector represents a sub token in the names of the program entities in the context.
for example in fig.
one gru is used for the interaction context including the sub tokens in the name body and interface of the callee method calculate flow layout .... another gru is used for the internal context including the body interface of the current method dimension calculate flow ... for each time step t we input one vector vtin thesenvectors and we get one hidden state vector htas the output for this time step.
by collecting all outputs for each time step we have a list of hidden state vectorshi which is the output of gru i. mathematically the input sequence vfi is learned and converted into a hidden vector hiby the encoder.
the decoder is used to transfer the representation hidden vectorhiback to the target sequence y .
ht f vt ht h0 xt g yt h0 t hi p ytjy1 y t vfi s yt ht hi formula is for encoder rnn.
htis the hidden state in time stept frepresents the rnn dynamic function.
formula is for the decoder.
h0 tis the hidden state for the decoder at time stept grepresents the rnn dynamic function.
formula is for prediction sis the possibility calculation function.
by putting together all the outputs his of all grus we obtain all the hidden states vectors h .
this is the output of the encoder and used in the attention layer.
because not all the sub tokens in a context are equally important we aim to put emphasis on certain sub tokens.
such emphasis is learned via an attention mechanism .
technically the attention layer uses a changing context ctinstead of one 578hidden vector hi.ctis calculated as the weighted sum of the encoder s hidden states ct nx i t ihi t i er h0 t hi pn i06 ier h0 t hi0 whereris the function used to represent the strength for attention approximated by a multi layer neural network.
b.context feature decoding this is our new component in which we modify the operation of the grus at the output layer of the decoder to integrate our newly developed non copy mechanism.
it operates in connection with the attention layer.
first at each time step tfor a gru the previous hidden stateh0 t 1is used as the input for the attention layer and the output of the attention layer will be used as the input of the gru at the time step t. in fig.
the vector for the sub token preferred obtained from the output of the attention layer at the time step is used as the input of the gru at time step .
this emphasizes on the important sub tokens while the decoding is performed at each time step.
next at the output layer of the decoder we also integrate the operation of two mechanisms copynet and non copy .
copynet copynet is the copy mechanism with the rnn seq2seq model and attention mechanism.
it calculates the possibilities of copying the input sub tokens to the output.
it has two modes generation mode denoted by gen and copymode cpy for prediction.
the attention mechanism with rnn uses one function.
the state update for copynet considers not only the word embedding but also the corresponding location specific hidden state in the set of rnn encoder s hidden states h .
p ytjh0 t yt ct h p yt genjh0 t yt ct h p yt cpyjh0 t yt ct h p yt genj zeugen yt yt2dicall yt2dicall dicin zeugen unk yt 2dicall dicin p yt cpyj zp j vj yteucpy vj yt2dicin otherwise ugen anducpy are the score functions for the generationmode and copy mode zis the normalized term used by two modes and dicallanddicinare the overall dictionary and the dictionary for the input only.
non copy mechanism we aim to determine the possibility of a sub token that must notfollow the current subtoken.
for instance if the sub token getat the m position is a known one and the sub token preferred follows it in the training dataset we calculate the possibility that preferred will notfollow the sub token get.
such calculation is based on our statistical analysis on the word distribution on the vocabulary.
mathematically for the method name ym 1at the position m we calculate the possibility that a certain sub tokenwill not be the next one at the position m. that possibility score denoted by p ym nonjym is computed as p ym nonjym count ymjym count ym 1ym ym 12dicall ym 2dicall others wherecount ymjym is the occurrence count for the subtokenmthat follows the sub token at m in the training dataset and count ym 1is the occurrence count for the subtoken at m .
this formula is to calculate the word distribution possibility which is between .
the larger value means that this sub token has higher possibility of notfollowing the previous sub token at m .
moreover because we have multiple context features as input which can pass to the copy mode we add all the possibilities together as the total copymode possibilities with different weights.
with this the copymode formula has now become p yt newj ix i 1wipi yt cpyj wnon p yt nonjyt wherewiis a trainable weight for different types of context features wnon is a trainable weight and always less than zero andiis the total number of types of context features.
method representation decoding we have modified the decoder part see our new component in fig.
to integrate the copy and non copy mechanisms.
in the traditional gru for an rnn decoder the output layer is computed according to formula .
we still keep that computation as one of the three factors to determine the output of the decoder which is shown as generation mode in fig.
.
for example for the subtoken size after preferred that computation is as p size gen .
in addition we also integrate the copynet mechanism to determine the possibility of a sub token based on the sub token copying as in formula in section v b1 p size cpy .
for non copy mechanism we integrate with the copy mechanism from formula .
thus the new formula for the combination of copy and non copy mechanisms is formula .
the possibility score of a sub token as the output of the decoder at a time step tis the summation of the three factors.
thus the possibility score of a sub token e.g.
w size is calculated as p w p1 w gen p2 w cpy p3 w non .
we will pick the representation vector for the sub token with the highest possibility score in the current time step tas the output of the decoder at that time step.
finally after having the prediction vector vtfor all time steps we put them together to obtain a set of vectors in the original order as the set of vectors vcurfor the current method.
vi.
c onsistency checking and name suggestion a.consistency checking to check whether the method name is in consistent we use a two channel cnn model as a classifier on the set of vectorsvcur which can be viewed as a matrix mcur.
to build the second channel we apply the same word embedding step to represent the name of the current method as the set 579of vectorsvexist.
we consider that set as the matrix mexist and combine with the matrix mcurto form the two channel representation matrix mclassification which is fed to the two channel cnn model.
the output is produced by the softmax function.
the value is between in which represents consistency and represents inconsistency.
b.method name suggestion we consider the sequence vcurproduced by the previous step is the vector representations for the method name under study.
specifically we consider each vector vkas the representation for a sub token in the suggested name of the method.
from the dictionary dicallfor all the vocabulary in the corpus we find the closest vector vskto the vector vkand use the sub tokenskcorresponding to vskas the suggested sub token forvk.
finally we obtain the sequence of the sub tokens for allvks.
the resulting sequence is considered as the suggested name for the method under study.
the order of the sub tokens is the same as the order of the vectors vks in the sequence of the representation vectors for the method vcur.
vii.
e mpirical evaluation a. research questions rq1.
method name consistency checking comparative study.
how well does d eepname perform in comparison with the state of the art method consistency checking approaches?
rq2.
method name suggestion comparative study.
how does deepname perform in comparison with the state of the art method name suggestion approaches?
rq3.
impact analysis of different contexts and weighting scheme.
how do distinct types of contexts and their different weights affect the overall performance of d eepname ?
rq4.
impact analysis of copy and non copy mechanisms.
how do copy and non copy mechanisms affect accuracy?
rq5.
method name suggestion accuracy analysis.
how does deepname perform on the un seen method names and the methods in various sizes?
rq6.
live study.
how does d eepname perform on the currently active real world projects?
b. experimental methodology datasets corpus for consistency checking.
for comparison we used the same dataset from liu et al.
which was also used in another baseline approach mnire .
the training dataset table i from that corpus was collected from the highly rated open source projects from four communities namely apache spring hibernate and google .
it contains the latest versions of java projects with commits.
in total it has methods which were considered as consistent names because the methods whose names are stable for a long time were selected.
for the testing dataset liu et al.
mined the methods whose names were modified by developers for the reasons of misleading names.
finally in those projects there are methods with inconsistent names.
they randomly chose another methods with consistent names to form a test dataset with methods.table i corpus for method name consistency checking testing data training data methods files projects unique method names table ii corpus for method name suggestion testing data training data total project file methods corpus for method name suggestion.
for comparison we used the same dataset as in code2vec and mnire with top ranked java projects from github.
it has methods and unique files.
we split the corpus based on the number of projects instead of files.
the project based setting reflects better the real world usage of deepname where it is trained on the set of existing projects and used to check for a new project.
the overloading overriding methods and generated method names were removed.
empirical procedure let us present our procedure.
rq1.
method name consistency checking.
we chose the following baselines liuet al.
an ir approach to search for similar methods to suggest similar names mnire an ml approach using seq2seq encoder decoder on the subtoken sequences in the method s body and interface.
we trained each model under study with the same training dataset and tested it with the same testing dataset.
for hyper parameter tuning for a model we used automl in nni to automatically tune the parameters.
we selected the parameter set that helps a model obtain the highest f score and accuracy .
for name consistency checking methods are for testing methods of the training data are for training and the remaining are for tuning.
rq2.
method name suggestion.
we compared with the following baseline approaches mnire code2vec code2seq and path based representation .
for comparison using the same procedure as mnire we randomly split the data by for training for parameter tuning and for testing.
rq3 rq4.
impact analysis on various components.
we varied our model e.g.
adding each context and measured accuracy.
we used the parameter settings as in rq1 rq2.
rq5.
accuracy analysis on method name suggestion.
we measure the accuracy on un seen method names and on the methods with different lengths in the same setting as rq2.
rq6.
live study.
we use our tool to check the method names in the active github projects make pull requests to rename inconsistent ones and evaluate the acceptance responses.
evaluation metrics for method name consistency checking we compared the predicted results against the ground truth on consistent and inconsistent method names provided as part of the name consistency checking corpus 580table iii rq1.
method consistency checking comparison c consistent methods ic inconsistent methods liuet al.
mnire deepname cprecision .
.
.
recall .
.
.
f score .
.
.
icprecision .
.
.
recall .
.
.
f score .
.
.
accuracy .
.
.
table i .
for name suggestion we compared the predicted names by a model against the good method names in the name suggestion corpus which was part of code2vec table ii .
for consistency checking we used the same evaluation metrics as in liu et al.
and mnire including precision recall and f score for both inconsistency ic and consistency c classes.
for icclass precision jtpj jtpj jfpj and recall jtpj jtpj jfnj.
forcclass precision jtnj jtnj jfnj and recall jtnj jtnj jfpj in whichtpis true positive icis classified as ic fn is false negative icis classified as c tn is true negative cis classified as c andfp is false positive c is classified as ic .
for bothicandc f score is defined as precision recall precision recall.
for the overall in both icandc accuracy is defined asjtpj jtnj jtpj jfpj jtnj jfnj jtpj jtnj.
for method name suggestion we used the same metrics as in code2vec and mnire precision recall and f score over case insensitive sub tokens.
that is for the pair of an expected method name eand its recommended name r precision and recall are computed as precision e r jsubtoken r subtoken e j jsubtoken r j and recall e r jsubtoken r subtoken e j jsubtoken e j subtoken n returns the sub tokens in the name n.precision recall and f score ofthe set of the suggested names are defined as the average values in all cases.
in all experiments we also counted the exact matched names exmatch and the case sensitive names.
c. experimental results rq1.
method name consistency checking as seen in table iii for inconsistent method name detection ic deepname has a relative improvement of .
.
.
and .
.
.
on precision recall and f score in comparison with liu et al.
and mnire respectively.
we found that for liu et al.
in many cases the inconsistent methods might not be classified as inconsistent lower recall and the predicted inconsistent methods might be incorrect lower precision .
the main reason is that the principle of methods with similar bodies have similar names and vice versa does not hold in many cases.
for mnire several inconsistent methods are not classified as inconsistent lower recall since the bodies use similar sub tokens but the methods do not have the same tasks.
for those cases deepname uses the caller and callee methods to complement for the internal context in the characterization of a method and is able to detect the inconsistencies since it can detect methods with the same usages but with different names.table iv rq2.
method name suggestion comparison code2vec path rep code2seq mnire deepname exmatch .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f score .
.
.
.
.
forconsistent method name detection c deepname has a relative improvement of .
.
.
and .
.
.
on precision recall and f score in comparison with liu et al.
and mnire respectively.
regarding accuracy as considering both consistent and inconsistent name detection deepname relatively improves .
and15.
compared to liu et al.
and mnire respectively.
we found several consistent methods with similar bodies however with different names.
for example the methods on inputstream and outputstream but have the same body ofreturn stream .
both liu et al.
and mnire relies on the body thus cannot work in those cases while d eepname distinguishes them via callers callees and siblings.
moreover there are methods with the same bodies but names are different due to different enclosing classes intended for different purposes e.g.
process.start process.stop .
the baselines detected them as inconsistent.
d eepname uses the callers callees to recognize its usage thus correctly detecting it as consistent.
rq2.
method name suggestion as seen in table iv .
of the cases suggested by d eepname at top positions are exactly matched with the correct method names given by developers.
it has relative improvements from .
.
compared with the baselines.
it also achieves higher f score than all the baselines.
specifically it has relative improvements of .
.
in precision .
.
in recall and .
.
in f score over the baselines.
with regard to exmatch code2vec and path based rep have lower values than d eepname as they mainly encode pathbased contexts with tokens which have shown as less repetitive than the sub tokens .
code2seq encodes the path based contexts as well as the sub tokens.
however it failed to capture the order of sub tokens for the exact name recommendation.
code2seq often recommends the relevant sub tokens but not in the right order.
d eepname improves code2seq by .
also mnire does not consider the callers callees and siblings thus it cannot identify more sub tokens than d eepname .
with regard to recall we found that code2vec and path based rep have lower recall than d eepname because the baselines mainly encode path based contexts within one method.
code2seq requires two methods with similar sub tokens and or path contexts.
mnire requires two methods with similar sequences of sub tokens to have similar names.
d eepname uses the bodies interfaces as well as the interaction sibling and enclosing class contexts thus is more flexible.
with regard to precision two methods can be realized in the same structure but are named differently since they are in different classes and are used differently.
because two methods have the same similar ast path contexts code2vec and pathbased rep suggest the same name thus they have lower preci581table v rq3.
context analysis on consistency checking internal a a enclosing b b siblings c c interaction deepname cprecision .
.
.
.
recall .
.
.
.
f score .
.
.
.
icprecision .
.
.
.
recall .
.
.
.
f score .
.
.
.
accuracy .
.
.
.
table vi rq3.
context analysis on name suggestion internal a a enclosing b b siblings c c interaction deepname exmatch .
.
.
.
precision .
.
.
.
recall .
.
.
.
f score .
.
.
.
sions.
mnire uses the enclosing class but does not consider the interaction and sibling contexts.
thus in several such cases mnire suggests the same name while d eepname suggests the correct name due to the callers callees and siblings.
rq3.
impact analysis of different contexts and weights a. context analysis.
the base model in this experiment uses only internal context the method body and interface .
as seen in table v when adding the enclosing class of the method a enclosing accuracy increases by .
.
relatively as both f scores for c and ic classes increase.
considering the sibling methods accuracy additionally increases .
.
relatively as comparing the columns b and c .
finally with all the contexts accuracy additionally increases .
i.e.
.
relatively.
thus all contexts contribute positively toward the overall accuracy.
with further analysis we have observed the following.
first the enclosing class provides the context related to the general theme of the current method e.g.
inputstream versus outputstream .
while the method bodies are the same return stream d eepname is able to derive the correct names getinputstream and getoutputstream by leveraging the enclosing context.
second the sibling context provides the names of the relevant methods to the current one.
for example in a class that provides mouse handling for a canvas the sibling methods onmouseup and onmousedown give useful sub tokens to suggest the method name onmouseover .
finally the interaction context helps suggest the names for the methods with little content in the bodies e.g.
in delegation methods.
unlike the existing approaches with only internal context we leverage the interactions siblings and enclosing contexts of the method as well as internal context body interface to achieve highest accuracy.
we also performed another experiment to leave one context out and compare the accuracy with d eepname s accuracy in order to determine the impact of each context.
without the interaction context accuracy and two f scores reduce by .
.
and .
respectively.
without the sibling context the performance decreases by .
.
and .
ontable vii rq4.
copy non copy in consistency checking seq2seq seq2seq seq2seq copy non copy copy d eepname cprecision .
.
.
recall .
.
.
f score .
.
.
icprecision .
.
.
recall .
.
.
f score .
.
.
accuracy .
.
.
table viii rq4.
copy non copy in name suggestion seq2seq seq2seq copy seq2seq copy non copy d eepname exmatch .
.
.
precision .
.
.
recall .
.
.
f score .
.
.
accuracy and two f scores respectively.
the enclosing context also positively contributes to high performance.
in brief the internal and interaction contexts contribute the most.
the contributions of contexts are also confirmed by the method name suggestion results table vi .
when adding the enclosing context to the internal one f score increases by .
.
when further adding the sibling context f score additionally increases by .
.
finally adding the interaction context f score additionally contributes .
.
b. impact of weight learning for different contexts.
the nature and the length of the sequences of sub tokens in each context might contribute differently .
to help our model learn the importance of each context we use weight learning with wiin formula .
we compared our model with the one having equal weights.
the result shows that with weight learning d eepname improves .
in accuracy for consistency checking and .
in f score for name suggestion.
thus our weight learning for contexts positively contributes to accuracy.
rq4.
impact analysis of copy and non copy mechanisms in this experiment we removed from our model both copy andnon copy mechanisms and used it as a baseline.
we then added each mechanism one by one to the baseline.
as seen in tables vii viii copy mechanism helps improve over the baseline model .
relatively in accuracy for consistency checking and .
relatively in f score in name suggestion.
the newly developed non copy mechanism helps additionally improve .
relatively over seq2seq copy in consistency checking accuracy and helps improve .
relatively in name suggestion.
thus non copy complements to copy mechanism.
illustration let us take an example in our experiment to illustrate the effectiveness of each component in d eepname .
fig.
shows the top ranked results for the name of the method given in fig.
whose actual name is processfinallystmt .
we show the top ranked resulting lists of the name for several variations of d eepname in which we gradually added each component context to the previous model.
the leftmost column is the result of the model using only internal context 582fig.
method name recommendation results top ranked list for fig.
1private static livecalc xxxxxxxxxxx finallystmt s liveset onentry name processfinallystmt 3return liveness s.getbody onentry fig.
a correctly suggested method name body and interface .
the next column is the result from a new variation with the addition of a new component.
for example the second column is the result from a model that considers both internal and enclosing contexts the third one is from the model with internal enclosing and sibling contexts etc.
let us explain the resulting lists from all the variants and the impacts of contributing components.
as seen both the models internal and internal enclosing cannot suggest the correct name in the top lists.
the body and interface contain the sub tokens finally and stmt but do not contain the sub token process .
by adding the enclosing context can only help improve the ranking of the candidate method names that include subtoken finally and stmt.
however by adding the sibling context internal enclosing siblings is able to rank the correct method name processfinallystmt at the 8th position.
the reason is that the enclosing class in this case has several sibling methods with the names starting with process e.g.
processoperation processlabeledstmtwrapper etc.
thus internal enclosing siblings is able to learn the sub token process from the sibling methods and ranks the correct name higher.
by adding the interaction context internal enclosing siblings interaction can rank the correct name at the 6th position.
the reason is that the body and interface of the callee method liveness contain the occurrences of process and stmts .
with the addition of the copy mechanism the new model internal enclosing siblings interaction copy improves the ranking of the correct name to the 4th position.
the reason is that copy mechanism can emphasize on the copying of the popular and important sub tokens e.g.
process liveness .
as seen the names with the sub tokens process and liveness are ranked at the top positions in the column corresponding to this model.
finally d eepname with the addition of non copy mechanism is able to rank the correct name at the top position.
the reason is that non copy can learn that the sub token set must not follow processfinally .
thus processfinallyset is pushed down and the sub token stmt following processfinally to produce the correct name processfinallystmt is pushed to the top.
fig.
accuracy by different methods sizes in test set table ix rq7.
pull requests of real world projects accept agree disagree no reply total rq5.
accuracy analysis on method name suggestion we study the results on the suggested method names thatwere not in the training data .
there are 173k .
out of 445k generated method name that were un seen during the training.
the precision recall and f score for this set are .
.
and .
respectively.
importantly in .
of these generated cases i.e.
.
total cases the generated names exactly match the expected names in the oracle.
these numbers show that d eepname performs well for the un seen method names and really learns to suggest natural names rather than retrieving method names that have been stored in the training dataset.
accuracy by the sizes of methods in test set.
as seen in fig.
d eepname works well on the methods with the regular sizes of locs.
even with the longer methods locs precision and recall decrease gracefully at .
and .
respectively.
this shows that predicting the name becomes harder for longer methods.
even so in 8k cases of 33k long methods with locs d eepname produced the exact match names with the correct ones.
rq6.
live study to evaluate the usefulness of our tool we conducted a study on randomly chosen active java projects in github.
we used d eepname trained as in rq1 to detect inconsistent method names then submitted pull requests prs of method renaming suggested by the tool and assessed pr acceptance rates.
overall it identified 3k out of 133k methods as inconsistent.
to avoid much work for developers we randomly selected and made only pull requests.
as seen in table ix among prs cases were approved and merged by the development teams.
additionally prs have been validated and approved by the team members.
for those cases the teams acknowledged that the current method 583names are misleading and agreed with the suggested names as providing more meaningful names.
however at the time of writing the prs have not been merged into the main branch due to the additional requirements of reviewing or testing.
in cases the developers disagreed with our suggested names.
in some cases the suggested names do not conform to coding conventions in the project.
some cases involve template code.
in some cases the names are of the methods that override the external libraries.
there are still cases that we did not get responses.
in brief in cases the developers confirmed that the names suggested by d eepname are more meaningful than the current names.
this shows that d eepname is useful in real world projects in both detecting inconsistent method names and suggesting new names.
threats to validity our data has only java code.
for code2vec we used the same metrics for comparison i.e.
the accuracy for a set of method names are the average of those for all names .
we did not have a statistical test in comparison since they did not provide individual resulting names.
running their tool requires a high computational machine.
we could not run liu et al.
s name suggestion on our dataset despite our efforts trying and contacting the authors without responses.
limitations despite the above successes d eepname also has the aspects that need to be improved.
as any other ml approaches it has the out of vocabulary issue.
that is it cannot generate a sub token that has never been seen in the training data.
however as shown in the empirical evaluation section d eepname is able to generate a new method name from the sub tokens that it has encountered in the training dataset.
because d eepname does not analyze the entire project it does not perform well for the overriding methods and the methods that override the apis in the external libraries.
a potential solution is to integrate our ml direction with program analysis to guide the process of the method name generation.
moreover d eepname does not work well for the method names with one single sub token or the methods with long bodies or the long callers callees.
viii.
r elated work there are two categories of approaches for method name inconsistency detection and suggestion.
the first one is information retrieval ir .
liu et al.
relies on the principle that methods with similar bodies have similar names.
in our experiment we showed that such principle does not hold in many cases.
jiang et al.
searches for the methods having similar return type and parameters as well as heuristics to derive method names.
the key advantage of these ir based approaches is that they are light weighted and do not require high computational power.
their key disadvantage is that because searching in the set of already existed names they cannot generate a new name that were not in the training data.
the second category is machine learning ml .
mnire explores the sub tokens appearing in the methods bodies and interfaces.
allamanis et al.
use a neural network with attention and convolution to summarize code into descriptive summaries.
allamanis et al project all the sub tokens inthe entities names in the method bodies into the same vector space and cluster them to compose method name.
mnire has been shown to outperform code2vec which outperforms allamanis et al.
and allamanis et al .
those ml based approaches all rely only on the method s body and interface.
there are several approaches for code embeddings.
code2vec abstracts source code by the paths over the ast to produce vectors.
code2seq generates a word sequence from the structure of source code.
ke and su s approach builds the embeddings to capture structures and semantics of a program.
however as shown in mnire using code structure ast pdg is too strict in predicting method names.
there are several approaches to predict the names or types of program entities within the method bodies .
while jsneat searches for names in a large corpus to recover variable names in minified code jsnice and jsnaughty use crf and machine translation.
naturalize learns and enforces a consistent naming conventions.
several approaches use ml to generate texts from code and vice versa or code migration .
zheng et al.
uses ast structure for such statistical machine translation to produce comments.
code nn uses lstm on code sequence to model the conditional distribution of a summary to produce word by word.
deepcom has a traversal on ast for flattening and uses seq2seq to produce code summary.
wan et al.
use a deep reinforcement learning on ast and code sequence.
there are several studies on name consistency and naming convention .
ix.
c onclusion we introduce d eepname a context based deep learning approach for inconsistency checking and method name suggestion.
the following key ideas enable our approach characterizing a method by the surrounding methods that are interaction or siblings method for the method we are studying learning the representation for the method with multiple contexts using sub token copying and non copying mechanisms to help better predict the name.
we conducted several experiments to evaluate d eepname .
our results showed high accuracy and usefulness of d eepname in real world projects.
for consistency checking deepname improves the state of the art approach by .
.
and .
relatively in recall precision and f score respectively.
for name suggestion d eepname improves relatively over the existing approaches in precision .
.
recall .
.
and f score .
.
.
in the assessment of d eepname s usefulness in real world projects the team members agree that our suggested method names are more meaningful than the current names in cases.
for future work we plan to integrate our ml direction with program analysis to improve both accuracy and efficiency.
acknowledgment this work was supported in part by the us national science foundation nsf grants ccf ccf twc1723198 ccf and cns .
584references the github repository for this study.
.
available m. allamanis e. t. barr c. bird and c. sutton learning natural coding conventions in proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering ser.
fse .
acm press pp.
.
suggesting accurate method and class names in proceedings of the 10th joint meeting on foundations of software engineering ser.
esec fse .
new york ny usa association for computing machinery pp.
.
.
available m. allamanis h. peng and c. a. sutton a convolutional attention network for extreme summarization of source code in proceedings of the 33nd international conference on machine learning icml new york city ny usa june ser.
jmlr workshop and conference proceedings m. balcan and k. q. weinberger eds.
vol.
.
jmlr.org pp.
.
.
available u. alon s. brody o. levy and e. yahav code2seq generating sequences from structured representations of code in 7th international conference on learning representations iclr new orleans la usa may .
openreview.net .
.
available u. alon m. zilberstein o. levy and e. yahav a general path based representation for predicting program properties in proceedings of the 39th acm sigplan conference on programming language design and implementation ser.
pldi .
new york ny usa acm pp.
.
.
available code2vec learning distributed representations of code proceedings of the acm on programming languages vol.
no.
popl pp.
jan. .
.
available v .
arnaoudova m. di penta and g. antoniol linguistic antipatterns what they are and how developers perceive them empirical softw.
engg.
vol.
no.
pp.
feb. .
d. binkley m. hearn and d. lawrie improving identifier informativeness using part of speech information in proceedings of the 8th working conference on mining software repositories ser.
msr .
new york ny usa association for computing machinery pp.
.
s. butler m. wermelinger y .
yu and h. sharp relating identifier naming flaws and code quality an empirical study in proceedings of the 16th working conference on reverse engineering wcre oct pp.
.
j. gu z. lu h. li and v .
o. li incorporating copying mechanism in sequence to sequence learning in proceedings of the 54th annual meeting of the association for computational linguistics volume long papers .
berlin germany association for computational linguistics aug. pp.
.
.
available x. gu h. zhang d. zhang and s. kim deep api learning in proceedings of the 24th acm sigsoft international symposium on foundations of software engineering ser.
fse .
acm pp.
.
t. gvero and v .
kuncak synthesizing java expressions from freeform queries in proceedings of the acm sigplan international conference on object oriented programming systems languages and applications ser.
oopsla .
acm pp.
.
t. haije automatic comment generation using a neural translation model .
e. w. host and b. m. ostvold debugging method names in proceedings of the 23rd european conference on ecoop objectoriented programming .
berlin heidelberg springer verlag pp.
.
x. hu g. li x. xia d. lo and z. jin deep code comment generation in proceedings of the 26th conference on program comprehension ser.
icpc .
acm pp.
.
.
available s. iyer i. konstas a. cheung and l. zettlemoyer summarizing source code using a neural attention model in proceedings of the 54th annual meeting of the association for computationallinguistics volume long papers .
association for computational linguistics aug. pp.
.
.
available l. jiang h. liu and h. jiang machine learning based automated method name recommendation how far are we in proceedings of the 34th acm ieee international conference on automated software engineering ase .
ieee cs .
s. kim and d. kim automatic identifier inconsistency detection using code dictionary empirical softw.
engg.
vol.
no.
pp.
apr.
.
k. liu d. kim t. f. bissyand e t. kim k. kim a. koyuncu s. kim and y .
l. traon learning to spot and refactor inconsistent method names in proceedings of the 41th international conference on software engineering ser.
icse .
acm pp.
.
z. liu x. xia c. treude d. lo and s. li automatic generation of pull request descriptions in proceedings of the 34th ieee acm international conference on automated software engineering ser.
ase .
ieee press pp.
.
.
available microsoft neural network intelligence.
last accessed may 9th .
a. t. nguyen p. c. rigby t. nguyen d. palani m. karanfil and t. n. nguyen statistical translation of english texts to api code templates in2018 ieee international conference on software maintenance and evolution icsme pp.
.
a. t. nguyen z. tu and t. n. nguyen do contexts help in phrasebased statistical source code migration?
in ieee international conference on software maintenance and evolution icsme pp.
.
a. t. nguyen h. a. nguyen t. t. nguyen and t. n. nguyen statistical learning approach for mining api usage mappings for code migration in proceedings of the 29th acm ieee international conference on automated software engineering ser.
ase .
new york ny usa association for computing machinery pp.
.
.
available a. t. nguyen t. t. nguyen and t. n. nguyen lexical statistical machine translation for language migration in proceedings of the 9th joint meeting on foundations of software engineering ser.
esec fse .
new york ny usa association for computing machinery pp.
.
.
available divide and conquer approach for multi phase statistical migration for source code in proceedings of the 30th ieee acm international conference on automated software engineering ser.
ase .
ieee press pp.
.
.
available s. v .
nguyen t. le and t. n. nguyen suggesting natural method names to check name consistencies in proceedings of the 42th international conference on software engineering ser.
icse .
acm pp.
.
j. pennington r. socher and c. d. manning glove global vectors for word representation in empirical methods in natural language processing emnlp pp.
.
.
available h. phan h. a. nguyen n. m. tran l. h. truong a. t. nguyen and t. n. nguyen statistical learning of api fully qualified names in code snippets of online forums in proceedings of the 40th international conference on software engineering ser.
icse .
acm pp.
.
m. raghothaman y .
wei and y .
hamadi swim synthesizing what i mean code search and idiomatic snippet synthesis in proceedings of the 38th international conference on software engineering ser.
icse .
acm pp.
.
v .
raychev m. vechev and a. krause predicting program properties from big code in proceedings of the 42nd annual acm sigplansigact symposium on principles of programming languages ser.
popl .
acm pp.
.
soot soot introduction.
last accessed july .
h. tran n. tran s. nguyen h. nguyen and t. n. nguyen recovering variable names for minified code with usage contexts in proceedings of the 41st international conference on software engineering ser.
icse .
ieee press pp.
.
b. vasilescu c. casalnuovo and p. devanbu recovering clear natural identifiers from obfuscated js names in proceedings of the 11th joint meeting on foundations of software engineering ser.
esec fse .
acm pp.
.
y .
wan z. zhao m. yang g. xu h. ying j. wu and p. s. yu improving automatic source code summarization via deep reinforcement learning corr vol.
abs .
.
.
available k. wang and z. su blended precise semantic program embeddings inproceedings of the 41st acm sigplan conference on programminglanguage design and implementation ser.
pldi .
new york ny usa association for computing machinery p. a v134.
.
available m. l. wenhao zheng hongyu zhou and j. wu codeattention translating source code to comments by exploiting the code constructs frontiers of computer science vol.
pp.
.
s. zagoruyko and n. komodakis learning to compare image patches via convolutional neural networks in ieee conference on computer vision and pattern recognition cvpr pp.
.