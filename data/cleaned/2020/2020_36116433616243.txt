baldur whole proof generation and repair with large language models emily first university of massachusetts amherst ma usa efirst cs.umass.edumarkus n. rabe augment computing palo alto ca usa markus augmentcode.com talia ringer university of illinois urbana champaign il usa tringer illinois.eduyuriy brun university of massachusetts amherst ma usa brun cs.umass.edu abstract formally verifying software is a highly desirable but labor intensive task.
recent work has developed methods to automate formal verification using proof assistants such as coq and isabelle hol e.g.
by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.
this paper introduces a new method to automate formal verification we use large language models trained on natural language and code and fine tuned on proofs to generate whole proofs at once.
we then demonstrate that a model fine tuned to repair generated proofs further increasing proving power.
this paper demonstrates that whole proof generation using transformers is possible and is as effective but more efficient than search based techniques.
demonstrates that giving the learned model additional context such as a prior failed proof attempt and the ensuing error message results in proof repair that further improves automated proof generation.
establishes together with prior work a new state of the art for fully automated proof synthesis.
we reify our method in a prototype baldur and evaluate it on a benchmark of isabelle hol theorems and their proofs empirically showing the effectiveness of whole proof generation repair and added context.
we also show that baldur complements the state of the art tool thor by automatically generating proofs for an additional .
of the theorems.
together baldur and thor can prove .
of the theorems fully automatically.
this paper paves the way for new research into using large language models for automating formal verification.
ccs concepts software and its engineering software verification formal software verification theory of computation automated reasoning computing methodologies neural networks machine learning approaches .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
proof assistants proof synthesis proof repair machine learning large language models automated formal verification acm reference format emily first markus n. rabe talia ringer and yuriy brun.
.
baldur whole proof generation and repair with large language models.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction formal software verification proving software correctness and other properties is one of the most challenging tasks software engineers can undertake.
it is highly effective at producing high quality software.
for example compcert a c compiler verified using the coq interactive theorem prover was the only compiler on a list including the ubiquitous gcc and llvm in which a comprehensive study found no bugs .
similarly the sel4 project resulted in an highly reliable operating system microkernel .
however the cost of manual formal verification writing the proofs is often prohibitive.
for example the proof of the c compiler is more than three times as long as the compiler code itself .
as a result recent research has focused on automated proof synthesis which can lead to fully automating formal verification.
there are two promising approaches for automating proof synthesis.
the first is to use hammers such as sledgehammer for the isabelle proof assistant.
hammers iteratively apply known mathematical facts using heuristics.
the second is to use searchbased neural theorem provers such as deephol gpt f tacticzero lisa evariste diva tactok and astactic .
given a partial proof and the current proof state which consists of the current goal to prove and the list of known assumptions these tools use neural networks to predict the next individual proof step .
they use the proof assistant to evaluate the proposed next proof steps which returns a new set of proof states.
iterating this procedure results in a tree like structure which defines a search through the space of possible proofs.
neural theorem provers rely on diverse neural architectures such as wavenet graph neural networks short long term memory models and language models with the transformer architecture .
esec fse december san francisco ca usa emily first markus n. rabe talia ringer and yuriy brun in this paper we propose baldur a different simpler approach to proof synthesis.
we show that using large language models llms fine tuned on proofs can produce entire proofs for theorems.
llms are scaled up transformer models trained on a large amount of text data including natural language and code that have proven to be remarkably effective across a wide variety of applications including question answering and text and code generation .
here we show their remarkable effectiveness for whole proof generation.
the main contributions of our work are we develop baldur a novel method that generates whole formal proofs using llms without using hammers or computationally expensive search.
we define a proof repair task and demonstrate that repairing incorrectly generated proofs with llms further improves baldur s proving power when the llm is given access to the proof assistant s error messages.
we demonstrate empirically on a large benchmark that baldur when combined with prior techniques significantly improves the state of the art for theorem proving.
we design baldur to be able to work with any llm internally but we evaluate our implementation using two versions of minerva one with billion parameters and another with billion parameters.
by contrast existing tools that use l lms for theorem proving either predict individual proof steps or rely on few shot prompting and require the existence of natural language proofs as hints .
we evaluate baldur on the pisa dataset of isabelle hol theorems and their proofs used in recent state of the art isabelle hol proof synthesis evaluations .
the dataset consists of 183k theorems of which we use for measuring effectiveness.
our evaluation answers the following research questions rq1 how effective are llms at generating whole proofs?
llms outperform small model driven search based methods.
baldur without repair is able to generate whole proofs for .
of the theorems completely automatically whereas search based approaches prove .
.
rq2 can llms be used to repair proofs?
llms can repair proofs including their own erroneous proof attempts.
baldur proves an additional .
of the theorems when given access to a previous erroneous proof attempt and the error messages produced by the proof assistant even when controlling for the computational cost of the additional inference.
the error message is crucial for this improvement.
rq3 can llms benefit from using the context of the theorem?
in context learning is remarkably effective for llmbased theorem proving.
with context baldur proves .
of the theorems but only .
without context for the same model size.
rq4 does the size of the llm affect proof synthesis effectiveness?
larger llms do perform better suggesting that our approach will continue to improve with further developments in llm research.rq5 how do llms compare to other state of the art proof generation methods?
baldur complements state of the art approaches by proving theorems they do not.
together with thor a tool that combines a learned model search and a hammer baldur can prove .
of the theorems whereas thor alone proves .
.
these findings suggest that llm and search based methods ideas complement each other and can work together to further improve the automation of formal verification.
an ensemble of different fine tuned baldur models proves .
.
by leveraging llms baldur simplifies the proof synthesis pipeline greatly reducing the complexity and cost of the fine grained interaction between the prediction model and the proof assistant that search based methods require.
this reduction enables us to leverage the power of llms which would be prohibitively computationally expensive if synthesis required as many llm queries as searchbased methods.
further those calls would require re encoding with each step the additional information the llm might need whereas our approach allows us to make a single call and process the context only once sampling multiple proofs of multiple proof steps at once.1overall our study strongly suggest that llms are a very promising direction of research for automating formal verification and identifies several new avenues for future explorations.
the baldur approach prior approaches to proof synthesis employ a neural model to predict the next proof step given the current proof state .
the proof step predictions then guide a search strategy such as best first search or depth first search.
throughout the search the proof assistant needs to check each proof step prediction to determine whether it is valid.
this means that existing proof synthesis tools require a tight interaction between the neural network and the proof assistant.
as we move to using llms this results in complex systems as llms need to run on specialized hardware gpus or tpus while proof assistants run on cpus.
we explore a simpler yet effective method fine tuning llms to generate complete proofs.
this simplification avoids the finegrained interaction between neural model and the proof assistant allowing us to run the jobs of generating proofs and checking completely separately.
besides reducing complexity this can also improve efficiency because it enables us to use large batch sizes which can significantly improve hardware utilization during inference cf.
and when providing additional context to the model the context now does not have to be reprocessed for each proof step but only once per proof.
we fine tune llms on proof data to generate entire proofs and explore the impact of giving the llms additional information.
our approach and implementation include the following we fine tune an llm to generate an entire proof given only the theorem statement.
we call this model the proof generation model section .
.
we provide a model a proof attempt that did not check along with the corresponding error message from the proof assistant 1alternatively path advanced caching strategies in the prediction servers of large language models could address this problem.
this is beyond the scope of our work.
1230baldur whole proof generation and repair with large language models esec fse december san francisco ca usa proof generation model theorem theorem statement proof isabelle proof assistant no error error success candidate proof failureinput figure an example of using the proof generation model to generate a proof.
so that the model may attempt to find a better proof.
we call this model the proof repair model section .
.
we provide text from the same theory file that the problem was taken from.
we add only the lines from the theory file that immediately precede the theorem we want to prove.
we call this added information the theory file context and we add it to the proof generation model section .
.
the llm that we fine tune at the core of all of this is minerva which is pretrained on a mathematics corpus.
we describe our baldur specific implementation details for how we use this model section .
.
these fine tuned llms and their interaction with the isabelle proof assistant make up our tool baldur.
this section details the baldur approach which includes creating training datasets and leveraging llms to generate and repair proofs.
.
proof generation existing proof generation methods using neural models generate the proof one step at a time.
in contrast our approach generates the entire proof as illustrated with a single example in figure .
we use only the theorem statement as input to our proof generation model .
we then sample a proof attempt from this model and perform proof checking using isabelle.
if isabelle accepts the proof attempt without an error then we have proven the theorem.
otherwise we can try sampling another proof attempt from the proof generation model.
explicitly the input and output of our proof generation model is as follows input theorem statement.
output candidate proof.
example.
to illustrate the power of the proof generation approach in our tool baldur we first consider as an example the theorem fun sum commute .
lemma fun sum commute assumes f and x y. f x y f x f y shows f sum g a a a. f g a the theorem states that for an additive function fwhere f and an arbitrary function g applying fon the sum of the set resulting from applying gon each element in a given set is equal to the sum of applying gfollowed by fto each element in that set.
in this context the sum over an infinite set is taken to be zero.
this theorem is from a project in the archive of formal proofs called polynomials specifically in the file utils.thy .
the human written proof distinguishes between two cases when the set is finite and when it is not.
induction is used for the finite set case.
proof cases finite a case true thus ?thesis proof induct a case empty thus ?case by simp add assms next case step insert a a show ?case by simp add sum.insert assms step qed next case false thus ?thesis by simp add assms qed if we were to derive a training example from this example the input would be theorem statement and the target would be this human written proof.
our tool baldur using the proof generation model is able to generate the following correct proof for this statement.
by induct a rule infinite finite induct simp all add assms baldur recognizes that induction is necessary and applies a special induction rule called infinite finite induct following the same overarching approach as the human written proof but much more succinctly.
it is interesting to note that sledgehammer the hammer for isabelle cannot prove this theorem by default as it requires induction.
training data creation.
to train the proof generation model we construct a new proof generation dataset.
existing datasets for training models in neural theorem provers contain examples of individual proof steps.
each training example includes at minimum the proof state the input and the next proof step to apply the target .
given a dataset that contains individual proof steps we want to create a new dataset so that we can train models to predict entire proofs at once.
so we extract the proof steps of each theorem from the dataset and concatenate them to reconstruct the original proofs.
we use this data to generate training examples for the proof generation model where the input consists of the theorem statement and the target consists of the proof.
1231esec fse december san francisco ca usa emily first markus n. rabe talia ringer and yuriy brun in particular this means that we drop the proof states from the dataset which make up most of the text in the dataset.
we argue that for isabelle proofs this is not necessarily a problem as isabelle uses a declarative proof language that is designed to be humanreadable.
this is in contrast to other proof assistants such as coq where the proofs are typically written in a procedural style that is not easy to interpret for humans without using the proof assistant to generate the intermediate proof states.
inference.
we fine tune an llm on our data to predict the entire proof given only a theorem statement.
to synthesize a proof using the fine tuned llm we provide a potentially unseen theorem statement and sample a fixed number of sequences typically or from the language model where a sequence is an entire proof attempt.
we tune the sampling temperature from a small set between .
and .
in increments of .
which is a multiplicative factor on the log probabilities of the distribution of tokens sampled in each step.
proof checking.
after sampling proofs from the model we check all of them with the proof assistant.
this means that we first load the context in which the theorem was originally proven and then replace the original proof of the theorem with the one we sampled from the model.
if isabelle accepts any of the sampled proofs we report the theorem as proven.
.
proof repair if a proof is not accepted isabelle returns an error message that is intended to help humans with debugging their proof script.
existing proof generation methods however have no way to leverage error messages.
building off our proof generation approach we explore the use of error messages to improve neural theorem provers by developing a proof repair approach.
starting with just the problem statement we apply the proof generation model from section .
to sample a proof attempt.
if isabelle accepts the proof attempt we can stop.
otherwise we use the error message returned by the proof checker and the incorrect proof attempt to construct an example to serve as input to the proof repair model .
as depicted in figure we use the theorem statement the incorrect proof and the error message as input to our proof repair model.
we then sample the proof attempt from this model and perform proof checking in the same way as the proof generation approach.
explicitly the input and output of our proof repair approach pipeline are as follows input theorem statement incorrect proof error message.
output candidate proof.
example.
starting from the theorem fun sum commute we illustrate an example of the proof repair approach in our tool baldur.
we apply the proof generation model to obtain more proof attempts.
the following is a proof attempt generated by baldur which fails in the proof checker.
proof induct a case insert x a thus ?case by simp add assms qed simp proof repair model theorem theorem statement incorrect proof incorrect proof error error message proof isabelle proof assistant no error error success candidate proof failureinput figure an example of using the proof repair model to repair an incorrect proof.
baldur attempts to apply an induction but fails to first break down the proof into two cases finite vs. infinite set .
isabelle returns the following error message step error unable to figure out induct rule at command proof line the error message details where the error occurs line and that the issue is regarding the induct rule.
with these strings as input using the proof repair model baldur can attempt to generate a correct proof for this statement.
if we want to instead derive a proof repair training example from these strings we concatenate the theorem statement the failed proof attempt and the error message to serve as the input and we use the correct human written proof recall from previous section as the target.
training data creation.
to train the proof repair model we need to generate a proof repair training set.
figure details the training data creation process.
using the proof generation model we sample one proof with temperature for each problem in the original training set used to train the proof generation model.
using the proof assistant we record all failed proofs and their error messages.
we then proceed to construct the new proof repair training set.
for each original training example we concatenate the theorem statement the incorrect candidate proof generated by the proof generation model and the corresponding error message to obtain the input sequence of the new training example.
for the target sequence we reuse the ground truth proof from the original training example.
we fine tune the pretrained llm on the proof repair training set to obtain the proof repair model.
.
adding context llms possess impressive in context learning abilities cf.
that allow them to flexibly use information that is provided as part of the input sequence and in fact as part of their own output .
in order to explore to what extent in context learning can help in the theorem proving domain we extend their inputs with potentially 1232baldur whole proof generation and repair with large language models esec fse december san francisco ca usa proof generation model isabelle proof assistant theorem theorem statement incorrect proof candidate proof error error message proof no error proof repair model training example theorem theorem statement proof proof generation model training example error ground truth proof ground truth proof no example candidate proof error message input output input output figure training data creation for the proof repair model.
helpful context.
adding to our proof generation approach we use the theory file contexts the lines preceding the theorem statement as input to our proof generation model with context .
explicitly the input and output of our proof generation model with context is as follows input theory file context and theorem statement.
output candidate proof.
example.
continuing the example the theory file context directly preceding fun sum commute is the following theorem statement and its associated proof.
lemma additive implies homogenous assumes x y. f x y f x f y a monoid add b cancel comm monoid add shows f proof have f f f by rule assms hence f f f by simp thus f by simp qed the proof generation model with context in baldur can leverage this additional information.
strings that appear in the theorem statement for fun sum commute such as f appear again in this context and so the additional information surrounding them could help the model make better predictions.
training data creation.
we add the lines of the theory file that precede the theorem statement to serve as additional context.
thismeans that context can include statements such as the previous theorems definitions proofs and even natural language comments.
to make use of the available input length of llms we first add up to preceding statements from the same theory file.
during training we first tokenize all these statements and then we truncate the left of the sequence to fit the input length.
premise selection.
many proofs make frequent use of definitions and previously proven statements also known as premises .
some neural theorem provers such as holist focus entirely on the problem of selecting the right set of premises which has been shown to be quite successful in theorem proving.
premise selection is clearly similar to the addition of context in some aspects but we want to emphasize some key differences adding context is an extremely simple technique that only requires rudimentary text processing by adding the preceding lines of the theory file the model can only observe a small fraction of the available premises most of the added context consists of proofs.
.
large language model we use minerva a large language model pretrained on a mathematics corpus based on the palm large language model.
specifically we use the billion parameter model and the billion parameter model.
the minerva architecture follows the original transformer architecture but has some noteworthy differences.
it is a decoder only transformer with maximum sequence length of tokens.
the model uses rotary position encodings instead of sinusoidal absolute position embeddings parallel layers which compute the feed forward layer and the attention layer in parallel and add up their results instead of computing them in sequence and multi query attention which uses a single key value pair per token per layer for faster decoding .
as this model is not a contribution of this paper we refer the reader to prior work for lower level details on the minerva architecture .
baldur specific implementation details.
the proof generation task naturally consists of an input which is the theorem statement potentially augmented with additional information and the output target which is the proof for the theorem.
to work with the decoder only model we concatenate the inputs and targets but the loss is only computed over the target during fine tuning so that the model is learning to conditionally generate the target given the input and not the input itself.
the inputs use bidirectional attention while the targets use causal attention as in prefixlm .
as the transformer has a maximum context length of we pad the sequences with zeros if they are too short and we need to truncate them if they are too long.
inputs to the model are truncated to the maximum input length by dropping tokens on the left.
the rationale for dropping tokens on the left is that the additional context is given before the theorem statement and can be truncated more safely than the theorem statement itself.
similarly targets i.e.
the proof to generate are truncated on the right to the maximum target length.
1233esec fse december san francisco ca usa emily first markus n. rabe talia ringer and yuriy brun we used a maximum input length of and a maximum target length of all experiments but the repair study and the 62b model which used and instead.
we use a drop out rate of .
for both generation and repair models to address overfitting.
during sampling from the language model we restrict the choice of the next token to the tokens with the highest score also called top k sampling .
we sample sequences with a maximal length of tokens.
the model was trained to generate up to tokens but since most successful proofs are relatively short this limitation has little impact on the proof rate while saving some compute.
we use a batch size of and fine tune for up to steps but we observed that the model begins to overfit to the training set after to steps.
for inference we selected checkpoints from just before the model started to overfit.
evaluation this section presents our experiments answering the following research questions rq1 how effective are llms at generating whole proofs?
rq2 can llms be used to repair proofs?
rq3 can llms benefit from using the context of the theorem?
rq4 does the size of the llm affect proof synthesis effectiveness?
rq5 how do llms compare to other state of the art proof generation methods?
to answer these questions we trained several language models using the approach from section and evaluated them on the pisa benchmark see section .
.
.
experimental setup machine specification.
for most of the training runs of the 8b model we used tpuv3 cores distributed across hosts.
for training the 62b model we used tpuv3 cores distributed across hosts.
for most inference jobs we used between inference servers using tpuv3 cores each.
proof checker.
we use the pisa codebase under a bsd 3clause license which allows us to interact with the isabelle proof assistant to check proofs.
to run large jobs of the proof checker we package it in a docker container and run it on gcp.
we extended the proof checker to discard any proofs that contain sorry or oops which are keywords that skip proofs but otherwise pass the proof checker.
we apply a timeout of seconds to each proof step in the proof checker.
.
pisa benchmark we derive our datasets from the pisa dataset which includes the isabelle hol repository under a bsd style license and the archive of formal proofs afp from october .
the afp is a large collection of isabelle hol proof developments.
pisa includes the core higher order logic library of isabelle as well as a diverse library of proofs formalised with isabelle.
this includes mathematics proofs and verification of software and hardware systems.
the pisa dataset comes with a split of theorems for the training validation test sets which we follow in this work as well.
for the test set prior work randomly chose theorems from the test set to report their results on.
we report our results onthe complete test set.
some entries in the dataset are not proper theorems starting with the keyword lemmas instead of lemma which we filter out as did prior work.
this leaves us with a total of theorems in our test set originally theorems .
it is worth noting that as with any llm based work there is the potential for proofs from the test set to have leaked into the llm pretraining data.
minerva was trained on a dataset consisting of scientific papers from the arxiv preprint server and web pages that include mathematical expressions .
while the pretraining data for the minerva llm at the base of our models does not include the pisa dataset it does contain code that may include some isabelle hol proofs found in pisa.
this should be kept in mind when interpreting the results.
.
rq1 how effective are llms at generating whole proofs?
we aligned our methodology with the methodology described in thor to enable a comparison between various methods.
the thor paper includes informative baselines for the pisa benchmark including sledgehammer a method relying on heuristic search and a language model approach using search.
sledgehammer and the search based language model approach achieve .
and .
respectively.
in comparison our naive proof generation approach with an 8b language model achieves a proof rate of .
with samples and of .
with samples.
the comparison is even more favorable if we consider the other variants of baldur which achieve a proof rate of up to .
.
we observe that the comparison depends on the computational cost that we spend during inference.
while comparing the cost required for the two methods is involved one measure we can use is the amount of computational resources reserved during proof generation.
for a single proof the language model approach using search requires a tpuv3 with cores for seconds 2while our methodology also requires a tpuv3 with cores for around seconds to sample proofs a difference of factor .
this argument disregards the time spent on proof checking which is intentional proof checking is done on cpus which is cheap compared to time spent on tpus.
so disentangling these two workloads can lead to significant reductions in computational cost.
ra1 these results demonstrate that llms can generate full proofs just as well as smaller language models augmented with a search strategy.
.
rq2 can llms be used to repair proofs?
we trained models for proof generation and repair as detailed in section .
if we sample from the proof generation model once with temperature collect the failed proofs and then repair once with temperature we generate an additional or .
correct proofs.
however in this comparison the generate repair approach uses two samples while the generate approach has only one sample.
for a fair comparison we have to compare the repair approach to the generate approach with additional inference attempts.
2jiang et al.
state in section .
that problems take around tpu hours.
1234baldur whole proof generation and repair with large language models esec fse december san francisco ca usa .
.
.
.
.
number of proof attemptsratio of proven theorems generate generate repair generate repair no err msg figure ratio of theorems proven vs inference cost.
model samples samples baldur 8b generate .
.
baldur 8b generate repair .
baldur 8b w context .
.
baldur 62b w context .
.
baldur 8b w context thor .
figure proof rate of different models.
the repair approach uses half the number of samples and then one repair attempt for each sample.
figure plots the proof success rate of the generate approach and the repair approach against the number of proof attempts.
note that the number of samples for the repair approach does not perfectly align with the number of samples for the generate approach.
this is because the generate approach tends to produce multiple copies of the same proofs which we deduplicate before repair and only generate one repair attempt per failed proof attempt.
for each of the number of samples of the generate approach we tune the temperature in the range of .
to .
in increments of .
and we always use temperature for the repair approach.
the repair approach consistently outperforms the plain proof generation model which only uses the theorem statement as input.
to shed some light on what causes the gains we trained another repair model with the same information except without the error message.
figure shows this model s proof success rate it does not surpass the performance of the plain generation model when normalized for inference cost.
this suggests that the information in the error message is crucial for the observed gains of the repair approach.
ra2 llms can be used to repair proofs including their own failed proof attempts boosting overall proving power.
.
rq3 can llms benefit from using the context of the theorem?
figure reports the impact of adding theory file context to our plain generation approach.
at samples the proof rate increases from .
to .
for the same model size.
figure plots the proof success rate of the generation model with and without context against the number of proof attempts.
we observe that the proof generation models with context consistently outperform the plain generation model.
we illustrate the complexity of generated proofs with several examples .
to get a better understanding of where these gains are coming from we inspected randomly sampled examples that the model using context was able to solve but the plain generation model could not.
we determined the lists of problems each model could solve computed their difference and then sampled examples uniformly at random.
for examples that had multiple correct proofs generated by the model we selected one at random.
while the sample size is not large enough to make quantitative judgements it appears that the model frequently makes use of similar proofs in the context.
we observe that for of the examples the model readily copies and adapts proofs that exist in its context.
for another example the model made use of a premise that did not occur in its context which happened to also be used in the ground truth proof but with a different tactic.
in the final example the model found a simpler proof that did not occur like this in the context.
this suggests that the addition of context does not play the same role as premise selection.
ra3 llms can benefit from the context in which the theorem occurred in the theory file both quantitatively by increasing proving power and qualitatively by copying and adapting nearby proofs.
1235esec fse december san francisco ca usa emily first markus n. rabe talia ringer and yuriy brun .
.
.
.
.
.
number of proof attemptsratio of proven theorems generate w context 62b t .
generate w context 8b t .
generate 8b t .
figure ratio of theorems proven vs. inference cost for models with different sizes and temperatures.
.
rq4 does the size of the llm affect proof synthesis effectiveness?
we fine tuned and evaluated the 62b version of minerva on the proof generation task with context.
figure reports that for samples the large model can prove an additional .
over the 8b model resulting in a total proof rate of .
.
for samples the large model can prove an additional .
over the 8b model resulting in a total proof rate of .
.
figure plots the proof success rate of the generation model with context for the 8b model and the 62b model against the number of proof attempts.
we observe that the 62b proof generation model with context outperforms the 8b proof generation model with context.
one caveat here is that we were not able to tune hyperparameters as well due to the higher cost of these experiments so an optimally tuned 62b model may perform even better.
ra4 theorem proving performance improves with the scale of the language model.
.
rq5 how do llms compare to other state of the art proof generation methods?
while comparisons across different neural theorem provers are hard in general we can compare to thor one of the most effective approaches available.
thor also relies on language models but uses smaller models 700m parameters and uses a different kind of proof step as its prediction target.
instead of using the human ground truth proofs thor generates a new training set and aims to solve each proof step by generating a declarative statement which is then solved using sledgehammer.
that is thor disentangles the planning stage of the next proof step which is the specification of the target state using a have statement and premise selection which is done by sledgehammer.
this enables thor to solve a total of of the problems.afp topic test set baldur thor computer science .
.
logic .
.
mathematics .
.
tools .
.
figure proof rate by afp topic classification and the number of theorems in each category.
there are only theorems in total in the test set but the projects these theorems appear in can be covered by multiple topics.
by contrast baldur solves up to .
of the problems.
while there is a significant gap we argue that the means by which the two techniques improve over plain language modeling are largely orthogonal.
figure reports a large gain from to .
when we consider the union of baldur and thor which supports this hypothesis.
additionally we find that an ensemble of different fine tuned baldur models proves .
.
we compare baldur s and thor s proof rates on different types of problems.
the afp is indexed by four overarching topics computer science logic mathematics and tools.
the authors of individual proof developments self identity which topics their projects fall into.
we use these provided topic labels to determine the categories of problems from our test that baldur and thor can most effectively solve.
figure shows the breakdown of which theorems in the test set fall into which topics and baldur s and thor s proof success rates on these theorems.
in terms of relative performance baldur performs better than thor on problems related to tools and similarly on problems related to logic.
we observe that thor outperforms baldur on problems related to mathematics and computer science.
for mathematics proofs we hypothesize that premise selection may be particularly useful and thor s use of sledgehammer is likely what gives it a leg up on solving these mathematics problems.
overall we observe some complementarity in baldur s and thor s 1236baldur whole proof generation and repair with large language models esec fse december san francisco ca usa effectiveness on problems of different topics though future work should examine this complementarity in more depth.
ra5 our findings suggest that llm based methods and search based methods are complementary and together can lead to large gains in proving power.
discussion what s next?
our evaluation shows that llms can generate whole proofs at once and can repair their own mistakes forming the basis for an effective and simple approach to proof synthesis.
moving forward we find three directions particularly promising integrating proof generation and proof repair models into a newlearnable proof search strategy investigating alternative data splits corresponding to different goals and evaluating these techniques across different proof assistants .
learnable proof search.
while our generate repair approach to proof synthesis lets us avoid costly proof search procedures it also lends itself to a new proof search strategy.
the search strategy would work as follows use the generation model to sample candidate proofs use the repair model to attempt to repair those proofs and continue to use the repair model to repair the repair modelgenerated attempts from .
this paves the way for a learnable proof search strategy.
we demonstrate a proof of concept of this new proof search strategy.
we sample once using the generation model repair the generated sample using the repair model and repair the repair model s attempt using the repair model.
when using both models we sample with temperature .
so the inference cost in this setup is for the first generation for the first repair and for the second repair .
the generate repair approach with inference cost of proves .
of the test set theorems.
with a second repair attempt it proves an additional .
for a total of .
.
the generation approach with inference cost of proves .
which is .
less than the second repair attempt for the same inference cost.
to make this a more viable proof search strategy future work needs to focus on generating proof repair training data that better mirrors the required changes for the subsequent repair attempts.
when proof checking the resulting error message is for the first occurring error typically from the first couple of lines of the predicted proof.
so the proof repair model will only learn to address these types of errors.
an alternative approach could be for example to take the training examples from the proof generation model and use the first few lines of the human written ground truth proof as aproof prefix .
we could then concatenate this proof prefix to the end of the input.
since it is a decoder only model we can simply sample the model s attempt at the rest of the proof.
if the proof prefix concatenated with the rest of the proof does not check then that can serve as a new training example for the proof repair model.alternative data splits.
the pisa benchmark that we use to evaluate our approach commits to a particular data split between training data and testing data.
it is interesting to note however that different data splits may themselves correspond to different goals even fixing the same evaluation task and metric.
moving forward it may be useful to consider different kinds of data splits corresponding to different goals even fixing the same dataset and benchmark suite.
here we consider two different splits theoremwise andproject wise .
pisa uses a random theorem wise split of the theorems appearing the afp.
this means that for any theorem in the test set the theorems and the corresponding proofs that appear before or after that theorem may be in the training set.
this split is useful to evaluate since a forward looking goal of neural theorem prover researchers is to integrate these tools directly into proof assistants where they could make use of the full project context.
that project context may include human written proofs of nearby theorems that look similar or even identical to one another automatically repurposing and adapting those proofs can be quite fruitful.
by contrast with pisa coqgym the neural theorem prover benchmark suite for the coq proof assistant uses a project wise split where training and testing data come from entirely different projects.
this is useful when the goal is to help proof engineers who start completely new projects and want an automated proof synthesis tool to prove as much as it can.
a tool that is trained and evaluated in a setting where it expects that it has seen proofs in a given proof development as may happen with a theorem wise split may not perform as well in this new setting.
explicit consideration for the data split and the goals it achieves may help drive neural theorem proving research even further.
different proof assistants.
to make better sense of new strides in neural theorem proving it makes sense to evaluate the same techniques across many different proof assistants.
but this remains challenging.
consider once again the problem of data splits since prover developments that evaluate on coqgym follow the same project wise split as coqgym it can be hard to make sense of how those developments compare to those trained and evaluated using theorem wise data splits like our own baldur.
we used an established benchmark of isabelle hol proofs to fairly compare baldur to prior work and to increase the chances that our results generalize.
however we observed that search based proof synthesis tools for other proof assistants tend to prove a smaller fraction of theorems than we have found in our work.
for example diva the current state of the art for the coq proof assistant proves .
of its benchmark automatically.
this could be a reflection of size and quality of the available training data or the complexity of the available evaluation data which by necessity is different from what we use because it involves theorems and proofs in different languages or a more fundamental difference in the complexity of synthesizing proofs in these respective languages.
future work should allow for direct comparisons by porting the developed techniques across proof assistants.
cross proof assistant benchmark suites may help substantially with this but still have their limitations.
for example minif2f implements the same benchmark suite for math olympiad problems across many different proof assistants.
but math problems are not evenly represented 1237esec fse december san francisco ca usa emily first markus n. rabe talia ringer and yuriy brun across proof assistants which draw different user communities with different emphases.
fair comparisons between proof assistants are hard but we do believe they are necessary.
related work existing methods for automating formal theorem proving can be classified into two categories hammers and search based methods.
hammers such as coqhammer and sledgehammer iteratively use a set of precomputed mathematical facts to attempt to hammer out a proof.
while hammers are powerful they lack the ability to employ certain tactics such as induction preventing them from proving certain large classes of theorems.
search based methods use a prediction model that given some information about a partially written proof the target theorem being proven and the current proof state predicts a set of next likely proof steps.
the methods then use metaheuristic search to attempt to synthesize a proof.
they iterate querying the prediction model for the likely next steps and using the proof assistant to get feedback on those steps and prune non promising paths generating a search tree of possible proofs.
the proof assistant also determines when the proof is complete.
the tools mostly differ in the prediction model they use which are typically learned automatically.
for example astactic uses only the proof state tactok uses the proof state and the partially written proof script diva which combines the use of many models also uses the proof term and passport also uses identifier information .
other search based techniques include tactician proverbot9001 and gamepad for coq tactictoe for hol4 and deephol for hol light.
prior work has found that hammers and search based methods are complementary each often proving theorems the other cannot though effective user interfaces are needed to help proof engineers use these tools .
thor combines a search based method with a hammer using both a prediction model and sledgehammer in its search.
by contrast our baldur uses an llm to generate an entire proof at once and then to one shot repair it.
the most closely related work to ours is lisa which finetunes a pretrained language model on a large isabelle hol proof corpus and uses it inside of a search procedure to predict proof steps.
gpt f likewise combines a generative language model with proof search to target the metamath proof language.
a montecarlo tree search approach outperforms gpt f in lean .
tacticzero learns not just tactics but also proof search strategies for end to end proof synthesis rather than relying on a single fixed proof search strategy like other neural theorem proving approaches.
the approach works by way of deep reinforcement learning and improves over the previous state of the art on a benchmark for the hol4 theorem prover.
a related problem to neural theorem proving is autoformalization the automatic translation of natural language specifications and proofs into formal machine checkable specifications and proofs.
llms have shown promise for autoformalization of specifications and automatically generated proofs of the resulting autoformalized specifications have been used to improve a neural theorem prover on a widely used benchmark suite in isabelle hol .
proofnet introduces a dataset and benchmark suite for autoformalization in lean based on undergraduate mathematics andshows preliminary promising results autoformalizing proofs on that benchmark using codex with few shot learning.
autoformalization of both theorems and proofs in coq shows promise on a small preliminary benchmark suite .
autoformalization for specification logics in verification is also promising .
the draft sketch and prove method dsp presents a hybrid between theorem proving and autoformalization which similar to our approach makes use of llms for theorem proving.
it provides informal proofs as drafts for the llm to translate into a formal proof sketch which is then proven via sledgehammer.
in contrast we use fine tuning for llms do not make use of sledgehammer and do not rely on the availability of natural language proofs.
pretrained language models can be used to answer naturallanguage mathematics questions .
large language models such as minerva and palm have been evaluated on natural language mathematics benchmarks such as gsm8k and math .
the proofnet benchmark suite mentioned above includes informal proofs alongside formal proofs as a benchmark.
we introduce the proof repair task with error messages.
this is a new machine learning task for formal proofs.
we show that solving this task improves neural theorem proving performance.
proof engineers perform proof repair constantly during formal proof development .
automating this task first arose with the advent of symbolic tools for automatic proof repair in the coq proof assistant and has since made its way into tools for other proof systems .
our work is among the first to explore proof repair in a machine learning context and the first we are aware of to use error messages for a proof repair task and to use repair to improve performance of proof synthesis.
there are numerous other tasks that machine learning tools for proofs consider that may either help users with proof development directly or improve neural theorem proving performance themselves.
for example pamper predicts proof methods alongside explanations in isabelle hol.
acl2 ml generates helper lemmas and suggests similar theorems in acl2.
other popular proofrelated tasks leveraging machine learning include premise selection and datatype alignment .
nonfunctional and data centered properties can also benefit from formal verification but more research is necessary both on manual and automated approaches to verifying such properties.
probabilistic verification has successfully provided guarantees for such properties for machine learning systems .
our approach can help minimize human effort in formal verification by automatically synthesizing proofs for some theorems.
other tools that assist humans writing formal verification proofs can similarly save time and can be complementary to our work for theorems baldur cannot prove fully automatically.
icoq and its parallelized version picoq find failing proof scripts in evolving projects by prioritizing proof scripts affected by a revision.
icoq tracks fine grained dependencies between coq definitions propositions and proof scripts to narrow down the potentially affected proof scripts.
quickchick a random testing tool for coq searches for counterexamples to executable theorems helping a programmer to become more confident that a theorem is correct.
roosterize can suggest names for lemmas and language models can also help automatically format proofs both improving readability and maintainability.
mutation analysis can identify 1238baldur whole proof generation and repair with large language models esec fse december san francisco ca usa weak specifications when mutating definitions does not break their proofs .
the mutation operators could hypothetically be applied in repair and in providing feedback for developers as to why a proof has broken.
the automated program repair field studies the task of taking a program with a bug evidenced by one or more failing tests and automatically producing a modified version of the program that passes all the tests .
generate and validate repair techniques use search based techniques or predefined templates to generate many syntactic candidate patches validating them against the tests e.g.
genprog prophet ae hdrepair errdoc jaid qlose and par ssfix capgen simfix hercules recoder among others .
techniques such as deepfix and elixir use learned models to predict erroneous program locations as well as the patches.
it is possible to learn how to repair errors together by learning how to create errors which can increase the amount of available training data but poses an additional challenge of learning to approximate making human like errors .
unfortunately these automated program repair techniques often overfit to the available tests and produce patches that while passing all the tests fail to encode the developers intent .
improving the quality of the resulting repairs can be done via improving fault localization strategies patch generation algorithms e.g.
heuristic based constraintbased and learning based and patch validation methodologies .
by contrast in baldur s domain of theorem proving it is impossible to produce a proof that appears to prove the theorems but actually fails to do so because the theorem prover acts as an absolute oracle for the correctness of the proof.
as a result it may be more difficult to produce a proof in the first place but if techniques in this domain do produce proofs they are guaranteed to be correct.
contributions this paper is the first to fine tune large language models to generate entire proofs of theorems without the need for proof search or hammers.
we demonstrate that this approach is more effective and more efficient than prior methods that use one step at a time search based generation and that it is complementary to existing search based and hammer based approaches together our baldur and prior tools can fully automatically synthesize proofs for .
of the theorems in a large isabelle hol benchmark establishing a new state of the art.
we further demonstrate that generate andrepair improves proof synthesis when the language model is given access to the error messages produced by erroneous proofs.
this work opens new avenues of research into using llms to automate theorem proving and simplify formal verification of software properties repair approaches both for proofs and potentially more traditional automated program repair tasks and the use of context e.g.
failed synthesis attempts and error messages in proof generation.
our very encouraging results suggest a bright future for automated proof generation and repair using llms.data availability this work uses t5x which is publicly available google research t5x.
the scripts to launch training and inference and to process the training data and results rely on proprietary google infrastructure which inhibits us from publicly releasing the code.
our evaluation uses the pisa codebase and dataset which are also publicly available