patching as translation the data and the metaphor yangruibo ding columbia university yangruibo.ding columbia.edubaishakhi ray columbia university rayb cs.columbia.edu premkumar devanbu university of california davis ptdevanbu ucdavis.eduvincent j. hellendoorn university of california davis vhellendoorn ucdavis.edu abstract machine learning models from other fields like computational linguistics have been transplanted to software engineering tasks oftenquitesuccessfully.yetatransplantedmodel sinitialsuccessat agiventaskdoesnotnecessarilymeanitiswell suitedforthetask.
in this work we examine a common example of this phenomenon the conceit that software patching is like language translation .
wedemonstrateempiricallythattherearesubtle butcriticaldistinctions between sequence to sequence models and translation model while program repair benefits greatly from the former generalmodelingarchitecture itactuallysuffersfromdesigndecisions built into the latter both in terms of translation accuracy and diversity.giventhesefindings wedemonstratehowamoreprincipled approach to model design based on our empirical findings andgeneralknowledgeofsoftwaredevelopment canleadtobettersolutions.ourfindingsalsolendstrongsupporttotherecent trend towardssynthesizing editsof codeconditional onthe buggy context to repair bugs.
we implement such models ourselves as proof of concept toolsand empiricallyconfirm thatthey behave inafundamentallydifferent moreeffectivewaythanthestudied translation based architectures.
overall our results demonstrate the merit of studying the intricacies of machine learned models in software engineering not only can this help elucidate potential issuesthatmaybeovershadowedbyincreasesinaccuracy itcan also help innovate on these models to raise the state of the art further.
we will publicly release our replication data and materials at ccs concepts softwareanditsengineering softwaremaintenancetools .
keywords neural machine translation big code sequence to sequence model automated program repair permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
reference format yangruibo ding baishakhi ray premkumar devanbu and vincent j. hellendoorn.
.patchingastranslation thedataandthemetaphor.in 35th ieee acminternationalconferenceonautomatedsoftwareengineering ase september21 virtualevent australia.
acm newyork ny usa pages.
introduction recentworkhasappliedawidevarietyofmachinelearningmodels to practical software engineering tasks including code completion automatedprogramrepair andcodecommentgeneration.these models excel at learning general patterns from large amounts of diverse data even when training data is relatively unstructured.
this combination enables one to simply transplant successful models from related fields e.g.
from computational linguistics to software engineering.yet evenifthesemodelsprovidereasonableperformance the transplanted model may still not be appropriate for the task manyofthesemodelsweredesignedforparadigmsthatdiffer subtly yet significantly.
in this work we conduct a systematic empirical case study to illustrate how transplanted models can fail in the targeted task domain focusing specifically on the concept of patching as translation as a typical example of this phenomenon.
a range of recent work has adopted neural machine translation nmt models to learn to repair programs by translating the buggy code to the repairedcode .wearguethattherearethreegeneral concernswiththistypeofapproach andshowconcretelyhowthese manifest in patching as translation through empirical analysis task design deep learning dl models transform their inputs intoacompactsetoffeaturesthatstorestheimportantinformation which it then uses to produce the required target.
a wide range ofdlarchitectureshavebeenproposedthatdoso butregardless of the specific architecture or task it is self evident that all the relevant information needed to generate the target must alreadyexist in the input.
while that is largely a fair assumption for naturallanguagetranslation wherewecanassumethattheinput output sentences express the same idea it is questionable for source code repair we show evidence that buggy fragments often lack the information required to repair them.
reliably choosing the correct repair may even be impossible without access to a very broadcontext includingsurroundingfiles intheabsenceofwhich this task is inevitably ambiguous for many real world bugs.
architecturaldesign givenataskwheredeeplearningisfeasible onemustchooseamodelarchitecturethatsupportsthetransformation from input to output in as realistic and simple a manner aspossible.thisisdonebyensuringthatpriorknowledgeofthe 35th ieee acm international conference on automated software engineering ase task including dependencies and structural properties are built intothemodeldesign.architecturesformachinetranslationrely heavily on the auto regressive nature of text language is generally producedoneword ortoken atthetimeinleft to rightmanner e.g.
inspeechorwriting thestandardnmtencoder decoderarchitecturegeneratestranslationscorrespondingly.whilethisworks very well for nmt its relevance to practical program repair is tenuousatbest empirically manyrepairsjustcopy nearlyall tokens fromthebuggyline withveryfewchangedtokens oftenjustone .
assuch bothbugandpatchsharealargeidenticalprefix butthe differenceinthesubsequenttokensiscrucial.wedemonstratethat models struggle to predict this transition as the large amount of copying distracts and inflates the training quality signal.
objective design finally models are trained by computing a loss fortheirpredictionsrelativetoa gold output usinga lossfunction.
this function is usually a differentiable proxy for the actual quality of themodel because suchqualitative assessments tendnot to be differentiable.inmachinetranslation thetraininglossisusually basedontheprobabilitiesofthecorrecttoken theactualquality of the trained model is measured with bleu scores or the like that measure overlap between the generated and ground truth translation.
however such overlap measures are inappropriate for program repair.
for reasons stated earlier few token changes lack ofcontextualinformation thequalityofaproducedrepairoften correlates very poorly with the number and placement of tokens itshareswiththedesiredoutput.forinstance thetrainedmodel emits many syntactically incorrect repairs as well as many verysimilar patches for a given bug rather than exploring a range of alternatives.thisyieldspoorperformanceinasearch basedsetting in which they are popularly used given failing test cases .
having studied these concerns empirically we provide strong evidence that the metaphor of patching as translation is inap propriate for the task.
on the other hand our results shed lighton the effectiveness of a recent competing approach repairingbugs by synthesizing editsof code conditional on the buggy context .notonlydoesthisapproachagreebetterwithour empirical findings we implemented a general form of this architecture which predicts insertions and deletions relative to the bug rather than theentire patch and showthat this change inparticular produces a model that both generates the correct patch moreoften and provides better sampling behavior i.e.
higher top k prediction accuracy using beam search .
we also implemented a generic contextual extension compatible with both models to assesswhetherourempiricalfindingsoftheimportanceofcontext couldbeintegratedeasilyintothemodels thisenhancementprovedlesseffective likelybecauseexistingmodelsstruggletomodellarge windows of context.
this finding highlights that empirically observing an issue and effectively addressing it are not always the same we leave this challenge for future work.
background to study the transplanting of architectures from neural machine translation nmt to automated program repair we need to un derstand both domains.
in this section we first discuss nmt its conceptualneeds and correspondingarchitectural designs andthen automated program repair its empirical characteristics and practical use.
.
neural machine translation nmt aims to convert an expression in a source language e.g.
english into a semantically equivalent expression in a target language e.g.
french .
this is generally both quite feasible and fairly deterministic a given english sentence almost certainly has a frenchtranslationthatisbothagoodfrenchexpressiononitsownand preservesalltheinformationintheoriginalenglishexpression i.e.
it could be translated back to a comparable english phrase .
a natural fit for this task is the encoder decoder architecture whichconsistsoftwocomponents anencoderthatlearnstocompactly encode the important information from the source language expression and2 adecoder whichtransformsthatinformationinto anequivalentexpressioninthetargetlanguage.encoder decoder stylemodelscanaddressmanytypesoftransformationsbetween two domains e.g.
from image to textual description and are typically instantiated with specific encoder decoder architectures for a givenproblemthatreflectsomeknowledgeaboutthatproblem s domain.thissimplifiestheotherwisecomplextaskofrepresenting andproducingaverywiderangeofinputs.forexample incomputational linguistics sequence to sequence seq2seq models are a well establishedway to generate text one tokenat a time in aleft to rightmanner thislinearorderreflectshowlanguageis often generated in speech and writing.
seq2seq models exploit this structure by both representing and generating expressions witha strong emphasis on the left to right relations between tokens especially in the decoder component which in nearly all popular models produces tokens auto regressively meaning that tokens are produced one by one and every previously generated token is fed back to the model to produce the next token.
practical seq2seq models.
seq2seq models have achieved great success in the nmt field.
recurrent neural networks rnns were popularfor manyyears buthad difficultiesin rememberinglongterm dependencies.
in an rnn all the information of a source sentence is encoded into a hidden state from left to right the final hidden state is then passed to the decoder which attempts to re construct the target expression from this information.
this puts inordinate strain on that single hidden state which tends to cause themodeltoforgettokensseenlongago.longshort termmemory lstm and gated recurrentunit gru wereintroduced to mitigate this bottle neck by better separating long term and tokenspecificinformation anddidsignificantlyimprovetheperformance of rnn based nmt models but ultimately suffered from the same concerns.attention basedmechanisms wereintroducedtoallowthedecoderto attend toanygivenintermediatestatefromthe encoder ratherthanonlythefinalone whichgreatlyimproved performance.mostrecently thetransformer modelgeneralizedthisideatorelyingentirelyonattentionmechanismstoboth encodeinputsandgenerateoutputs.thetransformermodelproposes multi headed self attention interspersed with feed forward networksthatenablesbothencoderanddecodertoattendtoany 1inpractice contextissometimesrequired e.g.
todetermineifanexpressionismeant sarcastically which may alter its translation.
there can also be multiple valid translationsforoneexpression e.g.
literalvs.idiomatic .evenso generatedtranslationsthat overlap strongly with the ground truth are rated highly by human translators .
276set of tokens across arbitrarily long distances.
these models are also highly parallelizable.
we adopt this model in our work.
seq2seqmodelsinse.
hindleet al.observedthatsourcecodeis natural viz.
withstronglocaldependenciessimilartonatural languages likeenglish.
many languagemodels have beenapplied to software engineering tasks.
more recently this includes a range of applications of the seq2seqarchitecture in modeling source code.
existingworkhasexploitedtheirpotentialinseveralsetasks such ascodesummarization codemigration andprogramrepair .
the prevalent approach is to treat source code as a sequence of tokens with implicit or explicit structures e.g.
abstract syntax trees .
the encoder learns the distribution of such structured language which is then translated into the target domain either program languages pl or natural languages nl .existingworkhasespeciallyspenteffortonlearningmeaningfulcoderepresentationstoadapt seq2seqmodelsforsourcecode modeling these approaches focus on encoding rich characteristics of programming languages besides just the tokens.
our worktakesamoregeneralview andaimstostudythefeasibility ofconstructinggoodtoolsbasedonboththeproblemdesignand generalmodelarchitecture whichislargelyorthogonaltolearningbetterrepresentationsofcode.werecognizethatresearchesinboth directions arenecessaryand significantto improvethe efficacyof deep learning approaches in se.
relevance of models to tasks.
all these models excel at learning generalizablepatternsfromlargeamountsofdiversedataandare prima facie at least somewhat applicable to source code to the extent that it reflects natural language characteristics.
however different tasks come with their own concepts and peculiarities andthemodelsshouldreflectthephenomenaspecifictothetask.
for example code summarization and code migration are more likenltranslationtasks sinceboththeirgoalsaretoencodeand preserve the semantics of their inputs code fragments just in different vocabularies concise natural language and another code contextrespectively .ontheotherhand softwareengineersbehave differently when repairing a program.
developers tend to fix abuggy fragment by making minor changes rather than entirelyrewriting it.
furthermore the semantics of the buggy fragmentare by definition not preserved the express goal is to introducesemantically new content and possibly remove some so as to changethemeaningofafragment.noneofthisdisqualifiestheuseofseq2seqmodels perse butitsbuilt inassumptionsshouldatleast be carefully evaluated empirically and if necessary its application should be changed to better reflect the domain.
.
automated program repair automated program repair apr is a task of keen interest in se.
theaimistofixsoftwarebugswithminimalhumanintervention.
classic apr techniques can be categorized into generate andvalidate g v or2 synthesis basedapproaches .
g vapproachesautomaticallygeneratepatchesandvalidatethe candidatesusingasetoftestcasesthatrevealsthebug.togenerate fixes oneeffectiveapproachistomutate e.g.
insert replace the buggycodeaccording tocodesnippetsin thecurrentprojectthat occurinsimilarcontexts .synthesis basedapproachescreate constraints that satisfy all test cases and then solve them and produce patches from the solutions.nmt for apr.. tufanoet al.
proposed to use machine translation to repair programs and empirically studied the feasibility of translating buggy programs into fixed ones.
they applied multilayerrnnswitheitherlstmorgrunodestopredictpatchesofab stracted realbugs andreportpromisingperformance.chen et al.
subsequentlyintroduced sequencer anend to endframeworktorepairone linejavabugs.theyusednmtmodelstolearntheimplicit bug repairpatternsbytrainingthemodelwith35kbug fixpairs.
besidesthebuggylineitself theyalsoconsideredcodecontextto allowlong rangedependenciesinfixes theyincludetheentireclassinwhichthebugislocated whichthey abstracttoreducetheinput size.
codit developed a tree based nmt model to produce codeeditsandbugfixes.itfirsttranslatesthetreestructureofcode andutilizesthestructuralinformationtoassistthegenerationof codetokens.coditalsoincludesthetreenodesaroundthebugas contexttopredictmeaningfulpatches.coconut ensembles multiple nmt models to capture diverse fix patterns.
the authors argue that incorporating context is essential for fixing bugs so theyapplyaseparateencoderspecificallyforlearningthebuggy context.
although these existing works apply a wide range of models theyalltreatprogramrepairasatranslationtask thesetoolsencode a limited program window around a bug and learn to transform ittorepairedcodebasedonhistoricalrepairs.thepremiseisthat translation is both a suitable model and that the buggy code with its context provides sufficient information to succeed.
it is thus pasttimetoaskthefollowing high levelquestion whichhasyet to be addressed from an empirical perspective rq1.is it generally feasible to translate buggy programs to repairs?
while these approaches all indicate that seq2seq learning holds promise for learning patterns of transformation between bugs and patches they struggle to outperform many g v tools that applied human designed rules to fix defects.
one explanation is that the searchspaceofrepairsisprohibitivelylarge amongothersdue tothelargeandhighlylocalvocabularyandpatternsendemicto software as well as the length of buggy fragments.
intuitively however the search space need not be so large at all in real world development modifications made to code during repair are mostly small limitedtoafewtokensratherthancompletelyreconstructingastatement.istherelianceontranslationputtingmodelsata disadvantage by artificially expanding the search space?
it is again worth determining this empirically by asking rq2.do machine translation architectures mischaracterize realworld fixing behavior and does this disadvantage their performance?
deep learning for apr.. besides translating buggy code to fix it recent work has proposed deep learning models that learn to specify the buggy locations that need to be modified together with theeditstobemade.deepfix implementeda seq2seqattention networktofixcompilererrors.asinput theprogramisrepresentedasasequenceof linenumber tokens pairs andthemodelpredictsasingle buggylinenumber patch pairasarepair.vasic et al.
proposed using pointer networks to jointly learn to localize and repair a specific class of bugs known as varmisuses.
their networkjointlypredictstwooutput heads onetolocatethebuggy token and one for its replacement.
tarlow et al.
introduced 277an edit based model called graph2diff that uses a graph neural networkasanencoderandatransformerasdecoder.thismodel transformsaprogramgraphintoa tocoposequenceofastedits that transform the buggy program into a repaired version.
by directly learning the locations of incorrect tokens and the edits to be made these edit based methods provide an approach to learning bug fixing with a very different loss which is not trivially reduced by maximizing the token overlap between the bug and repair.theimpactofthislosscanbesubstantialindeterminingthe kind androbustness oflocalminimathattheneuralnetworkfinds during training.
we thus implement a simple version of this model ourselves to empirically study the impact of the objective function on both our baseline model and this edit based model.
this way we study the impact of the objective function on the models by studying the models results themselves asking the following rq3.how well does the nmt objective function apply to automated program repair?
methodology thegoalofthisworkistoprovideanempiricalandconceptualanalysis of the relevance of deep learning models originally developed fornmt insecontexts.assuch weemphasizethatitis notourgoal toproduceastate of the artbugdetector orreplicatepriorwork.
rather we identify a general representative approach seq2seq for program repair that reflects a direct adoption of models from a related fieldtose tasks and studyits limitations.naturally prior workhascoveredawiderangeofapplicationsandmodificationsof this method and may be immune to some of our findings but this does not discount the general result of our analysis that adapting deep learning models designed for other fields to se requires a principled empirically and conceptually grounded approach.
.
scope concretely we focus on a relatively simple form of automatedprogram repair in which we translate a given buggy line to its repairedcounter part.wethusassumethatwehavethebugalready localized and that it is confined to exactly a single line.
this is the mostdirectformof repairastranslation inwhichanoff the shelf translationmodelisusedontwosoftware sentences thebuggy version and the repair.
.
data we collect our bugs from the history of the most starred javarepositoriesongithubonmarch30th .weanalyzedeach project s entire commit history and extracted any commits that altered precisely a single line in a single java file disregarding any spurious changes to whitespace.
we then compared the corresponding commit messages against a relatively simple keywordbased check to heuristically find commits labeled as e.g.
fix or bug .
we note that although this heuristic is not particularly precise the characteristics we found in our data were very similar betweenthosemarkedasfixesandotherone linechanges soweex pectthistohavelittleimpactonouranalysis.thisprocessresulted inca.
bug fixes across projects in our dataset.
in the course of our analysis of this data we manually checked a number of the collected samples and confirmed that the vast majority of these were indeed bug fixes.
.
experiment setup given the collected dataset we first analyze the characteristics ofrealfixesandthentrainnmtmodelsonthesesamplestopredict patches.
to answer our research questions we study bothcharacteristics of the real world bug fixing behaviors and of the model generated patches.
.
.
bug context.
wedesignexperimentstoexploretheimportance of a bug s lexical context when fixing defects.
in natural languages context thetextsurroundinganexpression hasadirect effect on the way people understand a specific expression and can helpavoidambiguityincommunication.similarly thecontextof a buggy line is the code surrounding as in both preceding and succeeding the bug.
this can variously be chosen to include up to nlines of code above and below the bug the surrounding function oreventhewholefile orproject .thiscontextcanprovide vital information e.g.
variable definitions conditional statements for understanding the defect and the necessary repair.
we studythe role of variously sized contexts for both disambiguation and providing necessary information in section .
.
.
.
similarity analysis.
wenotedearlierthatsoftwareengineers tend to make small changes when fixing defects likely because bugs correspond to only minor flaws in the code and perhaps also becausemakingfewchangesreducestheriskofintroducingnew bugs.
given this we evaluate the similarity between real bugs and patches empirically across three similarity metrics i editdistance precisely levenshteindistance isametricthat quantifiesthedifferenceoftwosequencesbytheminimumnumberofedits deletions insertions orsubstitutions requiredtotransform one into the other.
ii jaccard similarity effectively intersection over union calculatesthe ratioof overlapping n gramsbetweentwo sequencesdivided by their union.
jaccard similarity is usually just applied totoken levelsimilarity weextendittotheaverageof1through gram overlap to better capture both token level similarity and matches in their ordering.
iii bilingual evaluation understudy bleu is popularly used to evaluatethequalityofmachinetranslations andisconsideredto have a high correlation with human assessments of similarity .
thisisanasymmetricmeasurethatcaptureshowsimilarthemodel prediction the hypothesis is to the ground truth translation the reference .bleualsocountsn grammatchesbetweenthe prediction and the ground truth and normalizes these w.r.t.
the predicted sequence length which causes the asymmetry.
these three metrics above will be used frequently across our analysis to measure similarity in different aspects.
.
.
model training and bpe.
toinspecttheperformanceofnmt modelsonprogramrepair wetrainedandevaluatedavanillatransformermodel onourdataset.wesplitthewholedatasetinto three parts across organizations train valid test with a ratio of .
we trained the model on the ca.
55k bugs in the train set optimized it for held out performance relative to the valid set and finally evaluated the performance on test set.
inthefieldofnaturallanguageprocessing byte pair encoding bpe is a widely used method to encode rare and long words intofrequentsub tokens thisway tokensthatwerenotseeninfull 278duringtrainingcanstillbepredictedaccuratelyatinferencetime.
bpe splits a word e.g.
coding into a list of more frequent subtokens e.g.
.inprogramminglanguages vocabulary innovationisevenmorerampant asdeveloperstendtonameavariableormethodusingacombinationofwords e.g.
isnullorempty .
karampatsis et al.
show that bpe can effectively address thisissueinbigcodeapplications soweapplythistoourmodel input as and predictions as well.
analysis in this section we empirically analyse the characteristics of program repair on real world bug fixes with a joint focus on the relationtonaturallanguagetranslationandonfactorsthatinfluence accuracy for program repair.
in particular we study the character istics of the ground truth data i.e.
the real bugs and patches with their context and of the patches generated by our nmt model as trainedinsection3.
.
.fortherestofthissection wescrutinize theadequacyoftranslationasaparadigminsection4.
weidentify architectural concerns in section .
and we quantify their impact on model performance in section .
.
.
task design translation asatask isintendedtofacilitatecommunicationacross languagebarriers.hence bydesign itmustpreservethesemantics between the source language and the target language any translationthatchangesthesemanticsisunacceptable.bycontrast in programrepair thesemanticsofsourceandtargetaremeanttodiffer as the buggy version contains incorrect program behavior that thefixedversionissupposedtocorrect.todothat engineersdeliberatelychange add delete replace incorrecttokenswithcorrect ones.
imitating such changes with a machine learner is non trivial especially since the learner usually only has access to the bug and the fix but not the knowledge latent in the developer s mind to reason about the fix.
for instance the fix may introduce de novo tokens that are not inthebuggylines e.g.
anewapicall.insuchcases amodelhas to learn to pick those new tokens from across its entire known vocabulary.
if the replacement is a common fix pattern this might beeasyenoughtolearn otherwise thisleadstoavastsearchspaceofcandidaterepairs.thelattercaseiscommonenough developers oftenusemethodsoftheirowncreation definedinadjacentfiles or string or numerical patterns specific only to that project.
it isthus important to quantify even just approximately how much informationthe modelneedsand howmuchit hasaccessto from its training data which tends to comprise the buggy lines and anoptionalwindowofcodecontext.althoughitisnon trivialto inspect the learned black box model and extract what it infers about a given buggy line we can identify the gapsbetween what program repair needs and what machine translation can supply .
.
.
program repair needs lots of contextual information.
patches that introduce newvocabulary relative to thebuggy line require the model to conjure up novel tokens ex nihilo.
given that code vocabulary is highly diverse and often strongly specific to a given project packageandfile may require an unreasonable level of ingenuity from the model.
table1quantifiesthis first nearly90 ofpatchesintroducenew vocabulary relative to their buggy source which is true regardlessofsub tokenization evenusingthebpeapproach .furthermore thesearenotatalljusttypicalprogramtokensorlocalvariables wepairedthebuggylinewithincreasingwindowsofcontext explained in section .
.
and find that the unseen tokens introduced by the patch are still rarely borrowed from any immediate buggy context theyaresometimespresentinthefileasawhole butinlocations farawayfromthebug.nearbytokensareabitmorelikelytoshare some sub token s with the patch but rarely provide the entire missinglink.giventhatmodelinglargevolumesofcode i.e.
many hundreds or thousands of tokens at once is often prohibitively expensiveforcurrentdeeplearnedmodels thiscanseriouslyaffect models that incorporate only modest levels of context such as the surrounding few lines or function.
table1 ratioofpatcheswithnewvocabularyrelativetothebuggy snippet given a context window that ranges from none i.e.
only buggy lines a typical translation setting to a given number of lines symmetrically around the bug and finally to the entire file.
context includedpatches introducing unseen tokens without bpe with bpe none .
.
lines .
.
lines .
.
whole file .
.
this is not merely a matter of richer training data either a large proportion of project specific tokens are not found in any other projects soitisquiteunlikelythatourmodelwouldhaveseen many of these at training time.
we note that this is in contrast to other paradigms of program repair such many g v models which instead searchfor patches from across many surrounding files ratherthanaimtoencodeacontextdirectlyintoatranslation.
.
.
without context program repair is inherently ambiguous.
as discussed a learner would certainly struggle to capture enough informationfromthebuggyprogramalone.fortunately theselearners are equipped with the capacity to transfer many insights from theirtrainingdatatonewexamples.perhapstheycanpredictthe missing semantic information from those bug repair pairs?
although it is again impossible to quantify what the model can do wealsoagainarguethatitisperfectlysoundtolower boundits potentialbyestimatinghowmuchoftherequisiteinformationit hasaccesstoattrainingtime.concretely thetrainingdatacontainsmany similar bugstothoseseenattesttime whichwewillquan tifyinvariousways sothemodelmightlearnthetransformations that produced patches from those similar bugs and apply the same insight e.g.
topredictthemissingvocabulary.but thisiscontingentonsimilarbugsindeedproducingsimilarrepairs ifrepairsfor similarbugsroutinelydiverge thenthemodelisreasoningabout highly ambiguous data and will have to learn a wide range of valid transformations for a single defect in the training data.
tosimplifythis discussion let bbeabugin theheld outportionofourdatasetand pbethepatchof b.assumingwehadsome oraclethatcanprovide similar bugs b withpatch p forb specificallyfromourtrainingdata wewouldideallyexpectinformationabouttherelativechangeneededtorepair btobetransplantedfrom a intersection over union b bleu figure1 correlationbetweenbugssimilarityandpatchessimilarity.
x axis indicates the bug similar bug similarity and y axis indicates the patch similar patch similarity.
the corresponding countof each grid is normalized on a log scale.
p.ifthatisgenerallytrue thenourmodelcanlearnsimilartransformations for similar bugs and thereby generate new vocabulary and patterns that are not present in the buggy context.
to quantify this we find the top most similar bugs for each bintheheld outusing4 gramjaccardindex seesection3.
.
which we label b1 b2 b3 among bugs in the training data.
we then extract the corresponding pand piand evaluate the patch similarity sim in relation to the bug similarity sim .
to facilitate transferring repair patterns we should hope that similar bugs produce similar repairs.
we visualize this as a heat map figure1 toshowthecorrelationbetweenbugs similarityandpatches similarity foreachgridintheheatmap wecountthenumberof sampleswithsim andsim inthecorrespondingrange andthencolorthegridbasedonthesecounts.tomakethecolor contrast more identifiable we log normalize the counts.
wecalculatethe b bandp psimilarityscoresusingboth thejaccardindexandbleuscores thelatterismoreappropriate for translation because it is asymmetrical capturing the overlap fromtheperspectiveofthetranslationtarget.thisalignswellwith the task s directionality we want to quantify what information is transferred from training to held out not vice versa.
theresult is shown in figure both metrics yield a similar pattern bug similarity only partially correlates with patch similarity.
both graphs show a smeared out pattern in which similar bugs tendto produce patches with typically less similarity rather than astrongly pronounced diagonal that would indicate that patches relatetooneanotherastheirbugsdo.worse manybugshaveonly neighbors with low similarity to begin with.
these lower scorestend to just reflect spurious overlap due to the large portion of closed vocabulary tokens e.g.
brackets keywords insourcecode which is also evident from the main hotspot being at .
.
.
weareparticularlyinterestedinpairsthatsharearelativelylarge number of tokens and patterns i.e.
those with similarity scores greaterthan0.
.forexample thecode private boolean isname false andcode private boolean isname true yield a bleu score of .
and they indeed look alike only differ in boolean value .
if similar bugs are predominantly fixed in similar ways then we should expect that to translate into high patch similarity which would allow the model to copy the appropriate repair patterns.
unfortunately table which breaks down the highlysimilar bugs specifically paints a different picture here too the similaritybetweenbugshasnearlynodiscerniblerelationtothattable similar bugs do not always have similar patches.
bug similar bug bleu samplespatch similar patches bleu .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
of their patches.
even highly similar bugs patches do not score above0.5halfthetime whichisactuallylowerthantheirrespectivebugs.forinstance acommonbuginourdataset log.error e presentswith manydissimilarpatchesincluding log.warn e and log.error can t read settings for tool e .
thebleuscorebetweenthesetwopatchesisjust0.
andwecan tellthat thisbug wasfixed withvery differentintentions.in other words relyingonsimilarbugstotransplantpatchinformationis almost entirely ineffective.
thisdemonstratesasubstantial inherentambiguity inprogram repairbasedonjustabuggyline thoughnotnecessarilytoprogram repair in general for a given bug the learned program repairhistory provides a mixed signal of many candidate repairs withdistinct semantics.
this matches our intuition as well just how agivenfragmentisbuggy andwhatspecificrepairamongmany valid semantic transformations is appropriate depends on a vast array of factors many of which are not enshrined in the code at all e.g.
project requirements developer preferences let alone the buggy line or even function itself.
.
.
the challenges of new vocabulary.
finally it might still be feasible for the model to guess at novel tokens and break theambiguity if they can be constructed fairly obviously from the context e.g.
byapplyingknowntransformationstoexistingones likeconvertingsingulartopluralorincrementingaprovidedinteger.
whereas the former results provided a lower bound on what is feasible it is quite impossible to quantify precisely what the model could do as the patterns learned by its millions of parameters can be highly complex.
so we instead use the model s performanceitself studiedinmoredepthinsection5 asanempiricaldatapoint giventhatitistrainedcarefullyandwithamplecapacity weshould expectthatitprovidesatleastevidenceofthisabilitytoproduce correctnewvocabulary.incontrast westudiedthetrainedmodel s accuracy on our test samples the patch introduced one or moretokensnotpresentinthebuginover75 ofthecases yetthe modelpredictsthisnewvocabularyonly5.
ofthetime.worse many of the new tokens are not even entirely new they may just constitutetheadditionof nullcheck whichthemodelstilldoes notanticipate.evenwhen beam searchingacrossthetop25mostprobable sampled patches the model only anticipates of the requirednewvocabulary.westressthatthisisawell trainedmodel whichwasabletoachievehighaccuracyonitstrainingdataand forwhichweusedthemostgeneralizablecheckpointaftertraining for epochs.
as such machine translation models are already at a serious disadvantage here compared with nlp applications.
this allows us to conclude our investigation of rq1 280the lack of information in the training data vocabulary and immediatecontextmakesrepairingastranslationinitscurrent form largely infeasible.
.
architectural design our second point of concern with translation models for program repair relates to the structural constraints assumed inherent in naturallanguagegeneration thattextisauto regressivelyproduced left to right.
this constraint is built into the translation model s sequence to sequence architectureandimpliesthatasimpleadoption for program repair requires the model to output the entire repair producing the correct token at each point.
theflawwiththisparticulardecisionisdifferentfromtheone in section .
in that it does not affect the feasibility of the task generating the entire repaired line is just as possible as e.g.
generating the change only .
instead architectural mismatches between the model and the task impact the difficulty of training and the corresponding rate and even the ultimate limit of convergenceon test data.
this is because a our models do not have infinite capacity and b stochastic gradient descent is a local optimization thus these modelstend to find alocal minimum that matchesthe signalconveyedbythelossfunction.ifthislossfunctionprioritizesexactrepetitionofmanytokensfromtheinput orastrongreliance on left to right production this may negatively affect the actualquality e.g.
overall accuracy of the ultimate local minimum.
in this section wequantify this effect from thedata statistics in the next weexploreitfurtherbasedonthemodel sconvergentquality.
.
.
the patch preserves most of the tokens in the bug.2bug fixing modificationstocommittedcodeareoftenminor thebuggyline usually is already persea close approximation of the correct code withverysubtle minorflaws.toquantifythisassertion wefirst measurethesimilaritybetweenrealbugsandpatches.weusethree different metrics to evaluate the similarity of each bug patch pairinourdataset outlinedinsection3.
.
token leveleditdistance gram intersection over union whichcontributesadenominatortotoken leveloverlap andfinally meanbleu 4similaritytobalance the overlap between tokens and sequences.
the results are shown in figure .
the average edit distance for thesamplesinourdatasetis3 .
butthedistributionislong tailed sothismeanissomewhatinflatedbythefewlargeedits themediandistance is simply .
of the samples only edit a single token to fix the bug and .
of the samples have an edit distance up to .thus bugfixingmodificationsareoftenlimitedtojustaselect fewtokens.figure2bfurthershowsthatbugsandpatchessharethe majority of their vocabulary as well the average jaccard similarity is0.
andhalfthetimethepatchreservesmorethan80 ofthe bug s tokens.
this overlap extends to sequences of tokensas well the mean bleu score of a patch relative to its bug is .
.
twolines of code are considered very similar when their bleu score is greater than .
so bugs and their patches overlap strongly.
for reference the state of the art results in nmt at this time are ca.
.
dependingon thelanguage pair.
program repairachievesfar 2this result applies to our study of small one line bug fixes this may not hold for larger patches which may be more likely to reconstruct the whole buggy module.table3 proportionofrepairsinwhichthesyntacticstructuresremains unchanged relative to bug both for all samples in our training data and for those in which the patch both does and does notintroduce novel tokens relative to the bug .
setting proportion all bugs .
patches introducing new tokens .
patches without new tokens .
higherperformancebysimplycopyingthebugverbatim yet so would in no way approximate a goodrepair.
thisalsoconfirmsourintuitionthatbugsandpatchesarehighly similar and patches retain most tokens from the buggy version rather than assembling code de novo.
this principally suggests that searchingforthecorrectpatchtokenbytoken fromlefttoright isapooruseofsearchspace asmartprogramrepairtoolshould justpredictwhichtokensaresupposedtobepreservedandfocus onsearchingfortheonesthatrequiremodifications.butisitreally sobadtogeneratetheentirepatch wouldcopyingthepreserved tokens simply prove no concern for the models?
we answer this question in the negative in section .
first we further analyze the typesof changes made in real repairs.
.
.
the patch tends to make minor changes to the bug s syntax .
grammars vary widely across languages.
for example subjectverb object sequences i eat an apple are abundant in english butpeopleseldomusetheminverb finallanguagesliketamilor japanese.because of thisdistinction translatingby merelysubstitutingwordsinonelanguagewithanotherisofteninappropriate.
instead neural architectures capture the syntactic transformation between languages as well as the translation of the underlying words.
a recent study shows that the difference in word orderamongvariouslanguagesisasignificantfeaturethatmodels learn and e.g.
neural attention mechanisms are effective at this task.
we are similarly curious whether this feature is prominentin the conversion of bugs to patches as such information giveshints about how to adapt machine translation models appropri ately.forexample givenabuggyline if level damage damage with patch if level damage damage arealsamplefromourdataset wecanseethatthepatchdoes not modify the syntax in terms of the ast of the bug but only changes its semantics by changing the underlying tokens.
we thus empirically study how often modifications that fix logical errors introduce changes to the syntactic structure of code.
given a pair of bug and patch we tokenize the code and use javalang package3to identify the syntactic type e.g.
identifier separator integer of each token.
if a bug and patch have the exact same token type sequence their syntactic structure is unchanged.
table summarizes the resulting ratios.
slightly more than half of our patches preserve the exact syntactic structure of their bugs.
furthermore theresultsarestarklydifferentbasedonwhethera patch introduces new tokens relative to the bug see section .
.
those that do even more rarely .
change the syntax while the other patches are often some kind of permutation of the bug s tokens that very rarely preserves syntactic ordering.
a edit distance b intersection over union c bleu figure different similarity metrics regarding bug patch pairs.
x axis of each histogram indicates the similarity score w.r.t.different metrics and y axis shows the ratio of samples within the corresponding range.
the average edit distance between bugs and patches is .
.
the edit distances of .
samples are and .
samples have an edit distance .
the average intersection over union similarity is .
and .
samples have a similarity .
.
the average bleu score is .
and .
samples have a bleu score .
.
these observations have importantimplications.
forone they suggestthatsearchingforunseenwordsacrosstheentirevocabularyisrarelynecessary rather themodelcouldsimplysearchfor the tokens given a specific syntactic type e.g.
many patches replace just an operator to fix a bug.
more generally this suggests that the left to right generation process is thus not just inefficient but all but misdirected for such bugs it requires the model to both copyapreciseprefix andthengenerateasinglealternativefrom that context where the original token was often already close to being correct i.e.
in the right syntactic ballpark .
patching such bugs more than half of those in our dataset in this way likely puts inordinate and unnecessary strain on the model which we will quantify in the subsequent sections.
first we partially conclude our second research question the machine translation architecture s generation process is apoor fit for program repair which frequently retains most to kensfromthebugwhilereplacingjustafew andfromasmall candidate set.
.
program repair via nmt objective finally when generating natural language translations the goalis to correctly predict as many words of the target sentence aspossible.
the idea is that a translator that is likely to predict any onewordgiventheinputandpreviouslypredictedwords ifany is also likely to correctly generate the entire desired sentence by simply repeatingthis process until termination.indeed this tends to be quite accurate in general in part because of the naturally auto regressive markovian nature of text a given prefix typicallyhas only a small set of plausible continuations.
giventheobservationsintheprecedingsections thismarkovian assumptionseemsprecariousatbestforprogramrepair thebug and repair often share a large identical prefix and suffix thatis then followed by incorrect tokens in the former and different corrected ones in the latter.
as such we must question the validity ofanobjectivefunction bothlossandmetric thatvaluesper tokenpredictionqualitysostrongly.havingsaidthat theaforementioned observationsalonedonotprovethatthereisaproblemwiththis transplanted objective the buggy token s may simply have been a particularlyunnaturalsuccessortoitscontext fromwhichthe figure performance trends dashed line and per epoch results points on held out data as training progresses in terms of pertoken accuracy subject to teacher forcing accuracy at generating the complete sequence and top accuracy using beam search.
correctedtoken s do infact follownaturally.inthissection we empirically assess this concern.
specifically if the markovian auto regressive objective used in naturallanguagetranslationisagoodfitforprogramrepairaswell we would expect two things to be true the per token accuracy under auto regressive teacher forcingcorrelatescloselywiththequality i.e.
totalaccuracy of the produced patch.
that implies that the model correctly identifiesthe challenging tokens thatneedtobealtered as these dictate the overall correctness of the resulting patch.
themodelefficientlyexplorestherepairspacewhensamplingmultiplepatches e.g.
usingbeamsearch .thatimplies that choosing the repair point by first copying tokens unaltered and then auto regressively generating a different continuation is no distraction to the model.
weputboththeseexpectationsintothe empirical test.figure3 showsfirsttheprogressionofvariousaccuraciesonourheld out dataoverthecourseoftraining.atthetop theteacher forcedtoken levelpredictionaccuracyincreasessteeplyearlyon throughoutthefirstca.10passesthroughthetrainingdata butafterthat itall but 282plateaus.itdoes infact stillincrease butonlyveryslightlyafter epoch from ca.
to .
this clearly shows two phases abimodalpattern intrainingthistypeofmodel themodelfirst trivially minimizes its loss and thus achieves a high accuracy through simple copying but then struggles to match that strategy with predicting the correct change to achieve any more progress.
thisinitialcopyingtranslatesintolittlerealaccuracy thefull sequence i.e.
completerepair predictionreachesjust4.
after10 epochs making nearly all its substantial progress afterwards.
this has real training ramifications we also visualize the progression of the per token entropy transformed to probabilities .
in the first epochs the model quickly becomes very polarized assigning high probability to the copied tokens then it becomes clear that thisyieldsverylowprobabilitiesforthefewchangedtokens which entropypenalizesstrongly.asaresponse themodelinsteadadopts a more balanced prediction to achieve higher overall repair quality.
toquantifythecorrelationsinthefaceofthisbimodality anonparametric spearman s rho correlation test is in order.
this does showthatthetwometrics per tokenaccuracyandfullsequence accuracy are highly correlated .
even though less so afterepoch10 .
.thelatterresultreflectsthattheremaining per token accuracy increase translates into a disproportionally higher complete repair rate the missing token accuracy becomesca.
completerepairaccuracy nearlytriplethelevelsat epoch10.thisimpliesamixedanswertoourfirstpremise thecomplete patch accuracy certainly follows theper token accuracy but therelationisfarfromdirectandthelatterisahighlymisleading metricin ipsodue to its bimodal nature.
finally thefigureshowsthattheoddsoffindingthecorrectpatch in the top generated samples is only a little higher than the top prediction ca.
points at most that gap actually shrinks as the top 1predictionbecomesmoreaccurate whichsuggeststhatthe beamsearchfindsfewgoodnovel alternativepatches.wewould hopethat giventhenaturalambiguityinchoosingthecorrectpatch the model learns to sample a diverse set of plausible corrections.
instead frominspectionofthegeneratedsamples themodelproduces many very similar candidates usually differing by just a few tokens.thistooislikelyanartifactofthetrainingcriteria which prioritizes copying of the tokens over predicting the correct variation.thus weanswerourfinalresearchquestion theobjectivefunctionsofnmtmodelsare inappropriate for program repair leading to reduced training efficacy on more appropriate metrics.
seq2seq model for program repair asafittingconclusiontoourempiricalandconceptualevaluationof the basic transplanted approach to program repair as translation it is appropriate to try and redesign the existing approach.
this sectiondemonstrateshowobservingandquantifyingissueswith an outside approach relates to principled and innovative modeling design while observing concerns does not guarantee that improvements are straightforward as we show in relation to context it canimproveperformancebybetterrelatingthemodeltothetask.
we do this below by eschewing past practice of trying to generate patch tokens directly and instead generating edits.
.
model changes weobservedthreemaindeficiencieswiththeexistingtranslation approach the inadequacy of relying on just the bug for enough information to produce a patch the mismatch between typical repair actions and generating the entire corrected line and the related divergenceoftraining objective betweenper tokenaccuracyand whole repair bothtop 1andbeam search accuracy.hadwedesigned a machine learning approach for this problem from scratch wewouldcertainlyattempttoincorporatebothbugcontext and a notion of repair editsto reflect these aspects of program repair ashasalsobeenproposedbysomerecentwork .wepropose tomake bothchanges figure4 showsthe twomain architectural mechanisms we add to the base model to achieve this.
edits wemodeleditsdirectly asatoken level diff betweenthe bugandpatch.ouranalysisoftypicalchangesindicatedthatthe bugandpatchnearlyalwaysshareasubstantialprefixand orsuffix with the repair occurring at some point in the middle of the line.
wethusparseeachbug patchpairandfindthelongestoverlapping prefix and suffix.
our model is augmented with two additionalpointersthat correspond to insertion and deletion the original decodercomponent oftheencoder decoderarchitecture isnow pressedintoservicetooutputthediff ratherthandirectlygenerate the raw tokens in the fix .
there are three possible scenarios no additions the prefix and suffix combined span the entire bug.
thismeans thatonlytokenswere addedinthe patch.inthiscase the deletionpointer willjust pointto thestart ofthe line and the insertion pointer will indicate where the new tokens which the decoder will emit are to be added.noremovals theprefixandsuffixcombinedspantheentirerepair.
in this case only token deletions are needed to go from the bug to the patch.
so the insertion and deletion pointer should correspond to the start and end of the segment to be deleted within the buggy statement and the decoder should just emit the s termination symbol an empty patch .
additions deletions a non trivial change in both bug and patch.
as a combination of the above the two pointers shouldidentify the segment to erase from the bug while the decoder should generate all newly required tokens to insert instead.
context we also observed that the bug alone rarely provides enough syntacticandsemantic informationtoreliablypredictthe necessaryrepair.thenaturalsolutionistoaddalargeamountof contextual tokens from the file containing the bug.
unfortunately transformers struggle to model very long sequences as their memory usage increases quadratically with sequence length.
at the sametime section4.2showedthateven20linesofcontextisrarely enoughtoprovidemuchmissingvocabulary whichisitselfonly part of the information needed .
we do not provide a new solution inthispaper rather weempiricallyquantifythedeficitfromthe model s perspective by adding up to tokens of context and comparing the resulting performance.
we ensure that the model is aware ofwhichtokenstorepairbybiasingthedecoder sattention tothebuggytokensusingthesamebiasingmechanismasin in this case with a simple unary relation i.e.
is part of the bug .
283if linewidth !
encoder s decoder if linewidth if linewidth !
start insert end delete a an edit based repair model which emits two pointers based on theencoderstatesthatindicatetheinsertionstartpositionandthe removal end position.
the decoder generates any missing tokens.
... if linewidth !
g.setstroke ... linewidth ...encoderdecoder if linewidth b representationofacontext enrichedrepairmodel.theencoder functionsasusualonabroadersetoftokens thedecoder sattentionis biased towards the highlighted buggy tokens.
figure4 proposedarchitecturalchangestothebasicrepairmodel on an example from our test data.
table4 repairaccuracyonthe de duplicated testdataofthevarious models that we propose in this paper.
model top 1top 5top baseline .
.
.
edits4.
.
.
context .
.
.
edits context .
.
.
.
results astable4shows therearetwomaincharacteristicsoftheresulting models performances.
first the edit based enhancement clearly andsubstantiallyimprovestheaccuracyoverthebaselinemodel fixing an additional bugs on our test set with its top prediction alone.
second the contextual enhancement does not seem to help in its current form.
we discuss both these results here.
editmodel theedit basedmodelproducesbetter qualitypatches onourtestdatathanthecorrespondingbaseline.itsdesignisinformedbyourdataanalysis andsoitisarguablyabetterfitforthis task.
figure shows its training behavior to compare with that of thebaselinemodelinfig.
its per token teacher forcedaccuracy increases much more smoothly4and more in line with increases in thefullrepair predictionquality.italso displaysalargerimprovement in sampling accuracy between the top and top prediction whichremainsconsistentlywideduringtraining suggestingthat it better explores the search space with more diverse predictions.
its design also allows the edit model to predict more newly introduced vocabulary in the patch relative to the bug it doesso .
and .
of the time for the top and top samplesrespectively compared to .
and of the base model.
onenotable difference is the gap between top and top sampling accuracy the edit model is stronger in the former but loses to the baselineinthelatter.thisappearstobeduetotheeditmodelhavingtocommittoaninsert deletepointerfirst conditionaluponwhich sampling is more bounded.
to be clear we did also sample thesetwo pointers from their corresponding probability distributions 4their probability also displaying less of a spike in early training.
figure performance of the edit based model on held out data as training progresses in terms of per token accuracy subject to teacherforcing accuracyatgeneratingthecompletesequence andtop accuracy using beam search.
and initialized the beam search with the most probable different combinationsofstartandendpointers but inpracticethemodel tended to choose a single pair with very high probability so that it effectively only explored that set.
this may be an interesting issue to pursue in future work.
context information the second missing element was the reliance on the bug alone as a source of patching information in section4.
weshowedthattheabsenceofcontextisaninsurmountable obstacle that deprives the model of the necessary information to patch most bugs.
however identifying a problem and solving it are quite different things as our results in table4 show.
although we added a substantial amount of surrounding tokens i.e.
to the model s input the resulting models performance is quite poor actually performing slightly worse than their context free counterparts.thisislikelyduetothechallengeofmodelinglargeamounts of contextual information although our models were trained to similar accuracies they did so much slower and evidently with worse generalization.
thismaypointatseveralissues butnoneseemquiteresponsible.
forone theattentionmechanismweusedmaynotadequatelyhelpthemodellocatethebuggybits however themodelalwaysemitted patches that were very similar to the bug.
similarly the amount of context may simply be too little table suggests that many useful tokens are only available far away from the bug.
however that tablealsoimpliesthattheimmediatecontext shouldhelpwithca of missing tokens so this too does not explain the lack of performance.
the model itself may simply have insufficient capacity to capture this much context though we used a relatively large transformer architecture and the model was trained to high accuracy.
all this is to say that we do not know how to betterintegrate context in these models.
this is not a bad thing notall modeling improvements are obvious but it is important that weunderstandthedeficitsfirst.ourempiricalanalysishelpedus both identify it and has laid a useful foundation for the kind of informationtointegrateinfurtherimprovements evenifitisnot yet clear how.
threats to validity this paper presents a case study of a specific type of program repair which we explore in great empirical detail.
as such the main threats to the validity of our conclusion are external relating to the generalization of our findings to both other types of defects and other model transplants into se research.
first our data collection and analysis focused only on small one linefixes sincesuchbugs andsingle statementbugs areboth commonandimportant realistictargettocurrentprogramrepair models .inaddition manyexistingnmt basedprogram repair tools are trained and tested on one line or single hunk bugs.
assuch studyingsuchbugsisbothrepresentativeandimpactful.
having said that we do not claim nor believe that their empirical properties generalize to larger more complex defects these no doubthavetheirownnon trivialcharacteristicsthatdeservefurther investigation especiallyif whentheybecomethesubjectofnew models.
secondly we did not compare our model s section with stateof the art nmt based program repair tools.
the goal of this work is not to present models with the best performance rather we are evaluating the feasibility of the general idea of patching as translation usingageneral representativemodelingsetup especially in contrast to variations that depart from the translation metaphor.
more broadly there are manyother cases ofmodeling transplants into our community often with some alterations to fit the task thesemaynotallbeharmfulormismatched buttheydoalldeserve carefulempiricalanalysistoensurethattheyachievetheirpotential efficacy in our community.
conclusion in this work we first present a comprehensive study to evaluate theconceitthat softwarepatchingislikethelanguagetranslation as a prototypical example of model transplant from neighboring communities into se.
we empirically show that the translationparadigm does not capture bug fixing very well for a range of reasons.wealsousemodelsthemselvesasempiricaldevices we adapt the seq2seqmodels used for translation to generate edits ratherthanrawtokens whichleadstopromisingimprovements.
we hope this work inspires more empirically grounded research into transplanting machine learning models to program repair and other software engineering applications.