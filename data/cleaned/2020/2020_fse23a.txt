deepdebugger an interactive time travelling debugging approach for deep classifiers xianglin yang xianglin u.nus.edu shanghai jiao tong university shanghai chinayun lin lin yun sjtu.edu.cn shanghai jiao tong university shanghai chinayifan zhang zyifan828 gmail.com shanghai jiao tong university shanghai china linpeng huang huang lp cs.sjtu.edu.cn shanghai jiao tong university shanghai chinajin song dong dcsdjs nus.edu.sg national university of singapore singapore singaporehong mei meih pku.edu.cn shanghai jiao tong university shanghai china abstract a deep classifier is usually trained to i learn the numeric representation vector of samples and ii classify sample representations with learned classification boundaries .
time travelling visualization as an explainable ai technique is designed to transform the model training dynamics into an animation of canvas with colorful dots and territories.
despite that the training dynamics of the highlevel concepts such as sample representations and classification boundaries are now observable the model developers can still be overwhelmed by tens of thousands of moving dots across hundreds of training epochs i.e.
frames in the animation which makes them miss important training events.
in this work we make the first attempt to develop the model time travelling visualizers to the model time travelling debuggers for its practical use in model debugging tasks.
specifically given an animation of model training dynamics of sample representation and classification landscape we propose deepdebugger solution to recommend the samples of user interest in a human in the loop manner.
on one hand deepdebugger monitors the training dynamics of samples and recommends suspicious samples based on their abnormality.
on the other hand our recommendation is interactive and fault resilient for the model developers to explore the training process.
by learning users feedback deepdebugger refines its recommendation to fit their intention.
our extensive experiments on applying deepdebugger on the known time travelling visualizers show that deepdebugger can detect the majority of the abnormal movement of the training samples on canvas significantly boost the recommendation performance of samples of interest 10x more accurate than the baselines with the runtime overhead of .015s per feedback be resilient under the also with national university of singapore.
corresponding author also with national university of singapore.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
mistaken user feedback.
our user study of the tool shows that the interactive recommendation of deepdebugger can help the participants accomplish the debugging tasks by saving .
completion time and boosting the performance by .
.
ccs concepts software and its engineering software notations and tools software notations and tools .
keywords debugging visualization deep classifier user study acm reference format xianglin yang yun lin yifan zhang linpeng huang jin song dong and hong mei.
.
deepdebugger an interactive time travelling debugging approach for deep classifiers.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
.
introduction deep learning based classifiers or deep classifiers are learnable functions mapping a sample to a predefined class which have been widely used in software systems in areas such as computer vision finance education and transportation .
as showed in figure typical deep classifiers consist of two components representation learning a sample e.g.
an image a sentence a voice clip is transformed into a representation vector in a highdimensional space.
the transformation can be implemented as convolutional layers lstm layers or transformers .
representation fitting the learned representation is further fed into layers for fitting its label as prediction which is typically implemented as fully connected layers.
a well functioning deep classifier can learn the classification landscape consisting of representations of the samples distributed in the space and classification boundaries to distinguish them well.
mathematically the sample representations are in the form of high dimensional vectors and the classification boundaries are in the form of composite formulas on the high dimensional vectors.esec fse december san francisco ca usa xianglin yang yun lin yifan zhang linpeng huang jin song dong and hong mei input image voice sentence code ... representation learning cnn transformer ... representation fitting fully connected layer representation vector prediction figure general architecture of deep classifiers consisting of representation learning and representation fitting figure a visualized classification landscape of one training epoch by a time travelling visualizer timevis .a pink dot located in the red region indicates a correct prediction.
a pink dot located in the blue region indicates a mis prediction.
the white region indicates classification boundaries.
recently time travelling visualization techniques are emerging to help observe the training dynamics of the high dimensional representations and classification boundaries in a low dimensional canvas.
figure shows one example of such visualization.
the canvas stands for the universal high dimensional space where each dot stands for a training testing sample and each colored region stands for a classification region and the white boundaries stand for the classification boundaries.
by this means the process of learning a deep classifier is transformed into a visualized animation of an evolving canvas.
the visualization is useful for the model developers to generally observe how the model predictions are formed.
however the techniques still suffer from the following challenges to serve as a practical model debugging solution overwhelming training samples and events when visualizing the dynamics of tens of thousands of training samples as dots on the canvas it is challenging for the model developers to locate which samples and what patterns of sample movements need more attention.
even worse some movements are hard to be explicitly expressed as shown in figure .
lack of intention inference in the model debugging tasks the developers can raise different hypotheses e.g.
whether the unexpectedly low accuracy is due to noisy training samples?
or whether the model is trained with sufficient training samples?
as they have different intentions for different tasks.
unfortunately the presentation of animation still lacks the support to detect different task relevant intentions and validate the aforementioned hypotheses in a debugging task.
in this work we propose deepdebugger to further develop the model time travelling visualizers to the model time travelling debuggers with the support of validating diverse hypotheses from the model developers.
to this end we design deepdebugger as aninteractive solution for developers to explore samples of interest in the training animation.
specifically we design deepdebugger to recommend abnormal and suspicious samples i.e.
samples with potential issues such as data corruption and mislabeling detect task relevant samples by taking the developers feedback on the recommendation and tolerate mistaken feedback for resilient recommendation results.
technically we design an anomaly detection algorithm based on the location velocity and acceleration of the training samples on the canvas.
after the developers give feedback i.e.
accept or reject on the recommendation we formulate the feedback adaption problem as a regression model learning problem.
by this means our data driven feedback learning model can perform well even with a small set of feedback and resilient to mistaken feedback by ignoring inconsistent samples .
the tool supported debugging process is iterative and interactive.
the recommendation keeps being refined until the developers have validated their hypotheses or located the root cause.
we implement deepdebugger as a tensorboard plugin.
we extensively evaluate our approach by integrating deepdebugger with two known time travelling visualizers.
our experiment on anomaly detection shows that deepdebugger can detect the samples with abnormal movement with the precision of .
and the recall of .
.
our feedback simulation experiment shows that our recommendation can identify 10x more samples of interest than the random baseline with strong resilience to mistaken feedback.
our user study consists of participants in two model debugging tasks showing that our feedback based design in deepdebugger can help the participants to be more efficient to accomplish the debugging tasks by either saving .
accomplishment time or boosting .
performance compared to the deepdebugger version without feedback based recommendation.
in summary we make the following contributions interaction over observation we enhance the time travelling visualizer into time travelling debugger by introducing the capabilities of detecting abnormal movement user interaction and learning feedback.
the human in the loop recommendation can be task adaptive and resilient to mistaken feedback.
through interactive exploration the users can have a more intuitive understanding of the model behaviors.
tool support we build a tensorboard plugin deepdebugger based on our approach supporting various functionalities such as visualization recommendation sample query and interaction.
screenshots demos and source code are available at .
extensive experiments we design systematic experiments and user studies to evaluate the effectiveness of the designed recommendation and interaction and the practicability and usability of our tool.
the results show that deepdebugger can significantly improve the model debugging efficiencies in tasks such as identifying suspicious samples and selecting informative unlabelled samples to retrain the model.
background .
deep classifier consider a classification problem with input samples s s1 s2 ... sn and a predefined set of classes c c1 c2 ... cp a learneddeepdebugger an interactive time travelling debugging approach for deep classifiers esec fse december san francisco ca usa samplerepresentation learning representation fittingrepresentation vector prediction projectionlocation inverse projection position paint canvas figure an illustration of how a time travelling visualizer visualizes an individual snapshot of a classifier i.e.
the classifier trained at the i th epoch.
classifierm s cis a function which maps sto one of the predefined classes ci c denoted as m s ci c. typically a deep learning based classifier mcan be decomposed into a representation learning function f and a representation fitting function g i.e.
m f g as showed in figure .
therefore given a dataset s s1 s2 ... sn we can have the set of representation vectors r r1 r2 ... rn where each ri f si is a high dimensional vector.
moreover we also have m si g ri g f si .
.
time travelling visualization .
.
definition of time travelling visualization.
a time travelling visualizer takes as input a dataset s s1 s2 ... sn and a sequence of partially trained classifiers m m1 m2 ... mk wheremiis the classifier learned in i th epoch in the training process.
then it generates as output a sequence of frames f frm frm ... frmk wherefrmiis a two dimensional canvas for reflecting the classification landscape of i th representation space r. intuitively the classification landscape describes which region in rbelongs to what class and how the prediction confidence distributes over each region.
thus for each pixel in a frame frmi the visualizer can describe its predicted class and confidence by the corresponding color and its depth.
specifically the visualizer has a coloring functionr2 ... 3where the input is a pixel and the output is a depth aware color in the form of the rgb value.
the colors represent classes and the color depth represents the confidence.
the smaller the confidence the lighter the color.
as a result each frame is visualized as the canvas showed in figure .
.
.
visualizing one frame.
a time travelling visualizer is implemented as a visualization model v with auto encoder architecture where is the encoder and is the decoder.
figure shows how a general time travelling visualizer works which consists of two branches i.e.
the encoder branch the solid lines in figure and the decoder branch the dashed lines in figure .
the encoder r r2is for positioning the sample representation.
a sample representation r r will be projected to low dimensional space at loc r r2wherelocis a twodimensional location on the frame.
by this means any samples training or testing can find its position on the canvas.the decoder r2 r is for painting classification landscapes.
given an arbitrary location locon the canvas the decoder inverseprojects it to the representation space to have r loc so that we can paint locwith the color of c g r .
note that many representation fitting layers in deep learning model provides a measurement of confidence e.g.
by sigmoid or softmax function.
hence the color of depth can be calculated with confidence.
generally different time travelling visualizers learn the encoder and decoder for different visualization properties .
.
.
visualizing training process.
given a training process as a sequence of classifiers m m1 m2 ... mk we can generate one framefrmifrom eachmi m .
generally different time travelling visualizers have different strategies to make sure that two adjacent framesfrmiandfrmi 1are continuous .
.
.
state of the art visualizers.
the time traveling visualizers to the best of our knowledge are dvi and timevis .
both can transform the model training progress i.e.
a sequence of recorded models into an animation in the two dimensional canvas.
dvi has its advantage over timevis in preserving the spatial topology between the low and the high dimensional space.
on the other hand timevis prioritizes efficient learning of the visualization model albeit at the cost of some visualization effectiveness.
given the space limit the interested audience can refer to and for more details.
in the following we call the model being visualized assubject model and the autoencoder model implementing the visualization as visualizer .
problem definition despite time travelling visualizers providing a useful abstraction for intuitive inspection there are enormous training events such as representation movement and boundary evolution in the training dynamics.
in this work we aim to address the following research problems to equip the visualization with debugging functionalities problem training anomaly detection given a large number of training events such as representation movement how could we recommend the events significantly different from the majority to be inspected first?
problem intention detection assuming a model developer has an implicit intention for certain tasks how could the timetravelling debugger capture its intention and recommend samples of interest accordingly?
it is important to note that problem does not define a specific task such as detecting noisy training data or addressing vanishing gradients.
instead we require a solution that can infer the user s intention in an open ended setting.
analogous to a traditional debugger for software developers a model debugger should facilitate open ended exploration of changes in classification boundaries and representation embeddings during model training adapting to various scenarios and intentions.
approach overview figure shows an overall design of the deepdebugger framework as a time travelling debugger for model behavior investigation.
deepdebugger is built upon time travelling visualizers which transform the training process into an animation.
deepdebugger takesesec fse december san francisco ca usa xianglin yang yun lin yifan zhang linpeng huang jin song dong and hong mei time travelling debugger time travelling visualizer training process epoch ... epoch epoch epoch n epoch n frame frame frame ntraining animation sample of interesttraining dataset ...training anomaly detection movement representation1.
dynamic feature extractor2.
anomaly detector3.
inte ntion learner model developerintention detection visualize figure overview of the designed deepdebugger framework figure three trajectories of three learned representation vectors.aandbare similar in terms of the locations aandbcan be distinguishable in terms of velocity and bandcare similar in terms of acceleration.
the training animation as input and reports the samples with abnormal training samples and interactively learns and refines the samples of user interest from the feedback of the model developers.
to this end we will first extract the features of each sample s training dynamics dynamic feature extractor in figure .
then we design an anomaly detector to report the abnormal movement as the initial recommendation of samples of interest anomaly detector in figure .
the model developers can investigate recommended samples and provide feedback on their relevance with options to accept reject or ignore based on their current tasks.
we implement an intention learning solution to efficiently learn the feedback on the fly and generalize the small scale feedback to more samples without feedback.
this interactive iterative process continues until users can identify root causes e.g.
mislabeled training samples or validate their debugging hypotheses.
validate their debugging hypothesis intention learner in figure .
.
dynamic feature extraction given a sample its dynamic features consist of both its trajectory on the canvas and the change of its prediction during the training process namely trajectory features and prediction features.
they track the training dynamics of the representation learning layer and representation fitting layer respectively offering valuable insights into the underlying patterns of movement see figure .
.
.
trajectory features.
for the visual trajectory features we capture the movement by location velocity and acceleration.location.
the location of a sample represents regional information on the classification landscape which can capture regional patterns such as samples sharing similar predictions.
specifically given a samples we can have all its historical locations on the canvas l l1 l2 ... ln whereliindicates its location on i th frame or epoch .
each location lis a two dimensional vector x y representing a coordinate on the canvas.
thus we encode the location featurerloc x1 y1 x2 y2 ... xn yn .
note that the location features allow us to detect anomalies based on the embedding of the inputs.
figure shows an example with three trajectories a b and c. the location features make aandbshare more similarity than that ofaandcand that ofbandc.
velocity.
the velocity is the first order derivative of the location feature which quantifies the rate of change in position from its source to its destination.
specifically given a location sequence l we can have its velocity sequence v v1 v2 ... vn where vi li li x i y i .
thus we encode the velocity feature rv x y x y ... x n y n .
as in figure the velocity features enable differentiation between trajectories aandb asaexhibits a more complex zigzag movement pattern indicative of greater struggle compared to the relatively smoother path of trajectory b. acceleration.
the acceleration is the first order derivative of the velocity and the second order derivative of the location which quantifies the rate of change in velocity as a sample representation transitions from its origin to its destination.
specifically given the velocity sequence vof a sample we have its acceleration sequence a ac1 ac2 ... acn whereaci vi vi x i y i .
thus we encode the acceleration feature rac x y x y ... x n y n .
as in figure the acceleration feature identifies similarities between the trajectories bandc.
both trajectories exhibit smooth progression towards their respective destinations.
we concatenate three dynamic features as rt rloc rv rac .
.
.
prediction feature.
in this work we assume that the target classifier can provide us with a continuous value to indicate its confidence.
for example the softmax function predicts a probabilitypfor each class the higher the p the more confident the prediction is.
similarly the sigmoid function predicts a value pbetween and for binary classification the closer pis to or the more confident the prediction.
therefore given a sample s its confidenceconf is the score of the classifier on sat an epoch e deepdebugger an interactive time travelling debugging approach for deep classifiers esec fse december san francisco ca usa conf s e p softmax function p .
sigmoid function thus we have prediction feature rconf conf conf ... confn .
finally the overall concatenated dynamic feature is rmv rt rconf .
.
anomaly detector in this work we detect the anomalies by learning the concept of normality and the abnormality from the training dataset.
we denote a training sample for learning the ab normality as a seen sample and generalize it to unseen samples.
to this end we adopt a clustering algorithm for anomaly detection.
we assess sample similarity using the cosine similarity of the dynamic features extracted in section .
.
subsequently we assign an anomaly score to each sample based on its cluster size.
ultimately we identify and report samples belonging to smaller clusters as anomalies.
.
.
learning normality and abnormality.
let any dynamic feature e.g.
location velocity be r r1 r2 ... rn we use birch hierarchical clustering algorithm to splitrintokclusters.
we choose the birch clustering algorithm as its leading performance in time and space complexity among other alternatives such as k means .
then for any sample in cluster cls we quantify its movement abnormality as cls n. the smaller the size of cluster cls the more abnormal the trajectory in the cluster.
for example assume that we have clusters cls1 cls2 cls3where cls1 cls2 and cls3 .
therefore the abnormality of cls1is .
.
in contrast that of cls3is1 .
which is much higher than that of cls1.
we let the samples within the same cluster share the same abnormality score.
.
.
generalizing normality and abnormality.
as for an unseen samplesunk we can have its dynamic feature runkas introduced in section .
.
first we denote the centroid of a cluster clsas center cls cls cls i 1ri also we denote the radius of a cluster cas radius cls arg max ri clsdist center cls ri then we determine if an unseen sample sunkbelongs to a cluster clsiby comparing the euclidean distance between runkand the cluster center center clsi to the cluster s radius radius clsi .
if the distance is smaller than the radius we consider the sample to be part of the cluster.
the sample sunkis considered normal if it falls into a normal cluster and abnormal if it falls into an abnormal cluster.
ifrunkdoes not belong to any of the existing clusters we create a new cluster clsk .
in this case the sample s abnormality score is close to indicating a high likelihood of it being an anomaly.
we consider samples with scores exceeding a user defined thresholdthabas anomalies.
for instance we report all samples in cls3 the last cluster and runk which belong to the new cluster clsk as anomalies in both cases respectively.
ultimately for a given sample s if any of its features is identified as an anomaly we report the sample itself as an anomaly.
.
intention detector we design the intention detection scheme with the following consideration.
i it should perform well on limited and imbalanced feedback without overfitting as human effort is valuable and often scarce.
ii it must be efficient enough for ensuring timely responses to user input.
iii it should be robust against noisy feedback considering that humans can make mistakes.
to this end we design the recommendation system as follows.
initially we recommend the most abnormal training samples see section .
.
then we take as input the feedback from the users in the form of accepting or rejecting these samples and recommend relevant samples as output.
specifically we regard each feedback as a label for a sample and use an efficient robust intention learning algorithm to fit and generalize the feedback based on abnormal scores extracted in section .
.
this algorithm assigns an interest potential score to other samples without feedback.
we recommend samples with high scores and iteratively refine our recommendations based on user feedback.
.
.
interest potential estimation.
given a sample s we estimate its interest potential by an interest estimation function ie a1 a2 ... an withaibeing its attributes.
we utilize the scalar anomaly score to represent each dynamic feature i.e.
aibeing the anomaly score fori th dynamic feature.
this enables faster processing and reduces noise of raw dynamic features alleviating concerns i and ii .
then we regard each user feedback for a sample as a labell with acceptance being 1and rejection being .
further we can transform the interest estimation problem as a regression problem on the attributes a1 a2 ... an.
generally we can use different kernel functions to fit ie .
in this work we opt for linear ridge regression due to its learning efficiency and robustness which effectively address concerns i ii and iii .
the linear ridge regression predictor can be expressed as ie s ie a1 a2 ... an n i 1ci ai c0 in equation the coefficients e.g.
ci are learnable variables regarding users implicit intention.
.
.
the feedback framework.
we first initialize ciby n i 1ci andc1 c2 ... cn reflecting an equal importance score for all dynamic features at the start.
each time users provide feedback by accepting or rejecting a sample we label it as interested positive sample or uninterested negative sample .
then we can refine our interest estimation function with the new training data point continuously improving our recommendation with the evolving user feedback.
practitioners can enhance our work by incorporating additional attributes or employing a different kernel for special needs and use cases.
for the example in figure if the model developer is looking for samples with similar predictions his or her feedback is expected to lead to a high similarity between aandb.
in contrast if the model developer is to check the learning smoothness of each sample the feedback can lead to a high similarity between bandc.
for each intention users can start one session of interactions for locating the relevant samples of interest.
if users change their intentions they can start a new session.
technically all the learnedesec fse december san francisco ca usa xianglin yang yun lin yifan zhang linpeng huang jin song dong and hong mei coefficients of the regression model are reset and new feedback and interactions can derive a new model capturing their new intention.
gui of deepdebugger we implement deepdebugger as a tensorboard plugin as figure .
more screenshots and videos are available at .
deepdebugger is designed with four features a time travelling visualization tt vis .
the model developers can investigate the evolving visualized classification landscape frame by frame i.e.
epoch by epoch on the timeline at the bottom.
they can either play the animation or switch between different epochs by clicking the index on the timeline.
b canvas investigation canvas inv .
at each frame they can observe the training and testing accuracy.
the corresponding canvas of each frame can be zoomed in and zoomed out.
the users can query the samples and highlight them on the canvas observe the sample e.g.
image and select a subset of samples by dragging and dropping a bounding box on the canvas.
c sample of interest recommendation soi rec .
deepdebugger can recommend samples of interest based on the technique described in section .
and section .
.
the recommended samples will be listed in section d1 of figure .
hovering on any sample a detailed description of the sample such as its appearance prediction and label will be shown.
d interactive feedback interaction .
once the users accept or reject individual recommendations in section d2 of figure the runtime interest detector in deepdebugger will be triggered to provide a new recommendation.
the process is interactive explorative and iterative for the model developers to have a more informed investigation to understand the model training behaviors.
evaluation we evaluate deepdebugger with the following research questions rq .
readers can check for the replication details.
rq1 abnormality detection can deepdebugger effectively detect abnormal training movement on the canvas?
rq2 intention detection can deepdebugger recommend relevant samples based on user feedback with acceptable feedback efforts and runtime overhead?
and what is the importance scores of different features in different scenarios?
rq3 error resistance what is the performance of deepdebugger if the user provides incorrect feedback?
rq4 user study whether the interaction design in deepdebugger is useful to address the model debugging tasks in practice?
how do they use the tool?
to answer rq1 we generate ground truth normal and abnormal trajectories to evaluate their precision and recall to detect the abnormal ones.
to answer rq2 we design two types of intentions and simulate user feedback in relation to those intentions.
to answer rq3 we inject noisy feedback on the tasks designed in rq2 evaluating how the performance of deepdebugger is impacted.
to answer rq4 we conduct a user study involving two model debugging tasks assessing the effectiveness of deepdebugger when used by human participants.table performance of detecting out of distribution abnormal movements on visualized training processes.
dataset mnist fmnist cifar visualizer dvi timevis dvi timevis dvi timevis precision .
.
.
.
.
.
recall .
.
.
.
.
.
f1 score .
.
.
.
.
.
table selection of out of distribution datasets dataset cifar mnist fmnist ood dataset fmnist cifar mnist experiment settings.
in the evaluation we empirically chose k 30with the following consideration to ensure all the clusters are not excessively small we opted for a small value of k and to prevent a few anomalies from being merged into a single large cluster we selected a relatively larger value of k. additionally we set thab .
.
such settings are used throughout all the experiments.
.
anomaly detection rq1 .
.
datasets subject models and time travelling visualizers.
we first design some training processes to be visualized.
in this experiment we use mnist fmnist and cifar as the datasets.
we train the model architecture of resnet18 as the subject model on each dataset until the training accuracy converges.
the subject model takes epochs to converge on mnist epochs to converge on fmnist and epochs to converge on cifar .
we use all known time travelling visualizers to the best of our knowledge i.e.
dvi and timevis to visualize the training processes of three datasets.
thus we have visualizers processes visualized processes.
for convenience we usep dataset visualizer to denote the visualized process derived by the classifier trained by dataset withvisualizer .
for example p cifar dvi indicates the visualized training process for the model trained by cifar10 with dvi.
.
.
setup.
we define ground truth of unseen normal and abnormal sample movements as follows.
ground truth normal movement we take the trajectories of testing samples as the ground truth normal movements considering they are not used by the visualizers and share the same distribution with training data.
synthesized abnormal movement to generate abnormal movements we take into account both trajectory features and prediction features.
for trajectory features we create a random trajectory step by step that has the same shape as normal samples on the canvas but with random accelerations.
for prediction features we randomly sample a confidence sequence with each of its elements ranging from to .
natural abnormal movement for a given training process we use the samples in another dataset as the out of distribution data and record their movements as abnormal movements.
we assume that out of distribution samples behave differently from the training data.
specifically we choose the out of distribution datasets for our datasets as table .
for each visualization p dataset visualizer deepdebugger identifiesmabnormal movements.
suppose cout of these mmovementsdeepdebugger an interactive time travelling debugging approach for deep classifiers esec fse december san francisco ca usa figure screenshot of deepdebugger tool.
the interface includes four sections a as time travelling visualization tt vis b as canvas investigation canvas inv c as sample of interest recommendation soi rec and d as interactive feedback interaction .
are correct.
we calculate precision asc mand recall asc n. we assessed our anomaly detector using three distinct sets testing samples synthesized samples and out of distribution data points.
.
.
results.
the recall of synthesized abnormal movement is showing deepdebugger s strong ability in detecting synthesized anomalies.
table shows the good performance of deepdebugger on the precision and the recall of the natural abnormal movements.
overall the average precision is .
and the average recall is .
.
moreover the performance over different time travelling visualizers are comparable.
further we qualitatively analyze the false positive and false negative as follows.
the testing sample in figure is an example of a false positive in the mnist dataset caused by the distribution shift of the testing samples.
compared to the majority of normal pictures of digital as shown in normal the sample renders a difference in the inclining angle which might incur unexpected anomaly.
in contrast the ood sample in figure is an example of false negative for the fmnist dataset.
in this case the movement of the ood sample is normal despite it being regarded as an outof distribution.
this observation aligns with the reported findings in explaining out of distribution data that out of distribution could behave in an in distribution manner.
.
intention detection rq2 in this experiment we consider two types of samples of interest.
noise samples detection.
the first type is noisy samples i.e.
mislabeled samples .
model developers can assess if poor training results are due to noisy training samples by examining the data.
mispredicted samples detection.
the second type is mis predicted samples in the wild i.e.
mis predicted samples that are neither part of the training nor testing datasets .
given the high cost of labeling samples some developers might intend to have a partially trained 20normal 20testing 20normal 20oodfigure examples of true negative false negative and false positive.
the first and third figures are two true negatives from mnist and fmnist respectively.
the second figure is a testing sample from mnist which is reported as false positive.
the fourth figure is an ood sample for fmnist reported as a false negative.
model and then look for unlabelled samples with more potential to improve the model.
thus they would be interested in finding unlabelled samples on which the preliminary model mis predict.
.
.
setup.
we use the configurations and hyperparameter settings i.e.
datasets model architecture and the use of visualizers as described in section .
.
except that the training dataset is changed for the designated intention noisy samples before training we randomly change and labels to other classes in the training dataset.
mis predicted samples in the wild we only train the model using and of samples in the training datasets.
the remaining samples are viewed as unlabeled data from the wild.
note that those samples are not used for training and we don t have their label information.
following the notion defined in section .
we define p dataset visualizer type ratio as the visualization process for the model trained ondataset with an intention type of type an intention ratio ofratio and thevisualizer element.
we then perform a feedbacksimulation experiment for each configuration e.g.
p cifar dvi n esec fse december san francisco ca usa xianglin yang yun lin yifan zhang linpeng huang jin song dong and hong mei .
.
.
.
.
.0noise rate 50noise rate 50noise rate 20dataset mnist fmnist cifar10 method random dvi timevis a noise samples round0.
.
.
.6feedback accuracylabeled ratio roundlabeled ratio roundlabeled ratio b misprediction samples in the wild figure results on the accuracy of recommendation.
a the accuracy of recommending noise samples on mnist fmnist and cifar in rounds of feedback when noise rate is left mid and right .
b the accuracy of recommending mispredicted samples in the wild on mnist fmnist and cifar in rounds of feedback when training data is left mid and right .
times documenting the time taken in each feedback round under two scenarios.
in our analysis we benchmark deepdebugger against a random selection baseline.
in addition the feedback is automatically generated based on the target intention providing an acceptance i.e.
if the sample is of interest and a rejection i.e.
otherwise.
this feedback emulates user input within the context of the tested intention.
finally we evaluate deepdebugger s feedback performance by the number of accurate recommendations within k k feedback iterations.
samples are recommended in each iteration.
.
.
results.
figure shows how feedback can help recommend noise samples and mis predicted samples in the wild.
line colors and styles denote different datasets and visualization methods while the shaded area indicates the standard deviation.
overall deepdebugger is effective when paired with both dvi and timevis visualization methods.
there is a significant improvement in accuracy in aligning with user intention within just to rounds of feedback which substantially outperforms the random baseline.
furthermore the marginal benefits of feedback increase as there are fewer samples of interest available in the pool.
in figure 8a the gap between deepdebugger with dvi and deepdebugger with timevis on cifar dataset demonstrates that dvi produces higher quality visualization when the target training process is much longer i.e.
epochs for cifar epochs for mnist resulting in improved feedback accuracy.
table shows the importance scores extracted from equation of various dynamic features in situations with a noise sample rate of and a labeled ratio of for mis predicted samples.
more results in other settings are in .
the results highlight the importance of different features in varied scenarios emphasizing thetable the important scores of four dynamic features.
ft stands for the feature type.
scenarios ftmnist fmnist cifar10 dvi timevis dvi timevis dvi timevis noise samplesconf .
.
.
.
.
.
pos .
.
.
.
.
.
vel .
.
.
.
.
.
acc .
.
.
.
.
.
mispredicted samples in the wildconf .
.
.
.
.
.
pos .
.
.
.
.
.
vel .
.
.
.
.
.
acc .
.
.
.
.
.
round of feedback0.
.
.
.
.
.030time second dataset mnist round of feedback dataset fmnist round of feedback dataset cifar10visualization method dvi timevis a noise round of feedback0.
.
.
.
.
.030time second dataset mnist round of feedback dataset fmnist round of feedback dataset cifar10 b misprediction figure runtime efficiency.
box plot of runtime efficiency of our feedback algorithm on two visualization methods dvi and timevis and two scenarios tested.
robustness of our method across diverse situations.
for cifar dataset s noise sample scenario deepdebugger s importance scores differ noticeably between dvi and timevis.
deepdebugger with dvi favours trajectory features as dominant features while that with timevis depends on the confidence features.
this aligns with the discrepancy in feedback accuracy for the cifar dataset observed in figure between dvi and timevis given timevis s predominant reliance on the confidence feature.
figure shows the runtime overhead for generating feedback in the th th and th rounds in scenarios where the noise sample ratio is and the labeled sample ratio is .
full results are available in .
the time overhead to generate feedback is less than .015s which makes deepdebugger very practical.
in addition the cost is almost constant regarding different configurations.
.
error resistance rq3 .
.
setup.
we use the same configuration as described in section .
.
nevertheless we inject and errors in the simulated feedback and observe the accuracy of recommending samples of interest to simulate human error.deepdebugger an interactive time travelling debugging approach for deep classifiers esec fse december san francisco ca usa .
.
.
.
.
.0feedback accuracy misprediction in the wild dvi misprediction in the wild timevis round0.
.
.
.
.
.0feedback accuracy noisy samples dvi round noisy samples timevisdataset mnist fmnist cifar10 tolerance .
.
.
.
figure feedback tolerance.
line plot of feedback tolerance on two methods dvi andtimevis and two scenarios tested.
.
.
results.
figure shows the accuracy of recommending samples of interest under the noise feedback of and .
the colors of the line represent the dataset and the styles of the line represent the noise rate of feedback.
we only show mispredicted samples in the wild with the labeled ratio of and noise samples with a noise rate being .
please refer to for results in other settings.
as shown in figure minimal performance degradation is observed when noise is added to feedback.
therefore deepdebugger is robust against noise in diverse settings and the performance is still 10xbetter than the random baseline.
in detecting mis predicted samples in the wild scenario we observe a decreasing performance in the mnist dataset when the noise rate of feedback increases.
this is due to there being fewer and fewer remaining available mis predicted samples in the pool.
.
user study rq4 to answer rq4 we design a user study on two practical model debugging tasks to observe the effectiveness of deepdebugger and user s behavior in practice.
.
.
participants.
we recruit participants having computer science backgrounds from xxx in yyy anonymous university and country .
the participants have on average .
years of experience in model training.
out of participants have experience using tensorboard.
none of them have used deepdebugger before.
a more detailed demographical analysis can be found in .
we provide each participant with a tutorial on deepdebugger using a toy example a day prior to the study followed by a quiz consisting of five questions.
participants were then paired and split into two equivalent groups based on their quiz performance the experimental group eg which used deepdebugger with all functionalities and the control group cg which used deepdebugger with feedbackbased recommendations replaced by default recommendations i.e.
user feedback was not utilized for refining recommendations .
both groups interacted with the same user interface and were unaware of their group assignment.
.
.
tasks.
we design two model debugging tasks fault localization task in the one shot model training setting given that the training and testing accuracies do not meet the expectation how do we infer the root cause?
informative sample selection task in the continuous model training setting given that we have a model mtrained based on a limited set of training dataset and a set of unlabelled samplesdu how can we know what samples selected from du can be more useful to boost the performance of m?
task setup.
we inject mislabeled samples on cifar dataset and train a model with limited training and testing accuracy.
we present the participants with the training testing accuracy and choices of the root causes then ask them to choose the root cause with justification.
the choices include noisy training samples noisy testing samples inexpressive model architecture inappropriate learning rate insufficient training epochs gradient disappear and unbalanced training dataset.
note that participants selecting the noisy samples option must identify at least such samples.
all of the root causes can be pinpointed by our tool via direct canvas examination feedback recommendations observing accuracy and dynamics in alternate settings using provided visualization tabs or checking training metadata.
detailed elaborations of these aspects can be found in .
task setup.
we start by training a model on of the cifar dataset.
the participants are shown the visualization of the model s landscape and the distribution of training testing and unlabelled samples on this landscape participants are then asked to answer the following question given this model if we aim to label an additional unlabelled samples for retraining would using mispredicted unlabelled samples prove more beneficial for boosting performance than using uncertain samples?
.
in this task the uncertainty based selection is no less effective than the misprediction based solution.
after selecting and labeling the relevant samples the participants can use deepdebugger to retrain the model to validate their hypotheses.
both tasks require participants to identify relevant samples to affirm their hypothesis a process akin to programmers debugging a program.
each task has a one hour time budget but early submissions are acceptable.
participants are instructed to record their sessions using a video recorder i.e.
zoom for post analysis.
moreover we have instrumented all of deepdebugger s functionalities to record user behaviors quantitatively.
after the study participants are invited to provide feedback on the tool through a survey.
performance evaluation.
we evaluate the performance of task based on whether the participants make the correct choice if so whether they can find enough noisy samples to support their claim and task completion efficiency.
task performance is assessed based on whether participants reach the correct conclusion and the efficiency of drawing the conclusion.
.
.
results.
table and table display the performance of the eg and cg in tasks and based on their decisions the quantity of correct selected samples ofinterest soi and time spent.
both groups made the correct decisions for both tasks yet the eg selected more correct soi .
vs. .
in task and .
vs. .
in task and took less time vs in task and vs. .
in task .
table shows a wilcoxon signed rank test on four variables i.e.
soiandtime in both tasks.
the p values ofesec fse december san francisco ca usa xianglin yang yun lin yifan zhang linpeng huang jin song dong and hong mei table comparison of groups based on correct sample selection and time spent on task .
eg decision soi time cg decision soi time p1 correct p9 correct p2 correct p10 correct p3 correct p11 correct p4 correct p12 correct p5 correct p13 correct p6 correct p14 correct p7 correct p15 correct p8 correct p16 correct avg .
.
table comparison of groups based on correct sample selection and time spent on task .
eg decision soi time cg decision soi time p1 correct p9 correct p2 correct p10 correct p3 correct p11 correct p4 correct p12 correct p5 correct p13 correct p6 correct p14 correct p7 correct p15 correct p8 correct p16 correct avg .
.
table wilcoxon signed rank test of correct soiandtime both in task and task .
the subscript denotes different tasks.
varsoit1time t1soit2time t2 group eg cg eg cg eg cg eg cg avg .
.
.
.
p value .
.
.
.
timet1andsoit2fall below .
suggesting statistically significant differences.
specifically in task eg spent less time than cg with statistical differences while their soi selection performance was comparable.
in task eg selected more soi with statistical significance while time usage was comparable.
in summary these findings substantiate the efficacy of deepdebugger in improving feedback quality across both debugging tasks.
we gathered data on the frequency of each function s usage see section by individual users in task and task as table with the following observation how do the participants find the noisy samples as the root cause in task ?
given the seven options provided participants from both groups interacted with the visual canvas and employed the animation feature to identify the correct one.
as in table the function of tt vis andcanvas invare used frequently.
for example p3 p7 p9 and p10 root out gradient disappear by observing whether the visualized canvas starts being frozen at certain checkpoints.
some participants e.g.
p3 and p13 query the number of samples by labels to investigate training sample imbalance.
considering the dataset contains noisy samples it s unavoidable for every participant to come across some of these noisy instances.
however when it pertains to locating sufficient noisy samples to validate their hypothesis two groups exhibit divergent strategies.experimental group.
typically the eg employs the soi rec function to locate potential samples of interest.
upon encountering several noisy samples most participants express their interest by offering positive feedback on these samples.
with an average of rounds of interaction nearly all the participants in the eg gather sufficient samples and observe that these samples are evenly dispersed across the canvas.
consequently they verify the hypothesis concerning the noisy dataset and proceed to make their decision.
control group.
similar to the eg the cg initially utilizes the soi recfunction.
however participants quickly discern that while the tools are able to recommend a few noisy samples their feedback does not prompt deepdebugger to recommend more.
the quantity recommended is insufficient to draw conclusive results.
after engaging with approximately .
rounds of recommendations on average many of them decide to take matters into their own hands by exploring the visualized canvas directly.
this exploration involves a detailed investigation of various regions using the sample selection with bounding box ss function.
once they discover that certain regions contain a higher proportion of noisy samples they strategically focus their efforts on these areas to uncover more noisy samples.
these behaviors align with table which indicates that theinteraction function was utilized more than twice as frequently by the cg compared to the eg .
vs. .
similarly the ssfunction was employed four times as often by the cg vs. .
.
consequently both groups exhibit comparable proficiency in identifying the root cause although the eg achieves this in less time.
this indicates the potential time saving benefits of the tool s feedback mechanisms.
how do the participants find the mis predicted samples in the wild in task ?
compared to task task requires participants to collect and label a larger number of samples i.e.
.
in task the eg had developed trust in the interaction function leading them to primarily rely on this function.
in contrast more participants in the cg abandoned the interaction function and adopted the ssfunction with a bounding box to support their exploration and exploitation strategy.
as evidenced in table there is a fivefold difference in the use of the ssfunction and a threefold difference in the use of both interaction andsoi rec functions between the two groups.
although cg s strategy is effective it lacks the precision offered by theinteraction function.
as a result despite spending roughly the same amount of time on identifying samples the eg excels over the cg in terms of selecting a greater number of correct samples.
generally the ssfunction enables participants to devise an explore and exploit strategy as a mitigative measure once the recommendations lose effectiveness.
as per the post study survey and interviews the majority of participants from both groups agree that the interaction function is both intuitive and beneficial.
participants from the eg in particular found the interaction function useful in selecting samples of interest and validating their debugging hypotheses.
.
.
threats to validity.
our study has two main threats.
internally participants varied expertise could affect comparison fairness.
to mitigate this we conduct a pre study survey and a quiz to evaluate their expertise.
externally we limit ourselves to two model debugging tasks.
to mitigate this we select the most popular datasets and common debugging practices.
nevertheless moredeepdebugger an interactive time travelling debugging approach for deep classifiers esec fse december san francisco ca usa table the frequency of each function s usage by individual users in task and task .
si represents sample investigation ss represents sample selection with bounding box sq represents sample query.
section describes the function of tt vis canvas inv soi rec interaction.
for instance row column indicates that participant p1 used the interaction function times under task .
group participantstask task tt viscanvas invsoi rec interaction tt viscanvas invsoi rec interactionsi ss sq si ss sq egp1 p2 p3 p4 p5 p6 p7 p8 avg .
.
.
.
.
.
.
.
.
.
std .
.
.
.
.
.
.
.
.
.
.
.
cgp9 p10 p11 p12 p13 p14 p15 p16 avg .
.
.
.
.
.
.
.
.
std .
.
.
.
.
.
.
.
.
.
configurations such as more architectures are still needed to be investigated in future work.
related work visual explanation of deep learning models.
many researchers have proposed visualization techniques to understand ai models .
gradcam and its variants are designed to highlight a region of an image to explain the prediction.
seq2seq vis is designed to visualize the most influential words in an input sentence to its translation .
our approach focuses on improving the debugging capability of the time travelling visualization techniques such as dvi and timevis .
interactive debugging and fault localization.
comparing to the one shot fault localization techniques such as spectrum based fault localization and delta debugging many debugging approaches are designed in an interactive way on program execution trace and source code .
trace based interaction can enhance the time travelling debugging techniques .
ko et al.
propose whyline to generate why and why not questions about the program behaviors to the users.
their follow up works are further proposed to recommend suspicious steps on the trace to expedite the recommendation and trace exploration by pattern summarization probability inference and machine learning model .
source code based interaction is also designed to enhance existing approaches such as spectrum based fault localization and breakpoint recommendation .
different from those approaches deepdebugger focus on the model debugging functionalities by leveraging the time travelling visualizers for debugging deep classifiers.
conclusion we introduce deepdebugger the first interactive debugging tool that can cooperate with any time travelling visualization solution for debugging deep classifiers.
deepdebugger recommends potential samples of interest and iteratively and interactively refine the recommendation with the user feedback.
through extensive quantitative experiments and a case study we also show the effectiveness efficiency and robustness of deepdebugger.
data availability our code and full results can be downloaded on an anonymous website .
a tutorial on deepdebugger is also available.