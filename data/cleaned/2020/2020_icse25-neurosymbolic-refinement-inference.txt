neurosymbolic modular refinement type inference georgios sakkas uc san diego gsakkas ucsd.edupratyush sahu uc san diego psahu ucsd.edukyeling ong uc san diego k8ong ucsd.eduranjit jhala uc san diego rjhala ucsd.edu abstract refinement types a type based generalization of floyd hoare logics are an expressive and modular means of statically ensuring a wide variety of correctness safety and security properties of software.
however their expressiveness and modularity means that to use them a developer must laboriously annotate all the functions in their code with potentially complex type specifications that specify the contract for that function.
we present lhc a neurosymbolic agent that uses llms to automatically generate refinement type annotations for all the functions in an entire package or module using the refinement type checker l iquid haskell as an oracle to verify the correctness of the generated specifications.
we curate a dataset of three haskell packages where refinement types are used to enforce a variety of correctness properties from data structure invariants to low level memory safety and use this dataset to evaluate lhc.
previously these packages required expert users several days to weeks to annotate with refinement types.
our evaluation shows that even when using relatively smaller models like the billion parameter starcoder llm by using fine tuning and carefully chosen contexts our neurosymbolic agent generates refinement types for up to of the functions across entire libraries automatically in just a few hours thereby showing that llms can drastically shrink the human effort needed to use formal verification.
i. i ntroduction refinement types are a type based generalization of floydhoare logics where the programmer can specify correctness requirements by decorating classical types e.g.int with logical predicates e.g.
v that provide additional constraints on the values that can inhabit the type thereby providing amodular and expressive means of statically enforcing a wide variety of correctness safety and security properties of software.
refinement types have been developed for various languages from the ml family to c ruby rust typescript scala solidity racket .
a recent paper presented a user study of developers using refinement types for java that concluded that liquidjava helped users detect and fix more bugs and that liquid refinement types are easy to interpret and learn with few resources.
sadly as with other expressive and modular program verification tools like escjava or dafny the wider usage of refinement types is hindered by the fact that to effectively use refinement types across their codebase developers must laboriously annotate all the functions in their code with potentially complex type specifications that specify the behavior of that function to the rest of the code.
the expressiveness of refinement contracts means that unlike in classical type systems where often a type can be uniquely determined fromthe code there is an infinite space of possible specifications for each function which makes it tricky for the developer to determine the right one.
the problem is exacerbated by modularity which means that the refinement type or contract specified for a function fmay be correct for fin isolation but may not suffice to verify f sclients and so the developer has to go back and forth changing the annotations of functions to get the entire codebase to verify.
in this paper we present lhc1 a neurosymbolic agent that uses large language models llms to automatically generate refinement type annotations for the functions in an entire codebase using the refinement type checker l iquid haskell as an oracle to verify the correctness of the generated specifications.
we develop our approach via three contributions.
.
agent our main contribution is an agent that systematically traverses the codebase s call graph to generate each function s refinement type annotation.
if we think of the refinement type annotation as the analog of a procedure summary then we can think of our agent as a neurosymbolic program analysis that combines a bottom up analysis which uses neural llms to generate refinement type annotations summaries for functions with a top down analysis that kicks in when the llm fails to generate correct types that instead uses a symbolic predicate abstraction technique to generate refinement types from predicate templates obtained from the failed llm predictions.
thus even where the llm fails to generate the correct type its predictions can be used to generate an abstract domain that allows the symbolic analysis to succeed.
.
dataset our second contribution is a dataset comprising three haskell packages a suite of programs which are part of a tutorial on refinement types a haskell implementation of the salsa20 cipher and a widely used library that implements byte strings with low level pointer operations.
the dataset includes a diverse set of functions totalling about 5kloc annotated with refinement types that enforce a variety of correctness properties ranging from data structure invariants to low level memory safety.
this dataset was curated to deliberately exclude code present in the popular open source code llm training dataset the stack to ensure that successful type generation is not simply due to memorization.
.
evaluation our final contribution is an evaluation of lhc on our dataset using a variety of pre trained llms including starcoder and codellama which were nottrained on the code 1stands for liquid haskell copilot or lhc opilotin our dataset.
we demonstrate that by fine tuning these llms on a small set of about l iquid haskell programs we can greatly improve the agent.
we show how by combining the bottom up generation of the neural models with top down symbolic inference using qualifiers from the llms predictions lhc can automatically generate refinement types for up to of the functions across entire libraries.
furthermore the entire generation process can be completed in just a few hours a significant improvement over the several days or weeks of human effort that originally went into annotating the packages thereby indicating that llms can drastically shrink the human effort needed to use formal verification.
ii.
b ackground we start with some preliminaries showing how refinement types can be used to specify and verify properties of programs ii a and how llms can be used to automatically generate the type annotations required for verification ii b. a. refinement type checking with liquid haskell specification refinement type checkers like l iquid haskell let the programmer specify correctness requirements decorating classical types with logical predicates typically drawn from an smt decidable theory which provide additional constraints on the values that can inhabit the type.
a refined base type of the form v t p v defines the set of values vof type tsuch that additionally the constraint p v is true of the value v. for example the type v int v specifies the set of non negative integer values.
a refined function type of the form x in pre x v out post v x can specify pre and post conditions for the underlying functions via constraints on the input and output types.
for example the type x int x v int v x specifies a function that requires non negative inputs and ensures that the returned value is at least as large as the input x. refinement type checkers also allow the programmer to specify properties of data using measure functions which are pure and total functions that map data types such as lists trees etc.
to smt decidable values such as integers booleans sets etc.
.
for example the measure notemp defines a boolean predicate on lists that is true if the list is non empty measure notemp bool notemp false notemp true and we can use it to specify that a particular function should only be called with non empty lists head v notemp v a head x x head error empty list runtime crash verification refinement type checkers like l iquid haskell verify the specifications by generating verification conditions vcs logical formulas whose validity determined by an smt solver ensures that the program is type safe.
for example consider the code for the head function shown above and assume that error which aborts the program with a run time panic is a library function that is given the type error v string false a that is the precondition of error says it can only be called with string messages such that the predicate false holds.
since there are no such string s the program will only verify if at compile time the refinement type checker can prove that error is never actually called.
l iquid haskell verifies the code for head by generating the vc v.notemp v notemp v false the first antecedent comes from the precondition that the input list is a non empty list the second antecedent comes from the fact that in the second case where we call error the input list is matched against whose measure is false and the consequent false arises from the pre condition of error .
the smt solver proves the above vc valid to verify that head will never crash on non empty lists.
modularity and annotations refinement type checking is modular in that when we check a client e.g.head that calls a function e.g.error the only information known about the callee is its type signature .
this means that to analyze an entire package or module the programmer must annotate all the functions of the module with refinement type signatures.
for example consider the code in figure which shows a small haskell module that implements a function that computes the average of a list of integers by computing the sum of the integers and then invoking divide with the size of the list.
the divide function panics with error when the divisor is and otherwise calls the mathematical divoperator.
the size function recursively traverses the input list to count the number of elements in it.
to verify this module the programmer must annotate each of the three functions with a type signature.
first for divide they must specify that the second argument is nonzero so that l iquid haskell can verify that error will not be called at run time.
second for size they must specify that the function returns a strictly positive result ifthe input is nonempty.
finally for average they must specify that the input list is itself non empty which lets l iquid haskell determine using the annotation for size that total is strictly positive and hence that the call to divide is also safe.
symbolic type inference with qualifiers refinement type checkers require type annotations in many places e.g.
for recursive functions polymorphic type instantiation and so on.
these can be viewed as type based generalizations of the classic problem of having to specify pre and post conditions and loop annotation invariants in floyd hoare style verifiers like escjava or dafny .
as with loop invariants refinement type inference is undecidable in general but the type based setting allows l iquid haskell to use a form of abstract interpretation called predicate abstraction .
here the programmer provides a set of qualifiers predicate fragments or templates that l iquid haskell can then automatically conjoin to infer refinement types.
in our running example in figure we could provide templates type nonzero v int v type nelist a v notemp v divide int nonzero int divide int int int divide error divide by zero divide x n x div n size xs v nat notemp xs v size int size size xs size xs average nelist int int average xs divide total elems where total sum xs elems size xs fig.
haskell module with multiple dependent functions.
qualif qual v a v qualif qual v a xs b notemp xs v and then simply annotate average andsize with the wildcard types average andsize after which l iquid haskell will be able to automatically infer the refinement type annotations needed to verify the module .
additionally l iquid haskell can automatically extract qualifiers from annotated type specifications.
for example if the programmer wrote a specification x int v x len v a then l iquid haskell would automatically extract a qualifier qual2 v a x b x len v and then use it for subsequent type inference.
b. neural type inference with llms even with symbolic refinement type inference there is a substantial burden on the programmer as they must be able to either write down the types for all functions ordivine a set of suitable qualifiers from which types can be inferred.
we aim to reduce this burden by using large language models llms specifically code specific models trained on large tracts of source code to assist the programmer in generating the necessary type annotations needed to verify entire modules.
constructing prompts lhc infers refinement types for entire modules by repeatedly crafting prompts that can guide the llm to generate accurate and relevant refinement types for each function.
in the case of refinement types lhc uses llms that have been pre trained on infilling missing masked parts of programs and generate prompts that provide context about the haskell code while indicating where the refinement type is missing.
for figure lhc builds the following llm prompt to generate a refinement type for divide fill in the masked refinement type type nonzero v int v measure notemp bool notemp false notemp true type nelist a v notemp v divide mask divide int int int divide error divide by zero divide x n x div n few shot prompting with function dependencies few shot prompting is a technique used in the context of llms where the model is provided with some examples typically between one and a few dozen to illustrate the task it needs to perform.
this approach helps the llm understand the pattern and context of the task improving its performance on similar tasks.
in contrast zero shot prompting provides no specific examples to the llm relying entirely on the model s pre trained knowledge to perform the task based on a descriptive prompt.
few shot prompting generally yields better results than zeroshot prompting as it gives the llm concrete examples to learn from thereby reducing ambiguity and increasing accuracy.
in the context of refinement types few shot prompting can be particularly useful.
for instance when asking a code llm to generate refinement type annotations for haskell code a few shot prompt would include several examples of functions annotated with appropriate refinement types.
this helps the llm learn the patterns and constraints associated with these types.
by seeing specific examples the llm can more accurately predict and infill the missing refinement type annotations in new un annotated code.
specifically lhc adds all the functions and their types in the prompt that the target function depends on.
for our example in figure when we query a llm to generate types foraverage the functions divide andsize with their type signatures will also be added to the prompt as extra examples to help the llm generate the correct type for average .
iii.
o verview let s look at an overview of how lhc systematically infers the refinement type annotations needed to automatically verify a given haskell codebase by traversing its call graph prompting the llm to generate new type predictions that can be locally verified by l iquid haskell and then backjumping to a dependency when the predictions fail.
a. initialization the input for lhc is an un annotated program i.e.a program where some functions are not yet annotated with a refinement type.
based on the running example the initial program is the code from figure where we removed the orange specifications for the target functions divide size and average .
helper type aliases such as nelist andnonzero are standard in l iquid haskell and very commonly used by more complicated refinement types in order to simplify them.
therefore here they are left untouched for more context when prompting the llm to get more accurate predictions.
lhc state during the entire type inference process lhc maintains a global state sthat captures the current state sfof each function fwhich corresponds to a triple fuel type predicts .
the fuelrepresents the maximum number of times that lhc will attempt to infer a type for fbefore giving up and asking the programmer to provide a type.
the type represents the current type that the function fhas been assigned and against which the implementation of fhas been verified or if no such type has been assigned.
the predicts queue stores all the predicted types from a llm that are yet to be tried.
for our example the initial global state smaps each of divide size andaverage to a triple where fuel is type is andpredicts is empty.
dependencies next we identify the potential dependencies between the different functions because the order that we generate types and verify them matters for the correctness of our approach.
we build the program s call graph and get all function dependencies where depsfis the set of functions that fcalls.
for our example average calls divide and size each of which have no dependencies.
thus the call graph has a depth of the dependencies of divide andsize are empty and of average is .
worklist finally lhc maintains a worklist wklwith all the functions that are yet to be explored and verified by our approach.
we initialize the wkl with all the root functions i.e.functions that have no dependencies .
these roots will be the starting points at which lhc will infer types.
for our example we initialize wklwith .
b. building the llm prompt lhc starts by popping divide from the working list wkl.
since we have no type predictions so far we need to call the llm to generate some.
for that we make the following prompt as described in ii b measure notemp bool notemp false notemp true type nonzero v int v type nelist a v notemp v divide mask divide int int int divide error divide by zero divide x n x div n the prompt contains the program up to the function that we are generating refinement types for where we add a dummy refinement type mask that the llm needs to fill in.
dependencies optimization as described in ii b this is a few shot promping technique where all dependencies that the target function calls are added in the prompt.
while divide and size don t have any dependencies and their prompts remain as described above the prompt for average would include both of these functions when the d ependencies optimization is enabled since average calls both of them.c.
generating type predictions given the above prompt for divide the llm will generate the following types where the 3rd one is the correct one.
divide nonzero pos pos rejected divide nonzero nat pos rejected divide int nonzero int correct divide v int v nat nat divide nonzero int v int v d. verifying types the next step is to identify a type from the prediction queue above that is locally correct i.e.against which l iquid haskell will verify the given function here divide .
we iteratively check divide against each of the candidate types decreasing divide s fuel each time until we reach a locally correct type that is verified by l iquid haskell .
after this step the global state is updated so that for divide we have two remaining type predictions in the predicts the fuel has been decreased by since we tested that many type predictions and the current type is updated to int nonzero int .
function fuel type predicts divide 7int nonzero int nonzero nat nat nonzero int nat size average qualifiers optimization as described in ii a the programmer can provide a set of qualifiers and a wildcard type for the target function in order to enable l iquid haskell to automatically infer the appropriate refinement type .
when we enable the q ualifiers optimization we add the wildcard type for the target function at the end of the list of predicted types i.e.divide for our example in order for this type to be tested as a last resort if all other types fail verification.
additionally we extract automatically all possible predicates from the predicted refinement types to add the corresponding qualifiers in the program.
for example for divide we would extract qual1 v a v andqual2 v a v from the last two predictions from iii c e. updating the working list after we locally verify divide with one of the predicted types we look up all functions fthat call divide i.e.the functions fsuch that depsfcontains divide and we add them to the working list wkl ifall their dependencies are resolved i.e.all functions in depsfhave also been locally verified against their current type.
in this case average calls divide butaverage also depends on size which we have yet to explore.
therefore no new functions are added to the working list wkl which now only contains size .
as in iii b and iii c we perform the same steps for size to generate type predictions 2we consider here the top predictions but in reality can generate up to types in totalsize xs v int v size xs v int v size xs v int v size xs size xs v nat notemp xs v size xs v nat v len xs the first predicted type xs v int v is rejected by l iquid haskell as it is not locally correct assize can return 0on an empty list.
the second type xs v int v however is locally correct the output of size is always non negative.
note however it is not the type that is needed to verify the whole module in particular that is needed to verify average which we still have not explored.
we show next how this issue is resolved in our approach.
at this point the global state is updated to function fuel type predicts divide 7int nonzero int nonzero nat nat nonzero int nat size 8xs v int v xs v int v size xs xs v nat notemp xs v xs v nat v len xs average which corresponds to the partially annotated program divide int nonzero int divide int int int divide error divide by zero divide x n x div n size xs v int v size int size size xs size xs average xs divide total elems where total sum xs elems size xs at this stage since all of average s dependencies are also locally verified we can add it to the working list wkl so that we can generate and try types for it as well.
f .
back jumping to a dependency when predictions fail now lhc repeats the same generate and check procedure foraverage as it did for divide andsize .
however this time the llm predicted types are invalid in that none of them can be locally verified against the implementation of average .
this could mean one of two things one of average s function dependencies has a type that is either too weak i.e.does not specify what the client requires in its post condition or too strong i.e.has a pre condition that rejects the actual inputs provided by the client .
all type predictions for average were wrong.
in this case condition holds as size had indeed a problematic type as we hinted earlier which made average impossible to verify.
however at this stage lhc cannotdistinguish between or i.e.we do not know which condition actually holds and therefore we need to make a decision based on the global state.
since all of the predictions in the predicts queue for average are exhausted and we have available fuelfor it we choose to back jump to one of average s function dependencies in particular the one that has the highest remaining fuel i.e.the one that has been the least tested and thus has the most candidate type predictions left.
the dependencies that we can potentially back jump to are not just the immediate parent functions in the call graph but any possible ancestor which in this case includes divide andsize .
in this case we would indeed jump back to the problematic size that has the highest fuel of .
in this process we clear all current types for functions that directly or transitively call size and we end up with the following state where average s fuel has now fallen to as we made unsuccessful local verification attempts for it and additionally average andsize have no assigned type.
function fuel type predicts divide 7int nonzero int nonzero nat nat nonzero int nat size xs v int v size xs xs v nat notemp xs v xs v nat v len xs average we now repeat the process of trying type predictions from the predicts forsize until we get another locally correct type.
of the three remaining predictions predicts the first of these is rejected by l iquid haskell s local verification but the second is accepted yielding the state function fuel type predicts divide 7int nonzero int nonzero nat nat nonzero int nat size 6xs v nat notemp xs v xs v nat v len xs average at this point we again add average to the working list wkl and proceed to generate fresh candidates and to locally verify them.
in this second time the llm generates the candidates average xs v int size xs average nelist int int average nelist a int average xs nelist int int average v notemp v int and l iquid haskell rejects the first type to locally verify the second candidate nelist int int thereby completing the verification of the whole program.algorithm lhc s algorithm input code repository r output verified code repository r procedure verify coderepo r s deps build callgraph r wkl f r depsf while wkl do f poptop wkl ps getpredictions s f t trypredictions s f ps ift then wkl wkl backjump s f else sf.type t wkl wkl solved callers s f deps return r s g. asking the user for a type suppose that instead the llm generated a slate of incorrect types for average such that while trying out the generated candidates the fuel foraverage falls to .
in this case we are potentially in condition where all the llm predicted types are wrong i.e.fail to locally verify .
in this scenario lhc falls back to ask the user to provide a type for average .
if the verification fails even with the user provided type then we back jump again to one of the dependencies and repeat the whole process until the function is verified with the userprovided type.
that is we presume that the user provided type is correct and we get the llm to generate types for the other functions so that the whole program verifies.
of course our goal is to minimize the number of times we have to resort to asking the user for a type signature.
in our example assuming the user provides the correct type nelist int int the verification of the whole program succeeds and we return the fully annotated program shown in figure to the user.
iv.
a lgorithm we describe here the full algorithm of the lhc agent an interactive approach to verifying a code repository r comprising a set of functions by automatically annotating all the functions in rwith refinement types algorithm presents lhc s high level iterative approach.
global state we define as stheglobal state that maps each function fto its current state sfwhich is a triple of the form fuel type predicts .
the first element fuel is an integer representing the upper bound on the number of remaining verification attempts for f. the second element type is the current type that has been assigned to and locally verified for for denoting that no type has been assigned.
the third element predicts is a priority queue of the predicted types for fagainst which fhas not yet been locally verified.
the global state sis initialized such that for each fin the repository sffor each function fhas a maximum the sf.fuel is some maximum n the sf.type is and sf.predicts is empty.
call graph build callgraph generates the repository s call graph returning returns all function dependencies depsfalgorithm getpredictions s algorithm input state s function f output llm type predictions ps procedure getpredictions s f ifsf.predicts then r program s prompt makeprompt r f sf.predicts llmguess prompt ps sf.predicts return ps which is the set of functions that fcalls which we also call theimmediate dependencies off.
for example the program in figure has the following dependencies depsdivide depssize depsaverage .
worklist next we initialize a worklist wklof functions that our procedure is going to operate on.
the worklist wklis initialized with all functions f r such that the function fhas no dependencies.
these are the root functions of the repository from which lhc starts generating and checking types.
main loop the main body of the algorithm is on lines to which iterates till all functions are assigned types and wkl is empty.
in each iteration we pop the top function ffrom the wklstack.
then we get new or existing type predictions ps iv a for the function f. we try to locally verify fwith a type from the predictions ps iv b until we successfully find a type against which flocally verifies in the module with the current state s i.e.with the currently assigned types for the transitive dependencies of f. if none of the predictions ps verified the function f thus t we back jump tof s least tested dependency iv c to continue the iterations from there.
if instead a type tthat locally verified the function fis found then we update the state sf.type with the new type t and add in wklthe functions in s olved callers s f deps .
these are all the functions g rsuch that a gcallsf i.e.
f depsg and b all the dependencies of ghave an assigned type i.e.
h depsg.sh.type .
we then continue to the next iteration ensuring all functions in wklhave been locally verified.
a. generating type predictions algorithm describes the procedure of generating new type predictions for a function fgiven a global state s. initially we check if we have any remaining type predictions in the function s queue sf.predicts and if there are no new types are generated and the remaining queue is just returned.
otherwise we retrieve the relevant functions from the codebase given the current state s replacing any types that have already been locally verified i.e.already assigned to the type field in s and make a prompt for function fas discussed in iii b. this prompt is sent to the llm to generate new type predictions which are then added to f s prediction queue sf.predicts .
b. trying type predictions algorithm presents the process of trying new type predictions psfor a function fgiven a global state s to find the firstalgorithm trypredictions s algorithm input state s function f type predictions ps output successful type tor failure procedure trypredictions s f ps while ps andsf.fuel 0do t ps getnext ps sf.fuel sf.fuel iflhverify s f t then return t ifsf.fuel then return userhint f return algorithm backjump s algorithm input state s function f output transitive dependency f with max remaining fuel procedure backjump s f alldeps wkl f while wkl do f poptop wkl alldeps alldeps f wkl wkl g g depsf g alldeps f maxfuel for all g alldeps f do ifmaxfuel sg.fuel then f g maxfuel sg.fuel s reset dependencies s f return f prediction against which flocally verifies.
in each iteration we first check the remaining fuel for the current function f. if it has reached 0then we ask the user to provide a type as we discussed in iii g .
if there is more fuel left then we decrement f s fuel sf.fuelby one.
then we get the next prediction tfrom the queue ps and assign the type to fand query l iquid haskell to try to locally verify the program using the other types already assigned in s. if the verification process is successful we return the locally verified type t and otherwise we continue to the next iteration.
if we have tried all the predictions in the queue psand none of them locally verified the function f then we return signalling that none of the candidate predictions were successful.
c. back jumping to the least tested dependency algorithm outlines the b ackjump procedure which identifies and returns the transitive dependency of a function f that has the maximum remaining fuel.
the algorithm begins by finding all transitive dependencies of f. this is achieved using a worklist wkl initialized with f. the algorithm iteratively processes each function in the worklist by popping the top function f adding it to the set of all dependencies alldeps and then including all its immediate dependencies depsf that are not already in alldeps back into the worklist wkl.
this loop continues until the worklist is empty ensuring that all transitive dependencies of fare collected.
once all transitive dependencies are identified the next step is to determine the dependency with the maximum remaining fuel.
the algorithm initializes f to andmaxfuel to .
itthen iterates over each function ginalldeps excluding f. if the remaining fuel sg.fuel for a function gis greater than maxfuel it updates f togandmaxfuel tosg.fuel.
this ensures that the function with the maximum remaining fuel among the transitive dependencies of fis selected.
after identifying the function f with the maximum remaining fuel the global state sis reset for this function and its dependencies using the r eset dependencies procedure thereby restarting the verification process for f from scratch.
for each function fthat calls f reset dependencies will setsf.type to and recursively will be applied to each caller fto reset all functions that indirectly depend on f .
this allows the type verification process to strategically backjump to f and restart with updated predictions and fuel.
the selected function f is then finally returned.
v. e valuation next we describe our implementation of lhc lhc opilot and an evaluation that addresses three research questions rq1 how accurate are llms at generating single refinement types?
v a rq2 how precisely can lhc verify whole codebases?
v b rq3 how efficiently can lhc verify a given codebase?
v c large language models for our evaluation we focus on code llms i.e.llms that have been pre trained specifically for generating code.
such models don t usually require special system or instruction prompts to effectively generate the target code unlike c hatgpt or gpt o. our emphasis is also on smaller and public llms for two important reasons.
first they can be more robust in generating types for the hundreds of times it is necessary to verify the given codebase and likely just as effective .
second and more importantly to guard against the possibility of data leakage memorization we wish to ensure that our benchmarks are notin the training set for the models.
therefore we use the following llms s tarcoder 3b c odellama 7b and starcoder 15b with and billion trainable parameters respectively.
all the models are open source and have been pre trained on public datasets of code .
in each case the training data does not include the benchmarks considered here.
we run all experiments with s tarcoder 3b and c odellama 7b on an nvidia geforce rtx ti with gb vram and an nvidia a100 pcie with gb vram for s tarcoder 15b.
fine tuning datasets we fine tune our models using the stack v2 a public dataset of open souce code with permissive licenses that includes a vast number of programming languages.
while the dataset consists of less than .
of haskell programs those programs amount to over million.
of those programs only used l iquid haskell and refinement types.
from these programs we extract a dataset of unique refinement types and we use them to fine tune thellms.
specifically we use ql ora an efficient finetuning approach that reduces memory usage enough to finetune much larger llms on smaller gpus while preserving full bit fine tuning task performance.
ql ora backpropagates gradients through a frozen bit quantized pretrained language model into low rank adapters l ora .
we fine tune starcoder 3b and c odellama 7b for epochs while the larger s tarcoder 15b for epochs due to limited time and resources and its excessive cost.
benchmarks we explore different benchmarks in order to answer our research questions.
first for single refinement type prediction rq1 we evaluate the different llms on a new benchmark based on the publicly available online l iquid haskell tutorial .
we extract refinement types from the lht utorial3 corresponding to exercises with hidden solutions into separate programs along with their relevant context such as our running example in figure .
for each refinement type in this benchmark we build a llm prompt that includes the implemented haskell function with its type signature and any surrounding context from the corresponding exercise.
for example any relevant functions comments or possible input output test cases that can help verify the correctness of the target function.
additionally functions called by the target function that are already annotated with refinement types were also provided when available.
for the rest of our evaluation rq2 and rq3 we use two public haskell libraries hs alsa and b ytestring .
hsalsa is a haskell implementation of the salsa20 cipher with refinement types used to track the sizes of various ciphers and keys.
b ytestring is a haskell library for representing and efficiently operating on byte strings via pointer operations that is widely used across the haskell ecosystem we use refinement types to statically track pointer arithmetic and ensure low level memory safety.
hs alsa was annotated with refinement types by its developer including 96such annotated functions while we annotated b ytestring functions with refinement types in order to establish a ground truth type for these benchmarks.
while there are several other liquid haskell repositories available online they are in the training data for the llms we consider and hence are excluded from our evaluation.
emulating user interaction we selected benchmarks that are already fully annotated with refinement types for each function so that we could emulate the user interaction userhint in iv b in our experiments using these ground truth types.
that is when all the predicted types fail for a given target function we use the ground truth type used to annotate the function in the original benchmark asthe type that was provided by the user.
baselines we acknowledge the importance of evaluating against a baseline.
however as we discuss in section vi symbolic program synthesis methods have historically faced significant challenges in this domain.
recent advances in 3our benchmarks are available along with code for lhc at com gsakkas ref type predmachine learning and llms have enabled substantial improvements making this problem more tractable.
a purely random generation approach would be of limited utility given the immense size of the search space the refinement type space is much larger and more complex than the space of standard haskell types.
consequently there aren t any meaningful baselines in the existing literature for generating refinement types and verifying entire codebases effectively until now.
a. rq1 single type prediction accuracy table i presents the cumulative results on the lht utorial .
we have manually categorized the programs with single target functions that need to be annotated with a refinement type as easy medium and hard based on the complexity of the ground truth refinement type.
an example of an easy refinement type is average nelist int intfrom our running example in figure while a hard type would be for the following function zipornull as bs notemp as notemp bs len bs len as v if notemp as notemp bs then len v len as else len v zipornull zipornull zipornull zipornull xs ys zipwith xs ys this refinement type verifies the correctness of zipornull which zips two lists only if neither of them is empty.
other hard examples in our benchmark can include more complicated predicates or functions with more dependencies.
as with any code generation tasks longer programs or refinement types with more dependencies tend to be more complicated and difficult to generate with llms.
for each target function we generate refinement types using the pre trained llms and their fine tuned versions.
we also show the number of functions that the llms successfully verified at figure .
we observe that all models showcase great performance for easy types with slight improvements when fine tuned.
however there is great improvement for all llms in the medium and hard categories.
specifically s tarcoder 3b shows a program improvement in the medium category while c odellama 7b and starcoder 15b show a and program improvement respectively.
we see a similar improvement for the hard programs with an increase of and programs respectively.
we also present the pass kresults in table i. introduced the pass kmetric and the codex paper popularized it recently specifically for evaluating code generation llms.
to calculate pass k kcode samples are generated per problem and the problem is considered solved if any sample is correct where pass kis the total fraction of problems solved.
however as it has been observed in computing pass k this way can have high variance.
instead to evaluate pass k n ksamples per task are generated in this paper we usellm total easy medium hard pass pass pass pass pass starcoder 3b .
.
.
.
.
fine tuned .
.
.
.
.
codellama 7b .
.
.
.
.
fine tuned .
.
.
.
.
starcoder 15b .
.
.
.
.
fine tuned .
.
.
.
.
table i lht utorial results total single type benchmark where we divided the user intended type into difficulty categories.
we also present the pass kmetrics for the full benchmark.
starcoder 3b codellama 7b starcoder 15b010203040506068correct types lht utorial pretrained hard medium easy finetuned hard medium easy fig.
llm accuracy in generating single refinement types.starcoder 3b codellama 7b starcoder 15b010203040506070809096correct types hsalsa pretrained user lhc finetuned user lhc qualifiers user lhc dependencies user lhc starcoder 3b codellama 7b starcoder 15b01020304045correct types bytestring pretrained user lhc finetuned user lhc qualifiers user lhc dependencies user lhc fig.
lhc accuracy in generating and verifying refinement types for our haskell benchmarks.
n andk and the number of correct samples c n is counted in order to calculate the unbiased estimator pass k e problems n c k n k pass 50here represents the total accuracy of each llm for the lht utorial i.e.the percentage of the target functions that the llms generated at least one correct refinement type.
by fine tuning we observe great improvement for all pass kmetrics reaching a pass 10of85 and a pass of91 in some cases.
we also see that there isn t a big improvement for k 10samples especially for the fine tuned models indicating that sampling even refinement types per function can yield very accurate results with a fine tuned llm.
fine tuned llms even with a small number of trainable parameters learn to encode l iquid haskell programs and can generate correct refinement types for of the target functions an improvement of up to from the out of the box models.
b. rq2 whole codebase precision next we evaluate lhc on the two haskell codebases hsalsa and b ytestring .
we evaluate here four different approaches with each llm presented in table ii for hsalsa and b ytestring .
first we use the original pretrained llms as a backend for lhc in order to generate the various refinement types.
second we fine tune the models as described before.
next we enable the q ualifiers verification optimization an optimization which automaticallyextracts all qualifiers from the llm predicted types and adds corresponding pragmas in the context of the program as described in iii d. this optimization also adds the relevant wildcard type as a last candidate type that is tested when all llm predicted types are exhausted which tells liquid haskell to perform symbolic type inference using the qualifiers.
finally we enable the d ependencies prompt optimization for additional context.
this optimization uses few shot prompting to add the verified dependency functions to the llm prompts iii b .
automatically verified this metric first column of table ii indicates the number of functions that were automatically annotated and verified by lhc without human intervention.
for example s tarcoder 3b in its fine tuned version correctly inferred types for functions in hs alsa and verified up to functions when the q ualifiers and d ependencies optimizations are enabled.
c odellama 7b shows similar improvement.
however the larger s tarcoder 15b reaches up to functions that are correctly annotated by lhc.
correct unverified type predictions this metric shows the number of times that the llms generated a correct refinement type for a function but lhc ran out of fuel as discussed in iv b and the function was not automatically and locally verified by lhc thus not included in the previous metric.
in a real world setting however before asking the user to manually write a refinement for these unsuccessful functions as a preliminary step we could provide the list of llmpredicted types and have the user select or approve one in order to mitigate the manual effort.
while a higher number for this metric indicates less dependency on manual input it still represents wasted cycles for lhc where it couldn t arrive tobenchmark llmautomatically correct lhc total lhc type pred.
verification verified unverified iterations time mins time mins time mins hsalsa 20starcoder 3b fine tuned q ualifiers d ependencies codellama 7b fine tuned q ualifiers d ependencies starcoder 15b fine tuned q ualifiers d ependencies 153bytestringstarcoder 3b fine tuned q ualifiers d ependencies codellama 7b fine tuned q ualifiers d ependencies starcoder 15b fine tuned q ualifiers d ependencies table ii hs alsa functions and b ytestring verification results functions the correct combination of refinement types in order to locally verify the faulty ones.
we observe that all llms have relatively high numbers when q ualifiers and d ependencies were not used.
when we use both optimizations the numbers go as low as only for s tarcoder 15b since lhc was able to automatically verify the vast majority of the functions when using this llm as we showed earlier.
cumulative results figure also shows the cumulative results of the previous two metrics i.e.the total number of functions lhc was able to generate a correct type for and were either automatically or manually verified.
we observe that for hs alsa even without the last two optimizations lhc generates a significant fraction nearly of correct types but the optimizations nevertheless improve the accuracy of types automatically generated by lhc to more than .
however for the more complicated module b ytestring though we observe that without the q ualifiers and d ependencies optimizations we can only generate of the types and the addition of symbolic qualifier inference and context provides a significant improvement allowing lhc to generate correct types for up to of the functions.
lhc can automatically generate formally verified types for up to of a codebase s functions.
c. rq3 efficiency the last four columns of table ii summarize how efficiently lhc can verify codebases.
lhc iterations this metric counts the number of iterations that lhc needs to verify the full module.
fewer iterations suggest a more efficient verification process.
we observe that for hs alsa that has more functions to verify and deeper dependencies there is a significant improvement inthe time we spent verifying them when we use q ualifiers and d ependencies .
on the other hand for b ytestring we observe a slight overhead for unclear reasons but perhaps due increasing the size of the prompts and getting diminishing results from the extra context in terms of efficiency.
total lhc time this metric provides an overview of the total time in minutes that lhc spent in fully verifying the modules.
at a high level it is remarkable that lhc is able to annotate and verify entire codebases in hours as typically this work takes days or weeks for a human to do.
of course this comes with the caveat that with these benchmarks we know that suitable types exist and we emulated human assistance when the llm got stuck .
the results also indicate that the verification time varies significantly depending on the llm used and the specific optimizations applied.
for instance we see that lhc with s tarcoder 3b required minutes to verify hs alsa which was reduced to minutes with fine tuning then adjusted to minutes with q ualifiers but drops to minutes with d ependencies .
similarly codellama 7b and s tarcoder 15b models show notable reductions in verification time when dependencies are included.
in the b ytestring results the trend is consistent for s tarcoder 3b and c odellama 7b.
however starcoder 15b shows no significant improvement.
type prediction verification time these metrics evaluate the time spent by lhc on generating type predictions using llms compared to the time required for codebase verification with l iquid haskell .
our analysis reveals that smaller llms are notably more efficient generating refinement types more quickly and consistently than their larger counterparts.
specifically type generation by llms accounted for approximately of the total processing time when using s tarcoder 3b whereas it increased to when employing s tarcoder 15b.
verification times with l iq uidhaskell remain very consistent across different models.
finally introducing the q ualifiers and d ependencies optimizations led to slight reductions in verification time primarily due to the enhanced accuracy of our approach.
lhc can with modest emulated human assistance annotate and verify entire codebases in a few hours.
the fine tuning q ualifiers and d ependencies optimizations greatly enhance verification efficiency by reducing verification time by an average and up to across real world codebases.
d. threats to validity we note three threats to the validity of our results.
first we have only considered three codebases the lht utorial hsalsa and b ytestring .
it is entirely possible that larger or more complex codebases may require annotations that cannot be generated so effectively by llms.
second our approach currently doesn t support mutually recursive functions .
a potential solution to this is to break the cycles created by these mutually recursive functions by either requiring the programmer to specify a type for one of the functions of the cycle or speculatively breaking the cycle and letting our backtracking mechanism synthesize the types.
however we have not tried either of these approached as mutually recursive functions don t occur in our benchmarks.
third we have emulated human assistance in our evaluation meaning on our benchmarks we know a priori that suitable refinement type annotations exist.
in a more realistic scenario using lhc on a new codebase such types may not exist because bugs in the code may require it to be modified or because of limitations in the verifier l iquid haskell itself.
we defer the evaluation of lhc on new codebases with actual users to future work but note that this is challenging as realistically annotation requires days or weeks of human effort .
vi.
r elated work finally we discuss some related lines of work on using machine learning and llms to automate program verification.
generating proof annotations lhc is most closely related to several other neurosymbolic approaches to generating the annotations needed for formal verification.
c ode2inv and pei et al.
present techniques to synthesize loop invariants using neural networks and fine tuned llms respectively.
kamath et al.
and integrates llms natively with automated reasoners escjava and esbmc to additionally check whether the generated invariants are actually inductive and optionally further to repair the invariants by querying the llms and reducing the proposed invariants into an inductive set using the h oudini algorithm .
several studies have also explored the generation of annotations and formal postconditions using advanced techniques.
similarly nl2postcond investigates the transformation of natural language intent into formal method postconditions using llms proposing metrics for assessingthe quality of these transformations.
finally l aurel automatically generates helper assertions for proofs written in d afny by leveraging llms as well.
all the above focus on a single goal generating loop invariants or contracts for single functions in isolation.
in contrast lhc is an interprocedural method that aims to generate interdependent refinement type annotations which generalize invariants preand post conditions across the whole codebase.
generating code from specifications a different line of work looks at using llms to synthesizing code from formal specifications in proof oriented languages.
misu et al.
and clover use llms to synthesize verified d afny code corresponding to natural language and formal specifications.
similarly chakraborty et al.
investigate using llms to synthesize f programs and proofs from dependent type specifications.
in contrast to the above lhc s goal is not to generate code but only the refinement contract annotations needed for modular verification of existing code bases .
llms for proof generation several groups have looked into using machine learning techniques to automate proofs written in tactic based interactive theorem provers.
sanchez et al.
uses machine learning techniques to generate coqproofs.
b aldur explores the use of llms to generate entire isabelle hol proofs for program verification a departure from traditional proof assistants that generate one proof statement at a time.
complementing this yang et al.
introduces l eandojo a tool that combines llms with retrieval augmented mechanisms to enhance theorem proving in the l ean environment demonstrating improved proof generation capabilities.
wu et al.
evaluates the performance of llms in autoformalization in isabelle hol introducing a neural theorem prover trained on autoformalized statements.
unlike the above lhc is designed to work in the setting of smt based auto active verification where the only programmer input is the code and the type specification the rest is handled by the smt solver.
in context prompting in context prompting techniques have been explored to enhance the few shot learning capabilities of llms.
liu et al.
investigates the relationship between incontext example selection and gpt s few shot performance introducing a retrieval model for better example selection.
su et al.
proposes a framework for selective annotation to improve accuracy in in context learning scenarios.
lu et al.
demonstrates the importance of prompt order using modelgenerated sequences to find optimal prompts while sorensen et al.
introduces an information theoretic approach to prompt engineering maximizing mutual information to select the best prompts without relying on model weights or ground truth labels.
l aurel introduced a lemma similarity metric to import potentially related lemmas to the current proof.
these lines of work inspired our d ependencies optimization where we augment our prompts with additional context.
unlike the above lhc does not rely on similarity heuristics but instead it includes the function dependencies of the target function we are trying to locally verify.