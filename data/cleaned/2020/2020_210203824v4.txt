neural termination analysis mirco giacobbe university of birmingham ukdaniel kroening amazon inc. usajulian parsert university of oxford uk abstract we introduce a novel approach to the automated termination analysis of computer programs we use neural networks to represent ranking functions.
ranking functions map program states to values that are bounded from below and decrease as a program runs the existence of a ranking function proves that the program terminates.
we train a neural network from sampled execution traces of a program so that the network s output decreases along the traces then we use symbolic reasoning to formally verify that it generalises to all possible executions.
upon the affirmative answer we obtain a formal certificate of termination for the program which we call a neural ranking function.
we demonstrate that thanks to the ability of neural networks to represent nonlinear functions our method succeeds over programs that are beyond the reach of state of theart tools.
this includes programs that use disjunctions in their loop conditions and programs that include nonlinear expressions.
ccs concepts software and its engineering software verification and validation formal software verification computing methodologies machine learning machine learning approaches .
keywords artificial intelligence and machine learning for software engineering computer aided verification formal methods program analysis ranking function synthesis termination analysis introduction software is a complex artefact.
programming is prone to error and some bugs are hard to find even after extensive testing.
bugs may cause crashes undesirable outputs and can prevent a program from responding at all which causes poor performance and can be a vulnerability .
termination analysis addresses the question of whether for every possible input a program halts.
this is undecidable in general yet tools that work in practice have been developed by industry and academia .
in this paper we introduce a novel technique that effectively trains neural networks to act as formal proofs of termination and thanks to the expressivity of neural networks significantly extends the set of programs that can be proven to terminate automatically.
to argue that a program terminates one usually presents a ranking function for each loop in the program.
ranking functions map program states to values that i decrease by a discrete amount after every loop iteration and ii are bounded from below .
they are certificates of termination if a ranking function exists then the program terminates for every possible input.
the authors are listed in alphabetical order regardless of individual contributions or seniority.
this work was done prior to joining amazon.int x y z ... while x y x z x figure a simple program with disjunctive loop guard.
many existing methods find ranking functions by relying on symbolic reasoning .
moreover they typically focus on the case of linear ranking functions for programs that can be represented as conjunctions of linear constraints.
for this particular case farkas lemma offers a means to compute the ranking function efficiently .
however finding proofs for programs that use disjunctive e.g.
fig.
or nonlinear loop guards is much more difficult.
our approach is based on the principle that finding a proof is much harder than checking that a given candidate proof is valid .
we use machine learning to guess a proof followed by symbolic reasoning to verify it first we learn a candidate ranking function by training a neural network that decreases along sampled runs of the program then we use satisfiability modulo theories smt solving to check whether this candidate neural ranking function nrf decreases along every possible run of the program.
we use networks that are always bounded from below thus upon success we have a proof of termination.
our experiments with established termination benchmarks demonstrate that the idea is effective most loops can be proven to terminate using very tiny neural networks fewer than neurons and at most a thousand sample runs.
we give exemplars of neural architectures and loss functions for training monolithic and lexicographic ranking functions.
using a neural network with just one hidden layer and a straight forward training routine our method discovers neural ranking functions for over of the benchmarks in a standard problem set for termination analysis .
our method subsumes a broad range of existing termination analysis strategies not only do we discover linear ranking functions but also ranking functions for problems that require piecewise linear or lexicographic termination arguments .
furthermore we observe that the ability of neural networks to represent nonlinear functions enables termination proofs for programs that go beyond what the state of the art can handle.
programs that use disjunctive or nonlinear loop guards as well as programs that require piecewise linear ranking functions are proven terminating just as easily by our new technique.
while we perform our experiments with java programs our training procedure is agnostic with respect to the programming language and requires no information about the program other than execution traces.
it applies without modifications to software that constructs data structures as long as a procedure for verifyingarxiv .03824v4 sep 2022mirco giacobbe daniel kroening and julian parsert x y zrelu relu figure neural ranking function for the program in fig.
.
the candidate ranking functions is available.
the complexity of formal reasoning about the program is entirely delegated to the verification procedure which only has to solve the task of checking the validity of a given ranking function.
illustrative example we prove that programs terminate by showing that their loops admit ranking functions.
constructing ranking functions for loops that involve disjunctive loop guards or involve nonlinear constraints is hard for existing technologies.
as an exemplar consider the loop in fig.
.
this loop terminates for every arbitrary initialisation of the variables x y and zand only involves linear constraints.
yet a linear ranking argument is insufficient to prove that this loop terminates.
naively one might believe that xis a ranking argument because it decreases with every loop iteration however it is not bounded from below by a given constant coefficient.
notably the same holds for expression y z x which can be always assigned to a value that is smaller than any given constant coefficient with an adversary initialisation of yandz if we assume that x y and z can take any unbounded integer.
in fact under this assumption no linear combination of x y and zis a valid ranking function for this loop which requires a nonlinear ranking function.
a valid ranking function for the program in fig.
is f x y z max y x max z x .
this function not only decreases in every iteration but is also nonnegative for every valuation of x y and zand it is bounded from below by zero.
this function corresponds to the simple neural network with relu activation functions in fig.
which is effectively learned and verified by our method.
we argue that neural networks are a powerful model to represent ranking functions of non trivial programs.
loops that include disjunctions in their loop guards are not rare.
similar behaviour can be induced by conditional control flow early breaks or by throwing exceptions.
suffice it to say that the following loop is semantically equivalent to the example in fig.
while true if x y x z break x moreover we also argue that decoupling the process of guessing the ranking function from that of checking it enables us to effectively discover termination arguments for programs that involve nonlinear constraints.
as it turns out as of today neither aprove nor ultimate can determine within a time budget of s that the following loop terminates while i k j while j i j i l1 l2 l3 l4 l5l6iload 0 if icmpge l6iload 2 iload 1iconst 0 istore 1 if icmpge l5iload 0 iinc goto l3 iinc goto l1 a b figure a java program and the respective cfg.
while x x x y x by contrast our method learns and verifies the ranking function max y x in less than a second.
background programs and transition systems.
a computer program is a list of instructions that together with the machine interpreting them defines a state transition system over the state space of the machine.
a state transition system is a pair p s t wheresis a countable possibly infinite set of states and t s sis a transition relation.
a state contains all information that is necessary to determine its successor state s .
for example this can include the value of variables that are explicitly declared in the source code or that exist as registers in the interpreter e.g.
the program counter .
the transition relation determines the successors of a state.
a state without any successors is a terminating state.
a state may have multiple successors because of operations that are external to the program e.g.
non deterministic input assignments or are underspecified and may therefore have multiple outcomes.
a run of pis any sequence of states s s s ... such that s i s i tfor all i .
we say that a program terminates if all its runs are finite.
control flow graphs and loop headers.
a control flow graph cfg for program pis a finite directed graph g l e wherel is a finite set of control locations and e l lis a set of control edges.
for java programs a cfg can be obtained for its bytecode as illustrated in fig.
.
control locations correspond to source and target addresses of jump instructions or entry or exit points of a procedure.
control edges indicate whether there exists a sequence of instructions or a jump that lead from the respective source to the respective destination location.
a state is on a control location if the next instruction to be executed is on a control location in the java virtual machine this is determined by the program counter.
notably we have that for every finite run s ... s k such thats ands k are respectively on control locations landl the control graph must admit a path from ltol .
then letl lbe any subset of control locations.
we define sl sas the set of states on any location inl andtl sl sl as the the maximal relation of states on control locations in l such that s s k tl only if there exist a finite run s s ... s k s k that does not encounter any control location in l in between i.e.
s ... s k sl .
specialneural termination analysis ... ... ...... x y z r x y z r program code execution traces candidate neural ranking function terminates unknownhyperparameters learning verification figure schema of our framework.
control locations are loop headers that is the dominators entry locations of the strongly connected components in the graph for instance the loop headers for fig.
3b are l1 and l3.
they are important in termination analysis because every run along a loop i.e.
forandwhile statements necessarily enters and iterates over at least one of them.
we denote the set of loop headers as h l. ranking functions.
to determine whether a program terminates our method attempts to find a ranking function for it.
a function f sh ris a ranking function for the program pif f s f s for all s s th and the relation r is well founded .
the existence of a ranking function proves that the program terminates and every program that does not terminate necessarily lacks a ranking function.
a standard way of giving a ranking function is to identify a function that maps states at loop headers to sequences of numbers that i decrease by a discrete amount and ii are bounded from below.
another way is to define tuples of functions that decrease lexicographically at loop headers which is particularly useful for nested loops.
overview of the method we propose a framework for termination analysis using neural networks as ranking functions.
these neural ranking functions are first trained over execution traces of a program and subsequently verified in combination with the program code.
thus the three parameters of neural termination analysis are the tracing sampling strategy the neural network architecture and the verification procedure.
these steps are independent from each other.
hence changing the learning setup e.g.
considering different models other learning frameworks etc.
only requires a change in the second step.
similarly considering different input languages or alternative verification procedures only requires a change in the verification and potentially tracing procedure.as illustrated in fig.
in the first step we collect execution traces for a program pthat we want to show terminating.
these execution traces are used subsequently as training data to train the neural ranking functions.
these traces are obtained by first generating test input data for pand then tracing the execution of p with the test input data.
since the training phase exclusively works with these execution traces it is important that they adequately represent the behaviour of the program p. more details can be found in appendix a. in the second step we use the execution traces to train neural networks to become neural ranking functions.
we discuss suitable choices for neural network architectures in sect.
.
we always use neural architectures that guarantee that the neural network s output is bounded from below.
the training procedure minimises a loss function that punishes neural networks that do not decrease over the sampled observations.
as a result we obtain a neural network that behaves like a ranking function over the sampled executions traces.
finally to assert that a trained neural ranking function generalises to all possible executions we pass it to a formal verification procedure.
the formal verification procedure encodes both the program and the neural network symbolically more details about our encoding are shown in appendix b .
then it uses smt solving to formally decide whether the neural network is a valid ranking function for the program.
upon an affirmative result we conclude that the program terminates upon a negative result we return an inconclusive answer that is the program may or may not terminate.
the verification procedure guarantees that our method is sound.
the combination of a particular sampling strategy neural architecture and verification procedure is an instance of our approach which offers a flexible and extensible termination analysis framework that combines testing methods deep learning and symbolic reasoning.
neural ranking functions our method collects execution traces for a program and then proceeds in two phases first it trains and then it formally verifies.
themirco giacobbe daniel kroening and julian parsert first phase thus takes a set of execution traces as input and returns a neural network as output.
the set of execution traces forms the dataset which is used to train a neural network that mimics a ranking function along these traces.
the neural network is trained by minimising a loss function that ensures that the neural network decreases by a discrete amount after every pair of subsequent observations in the traces.
we analyse every loop in the program and provide a ranking function for each of them.
we provide two strategies for training these candidate neural ranking function.
the first strategy trains a monolithic ranking argument that is a neural ranking function that outputs one value that decreases along the traces.
the second strategy generalises the first and trains a lexicographic ranking argument that is a neural ranking function that outputs many values that decrease lexicographically.
whether to use one or the other strategy is heuristic lexicographic arguments are normally suitable for nested loops.
observation functions and traces.
we train our candidate neural ranking function from states collected along program runs.
however this is made difficult by the fact that the states of a program are large and complex and contain internal information that is not directly amenable to deep learning.
the standard approach to address this issue is to construct an embedding defined by means of an observation function.
an observation function sl rn extracts vectors of numerical values that can be taken as input by a neural network from states that are on a specific set of control locationsl l. these numerical values can be for instance the values of numerical variables in memory.
with an observation function we convert any run into a trace o o o ... which is the sequence of observations in rnrecorded every time a location in l is encountered.
in other words every such trace corresponds to a sequences s s ... of states insl such thato i s i and s i s i tl for alli .
general architecture for neural ranking functions.
we use feedforward neural networks as models of ranking functions.
generally this is a function f rn rmwithninputs moutputs and parameterised by defined in terms of interconnected neurons partitioned into one input one output and kintermediate hidden layers.
the intermediate hidden layers are defined as a parameterised function w b x relu wx b where the parameters are a matrix of weights wand a vector of biasesb.
these define an affine transformation of the input while relu applies a nonlinear transformation max element wise to each of the hneurons in the respective hidden layer that is relu x1 ... xh max x1 ... max xh .
the parameters of the network are thus a sequence of weight matrices and bias vectors w b ... w k b k each of which corresponds to a hidden layer.
we impose that the output layer has no bias its weights w k are not trainable and their coefficients are non negative.
the neural network thus defines the function f w k w k b k w b whose output is in turn guaranteed to be non negative for every valuation of inputs and parameters.
this ensures that the function...x1 x2 xn...... ... 1f x relu relu trainable parameters figure architecture for monolithic nrfs.
output is always bounded from below by zero.
to train this neural network so as it behaves as a raking function it remains to ensure that it decreases after every loop iteration.
monolithic ranking loss.
a monolithic neural ranking function is the special case where m one example is depicted in fig.
.
our goal is to train it in such a way its output decreases by a discrete amount 0along a set of sample traces of observations collected every time a loop header is encountered.
for this purpose we define an embedding using an observation function sh rn.
with this embedding we record multiple traces from the program under analysis and store a dataset of observation pairs d rn rnsuch that for every pair o o we have that ois immediately followed byo in the trace.
in other words dconstitutes a sliding window of size two over the trace.
note that dpossibly contains the pairs of multiple execution traces.
we train our network so as to decrease between every pair o o dof sampled observations f o f o .
to this end we solve the following optimisation problem arg min 1 d o o dmax f o f o z l o o functionl o o is the loss of the neural network over a given pair.
the higher the value of lthe more the network increases over the pair whereas for pairs that decrease by at least the value is always i.e.
negative values do not affect the sum .
as a result of the optimisation problem we obtain parameters that ensure the network decreases along all sampled traces.
example .
.
consider the program in fig.
.
for this program we define an embedding sh z3that observes the values of x y and zevery time a run hits the entry location of the loop.
we reason about the loop in isolation and sample random initial values for the variables.
two example traces are and .
the dataset corresponding to exactly these traces thus contains pairs of consecutive observations.
we use the monolithic architecture in fig.
which has exactly one hidden layer made of two neurons.
we set 1and observe that once the parameters inneural termination analysis ...x1 x2 xn... ... .... .
.
.
.
.... ... ...relu relurelu relu...f1 x fm x trainable parameters figure architecture for lexicographic nrfs.
fig.
are attained the loss function lmeasures zero over all pairs which is its minimum.
the corresponding function fin eq.
maps both traces to the sequence .
the smt solver successfully verifies that the following formula evaluates to true for every possible assignment to x y z x y z f x y z f x y z unprimed variables represent an observation before an iteration and primed variables after an iteration.
the validity of this formula confirms that fdecreases by for every possible assignment of the variables and is thus a valid ranking function.
note that fis bounded from below by construction owing to the use of relus.
lexicographic ranking loss.
neural ranking functions with m 2can learn lexicographic ranking arguments.
figure gives an example of an architecture for this purpose.
our goal is training the neural network to ensure that every pair of observations decreases some output neuron by as long as all other outputs with smaller index do not increase.
lexicographic arguments are suitable to programs with multiple loop headers.
in this case we associate to each loop header an index i ... m for the output neuron that we expect to decrease every time the header is visited.
for every header we define an embedding that records an observation every time that header is visited.
we thus obtain multiple datasets d1 ... dmfrom our sample runs one for each header.
we train our network to obtain that for each pair o o diat thei th header the output neurons decrease lexicographically as follows fi o fi o and fj o fj o for allj i. to train the network we solve the following problem arg min 1 d1 dm m i o o dili o o the loss of a pair of observations is determined by the dataset it belongs to li o o max fi o fi o i j 1max fj o fj o .
functionli o o takes its minimal value when both conditions and are satisfied.
if also attains value then all samples satisfy these conditions and the network constitutes a lexicographic neural raking function over the sampled traces.
example .
.
consider the program in fig.
which has two nested loops.
the outer loop has its header at location l1 and the inner at location l3.
we associate l1 with index and l3 with index and learn a lexicographic argument accordingly.
we define two embeddings 1 s l1 z3and 2 s l3 z3that map states at control locations l1 resp.
l3 to the values of i j and k. we take for this example one sample run with initial values and obtain the following two datasets of pairs induced by 1and 2respectively d1 d2 we use a neural network as in fig.
with one hidden layer and one hidden neuron in each of the two blocks.
the first output must converge to the following function f1 i j k max k i .
the second output may converge to either of the following functions which are both valid f2 i j k max i j f2 i j k max k j proving that these functions are a valid termination lexicographic argument relies on auxiliary invariants which as discussed in the appendix we extract using a syntactic heuristic.
for instance checking that decreases along the inner loop requires the auxiliary invarianti k checking that decreases along the outer loop requires an argument that the inner loop leaves kandiunchanged.
experiments we present an experimental evaluation to answer the following research questions rq1 can neural ranking functions be used to formally prove the termination of programs?
rq2 do neural ranking functions advance the state of the art in termination analysis?
rq3 how do neural ranking functions scale in terms of the complexity of the program?
to answer these questions we developed a prototype implementation of the methods discussed in the previous sections for proving termination of java programs which we name term .
our implementation strictly separates tracing learning and verification as discussed in sect.
.mirco giacobbe daniel kroening and julian parsert aprove 09 term crafted nuterm advantage combined combined term .
.
.
.
.
.
.
.
aprove .
.
.
.
.
ultimate .
.
.
.
.
dynamite .
.
.
table results of running term with neurons aprove ultimate and dynamite onterm crafted aprove 09 and nuterm advantage .
in the case of term we report the average results rounded to the first decimal.
the last two columns show the union of the first two problem sets and all three problem sets respectively.
to obtain program traces we first generate test data for the program in question using a multivariate normal distribution.
subsequently we execute the program using this test data while maintaining control of the execution and collecting memory snapshots using the java virtual machine tool interface jvmti .
for more details on sampling and tracing we refer to appendix a. once tracing is completed this data is used to train the neural ranking function where pytorch is used as the machine learning framework.
finally we encode the problem of certifying the neural ranking function into an smt formula and use z3 to solve it.
more details about the implementation of this step can be found in appendix b. benchmarks and setup we consider three sets of programs for our experimental evaluation.
the first and second set are comprised of problems from the termcomp termination competition and the the sv comp software verification competition .
both problem sets are publicly available and cover a wide variety of termination and nontermination problems as well as software verification in general.
from these sets we discard the non terminating programs as they do not have ranking functions.
furthermore for this evaluation we consider deterministic programs with a maximum of two nested loops without function calls.
hence we focus on the two problem sets aprove 09 from termcomp and term crafted from sv comp.
after dropping problems due to the aforementioned constraints we are left with problems from term crafted originally and problems originally from aprove 09 .
the problem set term crafted comprises problems from literature on termination analysis which are given as c code.
we therefore translate these problems into java by hand.
in addition we also split the main functions bodies into an initialisation and loop part.
note that this purely syntactic change does not alter the difficulty of determining termination of a program.
finally we present an additional problem set nuterm advantage consisting of problems created by us for showcasing notable strengths and weaknesses of the tested tools.
to answer rq2 we compare term toultimate aprove anddynamite which collectively represent the state of the art in termination analysis.
setup.
the experiments were conducted on linux kernel .
running on an intel core i7 5820k at .
ghz with gb ram andan nvidia gtx graphics card.
for learning we use the adam optimiser provided by pytorch and a learning rate of .
.
we run the benchmarks times with random seeds that were fixed a priori for reproducibility.
full instructions on how to reproduce the results including the seeds are part of the supplementary material.
we ran all tools with a timeout of seconds for each problem.
experimental results can neural ranking functions be used to formally prove the termination of software programs?
to answer this question we ran term on the three benchmark sets mentioned above.
the results are given in tab.
.
we observe the best performance of term when using a neural network consisting of neurons with sample traces with a maximum length of .
the exact strategy used is described in the supplementary material.
term proves termination for .
out of problems on average in the best out of the five runs which accounts for .
of the problems in the problem set.
when considering the different problem sets separately we solve .
aprove 09 .
term crafted and .
nuterm advantage of the problems.
note that even when disregarding the nuterm advantage set term solves .
of the problems.
given that very simple neural networks suffice to prove termination of a substantial subset of the standard benchmarks we answer rq1 in the affirmative.
do neural ranking functions advance the state of the art in termination analysis?
to compare with the state of the art we also ranultimate aprove and dynamite on the same benchmarks.
since ultimate and dynamite do not support java code as input we ran the experiments on the c versions of the problems.
the results are presented in tab.
.
overall the strongest tool is aprove which solves .
of all problems followed by term with .
and ultimate with .
and finally dynamite with .
.
by considering the preexisting data sets separately we see that term comes in first on the aprove 09 set and third on term crafted .
on these two sets combined term solves .
of the problems with aprove andultimate solving .
and .
respectively and dynamite trailing with of problems solved.
we conclude that on the existing benchmarks term performs comparably to the state of the art.
our hypothesis is that term advances the state of the art when applied to programs that have either disjunctive loop conditions or programs that are nonlinear.
however the existing benchmark setsneural termination analysis suffer from confirmation bias and focus on programs that avoid these features.
to show that term indeed advances the state of the art we have compiled the nuterm advantage data set with programs that feature non linear conditions and disjunctions in conditions.
the following code snippet is from dynamiteexamplex4.java from thenuterm advantage problem set int a while a a n a a this loop only uses two variables aandn wherenremains constant throughout the execution while ais incremented by in every iteration.
despite the fact that the loop guard a2 nis nonlinear there is a linear ranking function.
furthermore the execution traces of the loop only show the incrementing of a which is also linear.
our tool term can solve this problem with a tiny neural network consisting of a single neuron and reports the ranking function relu n a .
neither aprove 09 norultimate are able to prove termination of this problem but dynamite a tool which also utilises execution traces can solve it.
disjunctions increase the complexity of formal reasoning significantly.
this is illustrated by the following code snippet from square2varsdisj.java in the nuterm advantage set int a b while a a m b b n a a b b none of the tools we compare with can show termination of this loop while term proves termination by learning the ranking functionrelu m a relu n b .
note that dynamite is able to show termination if the loop head was either whilea2 mor whileb2 n. however once both conditions are connected with a disjunction dynamite fails to show termination.
the nuterm advantage problem set comprises problems that exhibit either nonlinear conditions disjunctions or a combination of both.
for this set term solves on average .
of the problems while dynamite comes second solving followed by aprove .
and ultimate .
.
in conclusion the experiments conducted and presented in tab.
show that on existing benchmarks term either performs either comparable to or stronger than e.g.
on aprove 09 the state of the art.
furthermore we identified weaknesses in the existing tools when considering a broader range of programs and show that neural termination analysis can solve these problems by providing a set of programs on which term outperforms all existing tools by a large margin.
thus we can answer rq2 in the affirmative.
how do neural ranking functions scale in terms of the complexity of the program?
our experiments have only required tiny neural networks consisting of no more than neurons.
the results presented in tab.
were obtained by a neural network consisting of neurons.
increasing the number of neurons further does not yield significant gains on the existing problem sets as shown in figure percentage of problems solved for neural networks with to hidden neurons.
fig.
.
we hypothesise that programmers avoid writing loops that require termination arguments that depend on a very large number of variables.
to evaluate how our technique scales in the number of variables that are required for the ranking argument we use the following program template designed to require at least kneurons.
int a while a a n 1 ... a a n k a we include the instances of this template for values of kup to 4in thenuterm advantage problem set.
note that neither aprove nor ultimate is able to solve this problem for any value of k dynamite is able to solve these problems for kup to with a noticeable increase in runtime from s to s but times out for any larger k. note that term solves these problems in the problem set within seconds.
trying programs with kup to it becomes clear that the learning procedure continues to scale well while the verification starts to become the bottleneck.
it is worth emphasising that neural networks with 7neurons for example which would be able to prove such a loop for k 7terminating are laughably small compared to state of the art neural networks used in other areas such as natural language processing.
hence it is likely that by increasing the size of the neural network one would sooner run into issues verifying the neural networks and generating meaningful traces than training the neural networks.
furthermore we hypothesise that the vast majority of loops in real world programs do not have termination conditions that involve hundreds of disjuncts.
in conclusion we can tentatively answer rq3 by suggesting that neural termination analysis scales well in the complexity of the program noting that other parts such as verification and tracing might not scale as well.
discussion owing to the inherent difficulty of the problem at hand termination methods for solving it are necessarily incomplete.
neuralmirco giacobbe daniel kroening and julian parsert termination analysis is no exception.
under this constraint we presented an experimental evaluation to answer three research questions regarding the efficacy of neural ranking functions rq1 their advantages over other approaches rq2 and their scalability rq3 .
despite the favourable answers for each of the questions it is important to point out weaknesses of our approach.
for example it is easy construct programs that other methods can prove terminating but where neural termination analysis fails.
usually these fall into one of the following three cases.
insufficient data.
neural termination analysis learns termination arguments from execution traces.
hence any program feature that limits the data that can be collected is a problem for our approach.
several benchmarks in the dataset exhibit behaviour such as the following while x x x this loop has more than one iteration if and only if xis one of 4and even then the trace is extremely short.
similar issues can occur when offsets or certain program branches only occur in rare cases.
such instances may lead to overfitting of the networks to the sampled traces.
as a result a learned neural ranking function may satisfy all required properties over the sampled traces but not when verifying it with respect to all possible inputs in the verification procedure.
solver driven test input generation may be a means to ensure that traces for these behaviours are included in the training data set.
another issue related to insufficient data is the existence of large constants in the problems.
for instance considering a problem such as the following while x x this would require complete traces letting xgo all the way to which take a long time to gather.
furthermore learning a bias that large can also pose problems.
model expressivity.
as ranking functions become more complex we need neural architectures that are able to express them.
one instance from the dataset where this problem manifests is a benchmark where the ranking function depends on whether an input variable is even or odd.
none of the neural architectures discussed in sec.
is expressive enough to capture the concept of even and odd .
one way of solving this problem is by considering further neural architectures which would require a more sophisticated data collection.
the key limiting factor when deploying such architectures will likely be the increased complexity of the verification process rather than the learning.
verification.
when there are multiple correct ranking functions the verification procedure may not be able to prove all of them correct.
the following loop can exhibit such behaviour int j i while i i j i when purely looking at the execution traces which is what the learning procedure does iandjhave the exact same values at the loop head.
hence if the learning process comes up with the ranking function jthe verifier would not be able to prove it correct unless it is supplied with the auxiliary invariant that i jat the loop head.
one way to solve this problem could be to integrate existing methods that discover such invariants .
threats to validity we discuss threats to the validity of our experimental claims.
benchmark bias.
all our claims depend on the choice of benchmark programs.
we focus on sequential java programs and programs in other programming languages or programs that use concurrency may require ranking functions that our neural networks cannot find.
while we use standard benchmarks from the literature introduced by others to enable a comparison of different termination tools these benchmarks may not be representative for software written by developers.
moreover a source of bias may be introduced by our collection of programs with either disjunctive conditions or nonlinear behaviour.
while we believe that both features are important in commodity software it remains to be quantified how large the benefit of supporting these features is on a larger repository of software.
test input generation.
we require that test inputs can be generated that exercise the programs to yield traces to train the neural network.
while our simple sampling method is successful on our benchmarks it may not be possible in general to obtain sufficiently diverse test inputs.
more sophisticated means to generate test inputs are known and can be used to mitigate this threat.
neural architecture complexity.
while our experiments suggest that tiny neural networks are able to prove termination of most program loops there may exist programs that require a large number of neurons increasing training and verification complexity.
auxiliary invariants.
we use a standard verification step for checking the validity of the neural ranking function which in some cases requires an auxiliary invariant.
the results of an endto end termination analysis are dependent on the quality of these invariants and the tools we compare with use a variety of different approaches to solve this problem.
our own tool uses a simplistic heuristic for guessing these invariants appendix b which may be exceptionally successful.
the documentation on the algorithms for generating these invariants is limited and a proper comparison of alternative methods for generating ranking functions requires an implementation in a single framework.
related work termination analysis many methods for automatically proving termination have been developed and implemented.
owing to the undecidability of the problem in general most techniques restrict the scope of the analysis in some way e.g.
to linear ranking functions and programs semi definite programs semi algebraic systems or formulae drawn from specific smt theories .
to deal with complex program loops lexicographic ranking functions neural termination analysis piecewise ranking functions disjunctively well founded transition invariants and implicit ranking functions have been used .
ranking functions have been synthesised symbolically using e.g.
farkas lemma and template based guessand check strategies .
to prove conditional termination also abstract interpretation by underapproximation and loop summarisation methods have been used .
alternative methods perform termination analysis by translating programs to alternative models of computation and show that the resulting model is terminating.
this requires a guarantee that termination of the translated program implies the termination of the original program.
models used for this purpose include term rewriting systems constraint logic programs recurrence relations and b chi automata .
more recently smt solving has been used to discover ranking functions from execution traces similarly to methods based on machine learning.
machine learning for termination analysis in the last years several termination analysis approaches that incorporate machine learning technologies have been presented.
early methods learn linear ranking functions from execution traces by constructing a linear regression problem whose solutions describe a loop bound .
recently machine learning models such as support vector machines svm have been used as representation for ranking functions in .
methods based on svm have been applied to single or nested loop programs defined using conjunctions of continuous functions for the guard and deterministic assignments defined as continuous functions as well .
another deep learning approach for termination analysis has recently been introduced .
this method uses neural networks with sigmoidal activation functions which are shown to be an appropriate ranking function representation for programs defined using continuous functions without disjunctions and conditional choices.
while this is suitable to describe deterministic dynamical systems in discrete time this language restriction makes the method inapplicable to software including the majority of our simple termination analysis benchmarks.
we estimate that out of cf.
combined in tab.
programs in the preexisting benchmark sets are in the scope of but not necessarily solved by their method and remark that our method solves out of these problems.
besides out of the benchmarks in nuterm advantage are in their scope all of which are solved by term .
unfortunately we cannot directly evaluate the effectiveness of their method on our benchmark set neither nor because an implementation is unavailable.
moreover their method cannot be easily implemented in our infrastructure.
in fact neural ranking functions with sigmoidal activation lack efficient and complete decision procedures for checking their validity.
notably their approach required the development of bespoke decision procedures for this purpose.
conversely our method uses relu activation functions which can be encoded into expressions in decidable theories for which efficient smt solvers are available.
our work goes a step further by showing that neural networks with relu activation functions are sufficient to obtain results that are comparable to state of the arttools and even enable the effective termination analysis of programs that are beyond their reach.
recently a data driven method has taken a similar approach and employed efficiently checkable templates to learn loop bounds .
they propose a portfolio of methods and templates for this purpose.
by contrast our method employs neural networks whose expressive power subsumes a wide variety of ranking function templates.
our learning phase only relies on optimising a loss function and can thus be implemented using generic optimisation algorithms that are readily available in machine learning frameworks.
a heuristic approach to termination analysis based on deep learning has been proposed by alon and david .
this approach uses graph neural networks on a program s abstract syntax tree to estimate the likelihood of termination.
specifically this results in attention networks that propose locations in the program that might be a cause for non termination.
importantly however the proposed method trains neural networks over a large dataset of terminating and non terminating programs in a supervised learning fashion.
the approach is envisioned to be part of a debugging workflow where potential issues are highlighted for consumption by a programmer or another analyser and it is not aimed at providing formal proofs of termination therefore it does not give correctness guarantees.
by contrast our approach has a distinct training phase for each program and does not rely on any a priori knowledge.
moreover it provides a formal certificate of termination the neural ranking function whose validity we check using smt solving.
our method is thus fully unsupervised and provides formal guarantees of termination when a valid neural ranking function is found.
deep learning for automated reasoning neural networks have been used in other areas of automated reasoning and verification.
in automated and interactive theorem proving neural networks have been used for premise selection and proof search in first and higher order logic .
recent approaches have made use of deep learning in program and control synthesis .
in software verification deep learning has been used to find loop invariants .
our method falls within the realm of approaches that use neural networks to represent rather than to output formal certificates of correctness with soundness guarantees.
exemplars are lyapunov neural networks and their generalisation into neural barrier certificates which have been used for the formal stability and safety analysis of dynamical systems .
more recently the neural ranking supermartingale model has been introduced for the termination analysis of probabilistic programs and subsequently applied to the stability analysis of stochastic control systems .
in a similar fashion also decision tree learning has been applied to the verification of probabilistic programs .
formal verification of neural networks automated reasoning methods that use neural networks as representation of proof certificates including our approach rely on formal verification technologies to check the validity of these neural certificates.
various methods for the formal verification of neural networks have been developed in the last few years driven bymirco giacobbe daniel kroening and julian parsert the quest for formal guarantees against adversarial attacks in computer vision .
significant effort has been made towards this goal by using out of the box smt solvers and subsequently developing tailored methods to reason about neural networks.
this effort led to the development of many effective tools and algorithms .
methods for adversarial attacks reason about neural networks in isolation while reasoning about neural ranking functions requires reasoning about neural networks together with constraints arising from the encoding of a program which are usually in theories other than real arithmetic such as integer or bit vector arithmetic.
reasoning about neural networks in combination with other systems has been treated in the context of safety analysis of neural network controllers for dynamical systems .
methods of this kind apply abstract interpretation for a bounded number of steps or compute invariants.
verification of neural networks under different theories has been considered for binarized and quantized neural networks in isolation from other systems specifically for adversarial attack problems .
in our work we use off the shelf smt solving to check neural ranking functions because in our experiments we rely on relatively small networks.
while we observe that for a wide variety of problems small networks are sufficient we do not preclude that our method may benefit from using larger networks.
using larger networks may pose limits to the scalability of off the shelf smt solvers.
our work adds a novel problem to the spectrum of formal verification questions for neural networks contributing to their relevance to software engineering applications beyond robustness to adversarial attacks.
conclusion we introduced a termination analysis method that takes advantage of neural networks by learning a ranking function candidate from sampled execution traces.
this neural ranking function is subsequently verified using formal methods.
we provide a prototype implementation of this method called term .
when using tiny neural networks with one hidden layer and a straight forward training script we solved .
of the problems in a standard set of benchmarks for termination analysis performing comparably to state of the art tools.
furthermore we identify problems with disjunctive or non linear loop guards where competing tools are unable to prove termination.
we show experimentally that term is able to solve these types of problems by creating a separate set of benchmarks that feature such loop guards.
on this set term can solve .
of the problems outperforming competing tools.
our result suggests future research both in machine learning and formal verification.
learning proof certificates from examples applies not only to imperative programs but also to functional programming and logic.
moreover separating proof learning from formal proof checking may also apply to further verification tasks such as non termination which is difficult for conventional formal approaches.
a tracing a trace is a sequence of snapshots observations of the program s state as the program runs.
traces are thus generated dynamically by running the program.
the process of tracing takes two inputs the program that is to be traced and a list of program locations in the program where a snapshot of the state is to be taken.
in our case these locations are the loop heads in the program.
our approach is conceptually simple and independent of the platform and programming language.
as we consider java we give implementation details specific to the java environment and the java virtual machine jvm but our approach could also be applied to more abstract models of computation.
tracing consists of three steps input sampling execution and snapshot .
we describe each of these steps below.
input sampling.
termination analysis is commonly applied to program fragments that contain some initialisation and a possibly nested loop.
therefore we work with programs that are not closed but require inputs.
we only consider deterministic programs i.e.
two traces that are generated with the same sequence of inputs are identical.
we use two sampling strategies based on a gaussian distribution pairwise anticorrelated sampling pas and gaussian sampling .
pas uses a multivariate normal distribution where we enforce the same variance for all inputs except for two randomly chosen inputs.
for these two chosen variables we create a covariance.
hence this is a standard gaussian distribution for all variables except for two where we enforce a covariance.
gaussian sampling on the other hand is a sampling strategy where each variable is sampled independently from another from a gaussian distribution with a variance of and no covariance.
execution.
we start executing the program with the sampled arguments.
we maintain control over the jvm during the execution using the java virtual machine tool interface jvmti .
once we hit a loop head location we halt the execution and take a snapshot.
snapshot.
using the jvmti we have access to the local variable table lvt .
the lvt contains all local variables of the function.
we create a memory snapshot by iterating through the lvt and reading the values of every variable that is in scope at the given location.
for variables that are out of scope we record a placeholder default value which depends on the type of the variable .
since the number of local variables does not change the size of the snapshots is always the same.
once the snapshot is collected we append it to the trace of the current program.
if the maximum length of a trace is reached we force a termination of the virtual machine otherwise we resume the execution.
the resulting list of snapshots constitutes an execution trace.
the goal is to sample the input data in such a way that we achieve a high coverage of the function and the data therefore best represent a possible ranking argument while keeping the required number of program runs low.
our experiments with different sampling strategies show that pas exhibits better performance than multivariate gaussian sampling.
when using the same neural network with neurons we solve of problem using traces obtained with pas and .
when using multivariate gaussian sampling.
it should be noted that both sampling techniques are extremely simplistic.
the results may be further improved by utilising more sophisticated test input generation or fuzzing .neural termination analysis b verification we verify that a candidate monolithic neural raking function f rn ris a valid neural ranking function for program p s t by verifying that it decreases every time a loop header is encountered.
for this purpose we construct a symbolic encoding of the transition relation between every two loop headers th.
thus the verification question corresponds to that of determining whether for any two states between loop headers that also satisfy an auxiliary invariant the trained neural ranking function decreases by .
this corresponds to checking the following validity question s s s s th s a f s f s note that is constant is this formula.
then if this formula is valid we have that f is a valid ranking function.
we verify this by checking the dual satisfiability question using an smt solver.
the dual satisfiability question is that of finding a counterexample where the candidate does not decrease by that is the following formula s s s s th s a f s f s z if the quantifier free formula is determined unsatisfiable by the smt solver then the ranking function is valid.
encoding involves encoding constraints for the program th anda constraints for the neural ranking function f and the interface between them which is the observation function .
we encode the transition relation between loop headers thusing a single static assignment encoding which introduces intermediate variables after each assignment and encodes operations using appropriate arithmetic expressions.
this is similar to a bounded model checking encoding which is possible because every run between adjacent loop headers has necessarily fixed length.
according to the semantics one wants to consider the program can be encoded in the theory of integers or the theory of bit vectors in our experiments we use the theory of integers .
also fand can be seen as bounded programs note that fis a feed forward network and therefore can be encoded similarly.
our neural networks are compositions of linear layers and relu activation functions which result in first order logic formulae in the theory of reals with linear arithmetic.
notably we use much smaller networks compared to common machine learning applications and we argue that these are sufficient to solve a broad variety of termination problems.
also our program encoding is smaller than those generated by bounded model checkers as it involves at most one loop unrolling.
for this reason our overall encoding ultimately results in formulae that are efficiently solvable by modern smt solvers.
identifying auxiliary invariants athat are strong enough for termination analysis is a difficult problem encoding them is straightforward .
our verification procedure uses a heuristic that syntactically extracts constraints from the program from for example conditional statements and checks whether these are loop invariants using the smt solver.
if these are valid invariants then they used to define a. as it turns out this naive heuristic was sufficient to obtain the results presented in this paper.
using more sophisticated loop invariant generation methods can only improve the effectiveness of our tool and is subject of future investigation.
methods forgenerating loop invariants include procedure based on theorem provers constraint based invariant synthesis .
invariants for java programs have been constructed using symbolic execution .
tools for the discovery of invariants from trace data include daikon and dig .
similarly for lexicographic neural ranking functions we verify the validity of the conditions in eq.
and over each loop header and respective output component of the neural ranking function.
let h h1 ... hm be the set of loop headers then for everyi ... m we verify the validity of the following conditions s s s s t hi s ai fi s fi s s s s s t hi s ai fi s fi s ... s s s s t hi s ai f1 s f1 s note that each condition can be checked independently the lexicographic argument is violated if any of the conditions is violated otherwise it is valid.
we remark that unlike the monolithic case t hi may represent runs of arbitrarily length when the loop with headerhihas nested loops.
to encode t hi as a bounded problem we substitute every inner loop with a summary.
several methods have been developed for this purpose .
as a heuristic we construct a loop summary that encodes the invariance of all variables that are never assigned within the loop together with a transition invariant that encodes the respective and previously verified lexicographic component in the neural ranking function and the respective auxiliary invariant.
using or developing more sophisticated summarisation techniques is matter of future research.