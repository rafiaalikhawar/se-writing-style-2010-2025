editsum a retrieve and edit framework for source code summarization jia li key lab of high confidence software technology moe peking university beijing china lijia stu.pku.edu.cnyongmin li key lab of high confidence software technology moe peking university beijing china liyongmin pku.edu.cnge li key lab of high confidence software technology moe peking university beijing china lige pku.edu.cn xing hu school of software technology zhejiang university ningbo china xinghu zju.edu.cnxin xia faculty of information technology monash university melbourne australia xin.xia monash.eduzhi jin key lab of high confidence software technology moe peking university beijing china zhijin pku.edu.cn abstract existing studies show that code summaries help developers understand and maintain source code.
unfortunately these summaries are often missing or outdated in software projects.
code summarization aims to generate natural language descriptions automatically for source code.
according to groset al.
code summaries are highly structured and have repetitive patterns e.g.
return true if... .
besides the patternized words a code summary also contains important keywords which are the key to reflecting the functionality of the code.
however the state of the art approaches perform poorly on predicting the keywords which leads to the generated summaries suffer a loss ininformativeness.
to alleviate this problem this paper proposes a novel retrieve and edit approach named e ditsum for code summarization.
specifically editsum first retrieves a similar code snippet from a pre defined corpus and treats its summaryas a prototype summary to learn the pattern.
then e ditsum edits the prototype automatically to combine the pattern in theprototype with the semantic information of input code.
ourmotivation is that the retrieved prototype provides a good startpoint for post generation because the summaries of similar code snippets often have the same pattern.
the post editing process further reuses the patternized words in prototype and generateskeywords based on the semantic information of input code.
we conduct experiments on a large scale java corpus 2m and experimental results demonstrate that e ditsum outperforms the state of the art approaches by a substantial margin.
the human evaluation also proves the summaries generated by editsum are more informative and useful.
we also verify that editsum performs well on predicting the patternized words and keywords.
index t erms code summarization information retrieval deep learning i. i ntroduction during software development and maintenance developers spend around of their time on program comprehension activities .
a code summary provides a concise naturallanguage description for a code snippet which can help developers understand the program quickly and correctly .unfortunately the code summaries are often mismatched missing or outdated in the software projects .
additionally corresponding authorsmanually writing summaries during the development is time consuming for developers.
therefore it is important to exploreautomatic code summarization approaches.
traditional approaches generate code summaries based on the template based approaches and information retrieval ir based approaches.
template based approaches firstlyextract the keywords from the source code and then fill thekeywords into the predefined templates to generate a codesummary.
the ir based approaches use code summaries ofsimilar code snippets as outputs directly.
among these ir based approaches they retrieve the similar code snippets byvarious similarity metrics from open source softwarerepositories in github or software q a sites .although the traditional approaches are simple they haveachieved good results.
this is because code summaries arehighly structured and contain many repetitive patterns e.g.
return true if... and create a new... .
the manuallycrafted templates and retrieved summaries provide a lot ofreusable patternized words which play an key role in the codesummaries.
however for template based approaches manu ally defining templates is time consuming and laborious andrequires a lot of expert experience.
for ir based approaches there may be semantic inconsistencies between the retrievedsummary and the input code.
with the development of deep learning there is an emerging interest in applying neural networks for automatic code sum marization.
previous studies often adopt the encoder decoder architecture to learn the mapping between wordsand even the grammatical structure from source code tonatural language based on the large scale corpus.
by virtueof the naturalness of the source code these neuralmodels can mine patterns for generating code summariesfrom a large corpus.
besides the patternized words a codesummary also contains important keywords which have a lowfrequency in training data but are the key to reflecting thefunctionality of source code more details can be found insection ii .
however the state of the art nerual models ui .
oufsobujpobm pogfsfodf po vupnbufe 4pguxbsf ohjoffsjoh 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
perform poorly on predicting keywords.
for example leclair et al.
found summaries written by humansin the test set contain words with the frequency of less than100 but only summaries generated by their proposedapproach contain these words.
the lack of keywords leadsto the generated summaries suffer a loss in informativeness which have a negative impact on program comprehension.
recently wei et al.
and zhang et al.
proposed two retrieval based neural models to address the problem of keywords.
they used the ir techniques to get the similarcode and its summary and then input the retrieved resultsand the input code into the encoder.
with the assistance ofthe retrieved summary their models can accurately generatepatternized words.
however their models only treated theretrieved results as auxiliary information and don t solve theproblem of keywords.
in this paper we propose a novel retrieve and edit approach e ditsumfor code summarization.
the improvement by template based approaches proves that the importance ofthe patterns in code summaries.
the improvement by irbased approaches shows that the summaries of similar codesnippets often have the same pattern.
so we treat the summaryof similar code as a prototype and extract the pattern fromthe prototype.
considering the inconsistencies between theprototype and input code we design a neural network tofurther edit the prototype automatically based on the semanticinformation of input code.
our motivation is that the patternin a prototype tells the neural model how to say and thesemantic information of input code tells the neural model what to say .
e ditsumconsists of two modules a retrieve module and an edit module.
in the retrieve module given an input code snippet we use ir techniques to retrieve the similar codesnippet from a large parallel corpus and treat the summary ofthe similar code snippet as a prototype.
then the edit module generates a summary by fusing the pattern in prototype andsemantic information of input code.
specifically we proposea sequence to sequence seq2seq neural network to learn to revise the prototype based on the semantic differences of theinput code and the similar code.
to represent the semanticdifferences we calculate an edit vector by concatenating the weighted sums of insertion word embeddings words in input code but not in similar code and deletion word embeddings words in similar code but not in input code .
after that werevise the prototype summary conditioning on the edit vectorto obtain a new summary.
to evaluate our approach we conduct experiments on a real world java dataset.
the dataset comes from the sourcererrepository 1and has been processed by leclair et al.
including removing duplicates and dividing into training valida tion and test sets by projects.
we employ the mainstream eval uation metric bleu meteor and rouge score that are widely used in summary generation task toevaluate the generated summaries.
experimental results show i the patterns of summaries in dataset.
real sampleswrite atestfinish tothemesa logger write thistilemap toanxml file write thebuffer totheoutput stream write grid data tothegeotiff file write cdlrepresentation tooutput stream pattern write to real samplesthis method sets thehelp button visible this method sets thevaule offield this method sets asearch argument forlist this method sets theclient id this method sets therange asadouble pattern this method sets real samplesconvert animage toanarray ofinteger convert thisippacket toareadable string convert ajingle description toxml convert thespecified string toaurl convert thedate tothegiven timezone pattern convert to that e ditsum performs substantially better than the irbased baselines and outperforms the state of the art neuralbaselines.
the human evaluation and qualitative analysis provethe summaries generated by e ditsumare informative and useful for developers to understand programs.
besides weverify that e ditsumnot only accurately generates patternized words but also generates more keywords.
our main contributions are outlined as follows we propose a novel retrieve and edit approach namely editsum for code summarization.
we use the summaries of similar code snippets as prototypes to assist in generating summaries.
we design an effective editing module for summary generation which can combine the pattern in prototypewith the semantic information of code.
we conduct extensive experiments to evaluate our ap proach on a large scale java dataset.
the experimental results show that e ditsumsubstantially outperforms the state of the art approaches.
paper organization.
the rest of this paper is organized as follows.
section ii describes motivating examples.
section iii presents our proposed approach.
section iv and sectionv describe the experimental setup and results.
section viand section vii discuss some results and describe the related work respectively.
finally section viii concludes the paperand points out future directions.
ii.
m otiv ating examples a closer look at the code summarization dataset shows that patterns such as creates a new returns true if load into convert into are very frequent .
table i showssome samples from the dataset provided by leclair et al.
.
the bold words are patternized words and the dashedwords denote the keywords.
such a code summary can beregarded as composed of patternized words and keywords.
the pattern ensures the readability of the summary and thekeywords reflect the functionality of the source code.
a good code summary should contains suitable patternized words andmeaningful keywords.
input code publiciterator getprefixes stringnamespaceuri listl urimap.
get namespaceuri return l null ?
null l.iterator similar code publicstringgetprefix stringnamespaceuri list string l urimap .get namespaceuri return l null ?
null l.get rencos input code returns aniterator over the values to aspecified url.
human written input code return an iterator over all prefixes to a url human written similar code return a prefix corresponding to a url fig.
an example of the input code and similar code.
however previous models perform well on predicting the patternized word ignoring the importance of keywords.
as figure shows for the input code we use the open source search engine lucene2to retrieve the most similar code snippet from the training corpus.
the retrieval metric is basedon the lexical level similarity of the source code.
in figure the summaries of input code and similar code have the same pattern return...to a url but there are semantic differences between the similar code and input code.although the two java methods are lexically similar the inputcode returns all prefixes while the similar code returns acertain prefix.
in figure the state of the art neural modelrencos can correctly predict the patternized words e.g.
return to but it performs poorly on keywords e.g.
prefixes .the code summaries generated by rencos achieve high scoreson the patternized words but they do not clearly express thepurposes of the programs.
in this paper we address that both pattern and keywords are important for a code summary.
inspired by previous studies we propose a retrieve and edit approach by combining the pattern in existing summaries and the semantic informationof input code to generate informative summaries with suitablepatterns.
iii.
p roposed approach in this paper we propose a retrieve and edit approach named e ditsumfor source code summarization which can combine the strengths of traditional approaches and neuralmodels.
the overall framework of our model is shown infigure .
our approach e ditsum consists of a retrieve module and an edit module and generates a summary in threesteps step selecting a suitable prototype summary.
we use a massive training set as the retrieval corpus.
given an inputcode the retrieve module uses the search engine to search forthe similar code summary pair from the corpus.
the retrievalprocess is explained in section iii a. step extracting the semantic information of the input code.
in figure we mark the lexical differences between thetwo java methods.
we find that the different words between two methods reflect their semantic differences to a certainextent such as iteration vs string and prefixes vs prefix .
therefore we calculate an edit vector based on thelexical differences between similar code and input code torepresent their semantic differences.
the details of this part isdescribed in section iii b. step combining the pattern in prototype with semantic information of input code.
to this end we design a neuraledit module to revise the prototype based on the semanticdifferences between the input code and similar code.
thedetails is presented in section iii b. a. retrieve module in our approach the retrieve module aims to retrieve the similar code summary pair from a corpus given the input code.
inspired by previous studies we choose thelexical level similarity as retrieval metric.
specifically weadoptbm25 as the similarity evaluation metric which is a bag of words retrieval function to estimate the relevance ofdocuments to a given query.
given a query and a document based on tf idf the bm25function calculates the term frequency in the document of each keyword in the query andmultiplies it by the inverse document frequency of this term.the more relevant two documents have the higher the valueofbm25score.
we leverage the open source search engine lucene to build the retrieve module.
since the size of the training set is quite large over .9m we use it as the retrievalcorpus.
we first tokenize the source code and summaries andprocess each code and summary pair into a document add itto the index library and store it on disk.
as shown in figure we use different strategies to select prototypes for training and testing.
in testing we search forthe most similar code from the training set and treat itssummary as the prototype.
during the training phase as wealready know the targrt summary we first retrieve top code summary pairs based on the summary similarity.
then wereserve the retrieved summaries as prototypes whose jaccard similarity to target summary in the range of .thejaccard similarity measures text similarity from a bagof words view that is formulated as j a b a b a b whereaandbare two bags of words and denotes the number of elements in a collection.
the motivation behind filtering out summaries with jaccard similarity .
is the edit module performs well only if a prototype is lexically similar to its target summary .
besides we hope the editmodule does not copy the prototype so we discard summarieswhere the prototype and target summary are nearly identical i.e.jaccard similarity .
.
we do not use code similarity to construct training data because similar code snippets maycorrespond to totally different summaries.
this is not con ducive to our model learning how to revise a prototype.
thepreliminary experiments also show that constructing trainingdata based on code similarity may cause the model to fail toconverge.
similar code publicstringget prefix stringnamespaceuri list string l urimap.
get namespaceuri return l null ?
null l.get !
!
!
!
!
!
!
!
!
!
!
!
!
input code publiciterator get prefixes stringnamespaceuri listl urimap.
get namespaceuri return l null ?
null l.iterator fig.
the overall framework of our approach.
b. edit module after that the key step is to combine the pattern in the prototype and the semantic information of input code to generate a new summary.
the structure of the edit moduleis shown in figure .
firstly we utilize the prototype encoderto get the vector representation of prototype.
secondly wecompute the edit vector based on the lexical differences of two code snippets.
the edit vector represents the semanticdifferences between the similar code and input code.
lastly the summary decoder is used to generate a new summaryconditioning on the prototype representation and edit vector.
prototype encoder the prototype encoder takes the prototype y primeas input.
we first map the one hot vector of a tokenw prime iinto a word embedding y prime i y prime i w latticetop ew prime i i wherenis the length of prototype weis a trainable word embedding matrix.
to leverage the contextual information weuse a bidirectional long short term memory bi lstm unit to process the sequence of word embeddings.
at i th time step the hidden state h iof the bi lstm can be represented by hi l s t m parenleftbig hi y prime i parenrightbig hi l s t m parenleftbig hi y prime i parenrightbig hi bracketleftbig hi hi bracketrightbig where is a concatenation operation.
finally the prototype y primeis transformed to vector representation hi n i .
edit v ector the edit vector zaims to reflect the semantic differences between the input code xand similar codex prime.
suppose that xandx primeonly differ by a single word w. then one might think that the edit vector zshould be equal to the word embedding for w. generalizing this intuition tomulti word edits the multi word insertions can be represented as the sum of the inserted word vectors and similarly formulti word deletions .
as shown in figure we define i w w x w x prime as a insertion word set and d w prime w prime x w prime x prime as a deletion word set.
because different words influence the edit ing process unequally we represent the differences betweenxandx primeusing the weighted sum of word embeddings fdiff x x prime summationdisplay w i w w summationdisplay w prime d w prime w prime where w is the word embedding of word wand denotes a concatenation operation.
wis the weight of a insertion word w that is computed by the attention mechanism w exp ew summationtext w iexp ew ew v latticetop tanh w where v andw are trainable parameters and hnis the final hidden state of prototype encoder.
w primeis obtained with a similar process.
then we compute the edit vector zby following linear projection which can be regarded as a mappingfrom code differences to summary differences.
z t a n h w f diff b where wandbare two trainable parameters.
summary decoder after that we revise the prototype based on the edit vector to get a new summary.
the purposeof the summary decoder is to generate a new summary.
as shown in figure the decoder takes the prototype representation h i n i 1and the edit vector zas inputs and generate a summary by a lstm unit with attention.
the hidden state of the decoder is compute by si l s t m si yi z fig.
the structure of the edit module.
wheresi 1means the previous hidden state of the decoder yi 1is the i th word embedding of ground truth summary.
we concatenate the edit vector to every input embedding of the decoder so the edit information can be utilized in theentire generation process.
to introduce the information of the prototype we then compute a context vector c iby attention mechanism which is a weighted sum of prototype representation hi n i ci n summationdisplay j 1 i jhj where attention weights are obtained by i j exp ei j summationtextn k 1exp ei k ei j v latticetop tanh w where v andw are two trainable parameters.
based on the previous word yi hidden state of the decoder siand the context vector cifrom prototype our model compute the probability of i th token yi p yi s o f t m a x wp b p where wpandbpare two trainable parameters.
c. loss function during training e ditsumtakes a token sequence of the input code x a summary of the input code y a token sequence of the similar code x prime and the prototype y primeas inputs respectively.
we optimize parameters of e ditsum by maximizing the probability of p y z y prime .
the overalltable ii the statistics of datasets.
dataset train valid test count avg.
tokens in code .
.
.
avg.
tokens in summary .
.
.
a code length distribution.
b summary length distribution.
fig.
length distribution of test data.
objective function of our model is to minimize the following loss function l n summationdisplay i 1l summationdisplay t 1logp parenleftbig yi t zi y prime i yi t parenrightbig where is all trainable parameters.
nis the total number of training instances and lis the length of each ground truth summary.
during testing we utilize the prototype encoder to represent prototypes and compute edit vectors.
then the summarydecoder is used to generate directly a summary conditioningon the prototype representation and edit vector in equation .
iv .
e xperimental setup a. dataset following previous studies we conduct experiments on a public large scale java dataset3provided by leclair et al.
.
the raw dataset contains .
million java methods which is collected by lopes et al.
from thesourcerer repository.
because the raw dataset contains a largenumber of samples such as repeated and auto generated code that are not suitable for evaluating neural models leclair etal.
cleaned and pre processed the dataset.
specifically they first extracted java methods and javadocs from the source code repository.
assuming the first sentenceof the javadoc describes the method s behavior theyextracted the first sentence of the javadoc as the summary of amethod and filtered out non english samples.
considering theauto generated and duplicate code might affect the evaluation they removed these samples using heuristic rules andadded unique auto generated code to the training set.
afterthat they split camel case and underscore tokens and set themto lower case.
finally they divided the dataset by project intotraining validation and test set meaning that all methodsin one project are grouped into one set.
they argued thatthe pre processing of the dataset is necessary for evaluatingthe performance of neural models.
the statistical results ofthe dataset are shown in table ii.
figure shows the lengthdistribution of source code and summary on the test set.
b. implementation details our model is implemented based on the pytorch 4framework.
we set word embedding and lstm hidden states to dimensions and dimensions respectively.
we set thebatch size to and train the model using adam withthe initial learning rate of .
.
the learning rate is decayedwith a factor of .
every epoch.
to mitigate overfitting we use dropout with .
.
to prevent exploding gradient we clip the gradients norm by .
according to the statistics ofthe dataset in table ii and figure the maximum lengthsof code and summary are set to and respectively.the vocabulary sizes of the code and summary are 000and respectively.
the out of vocabulary tokens arereplaced by unk.
we train the model for a maximum of 30epochs and perform an early stop if the validation performancedoes not improve for consecutive iterations.
during thetesting phase we use a beam search and set the beam sizeto .
we conduct all experiments on two nvidia gtx titanxp gpus with gb memory.
each experiment is run threetimes and the average results are reported.
c. evaluation metrics following the previous studies we evaluate all approaches using the metric bleu meteor rouge l and rouge w .
we regard a generated yas a candidate and a huamn written summary y as a reference.
bleu calculates the n gram similarity between the generated sequence and reference sequence.
the bleu score ranges from to as a percentage value.
the higher the bleu the closer the candidate is to the reference.
it computes then gram precision of a candidate sequence to the reference bleu n bp exp parenleftbigg n summationdisplay n 1wnlogpn parenrightbigg wherepnis the ratio of length nsub sequences in the candidate that are also in the reference.
in this paper we reportthe bleu1 bleu4 scores.
bp is brevity penalty.
meteor calculates the similarity scores between a pair of sentences by meteor parenleftbig frag parenrightbig p r p r wherepandrare the unigram precision and recall frag is the fragmentation fraction.
and are three penalty parameters whose default values are .
.
and .
respectively.
rouge l computes f score based on longest common subsequence lcs .
suppose the lengths of yandyarem andn then plcs lcs x y m rlcs lcs x y n flcs parenleftbig 2 parenrightbig plcsrlcs rlcs 2plcs where plcs rlcsandflcsis the value of rougel.
rouge w is an improved version of rouge l which is based on weighted longest common subsequence wlcs .
v. e xperimental results to evaluate our approach in this section we aim to answer the following three research questions rq1 how does the e ditsumperform compared to the state of the art neural baselines?
rq2 how does the e ditsumperform compared to the ir based baselines?
rq3 does e ditsumperform better than previous approaches for tackling the keywords problem?
a. rq1 editsum vs. neural baselines baselines to answer this research question we compare our approach e ditsumto six state of the art neural models.
code nn is the first neural network based model for code summarization task.
it maps the source code sequence into word embeddings then uses an lstm unitas a decoder to generate summaries and employs theattention mechanism to introduce information from theword embeddings.
table iii the performance of our model compared with baselines.
approaches params bleu1 bleu2 bleu3 bleu4 meteor rouge l rouge w retrieve module .
.
.
.
.
.
.
lsi .
.
.
.
.
.
.
vsm .
.
.
.
.
.
.
nngen .
.
.
.
.
.
.
code nn .3m .
.
.
.
.
.
.
deepcom .9m .
.
.
.
.
.
.
attendgru .7m .
.
.
.
.
.
.
ast attendgru .7m .
.
.
.
.
.
.
rencos .3m .
.
.
.
.
.
.
re2com .4m .
.
.
.
.
.
.
editsum .4m .
.
.
.
.
.
.
deepcom is a seq2seq model with an attention mechanism that uses lstm units as the encoder and decoder.
it proposed a sbt algorithm to convert theast into a token sequence.
it is the first model tointroduce structural information of source code into code summarization.
attendgru is an encoder decoder model with an attention mechanism which takes the code sequence as input and the summary as output.
ast attendgru is also a seq2seq model with an attention mechanism.
different from attendgru it introducesthe structural information of the source code by usingan encoder to process the traversal sequence of ast.
itconcatenates the information from the two encoders asinput to the decoder and generates code summaries.
rencos is a retrieval based neural model that augments an attentional encoder decoder model with the retrieved two most similar code snippets for better sourcecode summarization.
re2com is a retrieval based neural model that uses the summary of the similar code snippet as an exemplar to generate a code summary.
for a fair comparison we re implement the attendgru andast attendgru based on lstm units.
the embedding size andlstm states of all baselines are set to dimensions.
results we calculate the bleu meteor and rouge scores between the summaries generated by differentapproaches and human written summaries.
the experimentalresults are shown in table iii.
we notice that code nnperforms the worst among all approaches.
this is becausecode nn directly uses word embeddings as the input ofdecoder and does not further extract the semantic informationfrom the source code.
this shows that whether the semanticinformation of the code can be effectively mined has agreater impact on the performance of the code summarizationmodel.
both deepcom and attendgru use the encoder decoderframework but deepcom performs worse.
this is becausethe traversal sequence of the ast input by deepcom isabout times longer than the token sequence of code inputby attendgru.
this also verifies the weakness of lstm inprocessing long sequences .
the difference between ast attendgru and attendgru is that the former introduces thestructural information in the ast but the improvement islimited.
this is because custom identifiers are removed fromthe ast used in ast attendgru which limits the structuralinformation in the ast.
both rencos and re 2com combine the information retrieval technology with neural networks but the former is less effective.
rencos retrieved two similar code snippets from the corpus and directly used them as input to themodel.
re 2com retrieved a similar code from the corpus and then sent the summary of the similar code into the model as anexemplar.
the experimental results show that the summary ofsimilar code contains more valuable and reusable informationthan similar code that may contain noise.
this also proves thatit is reasonable for us to use the summaries of similar code asthe prototypes.
from table iii we can see that e ditsumperforms the best among all neural models which improves the state of theart re2com by .
in bleu1 in meteor and .
in rouge l. in particular compared with rencos and re2com e ditsumperforms much better on all metrics.
this is because rencos and re2com are the ensemble neural models and they directly enter the retrieved results and theinput code into the model.
while e ditsumregards the prototype summary as an initial draft for post generation whichprovides many reusable patternized words.
so e ditsum focuses on learning how to revise the prototype based onthe differences between the input code and the similar code.besides the number of parameters of e ditsumis the smallest among all baselines.
it also shows e ditsumis efficient.
compared to other metrics we find that e ditsum has a small improvement on bleu4.
this is because the im provement by e ditsummainly comes from predicting more keywords.
however the average length of the summaries inthe test set is and these keywords are mainly words.therefore e ditsumhas a great improvement on bleu1bleu3 and a relatively small improvement on bleu4.
b. rq2 editsum vs. ir baselines baselines to answer this research question we compare our approach e ditsumto four ir based baselines.
retrieve module is a component of our approach whose details are described in section iii a. we use the summary of similar code as output directly.
latent semantic indexing lsi is an ir technique to analyze the semantic relationship between terms in documents.
given a code snippet we use lsi to retrievethe similar code from the training set and use its summaryas output.
the retrieval metric is the cosine distance ofthe dimensional lsi vector of the source code.
vector space model vsm represents the code as a vector using term frequency inverse document frequency tf idf .
it uses cosine similarity to retrievethe summary of the similar code from the training set.
nngen is an approach for generating commit messages based on nearest neighbors algorithm.
it firstencodes code changes into the form of a bag of words then uses the cosine distance to select the nearest kcode changes.
finally it chooses the message of the code change with the highest bleu score as the final result.in this paper we set kas .
results we calculate the bleu meteor and rouge scores between the summaries generated by differ ent ir based approaches and human written summaries.
theexperimental results are shown in table iii.
from table iii the retrieve module performs better compared with otherir based approaches.
this shows that it is effective for ourretrieve module to retrieve similar code based on the lexicalsimilarity.
lsi and vsm use different ways lsi vectorsand tf idf to map source code into a vector and theirperformance is similar.
compared with lsi and vsm nngendirectly uses bleu score as the retrieval metric so it gets ahigher bleu score.
it is worth noting that the bleu3 andbleu4 score of the ir based baselines even exceeds thatof some neural models i.e code nn and deepcom .
thisshows that the summaries output by the ir based approacheshave better precision scores of the gram phrase and gramphrase.
this proves that the retrieved summaries contains alot of valuable words which can be used to generate thenew summaries.
however there is still a gap between thesummaries output by the ir based approaches and the human written summaries due to the differences between the similarcode and the input code.
our model significantly outperforms ir based baselines in terms of all metrics which proves the effectiveness ofthe our edit module.
compared to the ir based baselines.our approach e ditsum treats the retrieved summary as a prototype and then revise the prototype conditioning on thesemantic differences between similar code and input code.
by combining the advantages of neural networks and ir basedapproaches e ditsumachieves the best performance.
c. rq3 tackling keywords problem metrics according to the information retrieve technologies the keywords in the summaries often are informative and are more likely to be low frequency words.
the statisticsshow .
of tokens in the summary vocabulary of thedataset have a frequency of less than .
however as wedescribed in section i and ii previous neural network modelstable iv the number of correctly generated low frequencywords the rate of keywords in parentheses approaches ast attendgru rencos re2com .
.
editsum .
.
table v the results standard deviation in parentheses ofhuman evaluation.
approaches naturalness informativeness usefulness retrieve module .
.
.
.
.
.
ast attendgru .
.
.
.
.
.
rencos .
.
.
.
.
.
re2com .
.
.
.
.
.
editsum .
.
.
.
.
.
perform poorly on predicting low frequency words.
to mea sure the ability of our approach on generating keywords wecollect all correctly predicted words on the test set calculatethe frequency of these words on the training set and countthe words with frequencies less than and .
thecorrectly predicted words refers to the overlap between thegenerated summary and the reference summary.
for the wordswith frequencies less than and we manually counted therate of keywords among these words.
results the statistical results are shown in table iv.
compared with ast attendgru rencos and re 2com perform better on predicting the low frequency words.
this shows that the summaries of similar code snippets contain a lot of reusable information.
we also can see that e ditsum can predict more low frequency words and more keywords thanother baselines which means that e ditsum alleviates the problem of predicting keywords.
the goal of learning howto revise a prototype makes our model focuses to generate more keywords.
d. human evaluation metrics although the metrics in section iv c can calculate the lexical similarity between the generated summaries and the reference summaries they can not reflect thesimilarity at the semantic level.
moreover the ultimate goal ofthe automatic code summarization model is to help develop ers understand the functionality of the program.
therefore we conduct a human evaluation to measure the quality ofsummaries generated by different baselines on the test set.following the previous work we measure three aspects including the naturalness grammaticality and fluency of the generated summaries informativeness the amount of content carried over from the input code to the generated summaries ignoring fluency of the text and usefulness what extent the generated summary is useful for developers to understandcode .
all three scores are integers ranging from to from bad to good .
we invite volunteers with yearsof java development experience and excellent english ability for hour each to evaluate the generated summaries in the form of a questionnaire.
the volunteers are computerscience ph.d. students and are not co authors of this paper.we randomly select samples generated by five models from retrieve module from ast attendgru fromre 2com from rencos and from e ditsum .
the samples are divided into five groups with each questionnairecontaining one group.
we randomly list the summary pairs and the corresponding input code on the questionnaire and removetheir labels.
each group is evaluated by two volunteers andthe final result of a pair of summaries is the average of twovolunteers.
v olunteers are allowed to search the internet forrelated information and unfamiliar concepts.
results the evaluation results are shown in table v. the standard deviations of all approaches are small indicatingthat their scores are about the same degree of concentration.our model is better than all baselines in three aspects.
theretrieve module can generate more fluent summaries than the ast attendgru because its outputs are directly retrieved from thetraining set.
we also notice that the scores on informativeness of five models are higher than those on usefulness.
this indicates that the generated summaries really contain information about the code but this information may be redundant ornot completely correct so they only provide limited help fordevelopers to understand the code.
vi.
d iscussion a. qualitative analysis we present three examples generated by different approaches from the test set as shown in table vi.
these exam ples show that the summaries generated by e ditsumhave a very high semantic similarity with human written summaries.from table vi previous models cannot generate keywords accurately and the generated summaries cannot reflect the intention of the programs concisely.
for example in case the aim of this program is to set the color to a darker shade.however re 2com simply describes it as setting the selected color to the specified color which is useless for developers tounderstand the program.
while our model e ditsumperforms well in both patternized words e.g.
set to and keywords e.g.darker shade .
besides compared with retrieve module wecan find that our edit module can make good use of the patternin the prototype and revise it based on the semantics of theinput code.
b. performance for different lengths we also analyze the performance of different models on different code and summary lengths number of tokens .
we calculate the bleu score for each sample on the test set andaverage the scores by length.
the experimental results areshown in figure and figure .
from these figures we canobserve that e ditsumoutperforms the re2com with different code and summary lengths.
as the lengths of the code and summary increase e ditsumkeeps a stable improvement over re2com.
the performance of our model is always better thantable vi examples of generated summaries.
case id example 1public void drawselected if unselectedcolor instanceof color setpaint color unselectedcolor .darker else setpaint java.awt.color.yellow retrieve module set the series colors to the chart ast attendgru draws the selected set of the selected color rencos p method description p re2com set the selected color to the specified color editsum set the color to a darker shade human written set the color to a darker shade 2public void close throws ioexception this.servletinputstream.close retrieve module close the resources used by the work factory ast attendgru close the underlying servlet rencos close the server re2com close the resources used by the work factory editsum close the underlying stream human written close the underlying stream 3public int read throws ioexception if chunksize return if pos chunksize pos return in.read else readchunksize pos if chunksize return pos return in.read retrieve module read some bytes from the stream ast attendgru reads the next byte rencos reads the next byte re2com read some bytes from the stream editsum read the next byte of data from this input stream human written read the next byte of data from this input stream code lengthbleu score variable editsum re2com fig.
bleu scores for different code lengths.
the baseline on the complicated code snippets with a relatively large length.
this also shows the robustness of our model.
c. threats to v alidity there are three main threats to the validity of our model.
firstly we only conducted experiments on a java dataset.
although java may not be representative of all programminglanguages the experimental dataset is large and safe enough summary lengthbleu score variable editsum re2com fig.
bleu scores for different summary lengths.
to show the effectiveness of our model.
previous studies also only conducted experiments on this java dataset.besides our model uses only language agnostic features andcan be applied in a drop in fashion to other programminglanguages.
secondly we cannot guarantee that the scoresof human evaluation are fair.
to mitigate this threat weevaluate every code summary pair by two evaluators and usethe average score of the two evaluators as the final result.finally the retrieve module retrieves similar code based onlexical similarity.
this may result in retrieved code and inputcode being similar only at the lexical level but their summariesare quite different.
to address this threat we use a large scale java dataset 2m to increase the scale and diversity of retrieval corpus.
the experimental results in table iii prove that the performance of our retrieval module is comparable tothe performance of some neural network models code nn deepcom .
we also propose an edit module to alleviate thisthreat through revising the prototype according to the semanticdifferences between input code and retrieved code.
vii.
r elated work as an integral part of software development code summaries describe the functionalities of source code.
a conciseand clear summary can help developers quickly understand thepurpose of the program.
however it is very time consuming and labor intensive to write a summary manually.
therefore more and more researchers are exploring automatic codesummarization technology.
automatic code summarization ap proaches vary from manually crafted templates information retrieval and neural networks .
a. template based approaches early studies generated code summaries based on templatebased approaches.
given the signature and body of a method sridhara et al.
identified the content for the summary and generated natural language text that summarizes the method sactions.
moreno et al.
determined the class and methodstereotypes and used them in conjunction with heuristics to select the information to be included in the summaries.then they generated the summaries using existing lexical ization tools.
mcburney et al.
utilized the pagerankalgorithm to select the important methods in the givenmethod s context and used a template based system to generateenglish descriptions of java methods.
generating summariesbased on templates can improve the readability of summaries but defining templates is a time consuming task and requiresextensive domain knowledge.
besides templates of differentprojects cannot be migrated to each other.
b. ir based approaches information retrieval technologies are also widely used in automatic code summarization.
haiduc et al.
represented the source code as a vector using two algorithms vsm and lsi and retrieved relevant terms from a code corpus.
theserelevant terms were integrated into a code summary.
eddyet al.
proposed a hierarchical probabilistic model that retrieved relevant terms from the code corpus and fused theminto the summaries.
wong et al.
applied a token basedcode clone detection tool to retrieve similar code snippetsin large scale software repositories.
although promising ir based approaches have two main limitations first they failto extract accurate keywords used to identify similar codesnippets when identifiers and methods are poorly named.second they rely on the size and diversity of the retrievalcorpus.
c. neural network based approaches recently more and more neural networks are applied to generate code summaries.
iyer et al.
used a recurrent neural network rnn with an attention mechanism as adecoder to generate code summaries and achieved good resultson c and sql summaries.
because source code contains richstructural information hu et al.
proposed a neural modelnamed deepcom to utilize the structural information of code.they proposed a sbt algorithm to convert the ast into atoken sequence then designed a seq2seq model to generatesummaries for java methods based on the ast sequence.leclair et al.
proposed two neural models attendgruand ast attendgru to generate the summaries by combiningthe sequence information and structure information of thecode.
wei et al.
proposed an exemplar based summarygeneration framework that used the summary of the similar code snippet as an exemplar to assist in generating a target summary.
zhang et al.
proposed a retrieval based neuralmodel that augments an attentional seq2seq model with theretrieved two most similar code snippets for better source codesummarization.
different from the retrieval based neural models we regard the retrieved summary as a prototype and combinethe pattern in prototype with semantic information of inputcode.
while previous models formulate it as a multi sourceseq2seq task in which the input code prototype and similarcode are all fed to the decoder.
the experimental results alsoprove the superiority of our approach.
viii.
c onclusion and future work in this paper we argue that code sumaries are composed of patternized words and keywords and emphasize the shortcom ings of previous models in predicting keywords.
to alleviate this problem we propose a retrieve and edit approach named editsumfor code summarization.
e ditsumcontains two modules.
a retrieve module for retrieving the similar code snippet.
an edit module treats the summary of similar code asa prototype and combine the pattern in prototype and semanticinformation of input code to generate a target summary.
we conducted extensive experiments on a large scale java dataset.
the experimental results show that e ditsumsubstantially outperforms the state of the art neural baselines and the ir based baselines.
human evaluation and case analysis provethat e ditsum can generate concise and informative summaries which can effectively help developers understand theintent of the programs.
the analysis of the experimentalresults shows that e ditsumcan generate more keywords and performs well on code and summaries of different lengths.in the future we will explore how to generate standard andmeaningful code summaries for various software projects.