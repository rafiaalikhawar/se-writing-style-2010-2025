boosting coverage based fault localization via graph based representation learning yiling lou hcst department of computer science and technology peking university beijing china yiling.lou pku.edu.cnqihao zhu hcst department of computer science and technology peking university beijing china zhuqh pku.edu.cnjinhao dong hcst department of computer science and technology peking university beijing china dongjinhao stu.pku.edu.cn xia li department of software engineering and game design and development kennesaw state university kennesaw usa xli37 kennesaw.eduzeyu sun hcst department of computer science and technology peking university beijing china szy pku.edu.cndan hao hcst department of computer science and technology peking university beijing china haodan pku.edu.cn lu zhang hcst department of computer science and technology peking university beijing china zhanglucs pku.edu.cnlingming zhang department of computer science university of illinois at urbana champaign illinois usa lingming illinois.edu abstract coverage based fault localization has been extensively studied in the literature due to its effectiveness and lightweightness for realworld systems.
however existing techniques often utilize coverage in an oversimplified way by abstracting detailed coverage into numbers of tests or boolean vectors thus limiting their effectiveness in practice.
in this work we present a novel coverage based fault localization technique grace which fully utilizes detailed coverage information with graph based representation learning.
our intuition is that coverage can be regarded as connective relationships between tests and program entities which can be inherently and integrally represented by a graph structure with tests and program entities as nodes while with coverage and code structures as edges.
therefore we first propose a novel graph based representation to reserve all detailed coverage information and fine grained code structures into one graph.
then we leverage gated graph neural network to learn valuable features from the graph based coverage representation and rank program entities in a listwise way.
our evaluation on the widely used benchmark defects4j v1.
.
shows that dan hao is the corresponding author.
hcst is short for key lab of high confidence software technologies peking university ministry of education china.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
significantly outperforms state of the art coverage based fault localization grace localizes bugs within top whereas the best compared technique can at most localize bugs within top .
we further investigate the impact of each grace component and find that they all positively contribute to grace .
in addition our results also demonstrate that grace has learnt essential features from coverage which are complementary to various information used in existing learning based fault localization.
finally we evaluate grace in the cross project prediction scenario on extra bugs from defects4j v2.
.
and find that grace consistently outperforms state of the art coverage based techniques.
ccs concepts software and its engineering software testing and debugging .
keywords fault localization graph neural network representation learning acm reference format yiling lou qihao zhu jinhao dong xia li zeyu sun dan hao lu zhang and lingming zhang.
.
boosting coverage based fault localization via graph based representation learning.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction fault localization fl aims to diagnose buggy program entities i.e.
classes methods or statements fully esec fse august athens greece yiling lou qihao zhu jinhao dong xia li zeyu sun dan hao lu zhang and lingming zhang automatically and has been extensively studied to facilitate software debugging process.
more specifically fault localization techniques often leverage various static and or dynamic program analysis information to compute suspiciousness scores i.e.
probability of being faulty for each program entity.
program entities are then ranked in the descending order of their suspiciousness scores based on which manual bug fixing or automated program repair can further be applied.
to date researchers have proposed to leverage various information to facilitate fault localization such as coverage mutation predicate switching program repair bug report and code history .
among them coverage based fault localization has been intensively studied in the literature due to its effectiveness and lightweightness for real world systems .
spectrum based fault localization sbfl one of the most popular coverage based fl techniques identifies buggy program entities by statistically analyzing coverage of failed and passed tests.
in particular existing sbfl represents coverage by the numbers of failed and passed tests covering each program entity and regards the entities covered by more failed tests and less passed tests as more suspicious.
although widely adopted for its simplicity and efficiency sbfl has limited effectiveness in practice which results from two major drawbacks in its design.
sbfl utilizes coverage in an oversimplified way by abstracting it into the number of covering tests for each program entity which may ignore detailed coverage information.
sbfl considers coverage as the only input information which cannot always infer the actual causal relationships between program entities and faulty behaviours or distinguish program entities with similar coverage.
to address the limitations in traditional spectrum based fault localization recently learning based fault localization has been proposed which leverages advanced machine deep learning techniques to utilize coverage more exhaustively i.e.
learning to represent or integrate coverage with extra information more intelligently i.e.
learning to combine .
in particular learning to represent techniques summarize coverage by a finer grained representation i.e.
a boolean vector for each test based on which various learning approaches are further applied to learn causal relationships between test coverage and test outcome.
however such a representation can still be imprecise since it treats each program entity equally and analyzes each test independently.
in addition these techniques still consider coverage as the only input which suffers from the same issue of single information source as sbfl.
orthogonal to more exhaustive coverage utilization learning to combine fault localization learns to integrate coverage with extra information by using suspiciousness scores computed by existing sbfl and other information as features.
for example fluccs learns to rank program entities based on suspiciousness scores of existing sbfl code complexity and code history similarly the latest learning to combine technique deepfl utilizes neural networks to combine suspiciousness scores of spectrum based fl and mutation based fl code complexity and text similarity.
however these techniques directly adopt suspiciousness scores generated by existing sbfl which inherently suffers from the same issues of the compressed coverage representation in sbfl i.e.
summarizing coverage by numbers of tests .
moreover some information e.g.
bugreports and code change history used in these techniques cannot be always available while other information e.g.
mutation can be very time consuming to collect limiting their applications in practice.
in summary although achieving substantial improvement existing learning based fault localization techniques still fail to well address the limitations in coverage based fault localization.
in this work we present a novel coverage based fault localization technique grace which leverages graph based representation learning to fully utilize coverage information.
the intuition in grace is that coverage can be regarded as connective relationships between tests and program entities which can be inherently and integrally represented by a graph structure with tests and program entities as nodes while with coverage and code structures as edges .
therefore we first propose a novel graph based representation to reserve all detailed coverage information and fine grained code structures into one graph.
then we leverage gated graph neural network ggnn to learn helpful features from the graphbased coverage representation and to rank program entities in a listwise way.
different from traditional machine learning and neural networks which often preprocess graph structured data to a simpler representation before learning ggnn can directly analyze graph structured information with all topological relationships reserved and has a prominent capability in graph analysis enabling more powerful fault localization.
we evaluate grace on the widely used benchmark defects4j v1.
.
which contains real world bugs from six open source java projects.
our results show that grace significantly outperforms state of the art coverage based fault localization techniques including ochiai cnnfl fluccs and deepfl .
for example grace localizes bugs within top whereas the compared techniques can at most localize bugs within top .
we further investigate the impact of each component and find that the default listwise ranking is the most effective ranking loss function the default fine grained code structures with detailed coverage information can also positively contribute to grace representing tests by oversimplified numbers as prior work significantly degrades fault localization.
in addition grace can further boost state of the art learning to combine fault localization deepfl by integrating suspiciousness scores of grace as extra features for deepfl localizing bugs within top the best learning based fault localization results on defects4j v1.
.
to our knowledge.
this indicates that grace learns essential features from coverage which are complementary to various information used in existing learning based fl.
finally we evaluate grace in the cross project prediction scenario on extra bugs from the latest version of defects4j benchmark i.e.
defects4j v2.
.
.
our results show that grace consistently outperforms state theof the art coverage based fault localization techniques on the new benchmark indicating general effectiveness of our approach.
this paper makes the following contributions a novel graph based coverage representation that integrally reserves all detailed coverage information by representing program entities tests their coverage relationships and fine grained code structures into one unified graph.
this coverage representation is general and could be applied to other problems using code coverage as inputs e.g.
regression test prioritization and reduction .
665boosting coverage based fault localization via graph based representation learning esec fse august athens greece a novel ggnn based fl technique grace that leverages gated graph neural network ggnn to fully analyze the proposed graph based coverage representation and to rank suspicious program entities in a listwise way.
an extensive evaluation on two versions of well established defects4j benchmarks in both within project and cross project prediction scenarios.
the results demonstrate the effectiveness and general applicability of the proposed approach.
our replication package is available at .
background and related work since our work leverages graph based representation learning to boost coverage based fault localization in this section we discuss the closely related work in traditional coverage based fault localization section .
and learning based fault localization section .
.
.
spectrum based fault localization spectrum based fault localization sbfl one of the most popular coverage based fl techniques calculates suspiciousness scores probability of being faulty of each program entity based on the number of failed passed tests that cover it.
the basic intuition in sbfl is that program entities covered by more failed tests and less passed tests are more likely to be faulty.
more specifically given a buggy program the test suite with at least one failed test and coverage information sbfl first abstracts coverage information into the number of tests covering each program entity e including the number of failed tests covering e ef or not coveringe nf and the number of passed tests covering e ep or not coveringe np .
based on these numbers sbfl further leverages ranking formulae e.g.
ochiai dstar and tarantula to calculate suspiciousness scores for each program entity.
for example ochiai computes the suspiciousness score of the program entityeasochiai e ef ef ep ef nf .
although widely adopted for simplicity and efficiency sbfl has been shown to have limited effectiveness in practice .
in particular traditional sbfl suffers from two major drawbacks.
sbfl utilizes coverage in an oversimplified way which summarizes coverage by the number of tests.
such a compressed representation ignores detailed coverage information that may be essential for fault localization.
sbfl considers coverage as the only input information which fails to distinguish program entities with similar coverage.
in addition coverage alone cannot always help infer the actual causal relationships between program entities and faulty behaviours.
.
learning based fault localization to address the limitations in traditional spectrum based fault localization learning based fault localization has also been extensively studied to leverage advanced machine deep learning techniques to utilize more detailed coverage information i.e.
learning to represent or integrate coverage with extra information more intelligently i.e.
learning to combine .
in particular learning to represent fl techniques learn suspiciousness scores from a finer grained coverage representation .
different from existing sbfl summarizing coverage by the number of failed passed tests these techniques represent coverage of eachtest by a boolean vector that reserves its coverage relationships with each program entity.
given a test and its coverage vector v the element vishows whether the test covers the ith program entity.
learning approaches are then applied on the vectors to learn causal relationships between test coverage and test outcomes based on which suspiciousness of program entities can further be inferred.
researchers have proposed to utilize various learning approaches such as back propagation neural network radial basis function network multi layer perceptrons and convolutional neural network .
for example cnnfl the state of the art learning to represent technique leverages convolutional neural network to facilitate coverage vector analysis.
orthogonal to more exhaustive coverage utilization learningto combine fl techniques learn to combine strengths of coverage and extra information by adopting suspiciousness scores computed by existing sbfl and other information as features.
for example fluccs adopts the suspiciousness scores of existing sbfl code complexity and code history as features trapt leverages suspiciousness scores of existing sbfl and also mutationbased fault localization as features combinefl adopts suspiciousness scores computed by existing spectrum based mutation based slicing based and information retrievalbased fault localization as features.
similarly deepfl the state of the art learning to combine fault localization technique utilizes neural networks e.g.
recurrent neural network and multi layer perceptron to combine features of four dimensions including suspiciousness scores of spectrum based and mutationbased fl code complexity and text similarity.
although achieving substantial improvement existing learningbased fl techniques still fail to eliminate the limitations in traditional sbfl completely.
for learning to represent techniques representing coverage of each test as a vector can be imprecise which treats all program entities equally and analyzes each test independently.
moreover these techniques also suffer from the same issue of single information source as sbfl since they consider coverage as the only input.
for learning to combine techniques adopting suspiciousness scores computed by existing sbfl inherently suffers from the same issues of the compressed coverage representation in sbfl i.e.
representing coverage as numbers of tests .
moreover some information e.g.
bug reports and code change history used in these techniques cannot be always available while other information e.g.
mutation can be rather time consuming to collect further limiting their applications in practice.
different from existing coverage based techniques this work makes the first attempt to represent detailed coverage by graph structures and utilize gated graph neural network to directly cope with the proposed graph based representation.
in addition we integrate coverage with lightweight information i.e.
fine grained code structures to boost coverage based fault localization for the first time.
motivating example to better illustrate the limitations in existing coverage based fault localization we further present a motivating example in this section.
as shown in table we use a real bug lang from the widely used benchmark defects4j v1.
.
.
lang denotes 666esec fse august athens greece yiling lou qihao zhu jinhao dong xia li zeyu sun dan hao lu zhang and lingming zhang table motivating example lang id method signaturecoveragesbfl rankft1ft2pt1pt2pt3pt4pt5pt6 m1 public string getnulltext .
m2 public strbuilder appendfixedwidthpadleft object int char .
m3 public strbuilder appendfixedwidthpadright object int char .
m4 public strbuilder .
m5 public strbuilder int .
m6 public strbuilder ensurecapacity int .
the47th buggy version of apache commons lang in defects4j v1.
.
.
column id and column method signiture present the unique method identifier and signature column coverage presents method level coverage of both failed and passed tests column sbfl presents the suspiciousness scores of each method computed by ochiai i.e.
one of the most popular sbfl formula and column rank presents the position of each method in the sbfl ranked list.
this bug involves multiple buggy methods i.e.
appendfixedwidthpadright andappendfixedwidthpadleft highlighted in grey and triggers two test failures i.e.
ft1andft2.
figure presents code snippets of the correct method m1and the buggy method m2.
for space limits we present only methods and tests that are essential for fault localization.
publicstringgetnulltext return nulltext publicstrbuilder appendfixedwidthpadleft object int char if width ensurecapacity size width stringstr obj null?
getnulltext obj.tostring intstrlen str.length bug... figure code snippets of m1andm2 unfortunately as shown by the table the traditional sbfl fails to localize neither buggy method within top since it always considers the method covered by more failed test and less passed tests as more suspicious.
for example the correct method m1and the buggy method m3are both covered by three passed tests but m1 is covered by more failed tests than m3.
therefore sbfl is misled by its compressed coverage representation and inflexible ranking heuristics to make a wrong judgment.
in fact besides ochiai all the existing sbfl formula share a same ranking intuition and thus all fail to rank the buggy method m3before the correct method m1.
therefore the learning to combine fl that directly adopts suspiciousness scores computed by existing sbfl formula would also suffer from the same issue as sbfl.
for example in our experiment section deepfl the state of the art learning to combine fl technique fails to rank the buggy method within top .
as for learning to represent fault localization that represents coverage as vectors the failed test ft1and the passed test pt1have the same coverage distribution and thus have identical coverage vectors.
therefore it is challenging for the model to learn causal relationships between test coverage and test outcomes which finally results in an incorrect ranked list.
for example in our experiment section the representative learning to represent fl technique i.e.
cnnfl fails to identify the buggy methods neither.
based on the example several observations can be made at this point.
coverage relationships among all program entities and alltests should be represented and analyzed in an integral way.
existing coverage based fl compresses coverage as numbers or vectors which actually regards program entities or tests as equivalent individual instances and ignores the diversity embedded in their topological coverage relationships.
for example some infrequent coverage relationships e.g.
tests covering few program entities or program entities covered by few tests may be more helpful by reducing the search space of fault localization.
in the example the passed tests pt1andpt2cover almost all the methods i.e.
out of which can provide very limited hints to identify buggy methods compared the other passed tests covering less methods e.g.
pt3covering methods .
code structures can be helpful information for coverage based fault localization.
although both the correct method m1and the buggy method m2are covered by the failed testft1 i.e.
all statements listed in figure are covered by ft1 coverage on statements of different types may be not equally important for fault localization.
for example the correct method m1is simply structured with only one return statement which can be covered by any test executing m1 whereas the buggy method m2has nested structures with ifstatements which are more challenging to be covered for a test executing m2and thus may provide richer features on program behaviours.
in addition code structures e.g.
abstract syntax structures are lightweight information and always available with low collection costs compared to various other information e.g.
mutation or bug reports used in existing learning based fl.
approach inspired by observations above in this work we present a novel coverage based fault localization technique grace which fully exploits coverage information via graph based representation learning.
more specifically given a buggy program its test suite and coverage information grace identifies buggy program entities by two phases.
first to reserve detailed coverage information grace novelly represents all program entities tests their coverage relationships and fine grained code structures into one graph section .
then grace leverages gated graph neural network to learn key features from the proposed graph based representation and to rank suspicious program entities in a listwise way section .
.
.
graph based coverage representation in this section we formally define the proposed graph based coverage representation which regards program entities and tests as nodes and coverage relationships and code structures as edges in one graph i.e.
unified coverage graph .
for better illustration we first describe three key sub components in the graph including code representation showing how to represent code structures with code nodes andcode edges i.e.
definition .
test representation 667boosting coverage based fault localization via graph based representation learning esec fse august athens greece showing how to represent tests with test nodes i.e.
definition .
and coverage representation showing how to represent coverage relationships as coverage edges in the graph i.e.
definition .
.
definition .
.
code representation.
given a method min the buggy program and its abstract syntax tree ast its statementlevel abstract syntax tree gm astis a subgraph of its original ast containing only statement and block level nodes and their corresponding edges i.e.
the token level nodes are excluded .
gm ast vm ast em ast wherevm astandem astdenote code nodes and code edges.
in particular for each code node vc vm ast attr vc denotes a set of its attributes.
vm rootdenotes the root code node in gm ast.
instead of adopting a complete ast we use statement level ast to represent code structures.
in fact statement and token level representations are equally informative in terms of the widely statement coverage information but the latter significantly enlarges scales i.e.
the number of nodes and edges and substantially increases unnecessary computation costs.
as for node attributes we consider ast node type andtest correlation for code nodes.
in particular ast node type e.g.
if statement andreturn statement effectively distills the syntactic type of each node which can be helpful for fault localization.
test correlation represents the textual similarity between code nodes and failed tests which has been inspired by information retrieval based fault localization that identifies buggy code based on the textual similarity between source files and bug reports.
these two attributes aim to reserve syntactic and semantic features of the buggy program respectively and we would further describe their detailed construction in section .
.
.
definition .
.
test representation.
given the test suite t test nodesvtrefer to a set of all tests in t. in particular for each test nodevt vt its node attribute attr vt is its test outcome i.e.
attr vt .
we represent each test as an individual test node and distinguish failed and passed tests by using their outcomes as node attributes i.e.
or .
definition .
.
coverage representation.
given a method m in the buggy program and the test suite t statement level coverage c denotes a set of statements in mthat are covered by the test t. we represent coverage as a set of edges emcovbetween code nodes vm astand test nodesvt i.e.
emcov vc vt c c t t wherevcdenotes the corresponding code node of the statement cin gm astandvtdenotes the corresponding test node of the test tinvt.
we represent coverage relationships as coverage edges between code nodes and test nodes.
in the motivating example figure illustrates code test and coverage representations for the method m2.
for clear illustration we have not included all test nodes in the figure.
by now we have presented representations within each method.
we further merge representations of all methods in the buggy program into one graph i.e.
unified coverage graph as shown in definition .
.
the unified coverage graph integrally represents coverage information of the whole program including nodes of two categories i.e.
code nodes and test nodes and edges of two categories i.e.
code edges and coverage edges .
figure illustrates 1in this work we treat ast as an unweighted and undirected graph.the final representation for the entire program in the motivating example.
definition .
.
unified coverage graph.
given the method setmin the buggy program the test suite t and statement level coverage c the unified coverage graph gof the buggy program is a graph including code test and coverage representations of all methods in m.g v e where nodesv mi mvmi ast vt and edgese mi memi ast mi memicov .
in fact unified coverage graph is a general representation and could be applied to not only fault localization but also other problems that mainly use code test and coverage as inputs such as coverage based regression test prioritization and reduction .
in addition provided with extra code test related information the unified coverage graph can be further extended by representing new information as additional node attributes nodes or edges based on the basic unified coverage graph.
v1type method declarioncorrelation .25v2type if stmtv3type conditionv4type then blockv5v6type method invocationtype variable declarationv7type variable declarationoutcome failv8outcome passv9ft1pt1code nodetest nodecoverage edgecode edge figure representations for the method m2 .
.
.
.
.
.
1ft1pt5pt6v1v2v3v4v22v24v23v25v26v219v220root code nodetest nodecoverage edgecode edge root root v5v6 v8v7v9 figure unified coverage graph for lang .
proposed model .
.
inputs.
given a buggy program the test suite with at least one failed test and the statement level coverage information we construct the unified coverage graph gas follows.
we first parse the buggy program by javalang toolkit to obtain ast representations for each method based on which we further construct 668esec fse august athens greece yiling lou qihao zhu jinhao dong xia li zeyu sun dan hao lu zhang and lingming zhang its statement level ast gm astby removing token level nodes and relevant edges.
we further connect gm astof each method into one graph by including test nodes and coverage edges according to coverage information.
finally we annotate each node with its attributes in the graph.
as mentioned in section .
we annotate test nodes with test outcomes and annotate code nodes with ast node type and test correlation.
in particular for ast node type we adopt the node type generated by javalang during ast parsing including types in total for test correlation currently we consider this attribute only for each root code node by calculating the textual similarity between its belonging method name and failed test names since prior fault localization work has demonstrated the effectiveness of textual similarities between failed tests and buggy methods .
intuitively a method whose name has a higher textual similarity with failed test names is more likely to be faulty.
therefore following prior work we compute the textual similarity between two words as equation where wmandwtdenote the method name and the failed test name and len wm wt and len wt denote the number of their common tokens and the number of tokens in the failed test name after they are tokenized by camelcase.
in particular we adopt the maximum value when there are more than one failed tests.
cor wm wt len wm wt len wt to further represent the constructed unified coverage graph in a suitable format for the graph neural network we use an adjacency matrixato represent the graph structure and use an attribute sequencesto represent all node attributes in the graph.
in particular given the unified coverage graph g v e the element avi vjin the adjacency matrix a v v represents whether node viconnects with node vj.
to avoid gradient vanishing exploding issues caused by cumulative degrees in the matrix we further normalizeaas a d 2ad whereddenotes a diagonal matrix i.e.
dvi vi de i and de vi is the cumulative degrees of node vi.
in the attribute sequence s svidenotes the attributes of node vi.
in particular for the test node and non root code node that have only one attribute it includes one token which can be any of type and for the root code node that has two attributes it includes one token i.e.
ast node type and one float value i.e.
test correlation which can be a two tuple type cor .
by far the unified coverage graph ghas been represented by the systematic adjacency matrix aand the attribute sequence s which are further fed to the neural network as its inputs.
.
.
gated graph neural network.
traditional machine learning and neural networks often handle graph structured data with a preprocessing phase to transform the graph to a simpler representation during which important information may be lost whereas graph neural network gnn which can directly analyze graph structured information with all topological dependency reserved has gained increasing popularity in recent years.
gated graph neural network ggnn is a variant of gnn that further includes gated units to preserve long term dependencies such as gated recurrent unit gru and long short term memory lstm .
therefore in grace we design a ggnn model to learn key features in the unified coverage graph c1c2c v ...ggnnlayer s1s2s v ...embeddinglayerx1x2x v ...linear transformationsoftmax attribute sequencessystematic adjacency matrix aa1 1a1 2a1 v ...a2 1a2 2a2 v ...a v 1a v 2a v v ...............c1c2c v ...c1c2c v ...c1c2c v ...c1c2c v ... figure architecture of grace and to rank suspicious code nodes in the graph.
figure shows the architecture of our model.
we then describe key components as follows.
embedding layer.
the word embedding layer first encodes the attribute sequence sinto an attribute matrix x r v d where ddenotes the embedding size.
as mentioned above the test node and non root code node have only one attribute token which can be directly embedded into a d dimension vector.
for the root code node with two attributes we first embed ast node type i.e.
a token into a vector of d 1dimensions and then concatenate it with test correlation i.e.
a float value into a d dimension vector.
ggnn layer.
we apply the ggnn layer by five iterations.
in thetth iteration for each node we update its current states by incorporating information from its adjacent nodes and from the previous iterations.
in grace we implement the gated mechanism in ggnn by leveraging input gates andforget gates in lstm to control the propagation of cell states .
in particular c t vdenotes the cell state for node vin thetth iteration and initially c v xv .
a t vpropagates cell states of all its adjacent nodes in the t 1th iteration as shown in equation .
a t v av c t ... c t v in particular forget gates decide what information to be excluded from cell states i.e.
equation input gates decide what new information from current input i.e.
a t v to be included into cell states i.e.
equation .
based on new and forgetting information cell states can be updated as equation where denotes hadamard product.
f t v sigmoid wfa t v bf i t v sigmoid wia t v bi gc t v tanh wga t v bg c t v f t v c t v i t v gc t v 669boosting coverage based fault localization via graph based representation learning esec fse august athens greece to avoid the problem of vanishing gradients we further leverage residual connection and layer normalization between each of the two sub layers.
.
.
inference.
the outputs after the computation of all ggnn iterations are further fed to a linear transformation layer followed by a softmax activation.
in particular for node vi zidenotes its output of the last iteration in ggnn layer which is further linearly transformed into a real number y ias equation where w rd andb r. ingrace nodes are ranked in a listwise way and thus we leverage softmax function to normalize the outputs of all nodes as equation where p vi denotes the probability of node vibeing faulty.
since grace targets at method level fault localization we consider only root code nodes in the inference phase i.e.
nis the number of root code nodes.
y i wzi b p vi exp y i n j 1exp y j .
.
ranking loss function.
listwise pairwise and pointwise are three common loss functions that have been widely used in learningto rank techniques .
given a ranked list listwise function evaluates the entire list based on the order of all elements.
it inherently agrees with the intuition of grace that represents and analyzes all elements and their relationships in an integral way.
therefore grace adopts listwise ranking as its default loss function which can be computed as equation .
in particular g vi denotes the ground truth label for node vi andp vi denotes its inference results.
llist n i 1g vi log p vi in principle grace can also leverage the other two functions i.e.
pairwise andpointwise for loss calculation which can be computed as equation .
pairwise function compares buggy nodes v and correct nodes v in pair while pointwise function computes loss for each node vias a binary classification problem.
different from listwise function sigmoid activation function is used in the last layer instead of softmax i.e.
p vi sigmoid y i .
we would further investigate the impacts of loss functions in the detailed experiments.
lpair i v j v max p vi p vj lpoint g vi log p vi g vi log p vi experiment design .
research question rq1 effectiveness of grace .how does grace perform compared to state of the art coverage based fault localization techniques?
rq2 impact analysis of grace components.
rq2a impact of ranking loss function.
how does the ranking loss function impact the effectiveness of grace ?
rq2b impact of code representation.
how does the code representation impact the effectiveness of grace ?
rq2c impact of test representation.
how does the test representation impact the effectiveness of grace ?
rq3 integrating with other information.
can grace further boost state of the art learning based fault localization techniques that use various information?
rq4 cross project effectiveness on defects4j v2.
.
.
how does grace perform in the cross project prediction scenario on the new benchmark defects4j v2.
.
?
table benchmark information id name bug test loc lang apache commons lang 22k math apache commons math 85k time joda time 28k chart jfreechart 96k closure google closure compiler 90k mockito mockito framework 23k defects4j v1.
.
344k cli commons cli 4k codec commons codec 10k collections commons collections 65k compress commons compress 12k csv commons csv 2k gson gson na na jacksondatabind jackson databind na na jacksoncore jackson core 31k jacksonxml jackson dataformat xml 6k jsoup jsoup 14k jxpath commons jxpath 21k defects4j v2.
.
165k .
benchmark we perform our experiments on the widely used benchmark defects4j which contains hundreds of reproducible real bugs from a wide range of projects.
the benchmark currently has two versions an original version defects4j v1.
.
and a recently released version defects4j v2.
.
with extra bugs.
to our knowledge existing fault localization work uses only the original version defects4j v1.
.
for evaluation.
in our study we evaluate our approach and state of the art fault localization techniques not only on the original version i.e.
from rq1 to rq3 but also on the latest version i.e.
rq4 for the first time.
table shows detailed information of the benchmark.
columns id and name present the short name and full name of each subject column bugs presents the number of bugs in each subject columns loc and test present the number of lines and tests in the head version of each subject.
note that the first bugs in jsoup and all bugs in gson jacksoncore highlighted in gray fail to be reproduced.
thus we exclude subjects gson and jacksoncore and use the remaining bugs for jsoup.
in total our experiments are conducted on all bugs from defects4j v1.
.
and additional bugs from defects4j v2.
.
.
670esec fse august athens greece yiling lou qihao zhu jinhao dong xia li zeyu sun dan hao lu zhang and lingming zhang .
independent variables .
.
compared techniques.
in rq1 and rq4 we compare grace with the following state of the art coverage based fault localization techniques.
spectrum based fault localization.
we compare grace with all sbfl formulae studied in prior work and present the best one i.e.
ochiai in our results.
learningbased fault localization.
we also consider three representative learningbased fault localization for comparison including the state of theart learning to represent fault localization cnnfl the representative learning to combine fault localization fluccs based on machine learning and the representative learning to combine fault localization deepfl based on deep learning.
for cnnfl since its source code is not available we reimplement it strictly following the original paper .
for fluccs and deepfl we directly take their corresponding implementations from the deepfl github webpage .
note that in this study we focus on coveragebased fault localization that includes only source code tests and coverage as inputs while the original deepfl technique includes mutation based fault localization information which can be very time consuming to collect i.e.
hours of online collection time per bug .
therefore for a fair comparison with grace we modify the original deepfl implementation to exclude mutation related features and keep the remaining three dimensions i.e.
spectrumbased fault localization information code complexity and text similarities that can be derived from source code and coverage.
we denote such a variant as deepfl covto differentiate with the original deepfl which additionally includes mutation related features .
in rq2 we consider the following variants of grace to analyze impacts of each component.
ranking loss function.
we consider grace with different ranking loss functions as mentioned in section .
.
by replacing the default loss function i.e.
listwise with pairwise andpointwise loss respectively.
for distinction we denote these two variants as gracepairandgracepoint .
comparing the default grace with these variants can show the impact from ranking loss functions.
code representation.
we simplify current code representation i.e.
definition .
to investigate its contribution tograce .
more specifically instead of using code nodes node attributes and code edges to reserve fine grained code structures we use only one node with the number of containing statements to represent each method in addition coverage is adjusted to be edges between test nodes and method nodes.
we denoted the variant with such a coarse grained code representation as gracecode .
test representation.
we simplify current test representation i.e.
definition .
to investigate its contribution to grace .
in particular we remove test nodes and directly adopt the number of failed passed tests that cover each code node as its extra node attributes.
we denoted the variant with such a coarse grained test representation asgracetest .
in rq3 to investigate whether grace has learned novel features that are complementary to other information used in existing fault localization techniques we further integrate grace with the state of the art learning to combine technique deepfl .
in particular we extend the original deepfl with a fifth dimension of features which are suspiciousness scores computed by grace .
we denote such a variant of deepfl as deepfl grace .
comparingdeepflgrace with deepfl we can investigate the complementarity between grace and the other four feature dimensions used in deepfl i.e.
suspiciousness scores of spectrum based and mutationbased fault localization code complexity and textual similarity .
.
.
experimental configurations.
from rq1 to rq3 we perform within project prediction by leave one out cross validation on bugs for each project.
following previous work we split buggy versions in each project into two groups one buggy version as testing data for prediction and all the remaining buggy versions in the same project as training data.
besides within project prediction in rq4 we further perform cross project prediction on the additional benchmark defects4j v2.
.
by two fold cross validation.
in particular we use buggy versions of all six projects in defects4j v1.
.
as training data and randomly separate all buggy versions in defects4j v2.
.
into two folds which serve as testing set and validation set in turn.
.
measurement following recent fault localization work in this work we perform fault localization at method level because recent studies have shown that class level fault localization is too coarse grained to aid debugging while statement level might be too fine grained to convey useful context information .
we use the widely used measurements as follows .
recall at top n. top n computes the number of buggy versions that have at least one buggy element localized within top n positions in the ranked list.
previous studies have shown that developers inspect only a small number of buggy elements within top positions in the ranked list e.g.
.
developers inspect only the top elements in the given list .
therefore following prior work we adopt top n n .
mean first rank mfr .
for each buggy version the first rank is the ranking of the first faulty element in the list.
for each project mfr calculates the mean of first ranks for all buggy versions.
mean average rank mar .
for each buggy version the average rank is the average ranking of all faulty elements in the list.
for each project mar calculates the mean of average ranks for all buggy versions.
following previous work we use the worst ranking for the tied elements that have the same suspiciousness scores.
for example if a correct element and a buggy element are tied with each other and both ranked at kth position in the ranked list we consider both of them are ranked at k 1th.
.
implementation data collection.
we use asm and java agent to instrument bytecode for coverage collection.
for grace we parse source code via javalang toolkit to construct ast.
in line with prior work we use jhawk asm and indri to collect code complexity and textual similarity required by compared techniques deepfl and fluccs.
time costs.
table presents the time costs for grace on the head version of each project.
in particular column vertexes and column edges present the number of vertexes and edges in the constructed unified coverage graph column construct presents the graph construction time column train and column 671boosting coverage based fault localization via graph based representation learning esec fse august athens greece test present the training and testing time for grace .
to restrain the scale of the graph we consider only suspicious methods i.e.
covered by at least one failed test and tests covering at least one suspicious method during graph construction.
based on the table we can find that graph construction is highly efficient i.e.
only one minute for the largest project closure.
in addition training time varies from seconds to one hour i.e.
minutes for the largest project closure which is acceptable since training process is often performed offline.
after the training model is ready grace then takes seconds to perform testing process.
overall grace is a lightweight learning based technique in practice.
hyperparameters.
we globally use learning rate of .
and embedding size of for all projects.
for the sake of efficiency we maximize batch size based on the scale of graphs to make full use of gpu memory.
in particular we use batch size of for all projects except closure i.e.
batch size of since its scale of graph is significant larger than other projects as shown in table .
following prior work we use a default training epoch i.e.
when performing within project prediction.
the experimental results for configurations can be found at our github website due to space limit.
furthermore all experiments are conducted with fixed random seed to avoid randomness and guarantee reproducibility.
environment.
all experiments are conducted on a dell workstation with 300g ram intel xeon cpu e5 v4 .40ghz and eight 24g gpus of geforce rtx running ubuntu .
.
lts.
we build our experiments on pytorch v1.
.
.
table efficiency of grace subjectgraph representation model vertexes edges construct s train s test s chart .
.
.
time .
.
.
lang .
.
.
math .
.
.
mockito .
.
.
closure .
.
.
.
threats to validity threats to internal validity lie in technique implementations and experimental scripts.
to mitigate the threat we manually check our code and build them on state of the art frameworks e.g.
asm and pytorch .
we also directly use the original implementations from prior work .threats to external validity lie in benchmarks used in our study.
to reduce this threat we perform our experiments on the widely used benchmark with hundreds of real world bugs.
furthermore to our knowledge we also make the first attempt to evaluate fault localization techniques on the latest version of the benchmark i.e.
defects4j v2.
.
which contains additional over two hundreds real bugs.
in the future we plan to further evaluate our approach on extra bugs .threats to construct validity lie in measurements used in our study.
to reduce this threat we use multiple measurements which are all widely used in fault localization studies .
in addition we also perform our experiments under various settings e.g.
within cross project prediction and two fold leave one out cross validation to strengthen generality of the study.
result analysis .
rq1 effectiveness of grace table presents fault localization results of grace and state ofthe art coverage based fl techniques on defects4j v1.
.
.
the first two columns present corresponding subjects and techniques and the remaining columns present results in terms of top top top mfr and mar.
from the table we can observe that grace substantially outperforms all the compared techniques in all studied metrics.
overall grace successfully localizes bugs within top more than deepfl cov more than fluccs more than cnnfl and more than ochiai.
in addition mfr and mfr are also remarkably improved i.e.
.
improvement in mfr and .
improvement in mar compared to the best compared technique deepfl cov indicating that grace is effective for all buggy elements.
moreover grace consistently outperforms other techniques on each project.
for example the improvement of grace is prominent even on the largest project closure i.e.
with .
improvement in mfr and .
improvement in mar compared to the best compared technique deepfl covon closure.
on the contrary we notice that cnnfl performs extremely poorly on the closure project i.e.
no bug is localized within top .
such a poor performance actually results from its coverage representation that uses a boolean vector to represent the coverage of each test.
the boolean vectors can be extremely sparse i.e.
most elements are zero especially in large projects where a test can cover only a small ratio of program entities.
therefore based on such a coverage representation almost all the suspiciousness scores predicted by cnnfl are values close to zero.
however grace would not suffer from such an issue in large projects since we leverage graph neutral network on a graph structured representation which focuses on only adjacent nodes rather than all nodes during learning process.
this observation further demonstrates the advantage of our coverage representation and learning model on projects of different scales.
to further confirm the observations above we perform wilcoxon signed rank test with bonferroni corrections to investigate statistical significance between grace and other state of the art techniques.
in particular we compare the rankings of buggy elements generated by grace and each compared technique in pair at the significance level of .
.
the results suggest that the improvements in terms of mar mfr achieved by grace are all statistically significant i.e.
p value .
.
.
rq2 impact analysis in this rq we further analyze the impact of each component in grace .
figure compares mfr and mar metrics between variants and the default grace i.e.
with listwise loss fine grained code and test representations .
in particular figure a presents results of default grace and variants of different ranking loss functions i.e.
pairwise andpointwise figure b presents results of default grace and the variant of a coarse grained code representation figure c presents results of default grace and the variant of a coarse grained test representation.
note that the results for the 672esec fse august athens greece yiling lou qihao zhu jinhao dong xia li zeyu sun dan hao lu zhang and lingming zhang table comparison with state of the art subject techniques top top top mfr mar chartochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
langochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
mathochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
timeochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
mockitoochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
closureochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
overallochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
top n metrics are similar and are omitted due to space limit.
based on the figures we have the following observations.
rq2a impact of ranking loss functions.
listwise is the most effective ranking loss function which achieves the best top and average rankings outperforming pairwise andpointwise by .
and .
in mfr while by .
and .
in mar.
on the contrary pointwise is the least effective one in terms of all metrics e.g.
it localizes and less bugs within top than listwise and pointwise respectively.
this observation further confirms that the default listwise loss is the most suitable loss function for grace .
the reason may be that the proposed graph based representation and graph neural network can reserve relationships between entities during learning process which inherently supports the globally ranking mechanism i.e.
consider all elements during ranking in listwise loss.
in addition it is interesting that in our study pairwise outperforms pointwise substantially but the prior work presents the opposite conclusion that pairwise loss is less effective than pointwise when integrated in learning based technique deepfl .
such inconsistencies may also result from the difference in data representations and model architectures between grace and deepfl different from grace which integrally considers all methods as one training data item deepfl regards each method as an individual training data item and may ignore the relationships between methods.
thus pairwise cannot outperform pointwise for deepfl.
rq2b impact of code representation.
the results demonstrate that the proposed code representation i.e.
representing code structures as code nodes andcode edges in definition .
positively contributes to the effectiveness of grace .
in particular grace localizes less bugs within top and downgrades mfr mar by .
.
when adopting the coarse grained code representation.
it confirms our motivation that integrating with fine grained code structures can provide helpful hints from coverage based fault localization.
rq2c impact of test representation.
the results demonstrate that the proposed test representation i.e.
representing tests as individual test nodes in definition .
also positively contributes to grace .
in particular grace localizes less bugs within top and downgrades mfr mar by .
.
when removing the fine grained test representation.
this finding further confirms our motivation that abstracting tests into numbers can impair the integrity of coverage information and downgrade the effectiveness of fault localization.
a loss function b code represent.
c test represent.
figure impact of grace components .
rq3 integrating with other information in this rq we integrate grace with deepfl to investigate complementarity between features learned by grace and other information used in deepfl.
table presents fault localization results of the original deepfl and the enhanced deepfl i.e.
deepfl grace .
the results show that grace can further boost deepfl by localizing bugs i.e.
more bugs within top which has also achieves the best fault localization results on defects4j v1.
.
to our knowledge.
moreover mar and mfr are consistently improved by .
and .
respectively.
we further perform wilcoxon signed rank test with bonferroni corrections at the significance level of .
which confirms that the improvement is significant p value .
.
our results indicate that grace has indeed learned helpful information for fault localization by fully exploiting detailed coverage and the learned features are complementary to various information used in state of the art learning to combine technique deepfl including suspiciousness scores computed by spectrum based fault localization and mutation based fault localization textual similarity and code complexity.
in addition this finding also indicates that integrating grace with other information can further enable more powerful fault localization.
673boosting coverage based fault localization via graph based representation learning esec fse august athens greece table integrating grace with deepfl techniques top top top mfr mar deepfl .
.
deepflgrace .
.
.
rq4 cross project prediction on defects4j v2.
.
we further evaluate grace on the newer version of defects4j benchmark i.e.
defects4j v2.
.
to our knowledge which has been used in fault localization studies for the first time.
table presents fault localization results of grace and state of the art coverage based fault localization techniques in the cross project prediction scenario.
from the table we can observe that grace still substantially outperforms all compared techniques by localizing bugs within top i.e.
more than ochiai more than cnnfl more than fluccs and more than deepfl cov.
moreover mar and mfr are consistently improved at least by .
and .
compared to all the other coverage based techniques.
in addition we can observe that compared to within project prediction i.e.
rq1 on defects4j v1.
.
all techniques perform worse on defects4j v2.
.
in the cross project prediction scenario.
for example deepfl covcan localize .
bugs within top on defects4j v1.
.
while only .
bugs within top on defects4j v2.
.
as for grace it can localize .
bugs within top on defects4j v1.
.
while .
bugs within top on defects4j v2.
.
.
the observation is as expected since in the within project prediction scenario testing data and training data are from the same project which tend to share similar features whereas the cross project prediction can be more challenging since characteristics between projects can be very different.
even though we can observe that compared to other techniques grace exhibits the smallest effectiveness drop between within project and cross project prediction.
in summary our results demonstrate that even when trained in the cross project prediction scenario grace still consistently outperforms state of the art coverage based techniques on hundreds of extra bugs.
table cross project effectiveness on defects4j v2.
.
subject techniques top top top mfr mar overallochiai .
.
cnnfl .
.
fluccs .
.
deepflcov .
.
grace .
.
conclusion in this work we present a novel coverage based fault localization technique grace which fully utilizes coverage information with graph based representation learning.
we first propose a novel graph based representation to reserve all detailed coverage information and fine grained code structures into one graph with tests and program entities as nodes while with coverage and code structures as edges.
then we leverage gated graph neural network to learn valuable features from the graph based coverage representation and to rank program entities in a listwise way.
our evaluation on the widely used benchmark defects4j v1.
.
shows that gracesignificantly outperforms state of the art coverage based fault localization.
in particular grace localizes bugs within top whereas the best comparison technique can at most localize bugs within top .
we further investigate the impact of each component and find that they all positively contribute to grace .
in addition our results also demonstrate that grace has learned essential features from coverage which are complementary to various information used in existing learning based fault localization.
finally we evaluate grace in the cross project prediction scenario on extra bugs from defects4j v2.
.
and find that grace consistently outperforms state of the art coverage based techniques.