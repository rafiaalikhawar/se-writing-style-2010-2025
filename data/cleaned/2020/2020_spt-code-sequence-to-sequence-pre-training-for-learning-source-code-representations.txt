spt code sequence to sequence pre training for learning source code representations changan niu state key laboratory for novel software technology nanjing university nanjing china nougatca qq.comchuanyi li state key laboratory for novel software technology nanjing university nanjing china lcy nju.edu.cnvincent ng human language technology research institute university of texas at dallas richardson texas usa vince hlt.utdallas.edu jidong ge state key laboratory for novel software technology nanjing university nanjing china gjd nju.edu.cnliguo huang dept.
of computer science southern methodist university dallas texas usa lghuang lyle.smu.edubin luo state key laboratory for novel software technology nanjing university nanjing china luobin nju.edu.cn abstract recent years have seen the successful application of large pretrained models to code representation learning resulting in sub stantial improvements on many code related downstream tasks.
butthereareissuessurroundingtheirapplicationtosetasks.first themajorityofthepre trainedmodelsfocusonpre trainingonly theencoderofthetransformer.forgenerationtasksthatareaddressed using models with the encoder decoder architecture however there isno reason why the decoder shouldbe left out during pre training.
second many existing pre trained models including state of the artmodels suchast5 learning simplyreuse thepretrainingtasksdesignedfornaturallanguages.moreover tolearn the natural language description of source code needed eventually for code related tasks such as code summarization existing pretraining tasks require a bilingual corpus composed of source code and the associated natural language description which severely limits the amount of data for pre training.
to this end we propose spt code a sequence to sequence pre trained model for sourcecode.
in order to pre train spt code in a sequence to sequencemanner and address the aforementioned weaknesses associated with existing pre training tasks we introduce three pre training tasks that are specifically designed to enable spt code to learn knowledgeofsourcecode thecorrespondingcodestructure aswell as a natural language description of the code without relying on any bilingual corpus and eventually exploit these three sources ofinformation when it is applied to downstream tasks.
experimental results demonstrate that spt code achieves state of the art performance on five code related downstream tasks after fine tuning.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
concepts softwareanditsengineering softwaremaintenancetools computing methodologies artificial intelligence.
keywords pre training code representation learning sequence to sequence acm reference format changanniu chuanyili vincentng jidongge liguohuang andbinluo.
.spt code sequence to sequencepre trainingforlearningsource code representations.
in 44th international conference on software engineering icse may21 pittsburgh pa usa.
acm newyork ny usa pages.
introduction pre training has revolutionized the way computational models are trainedinthenaturallanguageprocessing nlp community .
for a long time supervised learning has been the most successful natural language learning paradigm.
the pioneers of thepre trainingideachallengedthisviewbyshowingthatavast amountofgeneralknowledgeaboutlanguage includingbothlinguistic and commonsense knowledge can be acquired by pre training a model in a task agnostic manner using self supervised learning tasks.
self supervised learning tasks are nlp tasks for which the label associated with a training instance can be derived automaticallyfromthetextitself.consider forinstance oneofthe most well known self supervised learning tasks masked languagemodeling mlm .givenasequenceofwordtokensinwhicha certain percentage of tokens is maskedrandomly the goal of mlm istopredictthemaskedtokens.ascanbeeasilyimagined amodel formlmcanthereforebetrainedoninstanceswhereeachoneis composedofapartiallymaskedsequenceofwordtokensandthe associated class value is the masked tokens themselves.
because nohumanannotationisneeded amodelcanbepre trainedona very large amount of labeled data can be automatically generated thereby acquiring a potentially vast amount of knowledge about language.
a pre trained model can then be optimized for a specific ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa changan niu chuanyi li vincent ng jidong ge liguo huang and bin luo taskbyfine tuningitsparametersusingtask specificlabeleddata in the standard supervised fashion.
inspiredbythesuccessesofpre trainedmodelsinnlp anumber of pre trained models for source code have been proposed and applied to a variety of se tasks including code summarization and codecompletion withnotablesuccesses .despitethesepromisingresults thereareissuessurrounding the application of pre trained models for source code to se tasks.
first the majority of these pre trained models focus on pretraining only the encoder of the transformer .
this is not ideal however.
for instance for generation tasks that are addressed using models with the encoder decoder architecture there is no reason why the decoder should be left out in the pre training process.
second these models have largely assumed as inputs the source code and the associated natural languagedescription .inparticular codestructure whichis alsocrucialtounderstandingsourcecode islargelymissingfrom thesemodels.thereason whycodestructureisleftout isthatse researchershaveforthemostpartsimplyreusedthepre training tasks designed for natural languages when pre training models by viewingsourcecodeasnaturallanguage butnoneofthesetasksare concerned with learning the structure of anatural language.
third thesepre trainingtasksassumetheavailabilityofabilingualcorpus where each method function henceforth collectively referred to as method is labeled with the corresponding docstring whenpre training a model on source code and the associated naturallanguage description .
however such a bilingual corpus tends to be small in size compared to a monolingual i.e.
sourcecode only corpus thus severely limiting the amount of data amodel can be pre trained on.
in general we believe the reliance onabilingualcorpuswouldhinderthedevelopmentofpowerful pre trainedmodelsforsourcecodeinthelongrun asakeystrength of the pre trained models developed in the nlp community stems fromtheirabilitytobetrainedusingself supervisedtasksforwhich very large amounts of labeled data can be automatically generated.
several attempts have been made to address the three problems mentioned above to different extents.
to address the first problem t5 learning and treebert are proposed which are sequence to sequence i.e.
seq2seq pre trainingmodelswiththe encoder decoder architecture and enables both the encoder and thedecodertobejointlytrained.toaddressthesecondproblem graphcodebert employstwo pre trainingtasks specifically designedtoacquirestructural information.onetaskinvolvespredictingtheedgesinthedataflowwhiletheotherinvolvespredicting the alignment between the nodes in the data flow and the code sequence respectively.however astheauthorsalsopointedout whilethedataflowcapturesinformationthatislargelysemantic innature itdoesnotcapturesyntacticinformation e.g.
thesyntactic structure encoded in an abstract syntax trees i.e.
ast which is arguably the most important type structural information about source code that is commonly exploited byse researchers.
treebert employs the set of constituent paths of asts as the input of its encoder.
however it only inputs code sequences at thedecodersideduringpre training tryingtomaketheencoder learnlexicalandsemanticinformation bothofwhichcanbeeasily obtainedfromthecodesequences fromtheast whichcontains mainlysyntacticinformation duringthepre trainingphase.butit is uncertain whether treebert can extract the complete lex ical and semantic information from the ast only by relying on pre training thus eliminating the need to input code tokens when fine tuning.toaddressthethirdproblem t5 learningtreatscode andnaturallanguageastwotypesofindependentdatainstances.
whilethisallowst5 learningtolearnfromamonolingualrather than bilingual corpus the connection between a piece of code and the associated natural language description is no longer present in the corpus.
hence it is no longer clear whether t5 learning canstilllearntoproduceanaturallanguagedescriptionofapieceof code.toourknowledge therehasbeennoattempttoaddressall three issues in a single model.
in light of the above discussion we propose spt code a new pre trained model for source code.
motivated by t5 learning sptcode is a seq2seq pre training model enabling both the encoder andthedecoderoftransformertobejointlypre trained.eachdata instanceforspt codeiscomposedofthreetypesofinformation derived from a method namely the code sequence its ast and the associated natural language description.
note that the incorporationofastsallowsspt codetoexploitstructural specifically syntactic information.inaddition toobviatetheneedtolearnnatural language descriptions from a bilingual corpus we will simply use the name of the method and the names of the methods thatare invoked in this method as a very succinct natural language description of the given source code.
wedesignthreecode specificpre trainingtasksforspt code each of which allows spt code to acquire exactly one of the three typesofinformationthatcompriseadatainstance.thefirsttaskisa versionthewell knownmaskedsequencetosequence mass pre trainingtaskfornaturallanguagethatweadapttosourcecode.
specifically ourmodifiedmasstaskseekstoacquireknowledge about source code via masking a random fragment of the code tokens.thesecondtask code astprediction cap isdesigned toenablethemodeltogainknowledgeofthesyntacticstructureofacodefragmentbypredictingwhetherthegivenastisthecorrect ast for the given piece of code.
the final task method name generation mng is a novel task that involves generating the subtokens of the method name which we take to be an extremely concise natural language description of the method.
afterspt codeispre trainedonthecodesearchnetdataset wefine tuneandevaluateitonfivedownstreamtasks including code summarization code completion bug fixing code translation andcodesearch.experimentalresultsshowthatspt codeachieves state of the art results under virtually all circumstances.
in sum we make the following contributions proposespt code aseq2seqpre trainedmodelforsource code that is built with the encoder decoder architecture and is applicable to both classification and generation tasks.
extend the input representation of pre trained models for sourcecodewithasimplifiedandlinearizedversionofasts.
toourknowledge wearethefirsttousebothnaturallanguage and ast as inputs in pre training for source code.
design special input representations and three code specific seq2seq based pre training tasks enabling spt code to bepre trained without relying on any bilingual corpus or labeled data.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
spt code sequence to sequence pre training for learning source code representations icse may pittsburgh pa usa pre trainspt codeonalargeunlabeledmonolingual i.e.
sourcecodeonly datasetacrosssixprogramminglanguages then fine tune and evaluate it on five downstream coderelated tasks achieving state of the art results on all tasks.
related work .
pre training models in nlp and se table presents an overview of the most prominent pre trained modelsinnlpandforcode.eachmodelischaracterizedalongfour dimensions modules whatisbeingpre trained e.g.
theencoder the decoder or both objectives the pre training objectives1 and inputs what information the model assumes as input e.g.
natural language code and structural information of the code .
wecanmakeafewobservationsfromtable1.first whilethemajority of work has focused on pre training the encoder the newest models t5 bart and t5 learning which is modeled after t5 are all seq2seq pre training models that allow theencoderandthedecodertobejointlytrained.second exceptfor graphcodebert andtreebert allpre trainedmodelsin sereusetrainingobjectivesdesignedfornaturallanguages with mlm being the most popular pre training task.
this indicates that the selection of pre training tasks which dictates what knowledge will be acquired and exploited by a model is an area of research thatis under investigatedin se.
finally while earlier pre training modelsinseassumeonlysourcecodeasinput thelateronesall use both code and language.
in our experiments we will use as our baselines the most recently developed and also the state of the art pre trained models for source code namely codebert graphcodebert cuglm t5 learning and treebert .
.
structural information of source code structural information is very important for understanding source code.
asts are widely used in code related tasks for representingstructureinformationofcode e.g.
which containsabundantsyntacticstructureinformationthatcannotbe expressed by code sequences.
an ast should be flattened withlinearization methods e.g.
pre order traversal in order traversal and structure based traversal sbt before being fed to an encoder.
code2vec code2seq and slm useamethodthatlinearizesanastasaseriesof path contexts representing two terminal nodes and the path between them.
jiang et al.
represent asts as the set of paths and then introduce node position embedding to obtain the position of the node in the 1forward lm aims to predict the next word given the preceding words in a sentence.
backward lm aims to predict the previous word given the words thatappearafteritinasentence.maskedlmisthemaskedlanguagemodelingtask described in section .
nsp aims to predict whether the second sentence in a sentencepairshouldimmediatelyfollowthefirstsentenceinthepair.permutation lm aims to predict a word using a set of context words randomly selected via the attentionmaskmechanism.rtdaimstopredictwhichtokenintheinputhasbeen replaced.
ep involves masking of the edges in a data flow and aims to predict the masked edges.
na involves masking a certain portion of edges of connecting dataflownodesandcodetokensandaimstopredictthemaskededges.tmlm masks paths in the ast input on the encoder side and tokens in the code sequence input on the decoder side then predicts the token of the masked code.
nop i st o exchange thepositions of somenodes in the path and distinguish whetherthe order of nodes in the ast is correct or not.ast.
besides neural networks that take trees as input e.g.
treelstms rvnns andgnns gcns utilize an ast directly instead of flattening it.
there are also approaches taking data flow and control flow extracted from code as structural information e.g.
and .
however these flows do not contain structural information as rich asasts .theiradvantagesoverastsarethattheyhavealower demandonhardwareandneedlesstrainingtimeunderthesame conditions .
takentogether wechoosetouseaststorepresentthestructural information in spt code.
spt code in this section we first introduce the architecture of spt code section .
.
we then describe the model input section .
and the pre training tasks section .
which are the key innovations of this paper.
finally we illustrates how to fine tune spt code when it is applied to downstream tasks section .
.
.
model architecture architecturally spt codeisessentiallyamulti layertransformer whichiswidelyusedbypre trainingmodelssuchasbart and t5 .
as far as the parameter setting of the encoder and decoder isconcerned spt codefollows codebertandgraphcodebert the number of layers i.e.
transformer blocks l the size of model dmodel the dimension of feed forward dff thenumberofself attentionheads h and the dropout rate pdropout .
.
the total number of parameters is 262m.
to pre train both classification and generation tasks with an encoder decoder structure we adopt the strategy used in bart.
in particular notethattheencoderanddecoderwillcontinuetolearnjointly and collaboratively when pre trained on classification tasks.
figure1showshowclassificationandgenerationtasksarepretrainedinspt code.specifically tousespt codeforclassification tasks theinputofthedecoderisthesameasencoder exceptthat a special symbol indicating the start of the sequence is added to the front and another special symbol used here as a placeholder for the classification position is added to the end.
the output of the corresponding position of the is then usedforclassification.forgenerationtasks figure1bdemonstrates the input and output of the model when translating a bcde to .the specialsymbol denotesthe endof the sequence and the process of generation stops when the model outputs this symbol.
.
model input the inputs of the model are three different types of components belongingtoacomplete method i.e.
itcanbeinvokedbyitsname namely the code tokens the linearized ast and the natural language.
in this subsection we will demonstrate this with a real java method2shown in the bottom of figure .
ee1 cdm src main java thredds client catalog property.java l56 l63 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa changan niu chuanyi li vincent ng jidong ge liguo huang and bin luo table pre training models in nlp and for source code.
domain modelsmodules objectives inputs lstmencoder decoder encoder decoder forward lm backward lm masked lm nsp permutation lm rtd nlcode structure nlpelmo check check check check bert check check check check xlnet check check check roberta check check check electra check check check check gpt check check check t5 check check check bart check check check codescemlo check check check check cubert check check check check c bert check check check check intellicode check check check codebert check check check check check graphcodebert check check epna check checkdata flow cuglm check check check check check t5 learning check check check check treebert check tmlm nop checkast a959c ojoxkizoutgr tiujkx zu 8kmxkyyo k kiujkx rghkr a 59c a when spt code is used for classification the inputs of the encoder and decoder are identical and the output of the decoder at the last time step is used as the label for the classification.
ojoxkizoutgr tiujkx zu 8kmxkyyo k kiujkx a959c a 59c b whenspt codeisusedforgeneration thewholeprocedure is the same as the regular sequence to sequence model.
figure spt code for classification and generation.
.
.
code tokens.
as we can see from figure the first part of the input is the code token sequence of a method.
we use a lexical analyzer to tokenize the source code and then obtain thetokens c c1 c2 ... cl wherelis the number of code tokens.
specifically weusethepythonstandardlibrary3totokenizepython codes.
for languages such as java javascript php and go we use thepythonbinding4ofantlr45togetthecodetokens.theruby sourcecodeistokenizedbythecallingofarubybinaryprogram.
thesourcecodesofotherprogramminglanguagesaretokenized by the nltk tokenizer6.
.
.
linearized ast.
to represent the second part of the input i.e.
the structural information of the code we convert an ast into a specially formatted sequence by traversing it and call the result ujk v hroi yzgzoi 2oyz 6xuvkxz a9 6c jkirgxgzout lux ol k vxkyyout ol xksu k j vy yo k iutzgoty gjj first one override public static list property removedups list property org list property result new arraylist org.
size for property p org if !result.
contains p o n result.
add p return result zuqkto kx vgxykxxksu k vy yo k iutzgoty gjjigskr ytgqk igyk yvroz gz k vxkyyout rk kr tgsk gtj ot uigzouty figure the input of a real world java code snippet.
due to spaceconstraints weabbreviatedthenodenamesintheast sequence.
nl denotes the natural language input.
ofthisconvertinga linearized ast.wefirstemployanastparser7 to get the corresponding ast then use a traversal method to parse the ast into a sequence.
instead of using the original sbt please refer to for more details which has been shown to be more effective thanclassical traversal methods i.e.
pre order traversal but tend to produce excessively long sequences that are on average more thanthree times the length of the code we propose a simplified version ofsbtcalledx sbt xml likesbt totraverseasts.x sbtcan reducethelengthoftheresultingsequenceoftraversalsbymore thanhalf figure3showsacomparisonofsbtandx sbt.itcanbe seenthatwhentraversingast sbttakestwotokens andthe node name as the starting flag of a certain ast node and takes andthenameastheendingflag.wemakeoneobservation fornonterminal nodes i.e.
nodes which are not leaves we can replace the starting flag with one token in an xml like form similarly for authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
spt code sequence to sequence pre training for learning source code representations icse may pittsburgh pa usa 2ktmzn a b e f b c d a 2ktmzn a b e e f f b c c d d a figure an example of the sbt and x sbt traversal sequences are formatted and indented for a better illustration.
the ending flag.
for terminal nodes i.e.
leaf nodes we can further merge the starting and ending tokens into one token.
therefore it is easy to prove that x sbt shortens the length by more than half.
however x sbtsequencesarestilllong.tofurthershortenxsbtsequences wemakex sbttraverseonlythenodesatorabove theexpression levelintheast.thiswillalsoreduceredundancy.
commonly theastcontainsbothlexicalandsyntacticinformation wherethelexicalinformationisalreadyrepresentedbythefirstpart of the input i.e.
code tokens .
since the lexical information in the astisconcentratedontheterminalnodes weonlykeepthenodes atorabovethe expression level sothatitcontainsonlysyntactic information.
figure shows the nodes in the ast that will be oleyzgzksktz vgxktznkyo kjek vxkyyout tgx ek vxkyyout skznujeot uigzout ojktzolokxk vxkyyouteyzgzksktz gxm sktzeroyz ojktzolokxskznujeot uigzout vojktzolokx ojktzolokx v xky rzojktzolokx iutzgotyxky rz gjjktngtikjeluxeyzgzksktz z vkeojktzolokx ojktzolokx ojktzolokx 6xuvkxz v uxm figure the ast of the for statement block of the java code snippet in figure .
non terminal nodes are ellipses and terminal nodes are represented as rectangles.
the green nodes will be traversed by x sbt at the expression level while the red ones will not.
traversedbythex sbtatthe expression level.obviously ittraverses onlyonesubtreeoftheast andconsequently itcanfurtherreduce the length of the sequence by ignoring some fine grained lexical informationthatisalreadypresentinthecodetokens.toconclude wetraversetheastusingx sbtatthe expression leveltoobtainthe tokensa a1 a2 ... am wheremis the length of the sequence.
.
.
natural language.
for extracting natural language informationfromthecodeonly wederivethemethodnameandapicall sequence of the code8.
for example the tokens extracted from the codesnippetinfigure2are removedups size contains add .
we further split each token of the form camelcase andsnake case soremovedups is split into removeanddups.
then we take the 8the reason we do not use documentation in code such as docstring and in line comments isthatdocumentationisnotalwaysavailableaswementionedinsection1.resulting linear sequence of tokens n n1 n2 ... np as our natural language input where pis the number of tokens.
aftercompletingtheconstructionofthethreeinputparts we concatenatethemanduseaspecialsymbol i.e.
toseparate the three inputs.
therefore the input is represented as input c1 ... cl a2 ... am n1 ... np .
pre training tasks inthissection weintroducethethreetasksintheordertheyare used for sequential pre training.
.
.
code ast prediction.
the first pre training task is codeastprediction cap .sinceweaddstructuralinformationtothe input in cap we expect to have the model learn such informa tion about the structure represented by x sbt.
so we introducethis binarized task that can be trivially generated from any code.
specifically whenweconstructtheinputrepresentation namely input c a n of the time ais the actual ast sequence corresponding to c labeled as isast and of the time itisarandomastsequencefromthedataset labeledas notast .
asweshowinfigure1a label isusedforcode astprediction.
note that cap is closely similar to the next sentence prediction task .
.
.
mass.
since spt code is based on the encoder decoder architecture we expect to pre train spt code in a sequence to sequence style.
therefore we then adopt mass which seeks toreconstruct a sentence fragment given the remaining part of thesentence in the encoder decoder framework.
we employ a mod ified version of mass as one of our pre training tasks with the intentionoftrainingthemodeltounderstand inferandgenerate code sequences.
given aninput input c a n wedenotec u v originas amodifiedversionof cwhereitsfragmentfromposition utovis masked that is c u v origin c0 ... cu ... cv ... cl where u v l. in contrast cu vdenotes the fragment of cfromutov.
in our work we merge a number of consecutive symbols into one then c u v originis replaced by c u v c u v c0 ... cu cv ... cl we utilize this modified version of mass to pre train our sequencetosequencemodelbypredicting thefragment cu vtaking theinputinput c u v a n.then k u v 1isthe number of the masked consecutive tokens and we follow song et al.
and set k oflto achieve the best performance.
.
.
method name generation.
with the last pre training task we would expect the model to learn information such as the intent and functionality of the code.
the method name which is present in every method can be seen as an extremely concise summaryofthemethod.xieetal.
analyzethemethodnameandcode summary in a java dataset built by leclair and mcmillan and findthatonaverage50.
ofthewordsinmethodnamesappear in the corresponding summaries and .
of the words in the summariesappear inthe correspondingmethod names.for about authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa changan niu chuanyi li vincent ng jidong ge liguo huang and bin luo of the methods all the words in the method names appear in the corresponding summaries.
therefore they improve codesummarization performance successfully by predicting method names in advance.
therefore we exploit method names in this pre training task which we call method name generation mng .
given the model inputinput c a n we denote name cias the method name in c whereiis the index of the method name token inc. from section .
.
we learn that the split name cialso appears inn so we remove the tokens from nthat are derived from the method name split.
assuming that the method name is split intossubtokens the final input of mng will be as follows inputmng c0 ... ci ci ... cl a1 a2 ... am ns ns ... np then we make the decoder try to generate the split method name i.e.
n1 n2 ... ns.
.
fine tuning wegroupalldownstreamtasksintotwocategories classification andgeneration.forclassificationtasks weusethesettingshownin figure 1a otherwise we use the setting in figure 1b.
for each task we simply plug in the task specific inputs and outputs into sptcode and fine tune all the parameters end to end.
for example in code search which we cast as a classification task when obtaining arepresentation vectorofanaturallanguage query wetake only thenaturallanguageasinputanddonotconsiderthefirsttwoparts of the input i.e.
code and ast.
for code summarization which we cast as a generation task we only include the natural language summaryintheoutput.wewilldescribethetask specificdetails in the corresponding subsections of section .
experiment inthissection wefirstintroducethepre trainingdataandsettings section .
and how we fine tune on the downstream tasks section .
.
then we answer four research questions section .
.
finally we manually evaluate spt code through quantitative and qualitative analysis.
.
pre training .
.
dataset.
the dataset we use for pre training spt code is codesearchnet whichhasalsobeenusedtopre traincodebert graphcodebert and t5 learning .
the codesearchnet corpus is programmaticallyobtained by scraping opensourcerepositoriesandpairingindividualfunctionswiththeir processed documentation as natural language annotation.
it includes more than .4m codes from programming language i.e.
java python javascript php goandruby.thedatastatisticsareshown in table .
sinceourinputcanbeextractedfromcompletelyunlabeleddata we can make use of all the .4m data instances in codesearchnet.
however codebert and graphcodebert use labels i.e.
the documentation in the input for training both code and natural language sotheyarebothpre trainedonallthedatainthe w documentation column of the all row which we named csnw doc.table pre training dataset size statistics language methods function w documentation all java python javascript php go ruby all t5 learning inputsthecodeandthedocumentationintothemodel separatelywhenpre training soitresemblesspt codeinthesense thatitdoesnotutilizethecode documentcorrespondence.nevertheless the publiclyavailable implementationof t5 learningusesonly the data in the all column of java row named csn java.
.
.
settings.
thethreepre trainingtasksofspt codeareperformedsequentially.wetrieddifferentordersanddifferentnum bersofepochforthesetasksandfoundthatbetterresultscanbe achieved by first pre training cap for epochs then mass for 30epochs andeventuallymngfor30epochs.allthreetasksuse cross entropy as the loss function.
we use adamw as our optimizer the initial learning rate is 5e and the warmup step is set as .
we pre train spt code on nivdia a100s9with a total batch size of .
we use byte pair encoding bpe to tokenize the code and thenaturallanguage aftercamelcaseandsnake casesplitting and use regular word tokenizer to tokenize x sbt sequences.
the tokenizer is built upon the whole pre training data and will be directly used on each downstream tasks without any modification.
.
fine tuning on downstream tasks in this subsection we detail the process of fine tuning the pre trained spt code on the five downstream tasks.
for each downstreamtask we giveabriefintroduction tell thedatasetsforfinetuning presentthecomparedbaselines andeventuallyillustrate the metrics for evaluating.
.
.
codesummarization.
codesummarization alsoknownas theprocessofcodesummaryorcodecommentgeneration isthe task of automatically generating a natural language description for apieceofsourcecodethatsummarizestheoverallactionsofthe codesnippetaccurately adequately andconcisely .workinthis area of research has generally focused on generating a typically short natural language comment from a given method.
datasets.
inadditiontocodesearchnet weusetwowidelyused classicaldatasetsthatarecollectedfromopensourcerepositories in github i.e.
the java dataset jcsd and the python dataset pcsd .
as for jcsd the comment of each method is the first 9thegpusareprovidedbyalibabagroup whichemployeflops andalibaba collectivecommunicationlibrary accl techniques.eflopsisahighperformancedistributedtrainingsystemthatcanachievenear linearscalabilityofoverall throughput andacclbringstheperformantefficiencyofeflopsarchitecturetogeneral cluster systems and cloud scenarios which is able to achieve fully non congested communication.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
spt code sequence to sequence pre training for learning source code representations icse may pittsburgh pa usa sentence extracted from its javadoc and the comment of a method in pcsd is its docstring.
baselines.
we adopt codebert graphcodebert cuglm t5 learning andtreebert asbaselinesfor alldatasets10.forjcsd pcsdandcodesearchnet sjava python datasets weadoptnerualcodesum whichemploysrelativeposition and copy attention upon vanilla transformer asanotherbaseline11.besides weusethreemorebaselinesforjcsd andpcsd namely deepcom rencos andsit which are widely compared or recently proposed rnn or transformerbased models dedicated for code summarization.
metrics.
weusebleu b. meteor m. androuge l r.l whicharewidelyusedinfieldssuchasmachinetranslationandtextsummarization to measurethesimilaritybetween the sentences generated by the model and the goal sentences.
.
.
codecompletion.
codecompletionistogeneratecodegiven itssurroundingcodeascontext.wefine tunespt codeonatask calledany code completion here.unlike restricted completion wherethetargetcodecontainsonlyprimitivetypes e.g.
intand string and excludes user defined functions any code completion aimstogeneratecodeinageneral purposeprogramminglanguage without any restriction on its vocabulary or structure.
specifically consider a program pand some part of the program p p anycode completion makes the model to predict pgiven the rest of the program p p p. datasets.
the dataset we use for any code completion is the javadatasetprovidedbyalonetal.
.itisbasedonthejava small dataset whichcontainstheleastcodeduplication .alonetal.
createany codecompletionexamplesbyselectingeveryexpression largerthanasingleastnodeasthetarget p usingthereminder ofthemethodasthecontext p.theyalsofiltermanymethodsto cleanthedatasetandmakethetaskharder.theresultingdataset contains .3m 10k 20k train dev test examples.
baselines.
besides codebert graphcodebert cuglm t5learningandtreebert somespecificmethodsforthistaskarealso compared.code2seq representsacodesnippetasthesetofcompositional paths in its ast and uses attention to select the relevant paths while decoding.
transformer w copy uses the implementationofopen nmt withacopymechanism .seq2treew copy learns to generate the linearized subtokenized target ast forcodecompletion.slm leveragesjointmodelingofanast anditsmissingsubtreeusingastructurallanguagemodel which estimates the probability of the program s ast by decomposing it into a product of conditional probabilities over its nodes.
metrics.
following alon et al.
we use exact match accuracy at acc and acc for evaluation.
an exact matchis counted if and only if the sentence generated by the model is identicaltothegoalsentence ignoringcasesandwhitespaces .if weletthemodelgenerate kcandidatesentenceswiththehighest probabilityandifanexactmatchiscountedwhileanyoneofthe candidates matches the goal sentence exactly the ultimate exact match accuracy is acc k e.g.
if only one candidate it is acc .
10we resize all the pre trained models here and in all the following downstream tasks identical to ours.
11wehavetriedtorunitasjavaforotherlanguagesincodesearchnetbutgotcompletely nonsensical output.
.
.
bug fixing.
bug fixing aka.
coderepairor code refinement aims to automatically fix bugs in the code.
it can help software developers locate and fix bugs and thus save a lot of time .
datasets.
we use the dataset collected by tufano et al.
whoextractmethod level pairsofbuggyandcorrespondingfixed code named bfps bug fix pairs from bug fixing commits in thousands of github java repositories.
each bfp is composed of a tuple bfp mb mf wherembrepresents a buggy code component mfrepresentsthecorrespondingfixedcode.basedonthecodesize tufanoetal.providetwodatasets bfp smallandbfp medium with theformerhavingacodelengthbelow50andthelatterhavinga code length between and .
baselines.
the original method proposed by the dataset creator is marked as tufano et al.
s2s copysapn i sa n extension of seq2seq models which can copy entire spans of the input to the output in one step and reduce the number of decisions requiredduringinference.codebert graphcodebert cuglm t5 learning and treebert are also used as baselines.
metrics.
wereportacc acc andbleuonbothdatasets.
bothmbandmfareabstracted pleasereferto formoredetails beforebeingfedtothemodel andwedonottranslatetheabstractedcodepredictedbythemodelbackintosourcecodebeforeevaluation using the metrics because the result before and after translation are the same.
.
.
code translation.
code translation is important for migrating legacy code in one language into an ecosystem built in a different language.
datasets.
following chen et al.
and guo et al.
we conduct our experiments on a dataset containing several opensource projects which have both a java and a c implementation.
baselines.
baselines are naive i.e.
directly copying the source codeasthetranslationresult vanillatransformer codebert graphcodebert cuglm t5 learning and treebert.
metrics.
we use acc and bleu as metrics.
.
.
code search.
code search aims to find the code snippet that mostcloselyresemblesthesemanticsofthegivennaturallanguagequerystatementfromasetofcodes whichisnamed codebaseinthis task.itisagoodchoicefortestingtheperformanceofspt codein classification mode.
for each code query pair c q we compute the representation vectors of candq i.e.
vcandvqrespectively.
then we randomly take another query statement from the dataset asanegativesample denotedas q whoserepresentationvectoris vq .toensuretheeffectivenessofthenegativesamples werestrict the bleu score betweenq andqto be lower than .
.
the training loss is lsearch summationdisplay.
c q q dmax cos vc vq cos vc vq cos vc vq vc vq bardblvc bardbl bardblvq bardbl wheredis the dataset is a fixed value of .
following gu et al.
andcosdenotes the calculation of cosine similarity.
in the evaluation we first get the representation vectors of all codes in codebase as candidates.
then for each vector of query statements we select a few code representation vectors from the candidates authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa changan niu chuanyi li vincent ng jidong ge liguo huang and bin luo thatareclosesttothevectorofquerystatements.finally themetric is calculated.
datasets.
we use codesearchnet .
baselines.
we use cnn i.e.
1d convolutional neural network bi gru andtransformer i.e.
vanillatransformer as baselines in addition to codebert and graphcodebert.
metrics.
we use mean reciprocal rank mrr for evaluation.
mrr is a statistic measure for evaluating search algorithms.
the reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer for first place for second place.
therefore the mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries q mrr q q summationdisplay.
i ranki .
evaluation we evaluate spt code by answering four research questions.
rq1 howeffectiveisspt codecomparedwiththestate of the art baselinesandothercodepre trainedmodelsonfivedownstreamtasks?
weconductexperimentsonfivedownstreamtasksintroducedin theprevioussubsection.theresultsonclassicalandcodesearchnet codesummarizationareshownintables3andtable4 respectively.
table results on classical code summarization.
methodsjcsd pcsd b. m. r.l b. m. r.l deepcom .
.
.
.
.
.
rencos .
.
.
.
.
.
nerualcodesum .
.
.
.
.
.8sit .
.
.
.
.
.
codebert .
.
.
.
.
.2graphcodebert .
.
.
.
.
.4cuglm .
.
.
.
.
.
t5 learning .
.
.
.
.
.1treebert .
.
.
.
.
.
spt code .
.
.
.
.
.
tables show the results of code completion bug fixing code translation and code search respectively.
firstofall wecanseethatwhetherthebaselinesarededicatedto aspecificdownstreamtaskorpre trainedmodels spt codeclearly outperforms them in the vast majority of cases including code summarization code completion bug fixing on bfp medium and codetranslation.thisisbecauseduringthepre trainingprocess spt code learns the representation of codes from a large amount ofdata aswellastheconnectionbetweencodesandstructuraland naturallanguageinformation.in addition themodelisenhanced to generate code and natural language sequences by pre training.
asforbugfixingonbfp small theaccuracyofspt codeisslightly lower than that of s2s copyspan which achieves the best ac curacy on bfp small.
the reason we believe is two fold.
the first is the dataset where the lengths of the code are up to which allowsrnn baseds2s copyspantoadequatelycopewithinputs oftheselengths.anotherreasonisthatthemechanismproposedby s2s copyspan to copy the entire span of the input is wellsuited for a task like bug fixing where only individual modificationsaremadeontheinput.however inthenextrq wewillshow that spt code still outperforms s2s copyspan on bfp smallafter removing mng from the pre training tasks.
finally we can learn that spt code performs comparably to graphcodebert on code search which suggests that although we designedourmodeltoperformbetterongenerationtasks itdoes not come at the cost of the performance on classification tasks.
rq2 how do the three pre training tasks as well as the ast and the natural language input contribute to the model s performance on the different downstream tasks?
in order to find the impact of each component to the overall performance of spt code we conductablation study on all downstream tasks.
we remove each all pretrainingtask andastor andnaturallanguagepartfromtheinput respectively.owingtospacelimitations forcodesummarization we only show results on java and python in codesearchnet12 and forcodesearch weonlyshowresultsonjavaandgo13.theresults are shown in the first three groups of table .
thefirstthingwenoticeisthatwhenwetrainfromscratch i.e.
remove all pre training tasks or remove the ast and natural languagefromtheinput i.e.
onlyinputcodetokens theperformance of spt code consistently drops considerably.
second when we removeeachofthethreepre trainingtasks theresultsdecreased in most cases particularly in code summarization code translation andcodesearch.ofthese capismostusefulforcodesearch mass is most helpful for code translation and removing mng has the greatest impact on code summarization.
interestingly wefindthatforcodecompletionandbugfixing spt code s performance w.r.t.
accuracy improves instead whenmngisremoved.thisisunderstandable.ontheonehand code completionandbugfixingaretaskswheretheinputandtheoutputarebothcode asismass whiletheoutputofthemngtask is natural language and thus the ability to generate natural lan guage trained by mng is not fully reflected in these two tasks.
on the other hand regarding code completion mass can be seen astotallyunrestrictedany codecompletion.bothpredictapiece ofcodebasedonitscontext withthedifferencethatinany code completion thepieceofcodeisrestrictedtobeanentireexpression while in mass the piece of code is selected completely at random.
therefore fine tuning code completion directly after mass i.e.
removing the mng yields a higher result.
whenweremoveeithertheastorthenaturallanguagefrom theinput theresultsofthemodeldrop indicatingthattheyboth help improve performance.
in addition for code summarization bugfixingandcodesearch theresultsarelowerwhenonlynatural languageisremoved whichindicatesthatincomparison natural languagehelpsthesethreetasksmorethanastsdo.onthecontrary astismorehelpfulforcodecompletionandcodetranslation.
through ablation we find that different pre training tasks show different degrees of influence on the downstream tasks as a result appropriate trade offs of pre training tasks for different downstream tasks can help the model achieve better performance.
asts 12java and python represent static and dynamic languages respectively.
13the results for both are above and below graphcodebert respectively.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
spt code sequence to sequence pre training for learning source code representations icse may pittsburgh pa usa table results on codesearchnet code summarization.
methodsjava python javascript php go ruby b. m. r.l b. m. r.l b. m. r.l b. m. r.l b. m. r.l b. m. r.l neuralcodesum .
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0cuglm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t5 learning .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5treebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
spt code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table results on any code completion.
methods acc acc code2seq .
.
transformer w copy .
.12seq2tree w copy .
.89slm .
.
codebert .
.
graphcodebert .
.00cuglm .
.
t5 learning .
.
treebert .
.
spt code .
.
table results on bug fixing.
methodsbfpsmall bfpmedium acc bleu acc bleu tufano et al.
.
.
s2s copyspan .
.
codebert .
.
.
.54graphcodebert .
.
.
.33cuglm .
.
.
.
t5 learning .
.
.
.89treebert .
.
.
.
spt code .
.
.
.
table results on code translation.
methodsjava c c java acc bleu acc bleu naive .
.
.
.
transformer .
.
.
.
codebert .
.
.
.10graphcodebert .
.
.
.39cuglm .
.
.
.
t5 learning .
.
.
.10treebert .
.
.
.
spt code .
.
.
.
and natural language both have a positive impact on the performance of the model regardless of the downstream task.table results on code search.
methods java python js php go ruby cnn .
.
.
.
.
.
bi gru .
.
.
.
.
.
transformer .
.
.
.
.
.
codebert .
.
.
.
.
.674graphcodebert .
.
.
.
.
.
spt code .
.
.
.
.
.
rq3 is the ability of utilizing more unlabeled data an advantage ofspt code?
asweknow codebert graphcodebertandt5learningarepre trainedonasubsetofcodesearchnet whereaswe can pre train on the entire dataset.
therefore there is a question of whethertheabilitytoutilizemoredataforpre trainingalsogives spt code an unfair advantage.
to ensure a fair comparison we therefore pre train spt code only on the same data as codebert andgraphcodebert i.e.
csn w doc andalsoonthesamedata ast5 learning i.e.
csn java .notice however thathereonlythe codefromthelabeleddataisutilizedandnotthelabels evenonthe csnw docwhere all samples have labels.
so in this case although we are using the data set of the same size it actually utilize less informationthantheydo.theresultsaredisplayedinthelastgroup of table .
comparing the results of spt code with those of csn w doc and csn java in table we find that there is a significant performancedecreasewhenthedatausedforpre trainingisshrunkfrom .4m to .3m or .5m.
however considering the other pre training models itstillmaintainsanadvantageorisatacomparablelevel inperformance.therefore wecanconcludethatspt codeissuperior given the same amount of pre trained data.
in addition the ability to use more unlabeled pre training data can help spt code achievehigherperformance.thisensuresthatspt codehasavery advantageous scalability at the data level compared to the other pre training models for source code.
rq4 how would the size of the training data for fine tuning spt code influence its performance on downstream tasks?
to answer this question we plot learning curves by varying the amount of task specific training datausedtofine tune spt code.
owingto space limitations we only report results of code summarization on java in codesearchnet and bug fixing on bfp medium.
we select k 10datafromthetrainingseteachtimefortraining andthentest on the same entire testing set.
the results are shown in figure .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa changan niu chuanyi li vincent ng jidong ge liguo huang and bin luo table ablation study on downstream tasks.
methodssummarization completion bugfixing translation search java pythonacc acc 5bfpsmall bfpmedium java c c javajavagob.m.
r.l b. m. r.l acc bleu acc bleu acc bleu acc bleu spt code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w ocap .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o mass .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o mng .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o all .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w oast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o nl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
only code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
csnw doc15.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
csnjava15.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
k14161820bleu meteor .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rouge l33.
.
.
.
.
.
.
.
.
.
bleu meteor rouge l a results on java codesearchnet code summarization.
k681012accuracy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bleu .
.
.
.
.
.
.
.
.
.
accuracy bleu b results on bfp mediumbug fixing.
figure results when kgoes from to .
itiseasytoseethattheperformanceshowsadecreasingtrendin allevaluationmetricsas kdecreases i.e.
thesizeofthetrainingset decreases .itprovesthatevenforpre trainedmodels thesizeofthe trainingsetplaysakeyroleintheperformanceaswell.meanwhile consideringtable4 weseethatforcodesummarization theresultsofspt codearecomparabletothatofcodebertwhenthetrainingsetsizeisreducedto1 andtheperformanceissimilartooreven higherthangraphcodebertwhenitisreducedto2 .further recallingtable6 wecanseethatwhenthetrainingsetofbfp medium isreducedto1 10oftheoriginalsize spt code sresultsarestill higher than other pre trained models and when it is reduced tohalf of the original size spt code s performance can reach the performance of the best baseline model i.e.
s2s copyspan.
the conclusion is that although there is an inevitable drop in spt code sperformanceasthesizeofthetrainingdatadecreasesduring fine tuning the performance of spt code is comparableto that of the other models when the data used for fine tuningspt code is reduced to a very small size.
this also implies the robustness of spt code.
.
quantitative analysis wecollecttheoutputofspt codeandbaselinesforawiderangeof test samples on each downstream task.
then we invited five participants all of whom are graduate students in software engineering whoarethemselvesnotauthorsofthispaper.foreachdownstream task thefivestudentsarerandomlyassignedtothesamenumberofsamples.iftherearemultipledatasetsforthatdownstreamtask the samplesofmultipledatasetsarethesame.fordownstreamtasks that use acc as a metric such as code completion we focus on the similaritybetweenincorrectandcorrectanswers andtheextent towhichtheincorrectanswer canbehelpfulwhenallmodelsfail.
the results are shown in table .
table results of quantitative analysis.
dowstream tasks sample better comparable worse code summarization code completion bug fixing code translation code search .
qualitative analysis byaskingtheparticipantsintheprevioussubsectionandbrowsingtheoutputofeachmodelbyourselves wefoundthatincomparison spt codecancapturethesemanticinformationofidentifierswithin code more accurately than other pre trained models and it can capture the semantic information of code segment globally instead of limiting to a certain region.
the extracted code semantics are relatively more widely and evenly distributed in the code segment.
inthecaseofcodesummarization forexample spt codeprovides more complete and accurate descriptions of the method s overall functionality.
table 11shows summaries generatedby different models for an example java method in codesearchnet data.
wefindthatthe queue objectisrecognizedbyallofthesemodels.
codebertandt5 learningfailtocapturetheoperationconducted authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
spt code sequence to sequence pre training for learning source code representations icse may pittsburgh pa usa table qualitative example of spt code and baselines.
void emitloop for appendonlylinkedarraylist object q q queue if q null emitting false return queue null q.foreachwhile this codebert the queue is being destroyed graphcodebert emits the next loop in the queuet5 learning this method is called when the queue spt code emits all of the elements in this queue w o ast emit the linked list w o nl emits all entries in the queue human written loops until all notifications in the queue has been processed onthequeue i.e.
emits emit butgraphcodebertandspt code does.comparedtospt code althoughgraphcodebertalsounderstand the relationship between the operation and the object it fails to choose appropriate words for describing the operation.
the reasonmaybethatthedataflowcanindeedpartially butnotfully capturecodestructureinformation.anotherinterestingobservation that may support this inference is that all models considering structure information generate in the queue correctly i.e.
graphcodebert spt code and spt code w o nl and models using ast all generate accurate readable and smooth summaries.
it also implies that data flow is less effectiveness than ast.
threats to validity construct validity.
like many existing code pre training models spt code uses codesearchnet for pre training.
since codesearchnet is also used in the evaluation of code summarization and code search it is possible that the samples from the test sets for these twotaskshavealready beenusedforpre training.thisisnotfair to methods that have not been pre trained with codesearchnet such as neuralcodesum in codesearchnet code summarization.
we also recognize the impact on the results of not removing duplicates.
however our decision of not removing replicates is based on two considerations ensuring fairness of the comparison the three code pre training baselines were pre trained with all data fromcodesearchnetwithoutduplicateremoval spt codeisnotaffected by duplicate data on one hand downstream tasks that use pre training dataset are csn code summarization and code search while the pre training tasks designed for spt code do not use the docstring i.e.
in testing spt code does not have the advantage of generating more accurate summaries because it has seen a piece of the tested code during pre training or has the advantage of better searchingforcodebasedonnaturallanguage.ontheotherhand other downstream tasks that do not use codesearchnet have little overlap with csn.
forrigorousconsideration weremovealltestsetsincodesearchnetandcodeduplicatedwithtestsetsofotherdownstreamtasks14.
thenre pre trainandfine tunethespt codeonthreedownstream tasks i.e.
jcsd code completion and bfpmediumbug fixing.
it is 14by using tools provided by allamanis we find in classical code summarization 2i nb f p mediumand code completion respectively.found that the results of spt code decrease very little after removing these duplicates15.
moreover it is still not sure whether the change in results is due to the smaller pre training dataset shrunk by about or the removal of duplicates.
internalvalidity.
itiswidelyagreedthathyperparametershavea significantimpactontheperformanceofdeeplearningmodels but hyperparameters of spt code are not tuned experimentally and are set empirically.
therefore other hyperparameter settings may yield better results.
external validity.
another threat posed by using codesearchnet as ourpre trainingdatasetisthatcodesearchnetdataisnotbalancedacrosssixprogramminglanguages whichcanbeseenintable2 so ourmodelmaynotperformthesameondifferentlanguages and we cannot guarantee the validity of spt code for programming languages other than these six.
conclusion we presented spt code a large model for source code based on an encoder decoder architecture.
first we design three code specific pre trainingtaskstopre trainspt code.secondly weproposea newinputrepresentationwhoseisthefirstmethodthattakeinto account both natural language and ast form of code where wealsoproposeaimprovedversionoftheasttraversalmethod x sbt.
both our pre training tasks and input representation allow spt codetobepre trainedonacompletelyunlabeleddataset.sptcodewasthenfine tunedonfivecode relateddownstreamtasks.
results indicate that fine tuning spt code enables it to achieve the state of the art performance on five code related downstream tasks.ablationexperimentsrevealthatthethreepre trainingtasks have different degrees of impact on different downstream tasks and ast and natural language input also helped improve sptcode sperformance.tofacilitatefutureresearch wealsomakeour code and other artifacts publicly available at nougatca spt code.