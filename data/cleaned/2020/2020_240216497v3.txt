sand decoupling sanitization from fuzzing for low overhead ziqiao kong eth zurich and nanyang technological university ziqiao001 e.ntu.edu.sgshaohua li the chinese university of hong kong shaohuali cuhk.edu.hkheqing huang city university of hong kong heqhuang cityu.edu.hkzhendong su eth zurich zhendong.su inf.ethz.ch abstract sanitizers provide robust test oracles for various vulnerabilities.
fuzzing on sanitizer enabled programs has been the best practice to find software bugs.
since sanitizers require heavy program instrumentation to insert run time checks sanitizerenabled programs have much higher overhead compared to normally built programs.
in this paper we present s and a new fuzzing framework that decouples sanitization from the fuzzing loop.
s and performs fuzzing on a normally built program and only invokes the sanitizerenabled program when input is shown to be interesting.
since most of the generated inputs are not interesting i.e.
not bugtriggering s and allows most of the fuzzing time to be spent on the normally built program.
we further introduce execution pattern to practically and effectively identify interesting inputs.
we implement s and on top of afl and evaluate it on real world programs.
our extensive evaluation highlights its effectiveness in hours compared to all the baseline fuzzers sand significantly discovers more bugs while not missing any.
i. i ntroduction fuzzing has been one of the most successful approaches to finding security vulnerabilities .
at a high level fuzzers generate a large number of new inputs and execute the target program on each of them.
fuzzers typically rely on observable test oracles such as crashes to report bugs.
however many security flaws do not always yield crashes and thus are not detectable.
sanitizers are designed to tackle this problem.
when sanitizers are enabled at compile time compilers heavily instrument the target program to insert various checks.
at run time violations of these checks result in program crashes.
fuzzing on such sanitizer enabled programs is thus more effective in discovering software bugs.
to date the most widely used sanitizers include addresssanitizer asan undefinedbehaviorsanitizer ubsan and memorysanitizer msan .
problems.
sanitizers despite their extraordinary bug finding capability have two main drawbacks.
first sanitizers bring significant performance overhead to fuzzing.
as our evaluation in section ii a will show asan ubsan and msan averagely slow down fuzzing speed by a factor of .3x .0x and 45x respectively.
since fuzzing is computationally intensive such high sanitizer overheads inevitably impede both the equal contribution corresponding author this work was done when ziqiao kong was a master s student at eth zurich.1 tifffree read ptr ... 3read buff read ptr 4if !read buff read buff limitmalloc buffsize else if prev readsize buffsize new buff tiffrealloc read buff buffsize if !new buff free read buff read buff limitmalloc buffsize else read buff new buff 17read buff fig.
a simplified use after free bug from libtiff in cve2023 .
line triggers the bug because the freed buffer in line is reallocated neither in line nor in lines and .
performance of fuzzers and the adoption of sanitizers.
many approaches have been proposed to reduce the run time overhead of sanitizers.
for example debloat optimizes asan checks via sound static analysis.
s anrazor removes likely redundant asan and ubsan checks through dynamic profiling.
fuzzan designs dynamic metadata structure to improve the performance of asan and msan.
notwithstanding these optimization efforts the overhead imposed by sanitizers remains considerable.
for instance as our evaluation will show the state of the art effort debloat can only reduce less than run time cost of asan.
moreover all these schemes require significant modifications to the existing sanitizer code base which hinders its compatibility with diverse infrastructures.
second some sanitizers are mutually exclusive.
for instance because asan and msan maintain the same metadata structure they can not be used together.
consequently a fuzzer has to fuzz asan enabled programs and msan enabled programs separately to maximize its bug finding capability.
key insight.
since sanitizers provide the security oracle all current fuzzers execute sanitizer enabled programs on every fuzzer generated input to verify validity.
we now raise this question can we effectively filter bug triggering inputs without executing a sanitizer enabled program?
theoretically it seems paradoxical and infeasible as only by executing an inputarxiv .16497v3 feb 2025can we know if the input is bug triggering.
however our empirical evaluation will substantiate its feasibility.
the key insight is that bugs are strongly connected to execution paths.
for instance figure shows a simplified code snippet from cve which contains a use after free bug in line .
normal and most execution paths are or where the freed buffer read buff in line is correctly reallocated.
however when the execution path is the freed buffer read buff is incorrectly used in line .
this buggy execution has a unique path not seen in other non bug triggering executions.
since triggering this bug requires exercising unique execution paths our intuition is that we can encapsulate inputs with unique execution paths by executing them on normally built programs then only feed these inputs with unique execution paths into sanitizer enabled programs to reduce overall sanitization overhead.
more interestingly our empirical evaluation in section ii c will show that nearly all bugs can be accurately captured by unique execution paths.
in fact previous studies have also implicitly shown that bugs correlate highly to executions.
our approach.
inspired by this observation we propose a new fuzzing framework that decouples sanitization from the fuzzing loop for acceleration.
in the framework the fuzzer performs fuzzing on a binary that is built normally i.e.
without enabling sanitizers then selects inputs that have unique execution paths and runs them on the sanitizerenabled binaries to check if they trigger any bugs.
take the program shown in figure as an example during fuzzing when we first encounter an input that has the execution path we re execute the input on the sanitizer enabled binary.
the result is that this input does not trigger any bug.
thus we will not validate all future inputs that have the same execution path on the sanitizerenabled binary.
when we first encounter an input that has the execution path similarly we re execute the input on the sanitizer enabled binary.
the result is that this input triggers a use after free bug.
as long as only a small fraction of inputs have unique execution paths and the buggy execution reliably has a unique execution path we can significantly reduce the sanitization overhead during fuzzing.
as section iv c will show only .
of all inputs on average have unique execution paths and more than of buggy executions have a unique execution path.
there is a key challenge in our approach how to efficiently obtain execution path during fuzzing?
previous research has demonstrated that obtaining a fine grained execution path is too costly to be practical .
we tackle this problem by introducing execution pattern to approximate execution path .
execution pattern discards the order information in execution paths as a trade off for efficiency.
for instance for the execution path its execution pattern is meaning that code regions 3and17are executed.
although such approximation may cause imprecision in theory ourevaluation will demonstrate that this design is accurate enough to identify unique execution paths.
our idea is generally applicable to the gray box fuzzer family.
since afl is the most popular gray box fuzzer in both academia and industry we realized our idea on top of it and implemented a tool named s and.
we use real world programs widely used by the fuzzing community to evaluate sand.
our evaluation shows that in hours compared to all the baseline fuzzers s and finds more bugs with statistical significance while not missing any bugs.
in summary we make the following contributions we identify that bugs are strongly connected with unique execution paths and further design an approximate yet accurate execution pattern to efficiently obtain execution path information during fuzzing.
we propose a novel fuzzing framework that decouples sanitization from the fuzzing loop by selectively feeding fuzzer generated inputs into sanitizers.
we implement our idea in a tool named s and.
we conduct in depth evaluations to understand its effectiveness in terms of bug finding throughput and coverage.
ii.
o bservation and illustration in this section we first show our observations on the high overhead of sanitizers and the rarity of bug triggering inputs.
then we use two real world bug examples to illustrate the strong connections between bugs and execution paths.
a. high overhead of sanitizers to benchmark sanitizer overhead in fuzzing we use all benchmark programs from our evaluation section.
for each program we compile five versions of it i.e.
native program asan enabled program debloat enabled program ubsan enabled program and msan enabled program.
the native program refers to a normally built program without using any sanitizers.
since debloat achieves the stateof the art optimization for asan we include it to understand the significance of its improvement.
we use afl as the default fuzzer.
for each program we step use afl to fuzz the native program and collect the first one million fuzzer generated inputs.
all these inputs are saved into disk.
we use tmpfs to reduce i o overhead.
step run afl again on the native program to benchmark its running time on the saved one million inputs.
afl is adapted to fetch inputs from the disk instead of generation.
step repeat step on four sanitizer enabled programs to collect their running time on the same set of inputs.
we ran the above experiment times and reported the average fuzzing speed.
all experimental settings are the same as our later evaluation in section iv a. table i presents the average speed i.e.
number of executions per second of each program.
compared to native programs asan ubsan and msan averagely reduce the speed by and respectively.
even for the best asan optimizationtable i execution speed i.e.
number of executions per second of native programs and sanitizer enabled programs.
the column slowdown refers to the ratio of the native speed to the sanitizer speed.
it is calculated by dividing the native speed by the sanitizer speed.
indicates a compilation failure.
indicates the incompatibility with the sanitizer.
programsnative asan debloat ubsan msan speed speed slowdown speed slowdown speed slowdown speed slowdown cflow exiv2 ffmpeg gdk pixbuf pixdata imginfo infotocap jhead jq sqlite3 lame mp3gain mp42aac mujs nm flvmeta objdump pdftotext tcpdump tiffsplit wav2swf average table ii ratio of bugger triggering inputs.
exec.
shows the number of all executions.
trig.
shows the number of bug triggering executions.
programs exec.
trig.
ratio cflow .2m .2k .
exiv2 .2m .
ffmpeg .7m .
gdk pixbuf.
.9m 246k .
imginfo .8m 65k .
infotocap .3m 31k .
jhead .1m 404k .
jq .7m .
sqlite3 .4m 350k .
lame .4m .
programs exec.
trig.
ratio mp3gain .3m 104k .
mp42aac .6m .
mujs .4m 19k .
nm .1m .
flvmeta .8m 531k .
objdump .8m 196k .
pdftotext .8m 1k .
tcpdump .0m 13k .
tiffsplit .2m 26k .
wav2swf .3m 451k .
average .4m 122k .
debloat its improvement over asan is rather insignificant compared to the native programs.
such huge sanitizer overheads inevitably hinder the fuzzing throughput.
b. rareness of bug triggering inputs fuzzers typically generate a large body of inputs for a target program.
it is intuitive that bug triggering inputs are rarely met during fuzzing.
to understand the ratio of bug triggering inputs to all the generated inputs we count the number of all generated inputs as well as bug triggering ones during hours of fuzzing.
the experimental data is from our later evaluation in section iv b. table ii shows the result.
we can find that averagely only of inputs are bug triggering.
for1int wav convert2mono struct wav dest int rate ... for i i src size i channels int j int pos2 int pos for j j fill j dest data dest data src data pos ratio ... fig.
a simplified buffer overflow bug from wav2swf in cve .
line triggers a buffer overflow when the for loops significantly change the buffer offset pos2 j .
some programs it is even rarer.
for instance on pdftotext less than out of 104inputs trigger bugs.
we can conclude that only a tiny fraction of fuzzer generated inputs are bugtriggering.
blindly sanitizing all of them is thus a huge waste.
c. illustrative examples in figure we have illustrated a use after free bug that has a unique execution path.
in this section we present motivating examples of different bug types.
buffer overflow.
a buffer overflow bug is triggered when buffer access exceeds the allocated range of stack or heap memory.
figure shows a real world buffer overflow bug from wav2swf in cve .
the buggy buffer access is located in line .
when the two for loops iterate a1void jbig2stream 2readtextregionseg guint segnum ... ... numsyms for i i nrefsegs i if seg findsegment refsegs if seg gettype jbig2segsymbol numsyms seg getsize else if seg gettype jbig2se codetables append seg ... syms jbig2bitmap gmallocn numsyms ... fig.
a simplified integer overflow bug from xpdf containing pdftotext in cve .
line triggers an integer overflow in numsyms when the ifbranch in line is evaluated to true many times.
significant number of times the offset pos2 j exceeds the buffer range of dest data .
the original cause is from the large values of both src size andfill .
but these unusual data subsequently lead to a unique execution path i.e.
a long chain of executions .in practice we observe that buffer overflow bugs often accompany execution path changes that normal executions do not exercise.
thus using unique execution paths as the indicator for sanitization can help us encapsulate bug triggering inputs.
the many bufferoverflow bugs identified by s and in our evaluation will further confirm this rationale.
integer overflow.
figure shows an integer overflow bug in line where the variable numsyms overflows its valid range when the ifguard in line is frequently evaluated to true.
this bug leads to an unusual execution path.
the overflowed value in numsyms subsequently causes a small allocated buffer in line which leads to buffer overflow and dramatic path changes in the rest of the execution .
iii.
o urapproach this section introduces the design of our new fuzzing framework s and.
section iii a defines execution path and its proxy approximation execution pattern .
section iii b describes the fuzzing framework.
section iii c clarifies technical details.
a. preliminary execution path and its proxy our approach is backed by the intuition that bug triggering inputs have unique execution paths.
we formally define the execution path as follows definition iii.
execution path .given an execution e the execution path ofeis defined as e where eiis the unique id of the code edge executed by e. note that eis ordered meaning that eiis executed before ejifi j .
execution path is a temporal transition sequence of all executed code when executing an input on the target program.
it contains the full information on control flow visits.
for instance suppose a buggy execution .
ithas the execution path as .
execution path is ordersensitive meaning that .
unfortunately obtaining the execution path of execution is too expensive to be practical in fuzzing .
since throughput is a key factor in fuzzing effectiveness we cannot directly use execution path in our design.
in this paper we propose to use execution pattern as an approximate yet accurate proxy for the execution path.
we define execution pattern as follows definition iii.
execution pattern .given an execution e the execution pattern ofeis defined as te e1 e2 em where ei ej i j andeiis the unique id of the code edge reached by e. note that teis orderinsensitive e.g.
e1 e2 e3 e2 e3 e1 .
execution pattern records all executed code edges of an execution.
for example the previous buggy execution has the execution pattern as .
execution pattern is order insensitive e.g.
.
this execution pattern design can effectively approximate the execution path.
below we discuss the soundness realization and alternative design of execution pattern in detail.
soundness of execution pattern.
since execution pattern abstracts over execution path it is theoretically possible that we cannot soundly capture all bug triggering inputs with unique execution patterns.
take figure as an example where inputs taking the loop once or multiple times will have the same execution pattern .
however this is only true within this specific function.
the actual execution pattern considers the whole program and consequently the same local execution pattern does not represent the overall execution pattern.
at a high level such loop iteration differences result from or will result in different data e.g.
src size andpos which affect the execution of other parts of the program and eventually lead to divergent execution patterns.
to further address this soundness concern we provide an extensive evaluation in section iv c to demonstrate that the proposed execution patterns can precisely filter bug triggering inputs.
realization of execution pattern.
an essential benefit of the execution pattern is its ease of acquisition during fuzzing.
fuzzers like afl utilize an efficient data structure bitmap to collect visited code edges of an execution.
figure shows an example of this procedure.
the bitmap is initialized to all zeros for a new execution.
for the execution path the corresponding positions in the bitmap are marked to .
this bitmap is then used to update a global coverage map with a logic or.
for the next execution a similar bitmap is initialized and then marked.
the coverage map is then cumulatively updated to record all code edges visited by all previous executions.
the design of execution pattern allows us to obtain it effortlessly from the bitmap of execution as shown in the middle right in figure .
alternative design of execution pattern.
theoretically we can design different execution patterns to abstract execution paths.
there are two key requirements to execution pattern design it should precisely distinguish bug triggering inputs 00010101010110000101011101010100000000000123456789bitmapcoverage map uni21d2 uni21d2executions uni21d2 execution patternfig.
executions left are recorded by bitmap middle which are used in afl to update the coverage map right .
our execution patterns can be derived from these bitmaps.
target programqueue010111coverage bitmap bugs unique execution patterncgf loopmutationcrashasan enabled programnew bitsmsan enabled programour augmentationcrashcrash... fig.
s and fuzzing loop.
from normal inputs and it is cheap to obtain during fuzzing.
our current design meets all these requirements.
here we discuss two possible alternatives.
the first one is hit count where we include the counts of visited code edges into execution patterns.
in principle this design can capture more bug triggering inputs.
however as our evaluation in section iv c will show more than of the normal inputs also have a unique hit count which means that hit count does not meet the first requirement.
another one is coverage where we simply use the code coverage as the execution pattern.
an input has a unique execution pattern when it increases code coverage.
unfortunately our evaluation in section iv c shows that on average of bug triggering inputs do not increase coverage causing this design to miss bugs during fuzzing.
our current design offers the first practical execution pattern implementation.
exploring a better design will improve the effectiveness of our approach and is orthogonal to our work.
b. sanitization decoupled fuzzing based on the above formalization to execution patterns we introduce our new fuzzing framework design.
we first describe the general workflow of a coverage guided fuzzer cgf .
the gray area in figure outlines the high level sketch of a cgf.
before fuzzing starts the fuzzer compiles the target program with fuzzer instrumentation and or sanitizers.
then it fuzzes the target as follows seed selection.
select one seed from the seed pool according to predefined strategies.
mutation.
mutate the seed to generate new test inputs.
executing on the target program.
execute a test input on the target program.
coverage and execution analysis.
collect coverage feedback from the execution.
if the execution increases coverage save it to the seed pool if the execution results in a crash report the corresponding input as bugtriggering and save it to the disk otherwise discard it.
as one can see cgfs rely on the execution result of a target program to detect bugs.
in order to maximize bug detection capability current cgfs usually compile the target program with sanitizers enabled.
this routine significantly slows down fuzzing speed due to the high overhead of sanitizers.
in this paper we tackle this problem by decoupling sanitization from the conventional fuzzing loop.
the green part in figure highlights our approach.
before fuzzing starts the fuzzer compiles multiple versions of the same program a normally built program without any sanitizer enabled denoted as pfuzz on which the fuzzer performs fuzzing and a set of sanitizer enabled programs e.g.
asan enabled program pasan and msan enabled program pmsan .
the fuzzer follows the same steps as a cgf to fuzz the normally built program .
but after each execution of the target program we introduce a new step conditional sanitization.
extract the execution pattern from the current execution s bitmap.
if the execution pattern has been observed before i.e.
not unique discard it.
otherwise the current input is identified as sanitization required .
the fuzzer then executes this input on each sanitizer enabled program pasan pmsan etc.
and reports any discovered crashes.
in conditional sanitization we only consider the unique i.e.
first seen execution pattern to be sanitization required.
one may be concerned that bug triggering inputs do not always have a first seen execution pattern and thus our design may potentially miss many bugs.
to address this concern we provide extensive experiments in section iv to demonstrate that nearly all bug triggering inputs .
have unique execution patterns and our new fuzzing framework does not miss any bugs.
in practice a single bug can be triggered hundreds or even thousands of times with unique different traces aligning with our observations during evaluation.
covering more than is sufficient to capture the bug at least one time both empirically and evaluation proven.
example.
figure illustrates the process of identifying sanitization required inputs.
starting from the first execution with pattern the fuzzer identifies it as a unique execution pattern and thus sanitization required.
the second execution has the same pattern as before thus sanitization is unnecessary.
similarly the third and fifth executions have unique patterns not seen before and thus require sanitization.
our hypothesis is that all inputs triggering unique bugs also have unique execution patterns.
assuming that the fifth execution is buggy the fuzzer can successfully identify the bug during sanitization.
since executions holding the same execution path are likely to have similar semantics e.g.
exercising the same functionality or triggering the same bug we only need to sanitize the execution with unique execution execution pattern sanitization required sanitization required sanitization required uniqueuniqueuniquefig.
out of eight consecutive executions from top to bottom three are identified as unique and thus require sanitization.
paths to identify the bug.
this newly introduced conditional sanitization does not alter the standard fuzzing logic.
execution patterns are obtained from the already available bitmap collected on the normally built program.
algorithm sketches the implementation pseudo code of our new fuzzing framework.
in each fuzzing loop line the fuzzer first selects a seed sand mutates it to generate a new input s lines .
next it executes the normally built program pfuzz on the input to collect its execution return ret andbitmap line .
then it extracts the execution pattern tefrom bitmap line and determines whether or not this execution pattern has been observed line .
if teis new the fuzzer labels it as sanitization required and executes each of the available sanitizer enabled programs psanon the input s lines .
meanwhile tewill be added to the hash table.
if any execution crashes meaning the input s triggers a bug the fuzzer sets the return status to crash lines .
finally the fuzzer continues the original procedure save the new input as bug triggering if the return status is crash lines or queue it to the seed pool if it increases coverage lines .
our new fuzzing framework decouples sanitization from standard fuzzing logic.
it has the following main advantages orthogonal to cgfs.
we introduce only an additional step to execute sanitizer enabled programs on selected inputs.
conceptually our approach can augment any afl family fuzzers without modifying their main fuzzing logic.
sanitizer inclusive.
some sanitizers like asan and msan are mutually exclusive meaning that they cannot be used together on a program.
current fuzzers can only perform fuzzing on a program with only one of such sanitizers enabled.
in our s and multiple sanitizer enabled programs can be used for sanitization simultaneously.
we will provide additional technical details in section iii c to explain how we support multiple sanitizers.
c. implementation unique execution pattern analysis.
we obtain the execution pattern of an execution from its bitmap.
in our implementation we use the simplify trace function in afl to achieve this goal.
this design allows us to efficiently get execution patterns during fuzzing.
to identify unique execution patterns we calculate checksums of all observed execution patterns and use a hash map to store them.
algorithm showsalgorithm the new fuzzing loop of s and input seed pool s. 1while abort do s selectseed s seed selection s mutate s generate input ret bitmap execute s pfuzz 6te getexecutionpattern bitmap ifisunique te then foreach psan p asan pmsan do retsan sanexecute s psan ifretsan crash then ret crash our augmentation ifret crash then crash?
saves to disk ifcovers new code then new coverage?
adds tos algorithm identify unique execution patterns 1isunique te cksum hash te ifhashtable then hashtable return true return false the pseudocode.
the hash table hashtable is initialized to all zeros at the start of fuzzing.
in our implementation we use xxh32 hashing algorithm because of its fast speed.
note that the hash function is used to identify unique execution patterns.
in theory any method that can separate unique patterns such as bit wise match can be used here as an alternative solution to the hash function.
however the selected method will not affect the effectiveness of s and as long as it has negligible overhead and all unique execution patterns can be precisely captured.
our evaluation in section iv f will show that the selected hash function meets all requirements.
program instrumentation in s and.the fuzz target pfuzz is instrumented by s and to include the necessary instrumentation code for coverage collection.
since all the sanitizerenabled programs are used for sanitization only no such instrumentation is needed.
thus we directly use the llvm compiler to compile pasan pubsan andpmsan .
because asan and ubsan are compatible we combine them as pasan ubsan .
to reduce the burden of invoking these programs we utilize theforkserver to create one forkserver to communicate with all sanitizer enabled programs efficiently during fuzzing.
iv.
e valuation we implemented s and based on afl .05c the latest version at the time of implementation.
afl is the state of the art gray box fuzzer and has been widely used as the baseline fuzzer in many previous study .
.
.
.0cflow x exiv2 xx .
.
.
ffmpeg xx 02flvmeta x 0510gdk pixbuf pixdata x imginfo infotocap x jhead .
.
.0jq x lame x mp3gain mp42aac x mujs nm x objdump x pdftotext x .
.
.0sqlite3 xx tcpdump tiffsplit x wav2swf afl native afl asan ubsan afl debloat ubsan afl msan afl post sandfig.
number of unique detected bugs across repetitions.
indicates a compilation failure or sanitizer incompatibility.
a. experimental setup benchmark.
we use real world programs from the benchmarking test platform u nifuzz for our evaluation.
we use the provided seeds from u nifuzz for all fuzzing campaigns.
to maximally understand s and s capability in different sanitizers we use all three popular sanitizers i.e.
asan ubsan and msan.
we evaluate all programs on asan and ubsan.
due to the compatibility issue of msan we failed to instrument programs with msan.
we thus exclude msan on their evaluation.
for cflow lame and jq all the unifuzz provided seeds will crash ubsan due to unaligned pointers near program entry points we thus use asan instead of asan ubsan for these three programs.
these programs cover a diverse range of input types including image imginfo jhead tiffsplit exiv2 gdk pixbuf pixdata.
audio mp3gain wav2swf lame.
video mp42aac ffmpeg flvmeta.
text infotocap mujs pdftotext cflow jq sqlite3.
binary nm objdump.
network tcpdump.
there are other available benchmarks such as magma and fuzzbench .
we did not use them because many sanitizer reported bugs are not covered by magma s bug set since magma uses manually analyzed bugs magma contains only projects while u nifuzz contains projects and fuzzbench s programs contain too few bugs as its evaluation metric focuses on coverage.
baseline.
since asan and ubsan are compatible with each other we combine them together when building binaries.
all fuzzers and programs are built with llvm the latest stable version at the time of implementation.
we compare the performance of s and against four baseline fuzzers afl native fuzzing normally built programs afl asan ubsan fuzzing pasan ubsan .
afl msan fuzzing pmsan .
afl debloat ubsan to understand if s and can surpass the existing sanitizer optimization schemes we also choose the state of the art asan optimization technique debloat .
because debloat optimizes asan it can also be used together with ubsan.
to maximize its bug detection capability we let afl to fuzz on debloat ubsan enabled program denoted as afl debloat ubsan .
all programs instrumented with debloat are built with llvm because this is the highest llvm version that debloat supports.
since compiling exiv2 ffmpeg infotocap mp42aac sqlite3 nm objdump pdftotext andtiffsplit with debloat results in compilation or instrumentation failures we exclude them for afl debloat ubsan.
afl post another workaround to reduce sanitizer overhead is post processing all the inputs saved in the corpus when fuzzing with afl native.
these inputs increase coverage during fuzzing normally built programs.
this baseline can also be viewed as using coverageincreasing information as the execution pattern.
hardware and setup.
we conduct all experiments on a machine equipped with an amd 3990x cpu and 256g memory running ubuntu .
.
following klee s standard we repeated all experiments times and ran all fuzzing campaigns for hours.
b. bug finding capability finding bugs is the ultimate goal of fuzzing.
in this section we evaluate the bug finding capability of all fuzzers.
in particular we would like to answer the following two questions q1does s and find more bugs compared to other fuzzers?
q2does s and miss any bugs found by other fuzzers?
to answer these questions we collect all crashes found by each fuzzer.
we triage all crashes according to their root causes to quantify the number of unique bugs each fuzzer finds.
ourtable iii mean number of unique bugs across repetitions.
indicates a compilation failure and indicates incompatibility.
the largest mean numbers are highlighted in green .
programsafl sandp val a12 nativeasan ubsandebloat ubsanmsan post cflow .
.
.
.
.
.
.
exiv2 .
.
.
.
.
.
ffmpeg .
.
.
.
.
.
gdk.
.
.
.
.
.
.
.
imginfo .
.
.
.
.
.
.
.
infotocap .
.
.
.
.
.
.
jhead .
.
.
.
.
.
.
.
jq .
.
.
.
.
.
.
sqlite3 .
.
.
.
.
.
lame .
.
.
.
.
.
.
mp3gain .
.
.
.
.
.
.
.
mp42aac .
.
.
.
.
.
.
mujs .
.
.
.
.
.
.
.
nm .
.
.
.
.
.
.
flvmeta .
.
.
.
.
.
.
objdump .
.
.
.
.
.
.
pdftotext .
.
.
.
.
.
.
tcpdump .
.
.
.
.
.
.
.
tiffsplit .
.
.
.
.
.
.
wav2swf .
.
.
.
.
.
.
.
deduplication is done via both the stack frame information from gdb and manual analysis.
new bugs.
before discussing the evaluation result of s and we would like to mention that during the evaluation we found nine new bugs using s and.
all of these bugs have been confirmed by the developers.
three of them were assigned cve numbers.
considering all the programs have been heavily fuzzed in both academia and industry these new bugs reflect the effectiveness of our approach.
number of unique bugs.
we plot the number of unique bugs in every repetition in figure .
table iii aggregates the results and reports the mean number of unique bugs.
on out of programs s and found more bugs than all other fuzzers.
for the remaining programs there is no statistical difference between s and and the second best fuzzer.
since afl post is the overall second best fuzzer we report the mannwhitney u test p val and vargha delaney effect size a12 when comparing s and with it.
the p val column shows that s and is statistically different from afl post p val .
on programs.
there is only one case mp3gain where s and has a lower mean number of bugs .75v.s7.
.
however there is no statistical significance pval .
between them.
intuitively the a12column measures to what extent or the probability that s and is better than afl post.
the result shows that programs have a12 .
conventionally large effect size and programs have a12 .
conventionally medium effect size .
notably there is no program with a12 .
meaning that afl post cannot surpass s and on any programs.
in summary our result answers q1 sand has a significantly stronger bug finding capability than all other fuzzers.
accumulative number of unique bugs.
to understand the overlaps of bugs found by different fuzzers we accumulate all unique bugs found by each fuzzer in each repetition.
table iv shows the unique number of bugs found by each fuzzer.
it shows that s and covers the most number of bugs .
the sand shows that s and does not miss any bugs found by any other fuzzers.
the s and shows that s and always finds more bugs than other fuzzers.
compared to the secondbest fuzzer afl post s and found more bugs which also confirms that simply post processing coverage increasing inputs will miss many bugs.
in summary we can answer q1and q2 sand does not miss any bugs and can find significantly more bugs.
we will discuss the potential false negative problem of s and later.
number of unique bugs reported by sanitizer enabled programs.
of all the unique bugs identified by s and more than are not detectable on normally built programs.
these bugs are reported after invoking sanitizer enabled programs in s and highlighting the necessity of using sanitizers.
bug types.
we now try to understand which types of bugs sand can cover.
our observation as illustrated in section ii c is that all bug types like buffer overflow and integer overflow can result from in execution path changes.
figure reports the types of bugs found by s and.
the result highlights that s and can indeed cover all bug types such as heap buffer overflow bugs integer overflow bugs use of uninitialized memory bugs and use after free bugs .
the small number of use after free bugs is because they are indeed relatively rare in practice .
false negatives.
despite the outstanding performance of sand we have no theoretical guarantee that sand can cover all bugs .
theoretically s and may have false negatives where certain bugs are missed.
this false negative impact can be inferred from the mujs performance in table iii where s and has a slightly lower mean number of bugs .
than afl debloat ubsan .
.
the reason is that one bug in mujs was not triggered in every repetition of s and but in every repetition in the other fuzzer.
the execution pattern for this bug can sometimes be seen in normal executions which leads to a less frequent discovery in s and.
due to the stochastic nature of fuzzing triggering a bug only once is sufficient for s and to detect it in practice.
given the overall superior performance of s and we believe that the moderate false negative issue is acceptable and does not impede its general effectiveness.
c. effectiveness of execution pattern we now break down s and to evaluate the effectiveness of its core design i.e.
execution pattern.
to understand if uniquetable iv number of unique bugs found by each fuzzer.
total row shows the total number of unique bugs.
s and means the number of bugs missed by s and.
s and means the number of bugs additionally covered by s and.
afl sandnativeasan ubsandebloat ubsanmsan post total sand sand heap buffer overflow23 integer overflow29 stack buffer overflow2 unknown segmentfault4 null pointer dereference1 global buffer overflow1 misc.
undefined behaviors14 memcpy overlap1 use of uninitialized value22 use after free1 float point exception1 out of memory1 fig.
distribution of bug types found by s and.
execution patterns can accurately encapsulate bug triggering inputs executions we extensively analyze all executions during fuzzing.
specifically we conduct the following experiments step use afl to fuzz the normally built program.
step for each generated input we first obtain its execution pattern then examine whether or not this execution pattern is unique i.e.
has been observed before.
step for each input no matter whether or not its execution pattern is unique we run it on asan ubsan and msan enabled programs to test if it triggers a bug.
to explore and measure other designs of the execution pattern we provide the following two alternatives alternative hit count .
recall our current execution pattern definition in iii.
where we do not present the hit count of each visited code edge.
in this alternative design we include the hit count of each code edge in the execution pattern.
alternative coverage .
in this design the execution pattern is the coverage increasing information.
whenever an execution increases code coverage we identify it as sanitizer required.
this design is actually equivalent to the strategy used in afl post.
we ran the experiments on our current execution pattern as well as two alternative designs for hours and repeated them twenty times.
with this set of experiments for each execution pattern design we want to answer q3out of all executions in step how many of them are marked as having unique execution patterns?
q4out of all bug triggering inputs executions in step how many of them are also marked as having unique execution patterns in step ?
the above q3 can tell us the ratio of unique execution patterns during fuzzing.
a smaller ratio indicates that sanitization is required less frequently leading to a higher speed.
the following q4can inform us how effective the unique execution pattern is in filtering bug triggering inputs.
table v shows the result.
the all s and column shows that more than half of all programs have less than unique execution patterns in our current design.
the average ratio is only .
.
we can thus conclude that only a small fraction of fuzzer generated inputs have unique execution patterns.
the bug s and column shows that on average .
of bug triggering inputs have unique execution patterns whichmeans that if we only pass inputs with unique execution patterns to sanitizer enabled programs we can successfully sanitize .
buggy inputs.
one abnormal case is lame with a .
ratio.
the reason is that lame is not stable i.e.
executing even the same input multiple times can lead to divergent execution paths.
in fact even vanilla afl discourages fuzzing unstable programs .
note that we do not deduplicate all bug triggering inputs here most of them are in fact duplicates.
considering the same bug can be triggered multiple times during fuzzing .
accuracy can already ensure that no bugs will be missed with a high probability.
the bug hit column shows that when we use hit count as the execution pattern averagely .
of bug triggering inputs have unique execution patterns higher than .
in sand but the average ratio of all inputs all hit column is as high as .
much higher than .
in s and .
this means that .
of inputs will be identified as sanitizerrequired hindering the fuzzing throughput dramatically.
the bug cov column shows that when we use coverage as the execution pattern averagely only of bug triggering inputs have unique execution patterns.
such a low ratio will cause many bug triggering inputs to not be sanitized consequently missing bugs.
our previous bug finding evaluation of afl post in table iv also shows that using coverage will miss out of bugs.
note that since we do not duplicate all bug triggering inputs here of bug triggering inputs does not imply of all bugs.
in summary compared to the two alternative designs our current execution pattern design achieves the best balance between the two ratios.
however this does not mean that our current design is perfect.
ideally the ratio of all inputs should be as low as possible while the ratio of bug triggering inputs should remain as high as possible.
exploring better execution pattern design is an exciting future work.
d. fuzzing throughput we analyzed the end to end fuzzing throughput i.e.
the total number of inputs executed during fuzzing.
figure shows the average throughput normalized to afl native.
since afl post has the same throughput as afl native we use afl native post to represent both.
compared to sanitizers enabled fuzzers.
sand achieves an average of .4x .1x and .0x throughput than afl asan ubsan afl debloat ubsan and afl msan re cflow exiv2ffmpeg gdk pixbuf pixdataimginfoinfotocapjheadjq sqlite3lame mp3gain mp42aacmujsnm flvmetaobjdump pdftotext tcpdumptiffsplitwav2swf average00.
.
.
.
.0relative throughput x x x x x x x x x x x x x x x x xafl native post afl asan ubsan afl debloat ubsan afl msan sandfig.
relative throughput normalized to afl native post.
indicates a compilation failure or an incompatible sanitizer.
average refers to the average throughput of all programs.
table v ratios of inputs that have unique execution patterns.
all refers to the ratio of all generated inputs that have unique execution patterns.
bug refers to the ratio of bugtriggering inputs that have unique execution patterns.
programsall bug hit cov s and hit cov s and cflow .
.
.
.
.
.
exiv2 .
.
.
.
.
.
ffmpeg .
.
.
.
.
.
gdk pixbuf.
.
.
.
.
.
.
imginfo .
.
.
.
.
.
infotocap .
.
.
.
.
.
jhead .
.
.
.
.
.
jq .
.
.
.
.
.
sqlite3 .
.
.
.
.
.
lame .
.
.
.
.
.
mp3gain .
.
.
.
.
.
mp42aac .
.
.
.
.
.
mujs .
.
.
.
.
.
nm .
.
.
.
.
.
flvmeta .
.
.
.
.
.
objdump .
.
.
.
.
.
pdftotext .
.
.
.
.
.
tcpdump .
.
.
.
.
.
tiffsplit .
.
.
.
.
.
wav2swf .
.
.
.
.
.
average .
.
.
.
.
.
spectively.
moreover s and has significantly higher throughputon all programs .
on some programs the speedup rate is even higher.
for instance on nm sand executes .2x and .8x more inputs than afl asan ubsan and afl msan respectively.
it is worth mentioning that sand is equipped with all three sanitizers including the slowest msan.
compared to afl native post.
overall s and achieves of afl native s throughput.
on programs s and achieves more than of afl native s throughput.
the result shows that sand successfully increases the speed of fuzzing on sanitizer enabled programs to a near native level.
e. code coverage we use the afl showmap utility in afl to collect the code coverage.
table vi presents the average code coverage achieved by each fuzzer on each program.table vi code coverage of fuzzers with statistical p val.
indicates a compilation failure.
the highest code coverage compared to afl native is highlighted in green .
programsafl native postafl sandasan ubsandebloat ubsanmsan cflow .
.
.
.
.
.
.
exiv2 .
.
.
.
.
ffmpeg .
.
.
.
.
gdk pixbuf.
.
.
.
.
.
.
.
imginfo .
.
.
.
.
.
.
.
.
infotocap .
.
.
.
.
.
.
jhead .
.
.
.
.
.
.
.
.
jq .
.
.
.
.
.
.
sqlite3 .
.
.
.
.
lame .
.
.
.
.
.
.
mp3gain .
.
.
.
.
.
.
.
.
mp42aac .
.
.
.
.
.
.
mujs .
.
.
.
.
.
.
.
.
nm .
.
.
.
.
.
.
flvmeta .
.
.
.
.
.
.
objdump .
.
.
.
.
.
.
pdftotext .
.
.
.
.
.
.
tcpdump .
.
.
.
.
.
.
.
.
tiffsplit .
.
.
.
.
.
.
wav2swf .
.
.
.
.
.
.
.
.
compared to sanitizers enabled fuzzers.
sand achieves higher coverage on all programs .
intuitively since s and has a much higher throughput than other sanitizer enabled fuzzers sand executes more inputs and thus achieves higher coverage.
compared to afl native post.
on out of programs there is no significant coverage difference between afl native post and s and.
for the remaining programs s and achieves almost the same code coverage as afl native.
as analyzed before s and can achieve throughput of afl native which accounts for the coverage drop in some programs.
since bug finding capability is the golden metric for fuzzing although afl native post can achieve higher code coverage it has a worse bug finding rate and thus is less favorable in practice.f .
hash function we evaluated both the hash overhead and hash collision to demonstrate that our selected hash function is sufficiently effective to support s and.
our first evaluation measured the overall time spent on the hash function during fuzzing.
the result shows that hashing operations in sand have an average of overhead which is negligible in fuzzing .
our second evaluation measured if there was any hash collision i.e.
two different execution patterns have the same hash value.
the result shows that nearly no hash collision was detected during hours of fuzzing .
these evaluations convincingly demonstrate the effectiveness of our selected hash function.
v. d iscussion compatibility to other advanced fuzzers.
sand does not touch the main fuzzing logic of a cgf.
it is orthogonal to many other fuzzer advances.
for example new mutation strategies effective seed scheduling schemes and hybrid fuzzing techniques can all be normally integrated into a cgf fuzzer which s and can build upon.
to understand s and s general applicability we port it to an alternative fuzzer mopt which uses a different mutation scheduling strategy and can be manually turned on in afl .
we include the details in the supplementary.
incompatibility to coverage guided tracing.
our current execution pattern is collected from the coverage bitmap.
some research efforts are trying to reduce coverage collection overhead such as hexcite and untracer .
such approaches break the coverage map and thus cannot be used together with s and.
however we would like to highlight that the coverage collection overhead is much smaller compared to sanitizers.
researchers have shown that the latest coverage collection approach used in afl only brings a median of overhead.
sanitizers like asan and msan can incur overhead.
even if these approaches can entirely eliminate coverage tracing overhead the overall benefit when sanitizers are used is small.
limitations.
despite that s and brings significant improvement to fuzzing it also comes with a few limitations.
the first limitation is the gap between the unique execution pattern ratio and the bug triggering input ratio.
our empirical evaluation in section ii b has shown that many bug triggering input ratios are below .
which is lower than the average unique execution pattern ratio of .
.
this gap indicates that there is still space for improvement.
designing more effective execution abstraction is an interesting future work.
the second limitation is that although our evaluation has confirmed that sand did not miss any bugs we can not provide a theoretical guarantee.
it would be interesting and useful to explore sound execution analysis to eliminate this concern.
vi.
r elated work reducing sanitizer overhead.
asap removes sanitizer checks to meet a required performance budget.
fuzzan dynamically selects an optimal metadata structure for asanand msan.
sanrazor and debloat remove redundant sanitizer checks via either static or dynamic analysis.
sanrazor supports both asan and ubsan while debloat only supports asan.
all of these techniques require significant modifications to sanitizer implementations which inevitably hinders their practical adoption.
s and on the other hand uses sanitizers without any modification which further highlights the orthogonality of s and to these efforts.
bug pattern.
igor observes that all bug triggering inputs have unusual execution behaviors.
for specific bug types uafl prioritizes memory operations of longer sequences to effectively detect user after free bugs.
dowser selectively checks instructions that access arrays in a loop for detecting buffer overflow bugs.
parmesan leverages the knowledge from sanitizer instrumentations to discover certain types of bugs faster.
pge finds that bug triggering executions correlate with execution prefixes.
at a high level the findings or insights behind these approaches share similar motivations to our execution pattern i.e.
bug triggering inputs tend to have unique execution features.
improving fuzzing performance.
since the success of afl the fuzzing community has seen a broad range of new fuzzer developments.
in particular coverage guided greybox fuzzers such as afl aflfast and aflgo are the most widely adopted and studied fuzzing techniques.
researchers have also put great efforts into optimizing various aspects of fuzzing such as seed scheduling mutation strategies and path explorations .
in theory all these improvements are not related to sanitizer enabled programs and therefore are orthogonal to us.
reducing coverage collection overhead.
some researchers point out that coverage collection in fuzzing brings extra overhead.
untracer and hexcite remove instrumentation code in visited code edges to reduce coverage collection overhead.
zeror shifts between diversely instrumented binaries to achieve low coverage collection overhead on most executions.
odin dynamically recompiles a binary when the instrumentation requirement changes.
because all these approaches need to modify the coverage bitmap our approach is not compatible with them.
as discussed in section v coverage collection cost is rather small compared to sanitizer overhead and thus our approach is more beneficial.
vii.
c onclusion we have presented a new fuzzing framework s and to decouple sanitization from the fuzzing loop.
s and performs fuzzing on the normally built program and only executes sanitizer enabled programs when an input is identified as sanitization required.
s and utilizes the fact that most of the fuzzer generated inputs do not need sanitization which enables it to spend most of the fuzzing time on normally built programs.
we have evaluated s and on real world programs to demonstrate its superiority.
our work represents an exciting research direction toward the overhead free adoption of sanitizers in fuzzing.