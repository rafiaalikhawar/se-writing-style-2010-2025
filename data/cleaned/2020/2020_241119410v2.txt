wdd weighted delta debugging xintong zhou zhenyang xu mengxiao zhang yongqiang tian and chengnian sun school of computer science university of waterloo waterloo canada emails x27zhou uwaterloo.ca zhenyang.xu uwaterloo.ca m492zhan uwaterloo.ca cnsun uwaterloo.ca department of computer science and engineering the hong kong university of science and technology hong kong china email yqtian ust.hk abstract delta debugging is a widely used family of algorithms e.g.
ddmin and probdd to automatically minimize bugtriggering test inputs thus to facilitate debugging.
it takes a list of elements with each element representing a fragment of the test input systematically partitions the list at different granularities identifies and deletes bug irrelevant partitions.
prior delta debugging algorithms assume there are no differences among the elements in the list and thus treat them uniformly during partitioning.
however in practice this assumption usually does not hold because the size referred to as weight of the fragment represented by each element can vary significantly.
for example a single element representing of the test input is much more likely to be bug relevant than elements representing only .
this assumption inevitably impairs the efficiency or even effectiveness of these delta debugging algorithms.
this paper proposes weighted delta debugging wdd a novel concept to help prior delta debugging algorithms overcome the limitation mentioned above.
the key insight of wdd is to assign each element in the list a weight according to its size and distinguish different elements based on their weights during partitioning.
we designed two new minimization algorithms wddmin and wprobdd by applying wdd to ddmin and probdd respectively.
we extensively evaluated wddmin and wprobdd in two representative applications hdd and perses on benchmarks across two languages.
on average with wddmin hdd and perses took .
and .
less time to generate .
and .
smaller results than with ddmin respectively.
with wprobdd hdd and perses used .
and .
less time to generate .
and .
smaller results than with probdd respectively.
the results strongly demonstrate the value of wdd.
we firmly believe that wdd opens up a new dimension to improve test input minimization techniques.
index terms test input minimization delta debugging program reduction i. i ntroduction a bug triggering test input which causes a program to fail often contains many bug irrelevant elements.
these elements usually complicate the use of the test input to debug the program.
test input minimization is a technique that automatically minimizes the size of the input by removing the irrelevant elements while keeping the failure inducing parts.
it helps developers to focus on the essential parts of the input that cause the failure.
many minimization techniques have been proposed and widely used in various scenarios especially in facilitating software testing and debugging .
delta debugging is a widely used family of algorithms to automatically minimize bug triggering test inputs.
typically delta debugging algorithms take a test input as a list of elements with each element representing a fragment of the test input e.g.
a token a line or a tree node .
then it partitions the list into sets of elements referred to as partitions at different granularities systematically identifies and deletes partitions that are bugirrelevant.
state of the art algorithms in this family include minimizing delta debugging ddmin and probabilistic delta debugging probdd .
the first delta debugging algorithm ddmin systematically minimizes the list of elements in a binary search style.
the generality effectiveness and efficiency of ddmin make it a fundamental minimization algorithm in many subsequently proposed minimization tools .
the other algorithm probdd is a recently proposed variant of ddmin.
it improves the efficiency of ddmin by leveraging a probabilistic model to guide the minimization process.
in practice delta debugging algorithms are often applied to the tree representations of the inputs rather than plain lists of tokens or lines to achieve better minimization performance a.k.a.
tree based minimization.
for example hierarchical delta debugging hdd proposed by misherghi and su represents the input as a tree structure e.g.
a parse tree and then uses ddmin to minimize each level of the tree from coarse to fine.
another example is perses a minimization technique that further improves hdd by leveraging context free grammar to ensure the syntactic validity during minimization.
perses applies ddmin on the child node list of quantified nodes i.e.
a type of nodes whose children are independent to each other in terms of syntax validity in the parse tree.
both hdd and perses show significant superiority in handling structured inputs compared to directly applying delta debugging to the flat list representations of the inputs.
limitations.
one significant limitation of prior delta debugging algorithms is that they overlooked the effect of element size in minimization and thus the efficiency or even effectiveness of minimization is impaired.
specifically ddmin performs a binary search style deletion and iteratively divides the list into smaller partitions evenly by length i.e.
the number of elements .
however due to the varying sizes of elements ddmin fails to achieve the true evenness1and generates partitions with significantly different sizes.
for 1the true evenness indicates that the size of each partition approximately equals to each other.
the size of a partition is normally measured by the number of tokens it contains that is to say the number of tokens in each partition is approximately equal.arxiv .19410v2 dec 2024example when hdd invokes ddmin to minimize the bugtriggering input of llvm the largest and smallest partitions produced by a partitioning operation can contain and tokens respectively.
however ddmin treats these uneven partitions equally neglecting an important statistical observation i.e.
larger partitions are more likely to contain the failure inducing elements and thus less likely to be removed .
as a result ddmin spends significant efforts in removing large but unlikely to be removed elements during the minimization process which restricts its performance in large and complex bug triggering inputs.
as for probdd while it successfully refines the partitioning strategy of ddmin with its probabilistic model it still lacks awareness of the varying sizes of elements during partitioning thus leading to suboptimal performance.
more details of this limitation and its affect is illustrated in iii.
weighted delta debugging.
in this paper we propose weighted delta debugging wdd a novel concept to improve prior delta debugging algorithms by overcoming the aforementioned limitation.
the key insight of wdd is to take the sizes of elements into consideration and assign each element a weight based on its size.
by so wdd can perform a more rational weight based partitioning strategy thereby enhancing minimization performance.
we apply wdd to two representative delta debugging algorithms ddmin and probdd and propose two new algorithms wddmin andwprobdd respectively.
at a high level wddmin improves ddmin by performing a weighted binary search style minimization while wprobdd enhances probdd by incorporating the weights of elements as a new factor into the probabilistic model which guides the partitioning.
we extensively evaluate wddmin andwprobdd on benchmarks across two languages i.e.
c and xml by substituting them for ddmin and probdd respectively in two application scenarios hdd and perses .
the results demonstrate thatwddmin andwprobdd significantly outperform ddmin and probdd in efficiency and effectiveness respectively.
on average after substituting wddmin for ddmin hdd and perses use .
and .
less time to produce .
and .
smaller results respectively.
moreover with wprobdd hdd and perses obtain .
and .
smaller results with .
and .
less time than using probdd respectively.
contribution.
this paper makes the following contributions.
we present weighted delta debugging wdd a novel concept that helps prior delta debugging algorithms overcome the limitation of being unaware of the different sizes among the elements in the input list.
we realize wdd in two representative delta debugging algorithms ddmin and probdd and propose two new algorithms w ddmin and w probdd respectively.
we comprehensively evaluate wddmin and wprobdd on benchmarks in different application scenarios.
the results demonstrate the superiority of wddmin and wprobdd over ddmin and probdd respectively thus highlighting the significance of wdd in improving test input minimization.
for replication we make the artifacts of this paper publicly available .
we also release the source code of wddmin and wprobdd in the perses repository for further research and applications.
ii.
b ackground test input minimization facilitates the software debugging process by automatically minimizing the size of the bugtriggering test input.
this technique is highly demanded as it helps developers to focus on the essential parts of the test input and saves the time and effort required to identify the root cause of the bug.
for example both gcc and llvm have explicitly announced that the bug triggering program should be minimized before being reported.
test input minimization also assists many other software engineering tasks such as program analysis and slicing .
to facilitate presentation we introduce the notations below edenotes the set of all possible elements in test inputs ldenotes a test input which is a list of elements with elements drawn from e ldenotes the universe of possible test inputs namely l l. b t f where tfor true and ffor false.
l bis a property test function returning tif the given input preserves a certain property fotherwise.
w e nis a weight function computing the weight a natural number such as and of an element.
with these symbols the problem of test input minimization can be formalized as follows.
definition ii.
test input minimization .given a test input l lfor a program and a property exhibited by l e.g.
triggering a bug or generating an unexpected output when the program executes with l the objective of test input minimization is to produce a test input lmin lthat has a minimal number of elements and still exhibits i.e.
lmin t .
many techniques have been proposed to automate test input minimization.
delta debugging algorithms e.g.
ddmin and probdd are among the most general and widely used techniques upon which many advanced tools such as hdd and perses are built.
since our approaches i.e.
wddmin and wprobdd are the improved versions of ddmin and probdd respectively we first explain the workflows of ddmin and probdd with an example.
fig.
a displays a program that triggers a real world compiler bug gcc .
it triggers gcc to crash when compiling the program.
we aim to minimize this program to the smallest size while still triggering the compiler bug thus facilitating debugging.
taking the program as plain text and performing delta debugging algorithms on it directly is inefficient as the program is highly structured.
in practice delta debugging is usually wrapped in tree based techniques e.g.
hdd and perses being applied on the tree level.
in the tree representation of the program e.g.
the parse tree there are eight nodes at the same level right under the root node highlighted in orange in fig.
a each corresponding to a distinct part of the program such as a typedef statement or a1typedef long long llong ........... 1w 2test2char64 char p ............ 2w 3test1char8 char c .............. 3w 4test1short32 short c ........... 4w 5test2short32 short p .......... 5w 6typedef llong vllong1 attribute vector size sizeof llong .. 6w 7vllong1 test2llong1 llong p llong c test1char8 vllong1 v c return v .................................. 7w 8int main ...................... 8w a a program that triggers gcc to crash.
b the search space of ddmin.
c the search space of w ddmin.
fig.
a motivating example.
in each subfigure the weights of the nodes or the partitions are highlighted in orange.
function definition.
to minimize the program both hdd and perses invoke ddmin or probdd to minimize the tree nodes starting from this level i.e.
.
a. workflow of ddmin given land ddmin works in the following steps.
step split lintonpartitions evenly by length.
for each partition p test if palone preserves i.e.
p t .
if yes remove all other partitions from land resume step with n otherwise go to step .
step test if the complement of each partition ppreserves i.e.
l p t .
if yes remove pfrom land resume step with n n otherwise go to step .
step terminate if each partition pcontains only one element otherwise double nand resume step .
starting from n and following the above steps ddmin performs property tests in total to minimize the program in fig.
a .
the specific property tests ddmin performs during the minimization process are shown in fig.
a .
note that ddmin may produce duplicate test inputs which are not listed in the figure since in practice they can be recognized and skipped by caching the tests that have been performed .
b. workflow of probdd different from ddmin which follows a predefined pattern to perform the deletion operations probdd employs a probabilistic model to guide the entire minimization process.
the key insight of probdd is to estimate the probability of each element appearing in the minimized result with a probabilistic model.
given land and a map probs that stores the estimated probabilities of each element in lappearing in the minimized result the initial probability of each element is set to a same value e.g.
.
probdd works in the following steps.
step sort the elements in lin ascending order of their probabilities.
select a prefix prefrom the sorted list that maximizes the expectation of the number of elements that can be removed i.e.
pre q e pre probs .
step test if the complement of prepreserves i.e.
l pre t .
if yes remove prefrom l set the probabilities of the elements in preto and go to step if not go to step .
step increase the probabilities of elements in preaccording to the probabilistic model then go to step .step terminate if the probabilities of all the elements in l reach otherwise go to step .
following the above steps the minimization process of the example program in fig.
a is shown in fig.
c .
each property test is represented with two rows where the first row displays the elements selected complement of pre for testing and the second row shows the probability of each element after the test.
the selected elements and the updated probabilities are highlighted with blue and yellow respectively.
starting from the same initial probability set to .
in this case probdd performs property tests to finish the minimization process.
c. minimality the ultimate goal of test input minimization is to obtain the globally minimal result where no smaller input can exhibit .
however previous work has proven that obtaining the global minimality is np complete .
in practice the goal is usually relaxed to local minima.
first presented by dd minimality has been widely adopted by a series of works as the criterion of minimality evaluation.
a minimized input is considered minimal if no single element can be further removed without losing the property .
hdd extends the principle of minimality to tree structures introducing tree minimality which promising that in the tree representation of the input no single tree node can be further removed without violating the property.
to achieve tree minimality tree based techniques e.g.
hdd and perses typically operate in a fixpoint mode .
in this mode the minimization process is repeatedly applied to the minimized result until no more tree nodes can be removed from the result.
iii.
m otivation as fig.
a shows the code snippets represented by different nodes vary in size.
for example while node 1represents atypedef statement containing tokens node 7defines the function test2llong1 with tokens.
this discrepancy in size of nodes can affect the efficiency and effectiveness of minimization.
however both ddmin and probdd fail to capture this information and treat all nodes uniformly thus leaving room for improvement.
this is where out concept of wddinputs for property tests 112345678f212345678f312345678f412345678f512345678f612345678f712345678f812345678f912345678f1012345678f1112345678f1212345678f1312345678f1412345678f1512345678f1612345678f1712345678f1812345678f1912345678f2012345678t211345678f221345678t23135678f24135678f25135678t2613678f2713678f2813678f2913678f3013678f a ddmin inputs for property tests 112345678f212345678f312345678f412345678f512345678f612345678f712345678f812345678t9123678f10123678f11123678f12123678f13123678f14123678f15123678f16123678f17123678f18123678f19123678f20123678f21123678f22123678t2313678f2413678f2513678f2613678f b w ddmin inputs for property tests 112345678f0.
.
.
.
.
.
.
.3212345678f0.
.
.
.
.
.
.
.3312345678f0.
.
.
.
.
.
.
.46412345678f0.
.
.
.
.
.
.
.46512345678f0.
.
.
.
.
.
.
.46612345678f0.
.
.
.
.
.
.
.46712345678f0.
.
.
.
.
.
.
.65812345678f0.
.
.
.
.
.
.
.65912345678t0.
.
.
.
.
.
.65101345678t0.
.
.
.
.
.6511135678f1.
.
.
.
.
.6512135678t1.
.
.
.
.651313678f1.
.
.
.
.651413678f1.
.
.
.
.001513678f1.
.
.
.
.
c probdd inputs for property tests 112345678f0.
.
.
.
.
.
.
.2212345678f0.
.
.
.
.
.
.
.2312345678f0.
.
.
.
.
.
.
.2412345678f0.
.
.
.
.
.
.
.2512345678f0.
.
.
.
.
.
.
.33612345678t0.
.
.
.
.
.337134678f0.
.
.
.
.
.718134678f1.
.
.
.
.
.719134678f1.
.
.
.
.
.7110134678t1.
.
.
.
.711113678f1.
.
.
.
.
d w probdd fig.
the detailed minimization process of ddmin wddmin probdd and wprobdd .
the elements selected for the property test in each iteration are highlighted in blue with the leftmost column indicating the index of each property test.
in fig.
c and fig.
d the probabilities updated after each test are highlighted in yellow.
the last column of each figure shows the result of the property test .
in this case all the four algorithms minimize the input list to the same result which is .
comes into play.
the key insight of wdd is to assign each element a weight that matches its size and perform weightbased partitioning.
we first define the weight of elements in delta debugging based on which we present two new delta debugging algorithms wddmin andwprobdd by applying wdd to ddmin and probdd respectively.
definition iii.
weight .the weight of an element in the input list of delta debugging is defined as the size of the fragment represented by the element.
the weight of a partition is the sum of the weights of all elements in the partition.
the size is typically measured by the number of tokens.
a. improving ddmin fig.
b visualizes the search space of ddmin in a tree illustrating that ddmin splits the list evenly by length to conduct a binary search style deletion.
however it fails to achieve the true evenness due to the effect of different weights of nodes.
as highlighted in orange in fig.
b the weights of partitions on each level vary significantly which can impair the efficiency of ddmin.
that is because statistically speaking a larger partition is more likely to contain the failure inducing elements and thus less likely to be removed.
however ddmin fails to capture this information and handles all nodes equally leading to its efficiency being hampered by spending a large amount of attempts on deleting nodes that are unlikely to be successfully removed.
for instance the largest node node in the previous example which is the core element to triggerthe compiler bug is attempted to be removed from the list with partitions for times during the minimization.
different from ddmin wddmin considers the weights of elements and performs a weight based partitioning to make the actual size of each partition as close as possible.
the search space of wddmin based on this strategy is shown in fig.
c .
following this search space wddmin finish the minimization of the example program with only property tests and attempts to remove node 7only times.
the detailed minimization process is shown in fig.
b .
this improvement is much more significant for larger and more complex inputs as demonstrated in vi b2.
b. improving probdd as described in ii b probdd strives to maximize the expectation of the number of elements that can be removed during partitioning.
however the number of elements does not necessarily correspond to the number of tokens that can be deleted.
for example given two elements with the same probability of being removed the one with more tokens i.e.
larger weight should be chosen to remove first since deleting it contributes more to global minimization process.
the performance of probdd is suboptimal since it fails to consider the weight of elements when constructing the probabilistic model.
to fill this gap wprobdd leverages the weight information of elements to refine the probabilistic model of probdd and uses this model to guide partitioning.
as shown in fig.
d boosted by the weighted model wprobddminimizes the example program with only property tests.
it is worth clarifying that although in this example probdd and wprobdd produce the same minimized result our evaluation in vi demonstrates the superior effectiveness of wprobdd over probdd in practice by producing smaller minimized results.
iv.
w eighted minimizing delta debugging this section describes the application of wdd to improve the efficiency of ddmin.
algorithm details wddmin with our extensions beyond ddmin highlighted with grey blocks.
compared to ddmin wddmin has a different partitioning strategy weightedpartition on line and an additional deletion pass ensureoneminimal on line to ensure minimality.
started with the whole input las the only partition line wddmin performs systematic deletion operations on the partitions and their complements and iteratively splits the partitions into smaller ones.
if a partition ptnalone preserves the property i.e.
ptn on line all the other partitions are removed and the algorithm restarts with this single remaining partition line .
if the complement of a partition exhibits the property i.e.
complement on line wddmin removes the partition and restarts with the remaining partitions line .
if no partition or complement exhibits wddmin calls weightedpartition line to split the partitions into smaller ones based on the weights of the elements in these partitions and then start a new iteration.
this process terminates when the partition list partitions is empty line .
then wddmin performs an additional deletion pass by calling ensureoneminimal line to make sure the produced result is minimal.
a. weighted partitioning strategy the main extension of wddmin is the partitioning strategy as shown in function weightedpartition line .
unlike ddmin which partitions the input list levenly by the number of elements wddmin aims to split levenly by the weight of elements striving to make the weight of each partition as close as possible .
line .
notably if a partition from the current iteration contains only one element the partition will be excluded from the partition list in the next iteration line because the partition cannot be further divided.
revisiting the example in fig.
a by applying the weightbased partitioning strategy the search space is reorganized as shown in fig.
c .
while the tree is not balanced in terms of the number of elements it achieves balance for the weight of each partition.
quantitatively wddmin strives to minimize the standard deviation of the weights of partitions during partitioning.
for example in the second iteration corresponding to the third level of the tree in fig.
b and fig.
c the standard deviation of the partition weights of ddmin i.e.
is .
whereas that of wddmin i.e.
is only .
.
b. minimality of w ddmin wddmin guarantees minimality with an additional deletion pass as shown in function ensureoneminimal line .
because of the weight based partitioning strategy largeralgorithm weighted minimizing delta debugging input l l the input list of elements.
input w e n the weights of each element.
input l b the property to be preserved.
output the minimized list that preserves the property.
1lmin l 2partitions 3lmin wddrec partitions lmin w 4return ensureoneminimal lmin 5function wddrec partitions l min w while partitions do foreach ptn partitions do if ptn then lmin ptn partitions weightedpartition w return wddrec partitions l min w foreach ptn partitions do complement lmin ptn if complement then lmin complement partitions partitions return wddrec partitions l min w partitions weightedpartition partitions w return lmin 20function weightedpartition partitions w result foreach ptn partitions do if ptn then continue skip this partition halfsum .
p e ptnw e p1 p2 split ptninto two partitions with weight sum of each close to halfsum result result add p1 p2to result return result 28function ensureoneminimal lmin loopstart foreach element lmindo complement lmin if complement then lmin complement goto loopstart return lmin elements are isolated earlier in the deletion process.
for example in fig.
c node 6is isolated as a separate partition in the third iteration and it cannot be removed in the current iteration.
however in practice the deletion of some nodes may benefit the deletion of other nodes .
to ensure 1minimality wddmin attempts to remove each remaining element individually in the end by calling function ensureoneminimal line .
the loop line iteratively checks whether each remaining element can be removed without losing the property.
if so the element is removed and the loop restarts.
this process continues until no element can be further removed so that minimality is guaranteed.
c. time complexity of w ddmin wddmin does not shrink or enlarge the search space of ddmin.
instead wddmin follows the similar deletion process as ddmin with a more rational partitioning strategy.
therefore by design wddmin has the same worst case time complexity as ddmin i.e.
o n2 where nis the number of elements in the input list.
average time complexity.
we argue that wddmin can achieve higher overall efficiency than ddmin in practice.
the key insight ofwddmin is that the probability of an element being removed varies with its weight and there is a negative correlation between them.
intuitively an element with a larger weight i.e.
representing a larger fragment of a test input is less likely to be removed than a smaller one as it is more likely to contain the failure inducing elements.
the statistical validation of this observation is provided in vi a .
with this insight we expect that wddmin can achieve better efficiency than ddmin.
we perform a simulation below to demonstrate this.
d. synthetic analysis for average time complexity the inherent complexity of delta debugging problem prevents us from proving wddmin is better than ddmin in all cases which is also not necessarily true in practice.
therefore we design this simulation to compare the efficiency of wddmin and ddmin.
analysis setup first we randomly synthesize a set of lists and predetermine their minimization results.
next we perform wddmin and ddmin on the lists and record the numbers of property tests required by each algorithm on each list respectively.
the minimization results are predetermined based on the probability of each element being removed and the probabilities are calculated based on the assumption below.
assumption iv .
randomness .for a random input each token has the same probability of being removed.
given this assumption and the probability of a token being removed p0 the probability of an element with wtokens being removed peequals to pw .
that is because an element can be removed only if all its tokens can be removed.
with the input lists of elements synthesized randomly this assumption helps quantitatively distinguish the probabilities of elements with different weights being removed so that we can predetermine the minimization result.
this assumption is not necessary for the correctness of w ddmin in practice.
with the above assumption we perform the simulation as follows.
to synthesize a random input list we first generate a length nof the list where nis a random integer between and i.e.
n and the total number of tokens represented by the elements in the list which is a random integer between nand10n.
the number of tokens for each element is distributed randomly for instance a list of length with tokens could be .
to predetermine the minimization result we first generate a random value p0 which represents the probability of each token being removed.
then we calculate the probability of each element being removed pebased on assumption iv .
.
after that we generate a random value p for each element and compare it with peto determine whether the element can be removed.
the element can be removed if p p e otherwise it cannot be removed.
based on the established result the property is preserved if all the non removable elements are included in the list.
we execute wddmin and ddmin to minimizethe synthesized list and record their numbers of property tests during the minimization process respectively.
the effect of randomness is eliminated by repeating the single process for a large number of times.
specifically we perform ddmin and wddmin on randomly synthesized lists.
number of elements0.
.
.
.
.
.
.
.00ratioratio of number of property tests of w ddmin to ddmin ratio .
mean value of ratio .
fig.
the simulation results of wddmin and ddmin on the synthetic data.
analysis result the detailed results are shown in fig.
.
on average wddmin uses fewer property tests than ddmin to finish the minimization.
the results emulatively demonstrate the superior efficiency of wddmin compared to ddmin in the ideal case where the probabilities of elements being removed are negatively correlated with their weights.
we verify this correlation and evaluate the practical efficiency of wddmin on real world benchmarks in vi b. v. w eighted probabilistic delta debugging to demonstrate the generality of wdd we applied the concept of wdd to improve probdd a representative variant of ddmin and thus proposed a new minimization algorithm wprobdd .
as described in ii b with the model that tracks the expected probability of each element remaining in the result the partitioning principle of probdd is to prioritize the deletion of elements with lowest probability and maximize the expected number of elements that can be successfully removed.
however the ultimate goal of the minimization is to delete the most tokens possible instead of the most elements.
due to the different sizes of elements there is a gap between the principle of probdd and the ultimate goal of the minimization which makes probdd suboptimal.
to bridge this gap wprobdd improves probdd by incorporating the weight of elements as a new factor into the probabilistic model.
algorithm shows the workflow of wprobdd and the key extensions beyond probdd are highlighted with grey blocks.
when deciding the partition to remove in each test implemented in function getpartitiontoremove the fundamental principle of wprobdd is to prioritize the deletion of elements that are likely to remove larger weight and maximize the expected value of weight that can be removed.
to realize the principle wprobdd first sorts the elements in the list in descending order by the expectation of the value of weight that can be removed by attempting to delete the element.
this value equals to the product of the probability of the element can be removed and the value of its weight line .algorithm weighted probabilistic delta debugging input l l the input list of elements.
input w e n the weights of each element.
input l b the property to be preserved.
input p0 the initial probability for each element.
output the minimized list that preserves .
1lmin l 2probs n p0 n l the probability function that records and returns the probability of each element in l 3while notshouldterminate probs do ptn getpartitiontoremove lmin probs w complement lmin ptn if complement then lmin complement else probs updateprobs ptn probs 8return lmin 9function getpartitiontoremove lmin probs w lsorted sort the elements in lminby the value of w element probs element in descending order result ptn gainmax foreach element lsorted do ptn ptn weight p ni ptnw ni probofdeletion q nj ptn probs nj gain weight probofdeletion ifgain gainmaxthen gainmax gain result ptn return result 21function shouldterminate probs implementation skipped.
same as probdd in .
22function updateprobs ptn probs implementation skipped.
same as probdd in .
after that wprobdd determines the partition to remove in each test with the sorted list.
technically starting from the first element in the sorted list wprobdd can include any number of elements in the partition to remove in the next test.
while including more elements increases value of weight that can be removed it also decreases the probability of the test passing.
to balance the trade off wprobdd chooses the number of elements for removal that maximizes the expectation of the value of weight that can be removed successfully.
to this end wprobdd redefines the gain function in probdd with the weights of elements as gain m pm i 1wi qm j pj where m is the number of elements to be removed wiis the weight of the i th selected element and pjis the probability of being remained of the j th element.
as shown in function getpartitiontoremove line wprobdd selects a certain prefix of the sorted list lsorted that maximizes the gain function as the partition and attempts to remove this partition in the next test.
the complexity of this process is o n where nis the length of the list.
the rest steps of wprobdd are similar to probdd including performing property tests and updating the probabilities of elements according to prior test results.
we exclude the explanation of these steps here instead and refer the readers to the original paper of probdd for details.a.
minimality of w probdd wprobdd promises the same minimality as probdd which is conditional minimality.
the result of probdd is minimal under the assumption that the deletability of each element is independent.
however this assumption typically does not hold in practice since the deletion of some elements may affect the deletability of other elements.
for example even if a statement that defines a variable is bug irrelevant it can only be removed after all the statements that use the variable are removed.
despite sharing the same minimality wprobdd is expected to generate smaller results than probdd since wprobdd always strives to maximize the weight i.e.
the number of tokens that can be removed in the next test.
we evaluate the practical effectiveness of w probdd in vi b. b. time complexity of w probdd wprobdd shares the same worst case time complexity as probdd which is o n where nis the length of the input list.
in practice the deletion strategy of wprobdd i.e.
maximizing the expected weight can be removed not only enhances effectiveness but also speeds up the minimization process.
that is because successfully removing a partition containing a large number of tokens can usually make the execution of subsequent tests faster.
therefore we expect that wprobdd can outperform probdd in terms of time efficiency.
this expectation can hardly be verified by a simulation so we directly evaluate the efficiency of wprobdd on real benchmarks in vi b. vi.
e valuation in this section we verify the significance of wdd by evaluating the effectiveness and efficiency of wddmin and wprobdd .
we explore to what extent wddmin andwprobdd outperform ddmin and probdd in different application scenarios respectively.
we select hdd and perses for evaluation as they are two state of the art test input minimization tools that rely on delta debugging.
for each of the two techniques we implement wddmin andwprobdd versions to replace their original versions with ddmin and probdd respectively and compare their performance with the original versions.
for ease of presentation we refer to hdd with ddmin wddmin probdd and wprobdd as hdd d hdd w hdd p and hdd wp respectively.
similarly the four versions of perses are referred to as perses d perses w perses p and perses wp respectively.
all the minimization techniques for evaluation are executed in the fixpoint mode as described in ii c. for fair comparison all experiments were conducted on an ubuntu .
server with an intel xeon cpu .60ghz and gb ram using a single process single threaded.
we aim to answer the following research questions.
what is the correlation between element weight and the probability of being removed in practice?
how does the performance of w ddmin compare to ddmin?
how does the performance of wprobdd compare to probdd?
benchmarks.
we conducted experiments with benchmarks.
each benchmark triggers a real world bug in a certain languageprocessor and is considerably large and complex aligning with real world application scenarios of test input minimization.
specifically we utilized the following benchmarks.
c we collected c programs from previous studies .
these programs trigger real bugs in llvm and gcc and are large complex with tokens on average.
xml to increase the diversity of the benchmark suite we included xml files with each triggering a bug in basex a widely used xml database and xquery processor.
these benchmarks are also large and complex containing tokens on average.
metrics.
we used the following metrics to evaluate different algorithms following .
s the number of tokens in the minimized result.
a lower value means a more effective minimization by removing more property irrelevant elements.
t s the processing time in seconds.
shorter time means higher efficiency.
speed the number of tokens deleted per second.
using processing time to gauge efficiency is not comprehensive for cases where one approach generates a smaller result but also takes longer time.
we measure the number of tokens deleted per second to balance the trade off between effectiveness and time consumption.
wilcoxon signed rank test to measure the statistical significance of the improvements our our approaches.
a small p value typically .
from this test suggests a statistically significant difference between the paired data.
a. rq1 element weight v.s.deletion probability correlation the first question we are curious about is the correlation between the weights of elements and their probabilities of being removed.
since the fundamental observation behind wddmin is that larger elements are less likely to be deleted than smaller ones we would like to verify if our assumption i.e.
the probability of elements being deleted is negatively correlated with their weights holds during the execution of ddmin in practice.
specifically for an input list land its minimized result lmin the probability of elements with weight wbeing deleted pdel w is defined as the ratio of the number of elements with weight wthat are deleted to the total number of elements with weight w i.e.
pdel w w l w lmin w l where w l denotes the number of elements with weight win list l. to evaluate the correlation we calculate the spearman s rank correlation coefficient between the probabilities of elements being deleted and their weights for each execution of ddmin.
being widely used in practice spearman s rank correlation coefficient is a non parametric measure of the strength and direction of association between two ranked variables.
the value of ranges from to 1indicates a perfect positive correlation 1indicates a perfect negative correlation and implies no correlation.
to answer this research question we use hdd dand perses d to minimize the test inputs in our benchmarks and record the weights of elements before and after each execution of ddmin.
cases where no elements are removed are excluded since is undefined in these scenarios.
we then calculate for each execution of ddmin.
since ddmin is normally performed multiple times when minimizing a test input the of each benchmark is calculated as the average of the values from all executions of ddmin for that benchmark.
hdd d perses d .
.
.
.
.
.
.
.
.09 .
.
a c programs hdd d perses d .
.
.
.
.
.
.
.
.
.
.06 .
.
b xml inputs fig.
the spearman correlation coefficient between the weights of elements and their probabilities being deleted in ddmin.
each data point represents the mean of the values of all ddmin executions on a benchmark.
as shown in fig.
overall in each scenario of hdd d and perses d and for both c programs and xml inputs our assumption is preserved.
as shown in fig.
in the four scenarios only cases of c programs and cases of xml inputs in perses dhave values greater than while all other cases have values less or equal to .
specifically when minimizing the c programs with hdd dand perses d the mean values are .
and .
respectively.
for the xml inputs the mean values are .
and .
respectively.
although the values vary across different applications and benchmarks all are less than indicating a negative correlation between the probability of elements being deleted and their weights in ddmin executions thus validating our assumption.
rq1 the probability of elements being deleted is negatively correlated with their weights in ddmin executions in both hdd and perses to varying degrees.
this validation provides a solid foundation for the design of w ddmin.
b. w ddmin v.s.ddmin for this question we compare the performance of hdd w and perses wwith hdd dand perses d respectively.
the detailed results are shown in table i. effectiveness overall wddmin is more effective than ddmin in both hdd and perses.
on average hdd wgenerates .
and .
smaller results than hdd don c and xml benchmarks respectively with a p value of .
5overall.
in perses the results of perses ware .
and .
smaller than those of perses don c and xml benchmarks respectively with a p value of .53overall.
notably while the above results demonstrate the superior effectiveness of wddmin over ddmin the improvement of wddmin over ddmin in perses is not as significant as that in hdd.
given the different design of hdd and perses this result is expected.
unlike hdd that fully relies on ddmin to perform tree node deletion perses customizes different deletion strategies fortable i results of all algorithms in hdd and perses on all benchmarks.
better results in each pair are highlighted in bold.
hdd d hdd w perses d perses w hdd p hdd wp perses p perses wp benchmarkt s s t s s t s s t s s t s s t s s t s s t s s clang clang clang clang clang clang clang clang clang clang clang clang clang clang clang clang gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc mean xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml xml mean different types of nodes.
in perses ddmin is only used to minimize the list of nodes under a quantified node .
that is to say compared to hdd the deletion operations performed by ddmin or wddmin constitute a smaller proportion of the total operations in perses.
therefore improvements to the effectiveness of ddmin have a relatively moderate impact onthe overall effectiveness of perses.
moreover the nodes under a quantified node in perses are syntactically independent from each other making the minimization less challenging.
thus ddmin can generate results comparable to w ddmin.
efficiency we first evaluate efficiency with processing time for which wddmin outperforms ddmin in both hdd andperses.
on average hdd wtakes .
and .
less time than hdd dto finish minimizing the c programs and xml inputs respectively with a p value of .
11overall.
similarly perses wreduces the processing time of perses dby .
and .
on c and xml benchmarks respectively with a p value of .
6overall.
furthermore considering the number of tokens deleted per second referred as tokens s as an additional metric while hdd dand perses ddeletes .
and .
tokens s respectively hdd wand perses wdeletes .
and .
tokens s which are .
and .
more than those of hdd dand perses d respectively.
these results strongly indicate that w ddmin is more efficient than ddmin.
similar with the improvement of effectiveness while wddmin consistently achieves higher efficiency than ddmin in both hdd and perses the improvement is more significant in hdd than in perses.
the reason is the same as that explained in vi b for effectiveness.
moreover the high efficiency of wddmin is based on the assumption that the probability of elements being deleted is negatively correlated with their weights which is validated in vi a. in fact the degree of this correlation can affect the efficiency of wddmin.
as shown in fig.
the values of perses dare generally larger than those of hdd d indicating a weaker negative correlation.
thus the improvement of wddmin over ddmin in perses is not as significant as that in hdd.
rq2 wddmin outperforms ddmin in both effectiveness and efficiency in hdd by generating .
smaller results in .
less time on average.
in perses wddmin exceeds ddmin in efficiecny by taking .
less time on average to generate the comparable results.
c. rq3 w probdd v.s.probdd for this research question we compare the performance of hdd wpand perses wpusing hdd pand perses pas baselines respectively.
the minimization process of probdd contains nondeterminism since it may randomly select elements when their probabilities are the same.
to mitigate the impact of such nondeterminism we repeat each experiment for times and report the average results.
w probdd largely eliminates the randomness of probdd by considering the weights of elements.
the detailed results are shown in table i. effectiveness.
overall wprobdd is more effective than probdd by generating smaller results.
on average hdd wpgenerates .
and .
smaller results than hdd pfor the c programs and xml inputs respectively with a p value of .
.
besides perses wpgenerates .
smaller and .
larger results than perses pfor the c programs and xml inputs respectively with a p value of .
.
there is no significant difference between the results of perses wpand perses p. in fact perses wpgenerates same results as perses p out of benchmarks of which are from the xml inputs because of the same reason explained in vi b .
especially for the xml inputs only .
property tests performed by perses wpare from wprobdd indicating that the effectiveness ofperses wpis largely determined by the inner deletion strategies of perses instead of w probdd .
efficiency.
wprobdd achieves higher efficiency than probdd in both hdd and perses.
we first evaluate the efficiency of wprobdd with processing time.
on average hdd wpshortens the processing time of hdd pby .
and .
for the c programs and xml inputs respectively with a p value of .
6overall.
similarly perses wpreduces the processing time of perses pby .
and .
on each benchmark suite respectively with a p value of .
5overall.
moreover in terms of the number of tokens deleted per second as an additional metric while hdd pand perses pdeletes .
and .
tokens s hdd wpand perses wpdeletes .
and .
tokens s which are .
and .
more than those of hdd pand perses p respectively.
rq3 wprobdd outperforms probdd in both effectiveness and efficiency by making hdd and perses produce .
and .
smaller results in .
and .
less time on average respectively.
vii.
d iscussion a. alternative weight assignment in our implementation of wdd in ddmin and probdd in this paper we utilize the number of tokens of each element as the weight.
although this assignment strategy is not accurate it achieves high efficiency and feasibility as it is static lightweight and generalizable .
other weight assignment could also be considered such as a dynamic weight assignment strategy based on runtime information including factors like memory usage io operations or execution time.
however such a dynamic weight assignment strategy may introduce additional overhead potentially hindering the performance of minimization.
furthermore runtime profiling techniques are typically language specific which may limit the generalizability of wdd.
overcoming these challenges and exploring the potential of dynamic wdd for language specific minimization techniques presents an interesting direction for future work.
b. limitations the primary limitation of wdd is its applicability mainly to tree structured inputs where it is most effective when the weights i.e.
token counts of elements vary significantly.
when the test inputs cannot be represented in a tree structure e.g.
random strings while the concept of weight still exists token count may not serve as an appropriate weight representation.
additionally if the tree representation of the test input is highly balanced wdd may offer only marginal improvement over traditional delta debugging methods.
nevertheless given the widespread use of tree based minimization techniques and the typically unbalanced nature of trees in real world inputs wdd remains essential for enhancing the performance of test input minimization in practical scenarios.c.
threats to validity threats to internal validity the primary internal threat arises from the implementation of the evaluated techniques including wddmin wprobdd and their respective baselines as well as hdd and perses.
to mitigate this threat we rigorously reproduced the the baseline techniques based on their descriptions in the original papers and wrote multiple test cases to ensure the algorithms functioned as expected.
additionally all authors of this paper participated in a thorough code review of the implementation.
prior to evaluating the full set of benchmarks we randomly selected several cases ran our algorithms on them and manually verified the detailed results to confirm the accuracy of our implementations.
we have also made our implementations publicly available for replication and facilitating further research.
threats to external validity a key threat to external validity is the generalizability of wdd across different input formats or languages.
although wdd is designed to apply to all tree structured inputs variations in the tree characteristics of different inputs may impact its performance.
to mitigate this threat we evaluated wdd on two types of benchmarks c and xml.
the c benchmarks represent traditional programming languages while the xml files represent structured inputs that are highly hierarchical but not programs.
our evaluation results demonstrate the superior performance of wdd across these diverse formats.
to further address this threat our future work includes expanding the evaluation of wdd to a broader range of benchmarks.
viii.
r elated work we introduce two lines of related work.
test input minimization.
delta debugging is the first systematic study that enlightens the research of test input minimization.
it introduced an minimizing algorithm named ddmin to minimize failure inducing test inputs which has been described in ii a. while ddmin is effective its efficiency is not satisfactory as it follows a predefined pattern to partition and delete elements overlooking the information of existing tests.
to fix this issue wang et al.
proposed probdd.
as explained in ii b probdd leverages a probabilistic model to guide the minimization process.
however both ddmin and probdd overlook the different sizes of elements in the list leading to suboptimal performance.
contrastively our approaches wddmin andwprobdd successfully distinguish different elements with their weights and make more rationale partitioning decisions with considering weights which significantly improves the performance of prior delta debugging algorithms.
in practice rather than being used directly to minimize test inputs delta debugging algorithms are often integrated into tree based minimization techniques for better performance.
two representative techniques are hdd and perses which are chosen for our evaluation.
hdd and perses apply delta debugging algorithms to minimize the listof nodes in the tree.
thus their performance can be further improved by equipping our new delta debugging algorithms.
program reduction.
program reduction is a special case of test input minimization where the input is a program.
since normally a program can be parsed into a syntax tree tree based test input minimization techniques e.g.
hdd and perses can be directly applied to program reduction.
moreover xu et al.
proposed vulcan which pushes the limit of minimality by performing predefined program transformations.
they further developed t rec a fine grained language agnostic program reduction technique guided by lexical syntax.
t rec is demonstrated to not only achieve smaller minimization results than vulcan but also aids in deduplicating bug triggering test inputs.
additionally zhang et al.
proposed lpr the first language agnostic program reducer boosted by large language models.
furthermore some program reduction techniques are specifically designed for certain languages.
for example creduce is specifically designed for reducing c c programs.
it incorporates various semantic specific transformations to effectively minimize c c programs.
j reduce ddsmt and js delta are specifically designed for reducing java bytecode smt libv2 inputs and javascript programs respectively.
herfert et al.
propose the generalized tree reduction gtr technique which minimizes programs with a series of language specific transformations generated by learning from a corpus of example data .
while these approaches are designed for specific languages some of them such as ddsmt apply delta debugging under the hood.
to this end introducing our novel concept of wdd to these tools to further improve their performance is a promising direction for future work.
ix.
c onclusion this paper introduces weighted delta debugging wdd a novel concept that incorporates the weight of elements into delta debugging.
the key insight of wdd is to assign each element in the input list a weight and distinguish different elements based on their weights during partitioning.
we realize the concept of wdd in two representative delta debugging algorithms ddmin and probdd and propose wddmin andwprobdd respectively.
the extensive evaluation on benchmarks demonstrates the superior performance of wddmin and wprobdd in both effectiveness and efficiency highlighting the significance of wdd in optimizing delta debugging algorithms.
we firmly believe that wdd opens up a new dimension to improve test input minimization techniques.