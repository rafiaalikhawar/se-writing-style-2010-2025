scenario driven and context aware automated accessibility testing for android apps yuxin zhang sen chen xiaofei xie zibo liu and lingling fan college of intelligence and computing tianjin university tianjin china college of cyber science nankai university tianjin china singapore management university singapore singapore abstract mobile accessibility is increasingly important nowadays as it enables people with disabilities to use mobile applications to perform daily tasks.
ensuring mobile accessibility not only benefits those with disabilities but also enhances the user experience for all users making applications more intuitive and user friendly.
although numerous tools are available for testing and detecting accessibility issues in android applications a large number of false negatives and false positives persist due to limitations in the existing approaches i.e.
low coverage of ui scenarios and lack of consideration of runtime context.
to address these problems in this paper we propose a scenario driven exploration method for improving the coverage of ui scenarios thereby detecting accessibility issues within the application and ultimately reducing false negatives.
furthermore to reduce false positives caused by not considering the runtime context we propose a context aware detection method that provides a more fine grained detection capability.
our experimental results reveal that a11yscan can detect .7x more issues surpassing current state of the art approaches like xbot vs. thereby reducing the false negative rate by .
.
additionally it outperforms established ui exploration techniques such as scenedroid vs. ui scenarios while achieving comparable activity coverage to recent leading gui testing tools like gptdroid on the available dataset vs. .
meanwhile with the context aware detection method a11yscan effectively reduces the false positive rate by validated with a .
accuracy rate through a user study.
index terms mobile accessibility accessibility testing android app ui exploration context aware analysis i. i ntroduction the increasing popularity of mobile applications apps has made them an essential element in people s daily lives and mobile apps have become an important medium for accessing information and services.
however many of these apps are inaccessible to people with disabilities making it difficult for them to use and enjoy the same benefits as nondisabled users .
the world health organization who estimates that approximately of the world s population has a disability which means that app developers must ensure that all users including disabled users can access their apps .
despite increasing awareness of accessibility a large number of developers still struggle to create accessible apps due to a lack of knowledge or understanding of accessibility guidelines .
thus effective tools are urgently needed to help developers detect accessibility issues in their apps.
sen chen is the corresponding author email tigersenchen .com .in recent years researchers have developed many tools to test and detect accessibility issues in apps and conduct empirical research on these issues .
these tools are mainly divided into two categories static and dynamic analysis tools.
static analysis tools such as android lint detect accessibility issues by analyzing static information such as code and resource files without running the apps however they are ineffective and time consuming in detecting mobile accessibility issues .
dynamic analysis tools on the other hand detect accessibility issues by running the app with different ui exploration methods such as manually dynamic methods script based dynamic methods and automated dynamic methods .
since the issue coverage of accessibility detection depends on the number of explored ui pages their common goal is to reach more ui scenarios and check whether the attributes of the ui components in them violate accessibility standards.
app ui scenarios include android activity itself activity dependent and activity sensitive ui scenarios.
each activity includes various ui scenarios beyond the initially rendered ui page during runtime such as fragment ui drawer menu and dialog which refer to activity dependent ui scenarios.
activitysensitive ui scenarios refer to the new ui states of the current activity when the states of other ui scenarios change due to user interactions.
as shown in fig.
clicking the menu button in the upper right corner of the first mainactivity can reach an activity dependent ui scenario i.e.
the second updated mainactivity while user interaction flows adding a new category in categoriesactivity can lead to an activity sensitive ui scenario i.e.
the last updated mainactivity .
failure to explore activity dependent or activity sensitive ui scenarios of apps would result in the inability to detect accessibility issues in them such as issue1 issue2 and issue3 in this example.
however existing accessibility testing tools have some significant drawbacks low coverage of ui scenarios causing false negatives.
the existing manual and scriptbased tools can only cover limited ui scenarios and the current automated dynamic accessibility testing tools primarily focus on detecting the accessibility of activities while overlooking the accessibility issues that may arise from activitydependent and activity sensitive ui scenarios.
in addition to accessibility detection tools app gui testing and ui exploration tools also constantly strive tofig.
new accessibility issues detected in activity dependent and activity sensitive ui scenarios.
extract more ui scenarios for different purposes which would enhance the ability of accessibility testing.
although these tools cover a wide range of ui components and paths to improve activity ui coverage their main goal is to improve coverage or identify general bugs often ignoring the different attributes of the same ui component in different ui scenarios which are crucial for accessibility testing as they can determine whether the ui component meets accessibility standards.
ignoring this difference can lead to missed detection of a component s accessibility issues in specific scenarios.
lack of consideration of runtime context causing false positives.
the existing accessibility testing tools directly rely on accessibility rules to check whether the attributes of the detected component violate the accessibility standards and lack consideration of the actual runtime context of the detected component which can lead to incorrect judgments about the attributes of the ui component in different usage scenarios resulting in false positives.
for example these tools may mistakenly identify textview components that have been set as invisible by developer as low contrast issues.
whether a ui component has accessibility issues requires not only checking whether its current attributes violate accessibility rules but also considering contextual information during app runtime such as the hierarchy of all ui components in the current ui page and the state of this ui component itself i.e.
in visible un filled and un checked .
to address these above problems we propose a11yscan an automated approach for accurately detecting accessibility issues in android apps.
specifically to solve the first problem we design a scenario driven ui exploration method that dynamically explores as many ui scenarios as possible of the apps to expand the scope of accessibility detection using two key strategies initial state exploration which uses the command line activity launches combined with a depthfirst exploration strategy featuring multi state combination to interact with the interactive ui components of each activity to increase activity coverage while simultaneously acquiring additional activity dependent ui scenarios and the various attributes of ui components in them and enhanced state re exploration which analyzes and re explores critical ui scenarios with two or more out degrees in the ui scenario transitions generated from initial state exploration to identify activity sensitive ui scenarios and the attribute changes of the same ui component in new detected ui scenarios.
throughhigh ui scenario coverage the scenario driven ui exploration method targets accessibility testing by capturing the different attributes of ui components across different ui scenarios ensuring comprehensive accessibility assessment of the app.
to solve the second problem we propose a context aware detection method that designs newly defined checking rules referred to as oracle by combining the pre defined rules of testing frameworks like accessibility testing frameworks atf and the contextual information gained from constructing resource trees of the ui components.
these new rules in this oracle are not about redefining accessibility standards but optimizing the detection implementations to make results more accurate and aligned with standards.
additionally the extraction of contextual information is independent of specific issue types.
while these designs are systematic the checking rules still need to be fine tuned according to different issue types ensuring accurate accessibility assessment of the app.
we conducted a series of experiments on real world android apps including both closed source and open source apps to evaluate the effectiveness and efficiency of a11yscan.
the experimental results indicate that a11yscan performs well in issue detection with .7x more issues reduced the false negative rate by .
compared to the state of theart accessibility testing tool i.e.
xbot .
furthermore a11yscan outperforms in extracting ui scenarios compared with the state of the art ui exploration tools like scenedroid vs. ui scenarios .
additionally to ensure the comprehensiveness of effectiveness evaluation we further take into account the recent state of the art gui testing tool i.e.
gptdroid .
we compared it using the metric of activity coverage and app dataset used in their paper rather than the number of explored ui scenarios and detected issues due to the unavailability of their tool.
a11yscan achieved a comparable result on activity coverage vs. which demonstrates that the ability of the scenario driven exploration method is comparable even compared with the cutting edge techniqueenhanced approach with the large language models.
additionally false positives identified through contextaware issue detection represent of detected issues.
subsequent user study verified an accuracy of .
highlighting the effectiveness of a11yscan in handling false positives.
the average time for testing one app is .
minutes.
in summary our main contributions are as follows.
we are the first work to attempt to address the significant problem of false alarms when testing app accessibility.
we proposed a scenario driven ui exploration method aiming to explore both activity dependent and activitysensitive ui scenarios to find more accessibility issues.
we introduced a context aware detection method aimed at reducing false positives by leveraging contextual information from ui components during the app s runtime.
we have released a11yscan as well as the used dataset on git.ii.
b ackground a. ui scenarios of android apps ui scenario is crucial for testing android apps.
the ui scenario covers a series of state changes during the interaction between users and apps reflecting the dynamics and interactivity of the software.
the following will explore three key types of ui scenarios activity activity dependent ui scenarios and activity sensitive ui scenarios.
activity is a fundamental component in android apps representing a single screen with a ui scenario.
it serves not only as a container for user interactions but also as the primary means for the app to present information.
activity dependent ui scenarios involve state changes triggered by user interaction with ui components such as edittext and button within a single activity.
these changes usually occur after users perform operations such as clicking buttons selecting drop down menu items or entering data.
the state changes in such scenarios are predictable as they directly depend on user interactions.
as shown in fig.
when the user clicks on the menu button in the first mainactivity circled in red a drawer is triggered which generates a new activitydependent ui scenario i.e.
the second updated mainactivity .
activity sensitive ui scenarios focus on ui state changes caused by user interactions which may indirectly affect the current or related activities.
the changes in these scenarios are indirect and may not immediately manifest as they depend on the operations accumulated by users during the use of the app.
for example a user s actions in one activity may affect the data display of another activity even if there is no direct interaction path between the two.
in the subsequent section of fig.
when the user creates a new category in categoriesactvity the mainactivity is updated and expanded which refers to an activity sensitive ui scenario i.e.
the last updated mainactivity .
b. accessibility standards mobile accessibility is an important aspect of ensuring equal access to technology for people with disabilities.
to improve the accessibility of mobile devices many accessibility standards and developer guidelines have been proposed including those from w3c web content accessibility guidelines wcag .
and .
google accessibility guidelines for android iso developed by the international organization for standardization iso the us revised section standards and the bbc mobile accessibility standards and guidelines from the uk among others.
these standards provide recommendations to better support people with different types of disabilities including those with mobility hearing and visual impairments.
besides these standards and guidelines companies have developed their own accessibility guidelines based on industry standards.
for example the android accessibility developer guidelines apple accessibility developer guidelines and ibm accessibility checklist provide developers with specific recommendations and best practices for creating accessible mobile apps.table i list of accessibility testing tools for android apps.
category strategy tools static analysisautomated static methodlint pmd checkstyle dynamic analysismanual dynamic methodaccessibility scanner ibm abilitylab mobile accessibility checker talkback switch access a11ypuppetry script based dynamic methodespresso robotium robolectric latte uiautomator appium automated dynamic methodmate groundhog puma forapp xbot there are also some relevant laws and regulations.
for example the us americans with disabilities act ada and the communications and video accessibility act require federal agencies to use technology that meets standards to ensure that people with disabilities can use these technologies.
similarly the european union has also issued the convention on the rights of persons with disabilities and requires member states to take measures to ensure that people with disabilities can access digital technologies such as mobile devices and the internet.
c. accessibility testing tools table i lists the commonly used accessibility testing tools for android apps which can be classified into two categories based on their used techniques static and dynamic analysis.
static analysis static analysis can detect accessibility issues by analyzing the source code or compiled code of the app.
however this type of tool often fails to capture accessibility issues that only appear at runtime and cannot consider the actual runtime environment of the app.
tools like lint pdm and checkstyle can be used to detect errors and potential issues in the code such as missing content descriptions and accessibility tags in xml layout files which can improve code quality.
dynamic analysis dynamic analysis can detect accessibility issues at runtime to test the accessibility of the app.
this approach overcomes the limitations of static analysis and can more accurately simulate how real users use the app.
dynamic analysis tools can be classified into three types based on their exploration strategy manual dynamic method scriptbased dynamic method and automated dynamic method .
a manual dynamic method this method requires developers to manually simulate user interactions and check the accessibility of the app.
therefore it requires developers to have a certain level of knowledge and skills in accessibility and is relatively time consuming and labor intensive making it less suitable for large scale app testing.
by interacting with the components on the ui page manually developers can visually determine whether the detected issues are true or not.
it can also effectively explore more complex components in some scenarios but reproducing issues may becolor related issuesoptimal color selectionattribute to repair localization repairaccessibility issue detectionreference db construction accessibility issue reportsdataset without issues panoramic ui exploration atf based issue detectionfine grained resource analysis resource files screenshots detected issues accessibility issue reports supervisor.apk logic driven uiexploration atf based issue detectionfine grained ui resource analysis resource files screenshots detected issues accessibility issue reports accessibility issue reportsinitial state exploration atf based accessibility checksscenario driven ui exploration supervisorsupervisor.apk context aware issue detection enhanced state re exploration runtime context ware analysisresource files screenshots runtime layout informationfig.
overview of a11yscan.
more difficult.
currently commonly used tools such as the accessibility scanner .
it can automatically scan while manually exploring the ui pages and provide detailed reports on accessibility issues and repair suggestions.
in addition the android system provides some assistive services to help the disabled use android devices such as talkback for visual impairments and switch access for motor impairments.
these tools can assist testers in simulating the behavior of different users when using the app.
b script based dynamic method this method uses scripts to simulate user interactions and check the accessibility of the app.
it is more automated and efficient than the manual dynamic and can be used for large scale app testing.
these scripts can be customized for different scenarios and can be easily executed repeatedly.
however writing suitable scripts is a challenge.
commonly used tools include espresso and robotium.
in addition others such as latte appium and uiautomator can integrate assistive services for testing and focus on meeting the needs of different users.
c automated dynamic method this method can fully automatically simulate user interactions and check the accessibility of the app.
it is the highest degree of automation and can quickly and accurately detect accessibility issues in large and complex apps making it suitable for large scale app testing.
existing tools attempt to explore ui pages using different methods to discover potential issues on more ui pages.
for example both mate and groundhog use the monkey to simulate user interactions with the app whose random strategy can lead to incomplete exploration.
xbot uses instrumentation technique and static dataflow analysis based on activity intent parameters to explore ui pages achieving a higher activity coverage but ignoring activity dependent and activity sensitive ui scenarios triggered by interactive components as well as user interactions.
iii.
a pproach to overcome the limitations of existing state of the art approaches in terms of false negatives and false positives we propose a fully automated tool for testing and detecting accessibility issues in android apps named a11yscan which takes an apk file as input and outputs detection reports for accessibility issues and other relevant parsing results.
as shown in fig.
a11yscan mainly consists of two phases scenario driven ui exploration which thoroughly explores more ui scenarios of the app to increase scenario coverage and detection range to discover more potential accessibility issues context aware issue detection which checks the accessibility of the detected ui components by newly defined checking rules combining the pre defined rules of current ac cessibility testing frameworks and the contextual information of detected ui components by constructing resource trees of the relevant ui components.
a. scenario driven ui exploration to minimize the occurrence of false negatives our goal is to capture as many ui scenarios as possible in the apps.
however this is a challenging task due to various limitations of existing tools.
firstly the complex code structure and diverse scenario designs of apps make it difficult to simulate a wide range of user interactions and consider the impact of user interactions between different ui components thus limiting the ability to explore activities and more ui scenarios with multiple ui states.
secondly existing tools lack consideration of the ui state changes generated after complex user interactions which may accompany attribute changes of ui components and thus fall short of identifying and exploring activity sensitive ui scenarios.
to achieve higher coverage of ui scenarios we have adopted two key strategies initial state exploration android provides an interface to directly launch activities from the console using the android debug bridge commands.
this involves using intent objects to specify the target activity and pass necessary data where intent serves as one of the mechanisms for inter component communication icc in android supporting message exchange and operation execution.
to generate intent objects a11yscan parses the androidmanifest.xml file and java code to obtain the basic attributes including action category data and type and extra parameters composed of basic structures such as string char and boolean of intent objects.
and fill in extra parameters based on data types.
a11yscan then uses these intent objects to directly launch the activity from the console for future testing tasks like fax and storydistiller .
this establishes the foundation for exploring various ui scenarios by adopting different strategies.
in android apps each ui page is made up of various ui components ranging from simple buttons and text fields to more complex image carousels and navigation menus .
user interactions with these ui components trigger new ui scenarios and interactions.
for example clicking a button component may open a new ui page or display additional information.
inputting text or numbers into an edittext component may guide the user to different pages or sections of the app.
therefore after launching a new ui page a11yscan retrieves the interactable ui components on the current page and employs a top down depth first exploration strategy to interact with each component individually such as filling edittextcomponents with the randomly generated inputs according to the attribute inputtype ofedittext and clicking on clickablemainact chatact settingact homeact profileact homeact s4 chatact s1 homeact s5 homeact s6 homeact s7 homeact s1 homeact s2 homeact s3 searchact homeact s9 homeact s8 texttextfig.
ui scenario transitions generated by initial state exploration.
the node represents ui scenarios and refers to their out degrees.
components.
additionally a11yscan evaluates all possible combinations of component states to capture the diversity and dynamics of real user interactions.
for example with anedittext filled blank a checkbox checked unchecked and a switch button on off it explores all initial ui states through multi state combination.
then a11yscan continues depth first exploration until all possible ui scenarios are explored or no new scenarios are found.
additionally scrollable pages will be fully presented through scrolling.
overall a11yscan utilizes command line activity launches followed by a top down depth first exploration strategy to trigger a wider variety of component states and ui states effectively reaching more activities and exploring activitydependent ui scenarios and capturing various attributes of ui components in them.
enhanced state re exploration after the initial ui exploration some ui scenarios might change due to user interactions.
these changes often involve activity sensitive ui scenarios where the ui state dynamically changes with the operations accumulated by users.
to accurately capture these changes we developed an advanced strategy to identify these activity sensitive ui scenarios and detect whether there are new ui scenarios triggered by previous exploration behaviors.
a11yscan identifies critical ui scenarios that are susceptible to state changes induced by interactions in other scenarios by analyzing ui scenario transitions following the initial state exploration.
fig.
represents the ui scenario transitions generated after the initial state exploration of a real app showing the transition path between activities and other ui scenarios.
each node represents a ui scenario including activity and activitydependent scenarios and on them refers to their outdegrees reflecting their significance in the app logic and their sensitivity to state changes.
for example mainact can reach event flows of ui scenario indicating that it carries more functions while profileact can only reach one with simpler logic.
if a ui scenario can reach multiple paths we consider it a critical ui scenario which is sensitive to changes in the ui state such as mainact homeact homeact s4 and homeacts5in fig.
.
subsequently a11yscan employs a bottomup depth first exploration approach to re explore the marked critical ui scenarios to identify activity sensitive ui scenarios as well as the attribute changes of the same ui component in new detected ui scenarios.
this bottom up strategy distinct from the top down used during the initial state exploration simulates potential random user interactions.
this explorationalgorithm scenario driven ui exploration input actall all activities with intent objects in the app.
output s all ui scenarios explored within the app.
1foreach act intent actalldo s s startact act intent s s initialexploration act 4s s enhancedreexploration s 5return s 6function initialexploration act comps int getinteractablecomps act foreach comp comps intdo s s interactwithcomp comp return s 11function enhancedreexploration s skey identifykeyscenarios s foreach sk skeydo ifdetectstatechange sk then s s reexplorescenario sk return s aims to verify if our initial actions have triggered new ui states or behaviors revealing more complex and dynamic ui scenarios.
it is worth noting that a11yscan dumps the layout structure of the ui page runtime and extracts the resource id class and package attributes of each ui component.
it uses the md5 hash algorithm to generate unique identifiers for each ui component thereby identifying and distinguishing unique scenarios and avoiding the exploration of duplicate scenarios.
algorithm delineates the entire process of scenario driven ui exploration which employs two exploration strategies.
the input is all activities with associated intent objects in the app and the output consists of ui scenarios explored using exploration.
specifically each activity actis directly started using intent objects intent lines followed by an initial state exploration through the method initialexploration line .
during initial state exploration lines all runtime interactable components comps intare first retrieved line and a top down interaction with these components is conducted to explore all reachable activity dependent ui scenarios lines .
after the initial exploration an enhanced state re exploration of the obtained ui scenarios is performed by the method enhancedreexploration line to capture activity sensitive ui scenarios.
in the re exploration lines the ui scenario transitions generated after the initial exploration are analyzed to identify critical ui scenarios skey line .
each critical ui scenario skis then checked for state changes resulting from interactions during the initial exploration lines if changes are detected indicating an activity sensitive ui scenario a bottom up reexploration of that scenario is conducted line .
finally upon completion of all explorations the collection of ui scenarios s is returned.b.
context aware issue detection rule based checks are pre defined methods based on accessibility standards making it difficult to consider the diversity of the runtime environment and the states of the detected components.
existing tools use uiautomator to capture ui layouts and screenshots to obtain ui component attributes for accessibility detection which can be affected by incorrect screenshots or missing context leading to existing rule based misjudgments.
to make the detection issues more comprehensive and reliable we newly defined the checking rules by combining the pre defined rules of the official accessibility testing framework atf developed and maintained by google and the runtime contextual information of detected ui components.
atf based accessibility checks to help app developers test the accessibility of their apps google provides an accessibility testing framework atf commonly used in accessibility testing tools which includes a set of apis for testing the accessibility of apps and websites and checking compliance with accessibility design guidelines such as wcag .
however atf itself is not a standalone testing tool.
to use atf developers need to integrate it into their existing app development and testing processes.
for the standardization of accessibility detection we still use pre defined atf as part of our newly defined rules which follows the principles of perceivable operable understandable and robust enabling it to detect accessibility issues related to color contrast touch size accessibility labels and compatibility with screen readers among others .
with the accessibility events from the view of the detected ui scenario as input the accessibility testing framework atf can perform pre defined accessibility checks on all components of the current ui scenario which means that atf can analyze every component in the ui including buttons text boxes labels etc.
to ensure they comply with pre defined accessibility standards and rules.
runtime context aware analysis although integrating atf can help us detect accessibility issues in the apps it is not a perfect solution.
atf only performs accessibility checks on input accessibility events using pre defined methods which may lead to potential risks.
these accessibility events refer to static data of the captured attributes of ui components and their ui screenshots.
lack of contextual analysis can easily lead to inaccurate judgments of attributes of ui components especially for some visual design issues or complex interaction scenarios.
here are some factors that could lead to false positives without contextual information external dependency.
external dependency issues arise when apps display third party services or images with significant visual content such as logos or brand names from other apps during runtime.
accessibility standards do not require contrast checks for these elements or for invalid issues like unused components .
detection without contextual information cannot recognize and categorize the types and uses of detected ui components leading fig.
examples of false positives under different scenarios.
to misjudgments that violate the definition of accessibility standards.1fp external dependency in fig.
shows the circled component was reported as an image contrast issue but it actually directly