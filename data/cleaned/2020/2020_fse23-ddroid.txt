automata based trace analysis for aiding diagnosing gui testing tools for android enze ma1 shan huang1 weigang he1 ting su1 jue wang2 huiyu liu1 geguang pu1 zhendong su3 1shanghai key lab of trustworthy computing software engineering institute east china normal university china 2state key lab for novel software tech.
and dept.
of computer sci.
and tech.
nanjing university china 3department of computer science eth zurich switzerland abstract benchmarking software testing tools against known bugs is a classic approach to evaluating the tools bug finding abilities.
however this approach is difficult to give some clues on the tool missed bugs to aid diagnosing the testing tools.
as a result heavy and ad hoc manual analysis is needed.
in this work in the setting of gui testing for android apps we introduce an automata based trace analysis approach to tackling the key challenge of manual analysis i.e.
how to analyze the lengthy event traces generated by a testing tool against a missed bug to find the clues.
our keyidea is that we model a bug in the form of a finite automaton which captures its bug triggering traces and match the event traces generated by the testing tool which misses this bug against this automaton to obtain the clues.
specifically the clues are presented in the form of three designated automata based coverage values.
we apply our approach to enhance themis a representative benchmark suite for android to aid diagnosing gui testing tools.
our extensive evaluation on nine state of the art gui testing tools and the involvement with several tool developers shows that our approach is feasible anduseful .
our approach enables themis the enhanced benchmark suite to provide the clues on the tool missed bugs and allthethemis s clues are identical or useful compared to the manual analysis results of tool developers.
moreover the clues have helped find several tool weaknesses which were unknown or unclear before.
based on the clues two actively developing industrial testing tools in our study have quickly made several optimizations and demonstrated their improved bug finding abilities.
allthe tool developers give positive feedback on the usefulness and usability of themis s clues.
themis is available at .
ccs concepts software and its engineering software testing and debugging .
enze ma and shan huang contributed equally to this work and they are co first authors.
ting su is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
android gui testing runtime verification trace analysis acm reference format enze ma shan huang weigang he ting su jue wang huiyu liu geguang pu and zhendong su.
.
automata based trace analysis for aiding diagnosing gui testing tools for android.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
.
introduction in our community benchmarking software testing tools against a set of representative ground truth bugs e.g.
defects4j lava magma is the well justified and widely used approach to evaluating and improving the tools bug finding abilities .
specifically in the field of gui testing for android apps a proliferation of automated testing tools have been developed to help find crash bugs in the apps .
however a recent study benchmarks several such testing tools against a set of real world bugs.
it reveals that these tools miss of the bugs the tool effectiveness gap for finding real world bugs is large.
in such a situation the users of a benchmark suite e.g.
the testing tools developers likely raise the question why does the tool miss these bugs?
in hope of knowing some clues of potential tool weaknesses for improvement.
however the classic benchmarking approach falls short in such a situation because it can only tell the false negatives i.e.
which bugs were missed without any explanation.
this shortcomings limits the advantages of benchmarking.
a real example .
figure a shows a real crash bug issue ofscarletnotes an app used to take notes and to do lists.
figure a shows this issue s minimal bug triggering trace which includes five pivot1input events steps c1 clicking the notebook creation button located at the bottom right on page l0 to create a notebook e.g.
named as notebook1 c2 clicking the created notebook notebook1 on page l1to enter into its directory c3 opening the menu by clicking the menu button located at the bottom left on page l2 c4 choosing the locked option on the menu to show the locked notes page l3 and c5 clicking the button on page l4to exit from notebook1 .
note that the bug triggering condition of this issue is filtering the locked notes under some notebook s directory i.e.
notebook1 in this case and then clicking the button to exit from that directory it does 1a pivot event is a necessary event for bug triggering.
if a pivot event is removed from the bug triggering trace the bug cannot be successfully reproduced.esec fse december san francisco ca usa e. ma s. huang w. gang t. su j. wang h. liu g. pu and z. su figure a bug triggering event trace for scarletnotes s issue and b list of executable widgets on the gui page l2.
figure the benchmark suite themis enhanced by our automata based trace analysis approach.
not matter which notebook s directory we are in or whether there exist some locked notes under that directory.
by replicating the aforementioned benchmarking study detailed in section we find that all of the evaluated testing tools missed this crash bug.
in such a situation the classic benchmarking approach cannot give any clue on this missed bug for aiding diagnosing the testing tools.
difficulties of current practice .
due to the preceding limitation tool developers have to manually debug their tools against the missed bugs to find some clues.
to understand the current practice we interviewed several tool developers by asking what kinds of clues you are looking for during analysis?
what difficulties you have in finding these clues?
.allthe developers responded that they hope to find the clues indicating potential tool weaknesses e.g.
which events cannot be exercised which ui pages cannot be reached .
however the main challenge of finding such clues is analyzing the lengthy event traces generated by a tool against its missed bugs .
it makes the manual analysis process time consuming and difficult.
specifically we take fastbot a popular industrial gui testing tool in our study as an example to illustrate the typical manual analysis of tool developers.
in face of the missed bug scarletnotes s issue the fastbot s developer manually checks whether each pivot ui event of the bug triggering trace i.e.
c1 c2 c3 c4 c5 is executable the developer navigates the app to specific screen pages i.e.
l0 l1 l2 l3 l4 dumps the ui layouts and checks whether the ui widgets corresponding to c1 c2 c3 c4andc5 are clickable .
in this case the developer finds that all the ui widgets are clickable which means all the pivot ui events could be exercised in theory.
however this clue cannot help explain why the bug was missed.
to this end the developer runs fastbot on the app e.g.
one hour or more to manually analyze the actual tool behaviors against themissed bug e.g.
analyzing whether fastbot can indeed reach the pages likel0 l1 l2 l3andl4 andexercisec1 c2 c3 c4andc5in the right order by its testing strategy .
unfortunately this manual analysis process is time consuming and difficult because testing tools like fastbot may generate a large number of random usually fast executing input events including the pivot events and many irrelevant ones during testing although it may sometimes help.
for example fastbot could generate about input events within one hour of testing.
it is difficult for human to find the clues bymanually analyzing such lengthy ui based event traces.
in this case fastbot s developer spent more than hours not including the tool s running time in finding the clues before giving up.
even worse this manual analysis becomes more overwhelming when the developer needs to analyze a number of missed bugs or debug different tool versions.
when the developer fails to find the clues they may lose the opportunities for tool improvement.
our approach and its novelty .
the keyproblem in our setting ishow to automatically and effectively analyze the lengthy event traces generated by a tool i.e.
the actual tool behaviors against a missed bug to find the clues .
to this end our key insight is to cast this challenging problem into automata based trace analysis .
specifically our idea is to model a bug in the form of an nondeterministic finite automaton named as the bug automaton which captures its bugtriggering traces.
in this way we can automatically match the event trace generated by the testing tool which misses this bug against this automaton to monitor tool behaviors.
when the event trace cannot be accepted by the automaton i.e.
the bug is missed we can analyze the matching results of the automaton to obtain the clues.
this automata based trace analysis approach tackles the painpoint of manual analysis and is applicable to any off the shelf gui testing tool without tool modifications.
at the high level our idea can be viewed as the adaption of runtime verification rv because the automaton can be interpreted as the specification of undesired app behaviors.
however applying existing rv techniques in our setting is difficult which we will discuss in section .
specifically inspired by the clues concerned by tool developers in the interviews and the classic conception of code coverage we introduce three automata based coverage metrics i.e.
event coverage event pair coverage andtrace based minimal distance detailed in section .
as the proxies of our clues the values of these coverage metrics are the clues provided by our approach .
we also compute some supplementary clues e.g.
the execution times of events and event pairs .
the novelty is that these clues offer theautomata based trace analysis for aiding diagnosing gui testing tools for android esec fse december san francisco ca usa figure constructing the bug automaton for scarletnote s issue in three steps a b and c .
c is the final bug automaton.
tool developers systematic fine grained insights on the potential tool weaknesses which are difficult to be achieved by the ad hoc manual analysis demonstrated by our evaluation in section .
.
application scenario of our approach .
the main application scenario of our approach is to enhance a benchmark suite thus improving the classic benchmarking.
figure shows the benchmark suite enhanced by our approach denoted by the blue box detailed in section .
specifically we provide each bug with its bug automaton denoted by the grey box .
in this way given a testing tool under evaluation the benchmark suite can report the missed bugs as well asthe clues on these missed bugs.
as the users of a benchmark suite tool developers can inspect the clues with the bug and the app to diagnose and improve their tools.
moreover this benchmark suite can routinely serve as a regression test suite for validating the effectiveness of testing tools whenever the tools are modified.
it can further reduce the repetitive manual analysis cost of tool developers.
note that the bug automata in our work are manually constructed with a one time effort .
we will explain how to construct the automata in section .
and give more discussions in section .
.
evaluation and results .
we implement a tool named ddroid to support the automata based trace analysis approach.
to evaluate the usefulness we integrate ddroid intothemis a representative benchmark suite with diverse types of real world bugs for android.
we named the benchmark suite enhanced by ddroid and the bug automata as themis .
specifically we use themis to evaluate nine automated testing tools for android with differenttesting strategies and implementations including ape combodroid stoat droidbot humanoid q testing google s monkey bytedance s fastbot and wctester from tencent s wechat team.
these tools represent the state of the art and state of the practice.
our evaluation shows that our automata based trace analysis approach is feasible anduseful .
first it enables themis to provide the clues on the missed bugs see section .
which cannot be achieved by the classic benchmarking i.e.
themis .
second all thethemis s clues are either identical or useful compared to those manually found by tool developers without any misleading information.
the clues have also helped developers successfully pinpoint several tool weaknesses which were unknown or unclear before.
allthe tool developers explicitly stated that they would enhance their tools based on the clues.
specifically the two activelydeveloping industrial testing tools fastbot andwctester have quickly made several optimizations and already demonstrated their improved bug finding abilities section .
.
a further interview with the tool developers reveals that allthe developers are positive on the usefulness and usability of themis s clues.
to sum up our work has made the following contributions we introduce an automata based trace analysis approach in the context of gui testing to enhance the classic benchmarking by providing the clues of tool weaknesses on the missed bugs.
we introduce three automata based coverage metrics as the basis of the clues which can give systematic fine grained insights on the potential tool weaknesses.
our evaluation shows that the benchmark suite enhanced by our approach is effective and useful.
the clues have helped find several tool weaknesses and improved some testing tools.
illustrative example we use scarletnotes s issue discussed in section to illustrate our approach.
.
bug automaton a bug automaton is represented in the form of a nondeterministic finite automaton with transitions nfa .
intuitively such a bug automaton captures different non minimal bug triggering traces of the bug.
figure c gives the automaton of scarletnotes s issue in which each node e.g.
s0 s1 s2 s3 s4 s5 s6 denotes an abstract program state and each transition e.g.
c1 c2 c3 c4 c5 denotes an event connecting two states.
for example the trace in blue corresponds to the bug s minimal bug triggering trace.
specifically s0 the initial state abstracts and corresponds to l0in figure a denoting the initial state in which no notebook is created s1abstractsl1 denoting the state in which some notebook is created after c1is executed s2abstractsl2 denoting the state in which the directory of some notebook is opened after c2is executed s3abstractsl3 denoting the state in which the menu is shown under the directory of some notebook after c3is executed s4abstractsl4 denoting the state in which the locked notes is filtered under the directory of some notebook after c4is executed ands5 the final state denotes the crashing state after c5.
for another example according to the app feature see figure a one can click the button on l2 similar toc5onl4 to return back to l1 so the automaton includes the transition from s2to s1 denoted by c5 .
this transition helps capture such non minimal traces as .
additionally one can press back on pagel2 l3orl4to jump back to l1 l2orl1 respectively denoted by the curved black lines in figure a .
as a result one can take some non minimal traces e.g.
c1 c2 c3 back c3 c4 c5 or to trigger the bug.
to capture such traces the bug automaton also includes these transitions enabled by back i.e.
the transitions denoted by from s2tos1 s3tos2 ands4tos1.
specifically denotes those events like back which are not pivot for bug triggering but could help capture other non minimal bug triggering traces.
we will define the bug automaton and explain the construction method in section .
.esec fse december san francisco ca usa e. ma s. huang w. gang t. su j. wang h. liu g. pu and z. su table clues for wctester andfastbot onscarletnotes s issue in a simplified textual report.
note that indicates the event or event pair is missed by the tools.
wctester fastbot event coverage .
event pair coverage .
.
minimal distance details of ec event executed times c1 c2 c3 c4 c5 c1 c2 c3 c4 c5 details of epc event pair executed times c1 c2 c2 c3 c3 c4 c4 c5 ... c1 c2 c2 c3 c3 c4 c4 c5 ... details of md is covered is covered .
clues provided by our approach the clues are presented in the form of the three automata based coverage values.
table shows the clues for wctester andfastbot on the missed scarletnotes s bug in the form of a simplified textual coverage report which we explain as follows.
clue i event coverage ec .
the event coverage tells which pivot events for bug triggering are covered or missed by a testing tool.
table shows that wctester misses the three events c3 c4 andc5 event coverage while fastbot covers all the five pivot events event coverage but still misses the bug.
clue ii event pair coverage epc .
the event pair coverage tells which event pairs are covered or missed by a testing tool.
the intuition is that bug finding requires covering the pivot events but also specific event pairs.
for example c1 c2 c2 c3 c3 c4 and c4 c5 are some typical event pairs of interest on the minimal bugtriggering trace in figure .
table shows that wctester and fastbot achieve .
and .
event pair coverage respectively.
for example fastbot misses the event pair c4 c5 .
clue iii trace based minimal distance md .
the trace based minimal distance tells how close a testing tool can reach a bug e.g.
which ui pages on the bug triggering trace can be reached .
it uses the number of pivot events to characterize the distance.
the smaller the distance is the closer the tool reaches the bug.
when one bugtriggering trace is covered the distance should be .
table shows thatwctester s minimal distance is i.e.
wctester can only cover in order while fastbot s minimal distance is i.e.
fastbot can cover in order but the last event c5 .
note that in practice themis visualizes the clues in the textual report based on the ui transition graph of the missed bug like figure instead of the bug automaton to ease user understanding and inspection see an example of the visualized clues at .
.
diagnosing tools based on the clues we present the clues in table to the developers for tool diagnosis.
based on the clues wctester s developer quickly locates the suspicious events i.e.
the missed events c3andc5 for diagnosing.
first he inspects the widget properties of c3andc5 presented bythemis and finds that c3andc5are executable.
next he inspects the widget types of c3andc5and finds the root cause i.e.
wctester fails to support viewgroup the widget type of c3andc5.
after he fixed this tool weakness wctester can find this bug.
based on the clues fastbot s developer quickly knows that the tool missesc5on pagel4 as the minimal distance is but executes c5on pagel2 asc5is covered .
note that l2andl4containc5 seefigure .
specifically fastbot executesc5 onl2 by only times while combodroid andape executesc5onl2by and times within the same testing time respectively .
note that the execution times of events and event pairs are recorded as the supplementary clues detailed in section .
.
based on these clues the developer quickly suspects why c5is seldom executed and locates the pivot pagel2for diagnosing.
figure b shows all the executable widgets onl2in the dotted boxes.
the developer notes that the six widgets w5 w10 onl2 including the widget of c5 i.e.
the buttonw5 have the same widget property values i.e.
the same widget type and resource id .
as a result fastbot assumes that these six widgets are of the same functional purpose and clusters them into a widget group to reduce ui exploration space.
in particular each widget in this group is purely randomly selected for execution.
but this widget group and the other four widgets on l2 i.e.
w1 w4 are selected at the same level.
as a result the probability of executing c5onl2 is1 .
which is rather small.
the probability of executing c5onl4is much smaller than .
because c5 onl4 is executed afterc4.
this explains why fastbot misses the event pair c4 c5 .
the developer confirmed that this is a design defect in the tool s event selection strategy and fixed it by prioritizing the widgets in a clustered group which have not yet been executed before.
the enhanced fastbot can find this bug.
we can see that the clues provide systematic fine grained insights to aid diagnosing testing tools which are hard to be achieved by the manual analysis.
approach and implementation .
problem definition an android app is a gui based event driven program p. each of its screen pages is a gui layout l i.e.
a gui tree .
each node of this tree is a gui view or widget w. a gui event e t w o is a triplet in which e.tdenotes its event type e.g.
click edit e.w is the widget on which eis executed and e.odenotes the optional data associated with e e.g.
a string input by edit .
definition .
.
an event trace .
an event trace tis a sequence of events which is denoted as t whereeiis an event.
when tis executed on an app p we can obtain a sequence of gui layouts l i.e.
l wherel0is the layout of the app starting page and liis the layout due to the execution ofeionli i n .
intuitively the execution of an event trace t can be represented as l0e1 l1...li 1ei li...ln 1en ln.
the main goal of an automated gui testing tool is to find potential crash bugs by generating an event trace tinteracting with an app p. based on definition .
given a known crash bug we can define the bug triggering trace as follows.
definition .
.
a crash bug triggering trace .
a crash bug r is a crash inducing fault of p and usually manifests itself as a runtime exception.
the bug triggering trace trofris an event trace which can deterministically reproduce r. we denote tras tr e ... e i ... e m e iis an event and the corresponding gui layouts oftraslr l ... l i l i... l m l iis the layout .
definition .
.
a minimal bug triggering trace .
given a bugtriggering trace trof the bugr if any single event in tris removed automata based trace analysis for aiding diagnosing gui testing tools for android esec fse december san francisco ca usa rcannot be reproduced we say this trace is minimal .
the events in such a minimal trace are named as pivot events .
without ambiguity all the bug triggering traces discussed in this paper are minimal unless we explicitly mentioned.
example .
for the bug in figure a the bug triggering trace is tr .
this trace is minimal and executes lr l5denotes the crashing page .
in practice rmay have multiple bug triggering traces trs with different sets of pivot events these traces lead to the identical exception stack .
without loss of generality we assume a crash bug rhas one bug triggering trace trin the following definitions and we discuss the case of multiple bug triggering traces trs later.
problem definition.
our problem is given a crash bug rofp and a testing tool how to automatically and effectively match the event trace t generated by againstr s bug triggering trace tr e ... e i ... e m to find the clues.
.
bug automaton and its construction to tackle the preceding problem our key idea is that given a bug r we construct a bug automaton mto represent rbased ontr and matchtagainst the automaton mto find the clues.
specifically we formulatemin the form of a nondeterministic finite automaton with transitions nfa for short .
definition .
.
bug automaton.
a bugr s automaton mis formulated as a nfa.
given r s the minimal bug triggering trace tr e ... e i ... e m and its corresponding gui layouts lr l ... l i l i... l m we definemasm s s f where is a finite set of input symbols i.e.
e ... e i ... e m .
here e e ... e mare the pivot events on tr and denotes any event like back in the automaton in figure c which are not pivot for bug triggering but could lead to other possible non minimal traces reaching r. sis a finite set of abstract program states which can be reached by executing the input symbols in on appp.
is a transition function i.e.
s p s wherep s is the power set of s. s0 sis the initial state of m. fis the set of final states.
specifically in our setting fonly contains one state which denotes the crashing state.
bug automaton construction given a bug triggering trace tr we follow three steps to manually constructr s bug automaton m. in the following we use scarletnotes s issue see figure to illustrate the automaton construction method shown in figure .
step initializing the automaton by the minimal bugtriggering trace.
based on the minimal bug triggering trace trand its corresponding gui layouts lr we can initialize the set of input symbols the set of abstract program states s and the transition function of the automaton m. specifically the gui layouts lrare abstracted to the set of states s i.e.
l iis abstracted to si.
here l the app s starting page is abstracted to s0 m s initial state and l n the app s crashing page is abstracted to sn m s final state .
according to the execution of tr if there exists a page transition l iei l i the corresponding state transition siei si 1will be added into the transition function .
in this way the initial bug automatonis constructed i.e.
s0e1 s1...si 1ei si...sn 1en sn.
take scarletnotes s bug in figure as example based on its tr we can decide that c1 c2 c3 c4 c5 s s0 s1 s2 s3 s4 s5 s0ands5 are the initial and final state respectively and the initial transition function corresponding to the transitions in figure a .
step adding other transitions enabled by the pivot events.
after step the automaton only captures the minimal bug triggering trace.
to capture those non minimal bug triggering traces we need to include other transitions enabled by the pivot events into the automaton.
to this end we check whether any pivot event in can be executed on each state in s excepts0andsn and lead to new transitions and or new states.
we will add any new transition and or state into the automaton and apply the same checking process on the new states until no new transitions or states can be found.
let us take s1in the automaton in figure a as an example to enumerate the input symbols in againsts1.
according to the app feature s1can takec1to reachs1itself because we can execute c1ons1 corresponding to l1 to create some notebook recall thats1denotes the abstract state in which some notebook is created s1can takec2to reachs2according to tr already included in the automaton s1can takec3to reachs6 a new abstract state denoting the menu is shown on top of the app s main page which is different from s3 becauses3denoting the menu is shown under the directory of some notebook s1cannot take c4andc5becausec4andc5do not exist on s1 corresponding to l1 .
as a result we added all the transitions enabled by the pivot events fors1.
similarly we can enumerate the input symbols in against the remaining states in s. after this step we obtain the automaton shown in figure b .
the automaton captures those non minimal bug triggering traces like .
step adding the transitions.
in addition to the pivot events one may take some non pivot events like back to reach r. thus we annotate such events as at this time is updated to c1 c2 c3 c4 c5 and include the transitions enabled by .
for example according to the app feature see figure a one can press back on pagel2 l3orl4to jump back to l1 l2orl1 respectively denoted by the curved black lines in figure a .
thus we add the transitions from s2 s3ands4tos1 s2ands1 respectively.
in this way the automaton can capture such new non minimal bug triggering traces c1 c2 c3 back c3 c4 c5 or .
after this step we obtain the final automaton shown in figure c .
discussion .
handling multiple bug triggering traces .
a crash bugrmay have multiple bug triggering traces trs with different sets of pivot events.
in such cases by definition .
we construct an nfa based on each bug triggering trace tr and merge these nfas together into a new nfa by connecting their initial and final states with .
the bug triggering trace trshould be minimal .
because such a bug triggering trace makes mexpressive succinct and precise.
if trincludes non pivot events mcould become unnecessarily complicated and may lead to misleading clues.
for example assuming event eiis an non pivot event but included in tr if a testing tool triggers rbut does not cover ei the clue that eiis not covered does not make sense.
in practice given a bug triggering trace we manually reduce it to a minimal one removing oneesec fse december san francisco ca usa e. ma s. huang w. gang t. su j. wang h. liu g. pu and z. su event at one time and then checking whether the bug can be reproduced .
it takes little effort because the bug triggering traces obtained from bug reports are already or close to minimal.
given all the bug triggering traces trs the bug automaton is precise and complete by construction .
section .
empirically validates the precision and completeness of the manually constructed automata.
after the construction we automatically convert min the form of nfa to its equivalent deterministic finite automaton dfa by eliminating the transitions.
formally the dfa mdismd sd d d q0 f where all the components have their similar interpretations as for the nfa and d .
we conduct this conversion because the dfa without transitions is algorithmically more convenient for defining and computing the coverage metrics detailed in section .
.
note that mandmdare equivalent and accept the same language so it is safeto matcht which only contains the symbols in d againstmd.
the conversion is not expensive as the sizes of nfas are relatively small.
moreover nfa is more intuitive for human understanding like the ui transition graph in figure and easier for manual construction than its equivalent dfa.
in table nfa sizes and dfa sizes show the sizes of nfa and its dfa respectively.
.
coverage metrics based clues we introduce three coverage metrics at the automaton level of md sd d d q0 fd the equivalent dfa of the bug automaton m nfa as the basis of the provided clues.
clue i event coverage .
leteabe the set of all the events in d and letecbe the set of events executed by a testing tool .
formula defines event coverage ec to characterize how many events could be covered by .
ec ec ea conceptually ecis similar to the statement coverage in classic software testing.
since the events in dare necessary to trigger r eccan assess the tool effectiveness when cannot execute all these events.
the higher ecis the more likely can find the bug r. if cannot execute some events it likely indicates some tool weaknesses.
for example as we illustrated in section .
wctester misses some events as it fails to support these events widget type.
clue ii event pair coverage .
letiabe the set of all event pairs ex ey inmd whereexandeyare the events in dand denote the events of two adjacent transitions in d. for example in figure c c4 c5 is an event pair as c4andc5are the events of two adjacent transitions.
formally ia ex ey si sj sk sd.
ex ey d. d si ex qj d sj ey sk .
leticbe the set of the covered event pairs.
specifically we say the event pair ex ey is covered if bothexandeyare executed in the order of eximmediately followed byey.
formula defines event pair coverage epc to characterize how many event pairs could be covered by .
epc ic ia conceptually epcis similar to the branch coverage in classic software testing.
epcis a stronger metric than ec.epccan assess the ability of a testing tool to execute two adjacent transitions and reflect the diversity of event traces generated by .
the higher epcis the more likely can stress test the interactions between pivot events.
if some event pairs are not covered it mayindicates some tool weaknesses.
for example as we illustrated in section .
fastbot missed the event pair c4 c5 which indicates some tool weaknesses.
note that this metric is identical to the eventinteraction coverage in traditional gui software testing and can be extended to length nevent sequence coverage n .
clue iii trace based minimal distance .
lett d e1 ... e i ... e n ei d be the event trace generated by a testing tool .
let sc s0 ... s j ... s m sj sd j mbe the set of states that t dcan reach when matching t dagainst the automaton md.
let distance sj fd be the minimal number of events or transitions required to take from sjto reachfdonmd.
formula defines the trace based minimal distance md to characterize how close a testing tool can reach a crash bug r. md min distance sj fd sj sc wheremin returns the minimal element of a set.
for example if sc s0 s1 s2 s6 is the set of states reached by a testing tool on the automaton in figure c the value of mdis .
because the minimal distance is from s2to the final state s5by following the three events c3 c4 andc5.
mdassesses the tool effectiveness from the perspective of pathbased testing in classic software testing.
this metric can assess whether a testing can exercise the events of the bug triggering trace in some specific orders and quantify how far is to reachr in terms of number of events to be executed.
it indicates the ability boundary of a testing tool.
if can find the crash bug r the md should be .
mdis a stronger metric than ecandepcbecause a tool may achieve ecorepcbut may not achieve mdas0.
other clues execution times of events and event pairs.
we compute the execution times et of covered events i.e.
the events inecofec and event pairs i.e.
the event pairs in icofepc respectively as the supplementary metrics.
etis similar to the execution count metric in classic code coverage tools like gcov for performance profiling in terms of statements and branches.
.
implementation figure illustrates the workflow of our automata based trace analysis approach denoted by the blue box .
specifically given a bug rof an buggy app pand its automaton m manually constructed according to the method described in section .
our approach conducts the following three automated steps to obtain the clues.
instrumentation .
the buggy app pis automatically instrumented at the pivot events in tr.
lettrbe e ... e i ... e m we instrument pat the event listener of each event e i. in this way e i will be logged when it is executed by the tool .
logging .
the testing tool is run against the instrumented apppto log the executed pivot events.
all the logged pivot events forms an event trace l. is allocated with enough testing time for running to reach the saturation point.
monitoring .
to ease the computation of coverage metrics we automatically convert the bug automaton mfrom an nfa to an equivalent dfa md.
next we match the logged event trace l againstmd and compute the coverage metrics i.e.
ec epc mdand et .
during the matching one event is taken from lat one time and matched against the transitions of md and all the covered events automata based trace analysis for aiding diagnosing gui testing tools for android esec fse december san francisco ca usa table selected automated gui testing tools in our study.
tool venue source main testing strategies stoat esec fse model based droidbot icse model based ape icse model based humanoid ase deep learning based combodroid icse model based q testing issta reinforcement learning based monkey google random testing fastbot bytedance model reinforcement learning based wctester wechat random reinforcement learning based event pairs the reached states and the execution times are recorded to compute the coverage metrics.
we developed a tool ddroid written in python shell and html to support the application of our approach.
we use jflap and its extension putflap to specify the bug automaton.
we use gradle transformer and asm to automatically instrument apps at the event handlers to uniquely log executed events .
we use automata lib to convert an nfa to an equivalent dfa and the floyd s algorithm to compute the mdmetric.
we visualize the clues via interactive html pages to ease user inspection.
empirical experiment .
research questions rq1 enhanced by the automata based trace analysis approach canthemis provide the clues on the bugs missed by automated gui testing tools compared to themis ?
rq2 how useful are the clues provided by themis for aiding diagnosing gui testing tools compared to the clues manually found by tool developers based on only the missed bugs?
rq3 how well other alternative trace analysis approaches perform in finding the clues?
can they outperform the automatabased trace analysis approach in finding useful clues?
rq1 investigates the feasibility of the automata based trace analysis approach to provide some clues on the tool missed bugs thus improving the classic benchmarking.
rq2 investigates the usefulness of the automata based trace analysis approach e.g.
better understanding testing tools behaviors diagnosing potential tool weaknesses and improving the tools bug finding abilities.
rq3 investigates the effectiveness of the automata based trace analysis approach compared to other alternative trace analysis approaches i.e.
to what extent our approach is really needed.
.
experimental setup experimental environment .
we deployed our experiment on a bit ubuntu .
machine cores amd 3995wx cpu and 128gb ram and google android .
emulators.
gui testing tools .
we selected nine gui testing tools including six academic ones ape combodroid humanoid droidbot q testing and stoat and three industrial ones monkey fastbot and wctester for our experiment.
these tools represent the state of the arts.
note that we used the latest versions of these tools at the time of our study.
the academic tools and fastbot are publicly available on github.
monkey is released with android sdk.
wctester is obtained on request from wechat s testing team.
table summarizes these selected tools and their main testing strategies.
readers canrefer to these tools papers for more information.
we did not include old tools like sapienz as it only works old android versions.
the benchmark suite .
we applied our approach to themis a benchmark suite with real world bugs for android among others .themis isrepresentative as it contains crash bugs with different complexities from different categories of apps.
each of these bugs is provided with its minimal bug reproducing traces and the corresponding buggy app version.
interested readers can refer to table in themis s paper orthemis s bug repository for bug details.
to build themis based on themis one graduate and one undergraduate students who participated in this research work manually built the bug automaton for each bug.
before constructing the automata the students spent some time in getting familiar with the apps and the bugs.
in our experience it roughly took minutes to build one bug automata depending on the complexity of the bug.
it took us about hours in total to build and validate all the bug automata.
in this process we excluded bugs of wordpress because the buggy app versions cannot be compiled anymore due to an obsoleted third party library bug of amazefilemanager which cannot be deterministically reproduced bug of phonograph because the bug requires adding music files which is unrealistic for automated testing tools and bug of frost for facebook avoiding violating facebook s user policy due to random fuzzing .
thus we finally got instrumented apks which can deterministically reproduce the corresponding bugs.
table column bugs lists these bugs.
evaluation setup for rq1 .
we benchmarked the nine selected testing tools on the bugs to identify missed ones and computed the automata based coverage metrics.
we followed the instructions ofthemis see section .
in to run these tools each tool is run against each bug on one emulator in one run each run was allocated with hours for thorough testing and repeated times to mitigate the randomness.
for the nine selected tools the whole evaluation took about machine hours.
evaluation setup for rq2 .
we invited the developers of seven tools listed in table s column tool to investigate the usefulness of the themis s clues.
monkey andq testing were excluded because monkey s and q testing s developers did not reply to our invitation.
we find that six of these seven tools except fastbot are developed and maintained by only one person respectively.
in this case it is difficult to involve different developers per tool to conduct the study with statistical tests.
therefore we involved developers one developer per tool in the study and designed a rigorous two step study which we believe is already enough and valid to answer rq2.
in the first step we gave the tool developers the missed bugs i.e.
the output of themis and let them try their best to manually find the clues based on the buggy app and the bug triggering traces.
the developers followed the similar manual analysis process described in section e.g.
running their tools against the missed bugs to find the clues without time limits.
this step aims to obtain the ground truth clues of developers with their best effort.
in the second step we gave the same developers thethemis s clues i.e.
the output of themis .
the clues are visualized based on the textual coverage report in table to ease inspection.
we let them validate whether the clues are useful identical ormisleading compared to their prior own clues.
specifically esec fse december san francisco ca usa e. ma s. huang w. gang t. su j. wang h. liu g. pu and z. su table evaluation results of the nine gui testing tools based on themis against the real world crash bugs.
bugs nfa sizesdfa sizeswctester fastbot ape combodroid monkey stoat droidbot humanoid q testing ec epc md ec epc md ec epc md ec epc md ec epc md ec epc md ec epc md ec epc md ec epc md ad ad afm afm afm ab ab ab ab ab anki anki anki anki anki anki anki apm collect comm comm comm comm comm fl fl fl ghd mfb nc nc nc nc on oeaa ol oe4a oe4a sn sf wp wp wp wp wp wp wp best values found missed we say identical if developers decided themis s clues are identical to their found clues useful if developers decided themis s clues provide more useful information for tool diagnosis than their found clues e.g.
the themis s clues cannot be found by manual analysisorare more precise than the clues found by manual analysis misleading if developers decided themis s clues are contradictory w.r.t.
their found clues.
note that all the involved developers are experts and have been actively maintaining the tools for years.
thus they have enough expertise to evaluate the usefulness of the themis s clues.
this study was conducted with developers online.
after the study we conducted an interview with each developer to solicit their feedback on themis s clues.
evaluation setup for rq3 .
we compared the automata based trace analysis approach with two simple alternative trace analysis approaches i.e.
simple trace comparison simple tc for short and simple runtime verification simple rv for short .
specifically simple tc represents a naive trace analysis method.
it directly compares the event trace tgenerated by a testing tool and the bug triggering tracetrof a known bug r. it reports the first differing event between these two traces tandtr.simple rv matches the event trace tgenerated by a testing tool against the constructed bug automaton m. it reports the first event which cannot be accepted by the automaton m. because the clues reported by these two approaches and ourscannot be directly compared.
to fairly compare these approaches we use the first missed event in the bug triggering trace of a missed bug as the comparison metric.
formally given a bug triggering tracetr e ... e i ... e m e iis the first missed event intrife i is missed but all the events e ... e i 1are covered in the order by t. note that simple rv used the bug automata constructed by us.
our approach computes the first missed event based on the trace based minimal distance md.
we used the event traces generated by the testing tools in rq1 for evaluation.
.
results of rq1 themis v.s.themis table gives the results of rq1.
column bugs lists the bugs.
for example sn denotes scarlet note s issue .
in table the last row found missed gives the output of themis on these tools in the form of x y where x and y are the numbers of found and missed bugs respectively.
we can see thatthemis can only identify the missed and found bugs.
with the help of our approach themis can provide the clues on the missed bugs which cannot be obtained by themis .
the columns with tool names e.g.
wctester fastbot give the achieved best coverage values of the three main metrics i.e.
event coverage ec event pair coverage epc and trace based minimal distance md for each bug tool among the five independent testing runs.
weautomata based trace analysis for aiding diagnosing gui testing tools for android esec fse december san francisco ca usa table validation results of themis s clues w.r.t.
the clues manually found by tool developers on the missed bugs.
tool identical useful misleading wctester fastbot ape combodroid stoat droidbot humanoid total focus on these achieved best values as they indicate the best tool performance.
take the results of wctester on bug sn as an example see row sn under column wctester the best achieved ec epc mdamong the five testing runs are and respectively.
from such metric values we can obtain the clues e.g.
which events and event pairs are missed andhow close a tool can reach the bug .
for example section .
illustrates the clues on the missed bug sn for wctester .
miscellaneous .
in table symbol denotes the coverage value is unavailable due to tool issues.
for example q testing only successfully ran on bugs we reported the tool issues to q testing s developer but did not get reply .
symbol denotes the coverage metric is not applicable.
for example apm does not have epcbecause its bug automaton only has one transition.
.
results of rq2 how useful are the themis s clues?
table gives the validation results on themis s clues.
column identical useful and misleading denote the numbers of missed bugs for which themis finds the identical useful ormisleading clues respectively compared to the clues manually found by tool developers.
from table we find that allthethemis s clues are identical or useful compared to the clues manually found by developers without any misleading ones.
specifically themis provided the identical and useful clues respectively for and of the missed bugs for all tools.
we provided the detailed validation results on each missed bug per tool in the supplementary material .
how can themis find identical or useful clues?
in cases themis can find the identical clues w.r.t.
the manual analysis of tool developers.
for example sf requires a multi touch event on an item list.
for the tool missing this bug the developers can find the clue that the tool cannot emit multi touch by manual analysis.
themis can find the identical clue as eccan tell the multi touch event is not covered.
in cases themis can find useful clues.
take nc as an example the bug triggering trace has five events e1 opening the sidebar navigation drawer e2 selecting auto upload in the drawer e3 selecting remote folder on the auto upload page e4 selecting new folder on the main page ande5 pressing the create button to create a new folder .
for this bug wctester s developer cannot find any clue although he observes that the tool could click all the widgets of e1 e5.themis finds the clue that wctester can indeed generate these events because theecis but these events are not executed in the right order because its mdis .
themis reveals that wctester never creates the folder by e4ande5 after the remote folder option is selected bye1 e2ande3 .
this clue is hard to obtain by manual analysis.
figure an example of event generation strategy.
can the themis s clues help diagnose tool weaknesses?
informed by the themis s clues the tool developers have successfully located several tool weaknesses which were unknown or unclear before.
we illustrate some found major tool weaknesses.
weaknesses in the event generation strategy .
most gui testing tools parse gui layouts to generate events.
specifically they check the properties e.g.
clickable long clickable of the ui widgets to generate the ui events e.g.
click long click .themis s clues helped reveal some weaknesses in the event generation strategies of fastbot anddroidbot which degrade their bug finding abilities.
for example figure shows a listview page simplified from a bug in our study and its gui layout.
in this layout listview is the root node and a b and c are the leaf nodes of textview wrapped by linearlayout .
from this layout a good testing tool should generate three click events for a b and c respectively.
however for this case wctester andape succeed but fastbot anddroidbot fail.
because fastbot generates an event only when a widget s clickable and enabled are both true while droidbot will not generate events for the nodes i.e.
a b and c if the clickable property of their parent node i.e.
listview is true .
as a result fastbot anddroidbot can only generate a click onlistview itself.
wctester andape succeed because they rewrite the clickable property of a leaf node i.e.
a b and c by that of its parent node i.e.
listview when the parent node is clickable .
informed by ec fastbot s developer located this weakness and fixed its strategy by following ape s. weaknesses in the event selection strategy .
most testing tools select events for execution by some heuristic strategy.
themis s clues helped reveal some design issues in the event selection which affect the bug finding abilities.
for example fastbot implements a clustering strategy to group similar widgets to reduce search space.
however as we illustrated in section .
this strategy may unexpectedly decrease the probability of executing the events in the group.
fastbot was affected by this strategy on bugs sn is one of them .
informed by epc mdandet execution times of events fastbot has fixed this strategy with careful design.
additionally ddroid s clues reveal that on the out of bugs some testing tools can cover all the pivot events of the bugtriggering traces i.e.
achieving ec but still miss these bugs.
informed by epcandmd we find that these tools fail to execute the pivot events in the right order.
for such tool weaknesses some tool developers plan to incorporate lightweight program analysis to improve the diversity of event selection.
other tool weaknesses .
based on themis s clues tool developers also found other tool weaknesses including failing to emulate the search event on the system keyboard or generate specific texts failing to interacting with external apps e.g.
camera file chooser setting and failing to support specific types of widgets or events e.g.
rotation and multi touch .esec fse december san francisco ca usa e. ma s. huang w. gang t. su j. wang h. liu g. pu and z. su table optimization results of wctester andfastbot .
tool missed actionable found improved wctester fastbot canthemis s clues improve the testing tools?
allthe tool developers explicitly stated that they would make tool enhancement based on the provided clues.
specifically the developers of two actively developing industrial testing tools wctester and fastbot have already made several improvements.
table shows the enhancement results of the two optimized tools.
column missed is the number of bugs missed by the original tools and actionable is the number of bugs for which the tool developers have devised actionable optimizations.
found is the number of newly found bugs among actionable and improved is the number of bugs which are still missed but their coverage values have been improved.
note that not all the missed bugs could lead to actionable optimizations see missed and actionable because some found tool weaknesses e.g.
failing to cover the pivot events in the right order are the open challenges .
in table we can see that wctester andfastbot have newly found and bugs respectively and have improved the chance of finding and bugs in terms of the three coverage metrics respectively.
it is clear that ddroid s clues have indeed helped improve these two tools.
note that the newly added optimizations are designed by developers in the general sense rather than overfitting specific bugs.
we follow the same evaluation setup in rq1 to assess the optimized tools.
how are the feedback of tool developers?
we conducted a semistructured interview with each of the seven tool developers.
during the interviews we solicited their feedback on the usefulness and usability of themis s clues.
to sum up allthe developers give high rates on themis s clues and appreciate that the visualized clues are intuitive for inspection.
in particular droidbot s developer commented i usually use droidbot s recorded ui trace graph to debug my tool but it is very time consuming for lengthy traces.
themis s clues are exactly what i want.
fastbot s developer commented themis smdmetric is very useful.
i can quickly know which events or screen pages i should focus on .
it can save me a lot of time.
wctester s developer commented i routinely improve my testing tool by adding new code.
but it is difficult to know how the new tool version works.
themis is nice as it can be used as a regression suite.
that s very useful.
ape s developer commented due to flakiness replaying the recorded event trace is very difficult.
i usually cannot find useful clues by manual analysis.
themis s clues helped me a lot .
.
results of rq3 from rq2 we know that the themis s clues are precise because no clues are contradictory with the manual analysis results of tool developers see table .
thus we used the clues computed by our automata based trace analysis approach as the ground truth and validated the precision of simple tc andsimple rv in identifying the first missed event in the bug triggering trace.
table gives the overall evaluation results the detailed results are provided in the supplementary material .
column tools lists the nine testing tools in our experiments.
column cases table the number of correct clues on the first missed event reported by simple tc simple rv and our approach.
tool simple tc simple rv themis cases wctester fastbot ape combodroid monkey stoat droidbot humanoid q testing total gives the total number of bugs missed by these tools according to the results of rq1 .
column simple tc and simple rv give the numbers of missed bugs for which simple tc and simple rv report the correct clues which are consistent with the results of our approach denoted by column themis respectively.
table shows that simple tc and simple rv achieve low precision in finding correct clues.
the precision of simple tc and simple rv ranges from .
.
computed by simple tc cases and simple rv cases per tool .
for example when analyzing the bugs missed by wctester simple tc and simple rv find correct clues for only and missed bugs respectively achieving .
and .
precision respectively.
we can see that simple tc and simple rv are error prone and unreliable.
it indicates that our approach is really needed and the three automata based metrics are useful.
.
discussion precision and completeness of the automata .
in our work given all the minimal bug triggering traces trs the bug automaton is precise and complete by construction.
moreover we empirically validated its precision we randomly generate random event traces from the automaton and allthe event traces reaching the final state indeed crashes the app.
thus all the automata are precise.
on the other hand if a bug automaton is complete the mdshould be when a tool triggers the bug i.e.
the logged event trace when the crash happens should be accepted by the automaton .
in table the symbol on the values of mddenotes that the bug was triggered by a tool at runtime.
we can see that among the tests running tools against automata for repeated runs only .
tests ab for monkey and ab for wctester fastbot apeandcombodroid fails the completeness.
it indicates the tools may find some bug triggering tracestrs which were not reported in themis thus not included in the bug automata .
when these traces are given the automata could be complete.
thus this is an orthogonal problem of our approach.
manual v.s.automated automaton construction .
in our work the bug automata are manually constructed for themis .
it is similar to manually writing program specifications in formal verification the bug automata can be viewed as the specification of undesired app behaviors .
in table column nfa sizes gives the sizes of the automata.
the minimal median and maximum number of automaton states and transitions are and and and respectively.
thus the complexity of bug automata is reasonable.
in our experience the construction effort ranges fromautomata based trace analysis for aiding diagnosing gui testing tools for android esec fse december san francisco ca usa figure a imprecise automaton b incomplete automaton minutes per automaton which is acceptable.
note that the construction is a one time effort themis is reusable for many different gui testing tools.
thus the benefits outweigh the effort.
although many automated algorithms exist in building finite state machine based gui models they are difficult to apply in our setting.
because defining one apply for all state abstraction criterion that fits all different apps is challenging .
as a result these algorithms are difficult to guarantee the automaton s precision and completeness which affects finding the right clues.
let us take scarletnotes s bug in figure as an example.
some state abstraction criteria e.g.
c lv3 c lv4 c lv5 defined in abstractsl2andl4into the same state s2 because the ui layouts of l2andl4are identical.
figure a shows the partial automaton under such criteria.
the automaton is imprecise because the trace reaching the final state is not a bug triggering trace.
on the other hand we know that c1can be executed on l1 to create new notebooks.
if the state abstraction is sensitive to the number of created notebooks e.g.
c lv4 c lv5 defined in a number of possibly infinite new states e.g.
s6 s7 will be included into the automaton shown in figure b .
it leads to an incomplete automaton.
additionally the incompleteness could also be caused by inadequate explorations of different bug triggering traces.
as a result we need extra manual efforts to validate and fix the automaton built by these algorithms.
coverage metrics .
the three coverage metrics ec epcandmd complement each other in finding the clues.
no one is the best.
for example when mdis0 i.e.
the bug is triggered ecandepcmay not reach .
because a bug may have multiple bug triggering traces and the tool may only cover one trace.
in this case ec or epccomplements mdin understanding tool effectiveness.
on the other hand a tool may achieve ecorepcbut may not achieve mdas0.
because the tool covers all the events but fails to cover them in the right order.
in this case mdcomplements ecorepc.
threats to validity .
the first threat is the representativeness of the bugs in themis .
we emphasize that themis s bugs are diverse collected from different apps nontrivial many bugs have long and complicated bug triggering traces and selected without explicit bias only selecting critical bugs labelled by app developers .
in the future we would consider non crashing bugs .
the second threat is that our study involves human factors e.g.
manually construct bug automata and letting the tool developers validate the clues from ddroid .
to counter this we empirically validated the precision and completeness of bug automata and the developers are required to follow our instructions to carefully validate the clues to mitigate possible biases and we cross checked the results.
related work analyzing gui testing tools for android .
to our knowledge little prior work exists in analyzing tool weaknesses based on toolmissed bugs.
for example some work only compares different testing tools or evaluates specific testing strategies in terms of the achieved app code coverage and the number of found app crashes.
they do not analyze potential tool weaknesses.
some work manually inspect the uncovered app code to analyze the tool weaknesses of failing to achieve high app code coverage.
vet uses two heuristic ui trace patterns to find the tool weaknesses in the form of ui exploration tarpits i.e.
a tool is trapped for an excessive amount of time within a small fraction of app functionalities .
however these work in general cannot help analyze tool missed bugs.
for example they can hardly help diagnose fastbot against the bug in figure .
because the bug does not have specific patterns of missed app code or ui exploration tarpits.
themis is the only close work.
but it can only manually analyze tool missed bugs to understand tool weaknesses.
our work improves themis by overcoming the difficulties of manual analysis.
runtime verification and automata based trace analysis .
runtime verification rv can help find un desired behaviors of the system under test .
the typical realization of rv is using a monitor e.g.
an automaton synthesized from some system specification to analyze the system s execution trace .
for example some work adapts the idea of rv to analyze system kernel traces or debug specification violations .
at the high level our approach can be also viewed as the adaption of rv as the bug automaton is one form of program specifications.
however applying existing rv techniques for android in our setting is difficult.
because existing rv techniques focus on verifying generic appagnostic properties e.g.
good programming practices and security policies which are manually described in temporal logic in terms of specific program apis .
however we concern app specific bugs involving diverse set of apis which are difficult to be captured by generic properties and thus difficult to automatically synthesize the monitor like the bug automaton in our approach .
the tools of these relevant work are not available for comparison.
ava uses a finite state automaton to represent the successful executions of a target system and use this machine to analyze the failing executions.
ava uses the deviated events from the failing executions to interpret why the system fails.
different from ava our approach uses the three different coverage metrics on the automaton itself to interpret why a target bug is missed.
conclusion in this paper we introduce an automata based trace analysis approach to tackling the challenge of manual trace analysis.
our approach can improve the classic benchmarking by providing the clues of tool weaknesses on the missed bugs.
the evaluation confirms the feasibility and usefulness of our approach.
our work opens up a new perspective of analyzing the weaknesses of testing tools.