usinggraph neural networks for program termination yoav alon yoav.alon bristol.ac.uk university ofbristol bristol ukcristina david cristina.david bristol.ac.uk university ofbristol bristol uk abstract termination analyses investigate the termination behaviorof programs intendingtodetectnontermination whichisknowntocause avarietyofprogrambugs e.g.hangingprograms denial of service vulnerabilities .
beyond formal approaches variousattemptshave been made to estimate the termination behavior of programs using neural networks.
however the majority of these approaches continue to rely onformal methods to provide strong soundness guarantees and consequently suffer from similar limitations.
in thispaper wemoveawayfromformalmethodsandembracethe stochasticnatureofmachinelearningmodels.insteadofaimingfor rigorousguaranteesthatcanbeinterpretedbysolvers ourobjective is to provide an estimation of a program s termination behavior and of the likely reason for nontermination when applicable that aprogrammercanusefordebuggingpurposes.comparedtoprevious approaches using neural networks for program termination we also take advantage of the graph representation of programs by employing graph neural networks.
to further assist programmers inunderstandinganddebuggingnonterminationbugs weadaptthe notionsofattentionandsemanticsegmentation previouslyusedfor other application domains toprograms.
overall we designedand implementedclassifiersforprogramterminationbasedongraph convolutionalnetworksandgraphattentionnetworks aswellas asemanticsegmentationgraphneuralnetworkthatlocalizesast nodes likely to cause nontermination.
we also illustrated how the informationprovidedbysemanticsegmentationcanbecombined with program slicingtofurtheraid debugging.
ccsconcepts computing methodologies artificial intelligence knowledge representation and reasoning neural networks .
keywords graph neural networks graph attention networks program termination program nontermination acm reference format yoav alon and cristina david.
.
using graph neural networks for program termination.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november14 18 singapore singapore.
acm newyork ny usa 12pages.
esec fse november 14 18 singapore singapore copyright heldby theowner author s .
acm isbn .
introduction termination analysis describes a classical decision problem in computabilitytheorywhereprogramterminationhastobedetermined.
it iscriticalfor many applicationssuch assoftware testing where nonterminatingprogramswillleadtoinfiniteexecutions.asproved by turing in a general algorithm that solves the termination problem for all possible program input pairs doesn t exist .
while there are a large number of works on termination analysis the majority of them employ formal symbolic reasoning .
in recent years various attempts have beenmadetoestimateterminationbehaviorusingneuralnetworks.
for instance giacobbe et al.
introduced an approach where neural networks are trained as ranking functions i.e.
monotone mapsfromtheprogram sstatespacetowell orderedsets .asimilar idea is employed in where abate et al.
use a neural network to fitrankingsupermartingale rms overexecutiontraces.giventhat programanalysistaskssuchasterminationanalysisaregenerally expected to provide formal guarantees these works use satisfiabilitymodulotheories smt solverstoshowthevalidityoftheir results.whilepromising theystillfacelimitationsspecifictoformal symbolic methods.
namely programs need to be translated to asymbolicrepresentationtogeneratetheverificationconditions thatarethenpassedtothesolver.additionally theseverification conditionsmaybeexpressedinundecidablelogicalfragmentsor may require extraprogram invariants for the proof to succeed.
in this paper we move away from formal methods and lean into thestochasticnatureofmachinelearningmodels.insteadoflooking forrigorousformalguaranteesthatcanbeinterpretedbysolvers ourobjectiveistoprovideanestimationofaprogram sterminationbehavior as well as localizing the likely cause of nontermination when applicable thataprogrammercanusefordebuggingpurposes.our workalsoservesasastudyoftheapplicabilityofmachinelearning techniques previously used for other classes of applications to programanalysis.inparticular asexplainednext weusegraphneural networks gnns and graph attention networks gats .
insteadoflookingatexecutiontracesliketheaforementioned works we are interested in using the source code with the assumption that it contains patterns that can assist in understanding its functionality.notably programanalysistechniquesgenerallywork on source code and specifically on graph representations of programs.toemulatethisformachinelearning wemakeuseofgnns which are a class of neural networks optimized to perform variousanalysesongraph structureddata.gnnsaregainingalotof interest as they are being used to analyze graph based systems denotingsocialnetworks physicalsystems knowledge graphs point cloud classification etc.
additionally gnns have recently been applied toprogram analysistaskssuchasvariablemisusedetectionandtypeinference andself supervised bugdetection andrepair .
thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse november14 18 singapore singapore yoav alon andcristina david inspiredby weusegnnstoestimateprogramtermination.
our baseline program termination classifier is based on graph convolutionalnetworks gcn .
onitsown estimatingaprogram sterminationbehaviordoesn t provide a lot of practical help to a programmer interested in understandinganddebugginganonterminationbug.rather wewould like to provide additional information such as the code location corresponding to the likely root cause of the failure in our case nontermination .thisobjectiveissimilartothatoffaultlocalization which takes as input a set of failing and passing test cases andproduces arankedlistofpotentialcauses offailure .
as opposed to fault localization techniques we are interested ininvestigatingusingthemechanismsofattentionandsemantic segmentationfrommachinelearning.tothebestofourknowledge we are the first ones to use attention and segmentation in the contextofprograms.
attentionisatechniquethatmimicscognitiveattention.intuitively it enhances some parts of the input data while diminishing otherswiththeexpectationthatthenetworkisfocusingonasmall butimportantpartofthedata.inourcontext weuseattentionto getanintuitionabouttheinstructionsrelevantfortheestimation of the termination behavior.
this allows us to visualize those parts of the program that the neural network focuses on to estimate its termination behavior.
to integrate attention in our work we buildanotherprogramterminationclassifierinspiredbythegraph attention network gat architecture described in .
given thevariedinfluencethatdifferentinstructionsinaprogramhave on its termination behavior we also expect the attention mechanism to improve the results of classification when compared to the gcn basedbaseline.
tolocalizethelikelycauseofthenonterminationbehavior we use semantic segmentation.
usually used in image recognition thegoalofsemanticimagesegmentationistolabeleachpixelof animagewithaparticularclass allowingonetoidentifyobjects belonging to that class e.g.
given an image one can identify a catthatappearsinit .inourwork weusethesameprinciplefor programstoidentifythosestatementsthatcausenontermination vs thosethat don t. to further aid debugging we also show how to use the informationprovidedbysemanticsegmentationtocarveoutanonterminatingslicefromtheoriginalprogram i.e.asmallersubprogram exhibitingthesamenonterminationbehaviorastheoriginalprogram .intuitively suchasmallerprogramiseasiertounderstand anddebug.
our experimental evaluation for multiple datasets both custom and based on benchmarks from software verification competitions confirmsahighabilitytogeneralizelearnedmodelstounknown programs.
the main contributionsofthis researchare as follows wedesignedandimplementedagcn basedarchitecturefor the binary classification ofprogram termination.
wedesignedandimplementedagat basedarchitecturethat improvestheterminationclassificationusingaself attention mechanism and allows visualization of the nodes relevant when estimating termination.
wedesignedandimplementedasemanticsegmentationgat thatlocalizesnodescausingnontermination.inparticular inthiswork wetrytolocalizetheoutermostinfinitelylooping constructs.we illustrate how the informationprovided bysemanticsegmentationcanbecombinedwithprogram slicing to further aid debugging.
wedeviseddatasetsforbothclassificationandsegmentation ofprogram termination.
preliminaries on graph neural networks gnns are an effort to apply deep learning to non euclidean data representedasgraphs.thesenetworkshaverecentlygainedalotof interest as they are being used to analyze graph based systems .acomprehensivedescriptionofexistingapproachesfor gnns andtheirapplicationscan be foundin .
thebasicideabehindmostgnnarchitecturesisgraphconvolution or message passing which is adapted from convolutional neural networks cnn .
each vertex node in the graph has a set of attributes which we refer to as a feature vector or an embedding.then a graphconvolution estimatesthe features ofa graph nodeinthenext layerasafunctionofthe neighbors features.
by stackinggnnlayerstogether anodecaneventuallyincorporate information from other nodes further away.
forinstance after n layers a node has information about the nodes nsteps away from it.
givenagraph g v e wherevdenotesthesetofverticesand erepresentsthesetofedges messagepassingworksasfollows.for each node i vand its embedding h l iat layerl the embeddings h l jof its neighbours j n i are aggregated and the current node s embedding is updated to h l iusing aggregation function aandupdatefunction u h l i u l h l i a l h l j j n i we usen i to describe the set of direct neighbors of i. each gnnarchitecturevariestheimplementationoftheupdate uand aggregation afunctions used for message passing.
in this paper we make useof two gnn architectures graph convolutional networks gcns and graphattention networks gats which we willdiscuss insubsequent sections.
description ofourtechnique inthissection weprovidedetailsaboutourtechnique.atahigh level we first convert programs to feature graphs and then feed themintoagnn.wewilldescribeeachstep includingthestructure ofthe modelsforterminationclassificationandsemanticsegmentation ofthe nodes responsible for nontermination.
.
generation offeaturegraphs therearemanygraphrepresentationsofaprogram.inthiswork we startfrom the abstract syntaxtree ast whichisahomogenous undirectedgraph whereeachnodedenotesaconstructoccurring in the program.
we picked asts as the starting point because theyaresimpletounderstandandconstructwhilecontainingallthe necessaryinformationforinvestigatingaprogram stermination.
911usinggraphneural networksforprogram termination esec fse november14 18 singapore singapore a b c d figure1 a ast forclarity wepointoutthecorrespondinglinesofcode .
b visualizationofattentionforbinaryclassification asedgecolorfromblueforlowattentiontoredforhighattention.theedgeswithhighattentionarethoseconnectingthe while nodestotheirloopguards.amongthese thehighestattentionisgiventothenonterminatingouterloop.
c resultofsemantic segmentation where the node corresponding to the outermost infinite loop is coloured in red.
d semantic segmentation visualized togetherwith theattentionextracted fromthesegmentationnetwork.
best viewed incolor.
asfuturework wemayconsiderdifferentgraphrepresentations ofprograms.
we start by generating the ast of each program in our datasets.
then allastsareconvertedinto featuregraphs byconvertingeach nodetoalocalfeaturevectorinlocalrepresentationor one hot encoding.
for this purpose all the nodes in the asts generated for eachdatasetaregatheredinadictionary.thedictionaryholdseach encoding as the key withthe instructionas the value.
running example.
for illustration we refer to our running example in figure which contains three loops.
out of them the firstouterloopisnonterminatingforanyinitialvalueof blessthan and ofagreater than b. note that for machine integers i.e.
bitvectors theinnerloopdoesterminateevenwhenstartingfroma cgreater than dbecausedwill eventually underflow thus triggering awrap around behavior.
therunningexampleisfirstconvertedtoitsastasshownin figure1 a .
then the ast is converted to the initial feature graph before any convolutions .
for reasons of space we don t show the wholefeaturegraph butratherjustthesubgraphcorresponding to the instructions while c d d in figure .
the dictionary maps each node in the ast to a one hot encoding.
for instance thewhilenodeismapped to .similarly both nodes correspondingtovariable dhavethesameencoding .
as we use the same dictionary for a whole dataset identical nodes inthe dataset willhave the same encoding.
our objective when picking asts to represent programs is to reduce the distance between nodes of interest e.g.
loop guards 912esec fse november14 18 singapore singapore yoav alon andcristina david figure2 conversionofanasttoalocalfeaturegraph using one hot encoding .
def main a b c d 3whilea b a whilec d d 8whilea a figure runningexample instructionsaffectingvariablesintheloopguards thusenabling gnns to aggregatefeatures relevant to termination with a small numberofmessagepassings.forinstance infigure a thedistance between the three loops and the instructions modifying variableashared between two of the loops is small allowing the gnn toeasilyaggregatetheirfeatures.atthesametime theastrepresentationissimpleenoughtofacilitatetheeasyunderstanding of the visualisation of those parts of the program potentially responsible for nontermination figure 1b c d .
understanding the decisions and warnings raised by program analyses is important to programmerswhoneedtoeithermarkthemasfalsepositivesor debug them as shownlaterinthe paper.
.
binaryclassification ofterminationbased on gcn for our first attempt at a binary classifier for program termination we make use of gcn.
in particular we take inspiration from the architecture in in a supervised training setting.
currently we donottrainonrecursiveprograms wherethenonterminationcould becausedbyinfiniterecursion.weplantodothisasfuturework.
for now our datasets contain programs with potentially infinite loops.
ourterminationclassificationisagraph leveltask wherewe predictanattributefortheentiregraph inourcasethetermination behavior of the corresponding program by aggregating all node features.
in therestofthissection we provide anoverview ofthe architecturein followedbyexplaininghowweadaptitforour task.as explained insection gnnsuse message passing to aggregatetheinformationaboutanode sneighborstoupdatethenode s value.according to we use h l i b l j n i cijh l jw l wherecij n i n j n i describes the set of direct neighbours of i and isthe activation function.
best results are achieved using a rectified linear unit relu activationfunction.the weights w l are initializedusing glorot uniform and the bias bwith zero.
the experiments of with other aggregation operators rather than the one used here such as multi layer perceptron aggregator and graph attention networks achievedsimilar results.
for the architecture in the graph convolution layers generate node wise features.
in our architecture given that termination classificationisagraph leveltask wechoosetomeanallthenodes once graph convolution is done and pass the resulting mean featurevectorthroughthreefullyconnectedlayers.then weapply a softmax function on the resulting two dimensional vector to achieve a binary estimation.
the resulting vector is then optimized bycomputing the cross entropy losswiththe ground truth.
runningexample.
forourrunningexampleinfigure ourclassifiercorrectlyconcludesthattheprogramisnonterminating.as aforementioned we operate in a supervised training setting meaning that our classifier has already been trained on the required dataset.
we will give more details on the training and testing in section4.
.
binaryclassification ofterminationbased on gat ifwerevisittheupdateruleofgcnsgivenbyequation ituses the coefficient1 cij n i n j .
intuitively this coefficient suggests the importance of node j s features for node iand it is heavily dependent onthe structure ofthe graph i.e.each node s neighbours .the main idea of gats is tocompute thiscoefficient implicitly rather than explicitly as gcns do by considering it to be a learnable attention mechanism.
in the rest of the section we refer to this coefficient as the attentionscore .
themainobservationthatledustousegatsisthefactthatnot allinstructionsinaprogramareequallyimportantwheninvestigatingitsterminationbehavior.forinstance loopstendtobemore important than straight line code and should therefore be given increasedattention.thus ourtasknaturallylendsitselftousing the attention mechanism.
moreover our intention for this work is not only to design a program termination classifier but also to gain insights into its decisions.
using gats helps us in this direction asitallowsustovisualizethosenodesthatinfluencethedecision relatedto termination.
inthissection weprovideanoverviewofthearchitecturein as wellas explainthe wayinwhich we adapt it for the termination classificationtask.westartbyshowinghowtheaggregationand update of node features h l itoh l ifor iteration lis derived.
initially eachnode feature is passedthroughasimplelinearlayer bymultiplyingthe feature withalearnableweightmatrix w l 913usinggraphneural networksforprogram termination esec fse november14 18 singapore singapore figure4 gat basedarchitectureforthebinaryclassification ofprogram termination.
z l i w l h l i toobtainthepair wiseimportance e l ijoftwoneighborfeatures h l iandh l jweconcatenatethepreviouslycomputedembeddings z l iandz l jto z l i z l j .then theconcatenatedembeddingsare fedintoanotherlinearlayerwithweightmatrix w l 2thatlearnsthe attention scale a l .
additionally a leakyrelu activation function isappliedto introduce non linearity a l leakyrelu w l z l i z l j aftercomputationof a correspondingtotheactivationscale the pair wiseimportance e l ijoftwoneighborfeatures is e l ij leakyrelu veca l t z l i z l j as noted by the resulting attention scale can be considered asedgedata.inourproblemsetting thisgivesusinsightintothe importance of two connected syntax tree nodes with respect to the program s termination.
to normalize the attention scores for all incoming edges we apply asoftmax layer l ij exp e l ij summationtext.
k n i exp e l ik then theneighborembeddingsareaggregatedandscaledbythe final attention scores l ij h l i arenleftt a arenleftexa arenleftbta j n i l ijz l j arenrightt a arenrightexa arenrightbta thearchitectureofourgat basedterminationclassifierisshown in figure 4and includes multiple connected gat layers with relu activation.
graph convolutions extract features to an intermediate representation denotedbythelastgatlayerwithreluactivation.
asopposedtotheoriginalarchitecturein classifyingprogram termination is a graph leveltask.
consequently we deviseda prediction on the program s termination by extracting the mean of node features and using dense layers with a final softmax layer.
additionally we need to associate attention information to eachastedge asshowninfigure b .consequently asopposedto the original architecture attention is extracted based on the last graph attention layer such that itcan be attributedto ast edges.
runningexample.
similartothegcn thegat based classifier identifiesthattherunningexampleisnonterminating.additionally in figure b we also obtain a visualization of edge wise attention fromblueforlowattentiontoredforhighattention.notably the edges with high attention are those connecting the whilenodes to their loop guards.
the extraction of attention gives us an intuition about which nodes have a high influence on the final prediction provided by the classifier.
although for this example the edge connecting the nonterminating outer loop with its loop guard gets the highestattention thedifferencebetweentheattentionscoreslinked to the three loops is not reliable enough to differentiate between the three loopsanddeterminethe cause for nontermination.
.
semantic segmentation ofnodescausing nontermination while gats provide some insight into the decision made by the classifier by allowing us to visualize those ast nodes influencing thedecision wewanttofindthelikelycauseofnontermination.
inparticular inthisworkwetrytolocalizetheoutermostinfinitely looping constructs.
note that in the case where we have several nested loops and the outer one is infinitely looping all of the inner ones will also be visited infinitely often.
in such a situation we identifytheouterloopasthelikelycauseofnontermination i.e.the outermostinfinitelyvisitedloop.asaforementioned fornow wedo nottrainonrecursive programs wherethenonterminationcould becausedbyinfiniterecursion.thus inoursettingnontermination can only be causedbyinfinite loops.
identifyingtheoutermostinfinitelyvisitedloopisnotpossible with the information that we gained up until this point.
although infigure b theedgebetweenthenonterminatingouterloopand its guard gets the highest attention the edges connecting the other twoloopsandtheirguardsalsohavehighattentionscores.thus itishardtodifferentiatebetweenthedifferentloopsbasedonthe attention score alone.
toachieveourgoalofidentifyingtheoutermostinfinitelyvisited loop we make use of semantic segmentation.
semantic segmentationhasbeenprimarilyusedinimagerecognitiontolabeleachpixel ofanimagewithaparticularclass allowingonetoidentifyobjects belongingtothatclass.sometimesintheliterature thesemantic segmentation of graphs is also named node wise classification or node classification .
inthiswork weattempttoextendthesameprinciplethatwas appliedinimagerecognitiontoprogramsbyusingsemanticsegmentation to identify thosestatements thatcausenontermination.
if we consider that program statementscan belong to two classes namelythosethatcausenonterminationandthosethatdon t we can envisionthat segmentationmayhelpidentifythe former.
thegraphnetworkmustconvertfeaturevectorstoanestimation.
forthispurpose itistrainedonthelossbetweengroundtruthand prediction on a node level.
this can be accomplished with most segmentation loss functions such as a simple cross entropy loss 914esec fse november14 18 singapore singapore yoav alon andcristina david figure proposed gat based architecture for the segmentation ofnodes causing nontermination.
for binary segmentationwithground truth yandprediction y l y y ylog y y log y althoughtheabovelossenablesthesetupofaninitialtraining session it is not ideal to eradicate hard negatives.
to improve andcontinuetrainingbeyondbinarycross entropyconvergence we use focal loss an extension of cross entropy which down weights simple samples and gives additional weight to hard negatives fl pt t pt log pt with andthemodulationfact pt wherept p fory 1andpt potherwise.
followingthesamemessagepassingmethodexplainedinsection3.
the newnode features holdthe segmentationprediction.
our architecture for semanticsegmentation is visualized in figure5.graphattentionconvolutiongeneratesasemanticsegmentation resulting in a node wise estimation of likelihood to cause nontermination.
by the attention mechanism edge wise scores denotethe weightoffeatures basedonrelationalpatterns.
runningexample.
theresultofsemanticsegmentationforour running example is highlighted in figure c where the redcolorednodedenotestheoutermostinfiniteloop.figure d shows the result ofsegmentationwithattention.
.
usingtheresultofsegmentation for debugging one issue with the result of segmentation is that it only highlights the node corresponding to the head of the outermost nonterminating loop rather than all the statementscontributing to nontermination.thisisobviousforourrunningexampleinfigure c where segmentation only identifies the node corresponding to the nonterminating whileloop.
thiswasaconsciousdecisiononourpart meanttosimplifythe construction of the required training datasets.
while there are a hugenumberofprogramsavailableonline it snotatallstraightforward to use them for training machine learning based techniques forprogramanalysis.themainreasonforthisisthatsuchprograms are not labeled with the result of any analysis and manually labeling them is difficult and error prone.
in particular for the programtermination classification task we must know which programs are terminating and which are not.
even more problematic for semanticsegmentation theannotationshould inprinciple identifyall theinstructionscontributingtonontermination.forinstance for our running example it should identify lines and .
in general it can be very difficult to annotate such datasets.
in this work to simplify the annotation for segmentation we only label the node corresponding to the head of the outermost infinite loop as the reason for nontermination.
more details on the waywe generateour datasets are given insection .
whileidentifyingtheoutermostinfiniteloopisalreadyuseful debuggingcanbefurtheraidedbyfindingtherestoftheinstructions that contribute to nontermination.
this can be achieved with slicing .ingeneral slicingisaprogramanalysistechniquethat aims to extract parts of a program according to a particular slicing criterion e.g.itcanextractinstructionsresponsibleforthewrite accessestoaparticularvariable .inourscenario givenanonterminatingloopreturnedbysegmentation wecanuseseveralslicing criteria in order to isolate the faulty loop while preserving the nonterminationbehaviorhighlightedbysegmentation.oneoption whichweillustratebelowontherunningexampleinfigure is a criterion aiming to preserve the reachability of a control flow point inparticular weareinterestedinthecontrolflowpointat the entrance to the nonterminating loop .
among other tools such aslicingcapabilityisprovidedbythesoftwareanalysisplatform frama c .
other slicing criteria can be used e.g.
preserving the valuesofthevariablesusedbytheguardoftheinfiniteloop.slicing has been previously used in the context of debugging nontermination e.g.failure slices in .
going back to our running example in figure once segmentation has identified the first outer loop as the likely culprit for nontermination we slice the program such that we preserve the reachabilityofthecontrolflowpointattheentrancetothebody of the nonterminating outer loop.
this gives us the program in figure6.
this sliced program preserves the nontermination behavioroftheoriginalprogram inducedbytheloopidentifiedby segmentation whilecuttingdownthesyntacticstructure which correspondsto reducing its state space.
intuitively cuttingdownthestatespaceoftheprogrammakes itmoreamenabletootherexistingdebuggingtechniquessuchas fuzzing.for illustration we have askedlibfuzzer alibrary for coverage guidedfuzztesting tosearchforaninputthattriggers the execution of the sliced program longer than 5s.
libfuzzer was abletofindtheinput a 165017090and b 183891446within6s.
thisinputtriggersthenonterminationbehaviorandcanassistin further debugging.
conversely when asking libfuzzer to find such input for the original in figure it required 12s.
we expect the difference between the two running times to increase for larger programs.
overall thedebuggingworkflowthatweenvisionfollowsthe followingsteps ourclassifierestimatesthataprogramisnonterminating ourtechniquebasedongnnsandsemanticsegmentation finds the likely root cause of nontermination slicing cuts down the syntactic structure of the program and implicitly its state space while aiming to maintain the faulty nontermination behavior fuzzingfinds faultyinputsfor the slicedprogram.
915usinggraphneural networksforprogram termination esec fse november14 18 singapore singapore def main a b 2whilea b a figure nonterminatingslice oftherunningexample ifaprogramcontainsseveralnonterminationbugs thedebugging workflow above may need to be repeated until all of the bugs are eliminated.
experiments .
system weimplementedtheproposedideasasatoolcalled graphterm .
we used the deep learning framework pytorch with the gnn packagedgl deepgraphlibrary andvariouspackagesforpreprocessing and visualization.
the experiments were performed using an rtx 8gb gpu on a local machine.
the architecture ofthegnnsused dependingontheparticularexperimentconsists of between and graph layers.
while this is typical for the effectivetrainingofgnns itcouldbeextendedusingresidualblocks .
.
binaryclassification ofprogram termination we startbydiscussing ourexperimentalevaluationfor thebinary classification of program termination.
given a program graphterm s classifier extracts a termination estimation i.e.
whether the programisterminatingornonterminating.concerningthelatter a programisconsiderednonterminatingifthereexistsaninputfor whichthe program s executionisinfinite.
.
.
classification datasets.
our experiments on classification and semantic segmentation of the cause for nontermination require different datasets.
this is because classification data requires program level annotations a program is labeled as terminating or nonterminating whereassegmentation requiresnode wiseannotations each node in the ast is either a cause for nontermination ornot .inthissection wediscussthedatasetsusedforclassification whereasmoredetailsonthoseusedforsegmentationcanbe foundinsection .
.
.
forclassification weuseatotaloffourdatasets outofwhich two are based on existing benchmarks and two are generated by us.concerningthedatasetscontainingexistingbenchmarks the first one ds sv comp contains c programs from the termination category of the sv comp competition .
in particular thisdatasetcontainsallprogramsfromthefollowingsubcategories termination crafted termination crafted lit termination numeric andtermination restricted .intotal thereare55nonterminating programs and terminating.
for a total of programs we count a total of loops with a maximum of five nested loops for the program no 04.c.
the second dataset of existing benchmarks ds term comp contains 150cprograms fromthetermination competition with a total of loops and a maximum of nestedloopsperprogram.thesebenchmarksareselectedsuchthat there isnooverlap withthoseinthe ds sv compdataset.the benchmarks in both ds sv comp and ds term comp expect inputs meaning that their termination behavior depends on non deterministic values.
these benchmarks come already labeled as terminating or nonterminating where the nontermination label indicates that there exists an input for which the program s execution doesn t terminate.
the pre processing of the datasets includes thegenerationoftheastrepresentationforeachprogramandthe conversionto afeature graph using dgl.
forefficiencyreasons weconsiderbatchesof30graphsusingthe dgl.batch .thetwodatasetshaveanassociateddictionaryfeaturing allthedistinctastnodesfromalltheprograms.intuitively this dictionary gives us the vocabulary used by the benchmarks.
for more details onthe generationof feature graphs see section .
.
aspreviouslydiscussedinsection .
oneofthemainchallenges ofourworkisthefactthat whiletherearemanyavailableprograms onlyafewarealreadylabeledasterminatingornonterminating as wecouldseeabove evenexistingsoftwareverificationcompetitions only consider a relatively small number of benchmarks .
however machine learning techniques generally require large training data whichis notavailablein oursetting.due tothis wechoseto also generateouradditionalcustomdatasets.forthefirstcustomdataset ds1 weusedthedictionarygeneratedfords sv companddsterm comp meaningthatthevocabularyofds1isthesameas thatofds sv compandds term comp.thisisimportantas it allows us to train on ds1 and only use ds sv comp and dsterm compfortesting.ds1contains950cprograms.thesecond custom dataset ds2 contains generated python programs out ofwhich800are usedfor training and150for testing.
tolabelthebenchmarksinds1andds2 wefuzztestfornontermination by generating a large number of inputs for each program.
iftheexecution timereachesa predefined timeout for at least one oftheinputs thenwelabeltheprogramasnonterminating.asa remark while this doesn t ensure that a program is indeed nonterminating it does signal a potential performance bug.
from a practical point of view a performance bug is equally important for programmers to debug.
alternatively we could use an existing termination analysis tool based on formal methods to label the generated programs.
the dataset is balanced by providing an equal number ofterminating andnonterminatingsamples.
apart from ds sv comp and ds term comp each dataset is splitintotrainingandtestsamplesbythegeneralruleofapproximately80 20dependingonthedatasetsize.theexactsplitforeach setisspecifiedintable .
.
.
training.
as explained in section we deploy two different neuralnetworks onebasedongcn andanotheronebasedon gat .inourexperiments wefoundthat4layersweresufficient for both the gcn and gat architectures respectively.
training is performedusinganadam optimizerwithaninitiallearningrate of .
.
we use a regular cross entropy loss function and record variousmetricssuchasthereceiveroperatingcharacteristic roc curve andtheprecision recall duringtraining both forvalidation andtesting.
for asignificant evaluation we perform a total of ten training sessions where each session is stopped as soon as the validationerrorreachesaminimum.inordertoavoidoverfitting we use the following techniques 916esec fse november14 18 singapore singapore yoav alon andcristina david table datasetsused inthiswork.
dataset origin language experiment training samples testsamples ds sv comp sv comp c classification ds term comp termcomp c classification ds1 custom c classification ds2 custom python classification ds seg py1 custom python semantic segmentation ds seg c custom c semantic segmentation amodelwithtoomuchcapacitycanlearnaproblemtoowell andoverfitthetrainingdataset.weavoidthisbyadapting the number and dimensionality of layers to simplify the model.
in particular asaforementioned we use4layers for the gcnandgatarchitectures respectively.
anetwork withlargevaluesforweightscanalsosignalthat thenetworkhasoverfitthetrainingdataset.weavoidthis byusingweightregularization whichinvolvesupdatingthe learning algorithm to encourage the network to keep the weightssmall.
anotherchallengeistheamountoftimespentontraining neuralnetworks wheretoomuchtrainingwilloverfitthe training dataset.
when training our model we use early stopping by stopping training as soon as the validation errorreaches aminimum.
additionally we use cross validation where we sample different portions of the data to train and test a model in different iterations.
therefore the modelmay performwell in sometrainingiterations butworseonothers.theseinsights allowedus totunethehyperparametersforthenetworkin order to achieve optimal training andreduce overfitting.
.
.
results and evaluation.
to judge the performance of graphterm s classifier we use the precision recall pr and receiver operating characteristic roc and their respective area under the curve auc and average precision ap .
auc takes values from to where value indicates a perfectly inaccurate test and 1reflectsaperfectlyaccuratetest.
pr is used as an indication for the tradeoff between precision andrecall fordifferentthresholds.consequently highrecall and highprecisionreflectinahighareaunderthecurve.highprecision isbasedonalowrateoffalsepositivesandahighrecallisbased onalowrateoffalse negatives.
the roc curve plots the true positive rate versus the false positiverateforeachthreshold.theclosertothetop leftcorneraroc curveis thebetter andthediagonallinecorrespondstorandom guessing.
a high area under the curve indicates a better classificationperformance.
theresultsoftheexperimentalevaluationofourclassifierare summarized in table where we record the auc for positive terminating instances the auc for negative nonterminating instances andthemeanaverageprecision map whichiscalculated as the average of the ap for terminating and nonterminating instances.
the first and arguably most important observation is that with values of over .
for all maps for both pr and roc we can conclude that classification results are significant and that we achieve ahigh abilityto generalize for unknowndata.
thesecondremarkreferstothecomparisonofgcnandgat.
asobservedintable theprandrocmapnumbersaregenerally higherforgatthangcn suggestingthattheapplicationofgraph attention improves the result of the binary classification we use bold font for the higher map numbers .
this indicates that the appliedself attention mechanismdoes assign a higherweightto patterns responsible for deciding the terminationbehavior.
the third comparison we are interested in is between the classificationofterminatingandnonterminatingprograms.wenotice that the pr auc negative tends to be higher than auc positive meaningthattheclassifierperformsbetteratidentifyingnonterminating programs.
we conjecture that nonterminating patterns are easier to identify based on relational probabilistic patterns that are likely to cause nontermination.
infigure 7wevisualizetheroccurvesperclassandlayertype.
thebetterperformancefornonterminationclassesisreflectedin higher red curves while the better performance of gat layers is represented by higher continuous lines compared to doted gcn curves.
.
semantic segmentation ofnodescausing nontermination inthissection wedescribethe graphterm experimentsforidentifying the outermost nonterminating node using semantic segmentation.
.
.
segmentationdatasets.
thereareseveralreasonsthatprevent us from reusing the same benchmarks from ds sv comp and ds term comp for semantic segmentation.
firstly they only containaprogram levellabeldenotingwhethertheprogramterminates.
for semantic segmentation we require node level annotationsthatindicatewhetherthecorrespondingastnodeislikely tocausenonterminationornot.secondly itisoftenverydifficult to determine the cause for nontermination in existing benchmarks as itisn tlabeled.
917usinggraphneural networksforprogram termination esec fse november14 18 singapore singapore figure roc curves per class class corresponds to nonterminatingprogramsandclass1toterminatingones and experimentalconfiguration.
for this reason we generate two datasets that consist only of nonterminating samples as we would only attempt to find the cause for nontermination for those programs that were deemed nonterminatingbyourclassifier .asexplainedinsection .
we restrictourannotationstoonlyidentifytheoutermostnonterminatingloopintheprogram.notethatifthereareseveraldistinct outermost nonterminating loops only one of them will be labeled ascausingnontermination.thereasonforthisisthefactthatwe use fuzz testing to determine the culpable loop and we stop the fuzzingoncewefoundthefirstnonterminatingscenario.weuse label to indicate that the corresponding ast node has no relation to nontermination and label to indicate a high likelihood of causing nontermination.
thetwodatasetsaredescribedinthelasttworowsoftable .
thefirst ds segpy1containspythonprograms whereasthesecond ds seg c contains c programs.
each dataset is composed of 180programsfortrainingand50fortesting.theprogramsample generator for custom data is probabilistic and generates programs that contain from twoto five loopsthat can be nested.
.
.
training.
toimproveandcontinuetrainingbeyondbinary cross entropy convergence wedeployedafocalloss an extensionofcross entropythatdown weightssimplesamplesand gives additionalweightto hardnegatives.
to achieve optimal results the experiments required modifying the learning rate optimizer network structure and the number of features compared to those used for classification.
the final configurationfeaturesanadamoptimizerwithalearningrateof0.
.
in our experiments we found that layers were sufficient for both thegcn andgatarchitectures respectively.while thegcn and gatarchitecturesusedforsemanticsegmentationarerespectively similarto thoseused for classification they donot contain a node mean function and linear layers.
the last gcn gat layer projects to a one dimensional feature indicating the confidence that a node iscausing nontermination.similar to the experiments for classification we perform a total of ten training sessions and use the same techniques for avoiding overfitting.
.
.
resultsandevaluation.
themetricsused forevaluationare jaccardlossbasedonintersectionoverunion iou therelateddicecoefficient andnode wiseaccuracy.acomprehensiveoverview ofthese metricsis providedby .similar toimage segmentation wherethe pixel wiseaccuracyisdetermined here the node wise distinction of true positives true negatives false positives and false negatives for the node prediction compared to the annotated node ground truthisused.
we calculate the mean metrics for an average of models that were trained to a convergence of validation scores for each experimentalconfiguration.intheevaluation weprovidemeanvalues withstandarddeviation.
from the results for semantic segmentation detailed in table we can infer several observations.
with mean values of more than .84fordiceandiouandanodeaccuracyof0.81inallexperimental validationsets theabilitytogeneralizeforunknownprogramsis high.thelowstandarddeviationindicatesarobustresultforthe state ofconvergenceofvalidation metrics.
for datasetseg py1themeandice coefficientishigherforthe gatarchitectureby3.3percentthanforgcn.similarly nodeaccuracyisbetterby4.7percentwithonlyadecreaseof0.8percent for the iou.
this indicates an improvement in segmentation performance for gat compared to gcn.
similarly for c programs using dataset seg c all metrics improved significantly with a percent better performance for dice coefficient and node accuracy and an increase of percent for iou.
from the comparative better performanceofgat layersfornode wisesegmentation weinfer thattheuseofself attentionmechanismsenablesthenetworkto weigh more relevant node transitions and therefore achieve better performance.
higher attention is assigned to edges with a high likelihoodofcausing nontermination.
.
comparisonto recurrentalgorithms in section we discussed the suitability of gnns for analyzing programs given the inherent graph structure of programs.
the objectiveofthecurrentsectionistojustifythisstatementbyevaluating whether gnns are better at estimating program termination than recurrent algorithms.
note that the latter are based on a sequentialprogram representation.
for this purpose we create an additional set of experiments forthebinaryclassificationofprogramtermination.tomakethe comparisonfair theexperimentalconfigurationensuresthatthe numberoflearnableparametersofallrecurrentalgorithmsishigher than the learnable parameters of the gnns they were compared against.
we use the pytorch implementations for rnns long short term memory lstm and gated recurrent unit s gru .
for this experiment we only use the ds2 dataset containing python programs.
conversely to the previous experiments for therecurrentalgorithms theprogramsarenotconvertedintoan astbutaredirectlyencodedintoaone hotencodingbasedona dictionary ofunique instructions.
918esec fse november14 18 singapore singapore yoav alon andcristina david figure8 binaryclassificationforterminationinrecurrent andgraphneuralnetworks.thegraphneuralnetworkbased approaches perform significantly better than classical recurrentapproaches.
forasignificantevaluation weperformatotaloftentraining sessions until convergenceofthe test metrics.
.
.
results.
similartothepreviousclassificationexperiments werecordthe precision recall pr andreceiveroperatingcharacteristic roc andtheirrespective auc.
we summarize our experimental results in table .
notably graph based methods outperform recurrent approaches by more than 5percent forthe bestrecurrent roc aucand pr aucmetrics.intuitively weinferthatthegraphrepresentationofprograms isabetter fitforterminationestimationthan thesequentialrepresentation.
furthermore the understanding of the context within the recurrent networks is limited by the model capacity and by the naturalconstraints that don tallow for large sequences of data to be processed.
additionalinsightintothecomparisonoftheclassificationresultscanbegainedbyexaminingfigure whereitcanbeobserved thatthegnnandgat basedarchitecturesperformbetterthanthe recurrentapproaches.
related work .
terminationanalysis termination analysis has been studied for a long time with approachesgenerallymakinguseofsymbolicmethodssuchasloop summarization programsynthesis quantifierelimination abstract interpretation .
most of the techniques work by computing ranking functions i.e.
monotone maps from the program s state space to well ordered sets for linear programs over integersorrationals .
recently there were attempts to employ machine learning techniques for programtermination.
for instance neuraltermination analysis trains neural networks as ranking functions.
this method uses sampled executions to learn ranking functions whichare then verified with an smt solver.
although able to provide strongsoundnessguarantees thistechniquesuffersfromcertain limitations e.g.
loops with a limited number of iterations may not provideenoughdatatolearnarankingfunction theverification oftherankingfunctionmayrequireadditionalloopinvariantsto succeed.caludeanddumitrescuproposedaprobabilisticalgorithm forthehaltingproblembasedonrunningtimes wheretheydefine a class of computable probability distributions on the set of halting programs .in abateetal.presentthefirstmachinelearning approach to the termination analysis of probabilistic programs where they use a neural network to fit a ranking supermartingales over execution traces and then verify itover the sourcecode with an smtsolver.
asopposedtotheseexistingworks wedonotattempttoprovidestrongguaranteesabouttheterminationdecision.instead our objective is to provide an estimation of a program s termination behavior aswellaslocalizingthelikelycauseofnontermination when applicable that a programmer can use for debugging purposes.
for this purpose we employ the attention mechanism to identify those nodes relevant for the termination estimation.
moreover forprogramsclassifiedasnonterminating weusesemantic segmentationtodistinguishtheoutermostloopcausingtheinfinite execution.
.
graphneuralnetworks theoriginalapproachtognnsaspresentedbykipfandwelling used the sum of normalized neighbor embeddings as aggregationinaself loop.withamulti layer perceptronasanaggregator zaheer et al.
presented an approach that propagates states through a trainable mlp.
with the development of advanced attention networks the approach of velickovic et al.
focused on attentionweightsthatallowtoprioritizetheinfluenceoffeatures based on self learned attention.
for heterogeneous graphs with additionaledgefeatures relationalgraphconvolutionnetworks wereintroducedbyschlichtkrullet.al.
toenablelinkprediction and entity classification allowing the recovery of missing entity attributes for high dimensional knowledge graphs.
conclusions we proposeda techniquefor estimating thetermination behavior of programs using gnns.
we also devised a gat architecture that uses a self attention mechanism to allow the visualization of nodes relevantfortheterminationdecision.finally fornonterminating programs we constructed a gat for the semantic segmentation of those nodes likely responsible for nontermination.
additionally we illustrated the use of our technique together with slicing and fuzzingfordebuggingofnonterminationbugs.weimplementedthe toolgraphterm andexperimentallyevaluateditonprogramsfrom twoverificationcompetitions aswellasonotherprograms.the experimental results for graphterm confirm its ability to generalizetounknownprogramswhenestimatingtheirterminationand locatingthecauseofnonterminationfornonterminatingprograms.
data availabilitystatement our prototype implementation of graphterm is publicly available at .
919usinggraphneural networksforprogram termination esec fse november14 18 singapore singapore table experimental results for the binary classificationof program terminationusing different network architectures and datasets.rocandprecision recallare recorded after atestaccuracyconvergence.
precision recall roc approach dataset map aucnegative aucpositive map aucneg.
aucpos.
gcn ds sv comp .
.
.
.
.
.
ds term comp .
.
.
.
.
.
ds1 .
.
.
.
.
.
ds2 .
.
.
.
.
.
gat ds sv comp .
.
.
.
.
.
ds term comp .
.
.
.
.
.
ds1 .
.
.
.
.
.
ds2 .
.
.
.
.
.
table3 experimentalresultsforthesemanticsegmentationofterminationusingdifferentnetworkarchitecturesanddatasets.
dice coefficient jaccard index iou nodeaccuracy approach dataset language value value value gcn seg py1 python .
.
.
.
.
.
seg c c .
.
.
.
.
.
gat seg py1 python .
.
.
.
.
.
seg c c .
.
.
.
.
.
table experimental comparison of binary classification usingrnns vs.gnns.
approach network roc auc pr auc recurrent rnn .
.
lstm .
.
gru .
.
graphbased gcn custom .
.
gat custom .
.