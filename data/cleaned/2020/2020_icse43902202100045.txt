measuring discrimination to boost comparative testing for multiple deep learning models linghan meng1 yanhui li1 lin chen1 zhi wang1 di wu3 yuming zhou1 baowen xu1 .
state key laboratory for novel software technology nanjing university china .
department of computer science and technology nanjing university china .
momenta suzhou china menglinghan wangz smail.nju.edu.cn yanhuili lchen zhouyuming bwxu nju.edu.cn wudi momenta.ai abstract the boom of dl technology leads to massive dl models built and shared which facilitates the acquisition and reuse of dl models.
for a given task we encounter multiple dl models available with the same functionality which are considered as candidates to achieve this task.
testers are expected tocompare multiple dl models and select the more suitable ones w.r.t.
the whole testing context.
due to the limitation of labeling effort testers aim to select an efficient subset of samples to make an as precise rank estimation as possible for these models.
to tackle this problem we propose sample discrimination based selection sds to select efficient samples that could discriminate multiple models i.e.
the prediction behaviors right wrong of these samples would be helpful to indicate the trend of model performance.
to evaluate sds we conduct an extensive empirical study with three widely used image datasets and real world dl models.
the experimental results show that compared with state of the art baseline methods sds is an effective and efficient sample selection method to rank multiple dl models.
index terms testing deep learning comparative testing discrimination i. i ntroduction deep learning dl supports a general purpose learning procedure that discovers high level representations of input samples with multiple layers of abstraction based on artificial neural networks anns which has shown significant advantages in establishing intricate structures of high dimensional data when tackling complex classification tasks .
along with increases in computation power and data size dl technology achieves great success in constructing deeper layers of more effective abstraction to enhance classification performance and has beaten human experts and traditional machine learning technology in many areas including image recognition speech recognition autonomous driving playing go and so on.
meanwhile concern about the reliability of dl models has been raised which calls for novel testing techniques to deal with new dl testing scenarios and challenges.
most current dl testing techniques try to validate the quality of dl models in two testing scenarios debug testing and operational testing .
on the one hand debug testing considers dl testing as a technology to improve reliability by yanhui li is the corresponding and co first author.finding faults1 where various testing criteria e.g.
neuron activation coverage and neuron boundary coverage have been proposed to generate or select error inducing inputs which trigger faults.
on the other hand operational testing aims to make reliability assessment for dl models in the objective testing contexts.
li et al.
proposed an effective operational testing technique to estimate the accuracy of a single dl model by constructing probabilistic models for the distribution of testing contexts .
the boom of dl technology leads to dl models with everincreasing functionality scale and complexity i.e.
complex dl models combine multi function from multiple primitive dl models.
exposing code and data to build models and sharing model files e.g.
h5 files boost the acquisition of dl models which drive developers to build complex models by reusing available dl models achieving specific primitive functionality.
one statistic in the previous study indicates that more than .
of complex dl models on github reuse at least one primitive dl model.
on the positive side this plugand play pattern has greatly facilitated the construction and application of complex dl models.
on the negative side for a given dl task it is tough to select suitable models because of the advent of numerous dl models constructed by mass developers.
these multiple models are produced by third part developers and are trained on samples with different distributions.
therefore their actual performance on the target application domain is not guaranteed and they are needed to be tested.
these above points expedite the emergence of a new testing scenario comparative testing where testers may encounter multiple dl models with the same functionality built by different developers all of which are considered as candidates to accomplish a specific task and testers are expected to rank them to choose the more suitable models in the testing contexts.
generally speaking comparative testing is different from current dl testing i.e.
debug and operational testing in the following two points the testing object is multiple dl models instead of a single dl model 1the faults of dl models are usually considered as the mismatching between the real labels and predicted labels of the input samples.
ieee acm 43rd international conference on software engineering icse .
ieee developers model 1github.com utkumukan cnn model developers model 2github.com coreywho mnist cnn developers model 3github.com kj7kunal mnist keras developers ...github.com ... ... developers modelngithub.com kiranu1024 keras api testingsamples suitablemodelscomparative testing for multiple dl modelsrank modelsselectsamples testerstestingcontextfig.
.
an example of comparative testing scenarios with multiple real world dl models designed for written digit identification all of which are trained on mnist dataset and available on github.
the testers need to evaluate and rank these dl models on the testing context.
the testing aim is comparing performances among multiple dl models instead of improving assessing performances for a single dl model.
figure shows an example of comparative testing scenarios considering multiple real world dl models available on github.
hypothetically in this scenario the target application requires an implementation of written digit identification and multiple candidate dl models are found to achieve this functionality.
testers are expected to compare the accuracy of written digit identification among these models and choose the more suitable ones to meet the requirements.
as stated in many previous studies sample labeling is the bottle neck of testing resources for dl models which spends much manpower and is time consuming.
due to the limitation of labeling effort testers can label only a very small part from the whole testing contexts.
therefore as shown in figure testers are asked to execute comparative testing by selecting and labeling a small but efficient subset of testing samples extracted from the testing context and ranking multiple models based on their performance of the selected samples.
as mentioned above comparative testing brings out a new problem of dl testing given limited labeling effort how to select an efficient subset of samples label and test them to rank multiple dl models as precise as possible ?
to tackle this problem we propose a novel algorithm named sample discrimination based selection sds to measure the sample discrimination and select samples with higher discrimination.
the main idea of our algorithm is to focus on efficient samples that could discriminate multiple models i.e.
the prediction behaviors right wrong of these samples would be helpful to indicate the trend of model performance.
specifically sds combines two aspects of technical thoughts majority voting in ensemble learning and item discrimination in test analysis which are introduced to estimate the sample discrimination with the lack of actual labels details are insection iii .
we evaluate our approach on three widely used image datasets mnist fashion mnist and cifar10 each of which contains testing samples.
to simulate the comparative testing scenarios where multiple dl modes are developed submitted for the same task e.g.
digital identification with mnist and clothing classification with fashion mnist we introduce totally models from github including models for mnist for fashionmnist and for cifar .
to assess the performance of sds we introduce three sample selection methods as the baselines one state of the art method from debug testing deepgini at issta one state of the art method from operational testing ces at fse and the simple random selection srs .
the experimental results indicate that our algorithm sds is an effective and efficient sample selection method for comparative testing to solve the problem ranking multiple dl models under limited labeling efforts .
our study makes the following contributions dimension.
this study opens a new dimension of dl testing comparative testing for dl models which focuses on comparing multiple dl models instead of improving assessing a single dl model.
strategy.
this paper proposes a novel selection method sds to measure the discrimination of samples and select samples with higher discrimination to rank multiple dl models.
study.
this paper contains an extensive empirical study of models with three datasets containing testing inputs.
the experimental results indicate that compared with the baseline methods sds is an effective and efficient sample selection method for comparative testing.
the rest of this paper is organized as follows.
in section ii we introduce a motivation example to show the difference between comparative testing and debug operational testing.
in section iii we present a detailed description of our algorithm sds.
in section iv we present our experimental settings including studied datasets and models baseline methods research questions and so on.
section v explains experimental results and discoveries.
section vi further discusses some important experimental details.
sections vii and viii are threats to validity and related works respectively.
section ix presents the conclusion of our paper.
ii.
t hemotivation example as we mentioned in section i the aim of comparative testing is comparing the performances of multiple models.
here we introduce an example to show the differences between comparative testing and debug operational testing.
figure presents an example of comparative testing scenarios containing six testing samples s1 ... s 6and three dl models m1 m2 m3 with the prediction results of samples predicted by models.
x indicates that the prediction results of these models running against samples are right wrong i.e.
the predicted labels are identical different with the actual ones .
by calculating the numbers of x 386no.prediction resultsaccs1s2s3s4s5s6 m1xxxx m2x x x m3 x x fig.
.
an example of six testing samples with prediction results under three dl models.
xand show the prediction result right and wrong.
we can obtain the accuracies of three models i.e.
6for m1 6form2 and2 6form3 respectively.
as a result the actual rank of accuracies acc for these models is acc m1 acc m2 acc m3 .
as shown in figure we have the following observations as only six samples are considered we can easily find that the most efficient subset to indicate the actual rank of these models is s s1 s2 m1has two xunder s m2has one and m3has none.
we can obtain the same rank of models for the accuracies acc0 w.r.t.
s acc0 m1 acc0 m2 acc0 m3 .
s is not the target sampling subset in operational testing as it assesses model performance imprecisely the accuracyacc0 m1 acc0m2 acc0 m3 under s is2 which is much different from the actual4 .
s is also not the target sampling subset in debug testing.
debug testing would consider s5with the highest priority since it triggers the mismatching behaviors of all models.
these observations indicate that the differences of aims between comparative testing and debug operational testing lead to the different sampling priority.
in comparative testing we focus on the samples that could discriminate multiple models e.g.
s1ands2in figure .
in the next section we will introduce a novel algorithm to measure sample discrimination and select samples with higher discrimination.
iii.
m ethodology in this section we present the detailed description of our approach.
first we present the studied problem.
next we show an algorithm named sample discrimination based selection sds to measure the sample discrimination and select samples with higher discrimination.
a. the studied problem we first introduce some symbols and definitions which are helpful for readers to understand the rest of our paper.
definition dl models .
a dl model mis usually regarded as an implementation of complex classification task based on the layer structure of artificial neural networks which achieves a function mapping the high dimensional samples s e.g.
a gray value matrix for figures to labels lin a given label set sl l1 l2 .
.
.
lc m s 2sl.
definition accuracy .
a dl model mis tested under the testing context ctcontaining samples s. let m s and l s be the predicted label generated by mand the actuallabel of s respectively.
the accuracy acc m ct ofmw.r.t.
ctis defined as follows acc m ct s s2ct m s l s ct we introduce accuracy as the main indicator to measure the performance for comparing multiple dl models as it has been widely used in evaluating the performance of dl models .
based on above definitions and symbols we present the studied problem given limited labeling effort for multiple dl models tester aim to select an efficient subset of samples label and test them to rank these models as precise as possible specifically problem.
m1 m2 mnare tested under the testing context ct and all samples sinctare unlabeled.
given limited labeling effort e e ct the task is to select and label an efficient subset cr cr e from ct and employ the results i.e.
acc mi cr oncrto estimate therank of model performance i.e.
acc mi ct on the whole testing context ct with an as small rank error as possible.
b. sample discrimination based selection as shown in the motivation example comparative testing need samples that could discriminate the multiple models.
in this subsection we propose a novel algorithm named sample discrimination based selection sds to measure the sample discrimination and select samples with higher discrimination.
generally sds combines two aspects of technical thoughts majority voting .
majority voting is a simple weighting method in ensemble learning which selects the class with the most votes as the final decision.
as our algorithm has the precondition that all samples are unlabeled we employ majority voting as a procedure to deal with the lack of actual labels i.e.
for a given sample we choose the predicted label with the most models as the estimation of the actual label.
item discrimination .
item discrimination is an indicator to describe to what extent test items can discriminate between good and poor students which is widely used in test analysis2.
we introduce the idea of item discrimination to measure sample discrimination i.e.
estimate discrimination by calculating the difference performance between good and bad models under each sample.
specifically given multiple dl models m1 m2 mn the testing context ct s1 s2 sm with unlabeled samples si the label set sl l1 l2 .
.
.
lc and the labeling effort e sds is composed of the following five steps as shown in algorithm .
step extract prediction results.
we run multiple dl models against the testing context line .
for model mi 387algorithm s ample discrimination based selection sds sm ct sl input the set of dl models sm m m2 mn the testing context ct s1 s2 sm with unlabeled sample si and the label set sl l1 l2 .
.
.
lc .
output the subset crwithcr ctand cr e. 1initialize cr 2initialize an array ad ad i m 3initialize a two dimensional n m array apthat stores the prediction matrix of nmodels on msamples i.e.
ap i n j m indictors that mipredicts sjas the labelap with ap null 4initialize an array af that stores the frequency of labels in the prediction results with af k c 5initialize an array av that stores the voting labels of m samples with av null j m 6initialize an array as that stores the scores of nmodels withas i n 7fori tondo extract prediction results forj tomdo runmionsjand get the prediction label lp2sl assign the prediction label to ap ap lp 11forj tomdo vote for sample labels fork tocdo count the frequency of lkin the nprediction results ap of sample sj af freq lk ap use the majority voting results as the actual labels k a r gm a x1 k c af av lk 15fori tondo classify top bottom models initialize score forj tomdo ifap av then score score as score 21sort ndl models in descending order by as 22select the top and the bottom models into standsb respectively 23forj tomdo compute sample discrimination initialize discrimination fori tondo ifmi2stthen ifap av then discrimination discrimination else if mi2sbthen ifap av then discrimination discrimination ad discrimination st 33sort msamples by their discrimination ad in descending order select with higher discrimination 34select the top samples into the candidate set sc 35randomly select esamples from scintocr 36return cr and sample sj we record the predicted label lpin the element ap of the prediction matrix ap line .
step vote for estimated labels.
for any sample sj we compute the frequency of predicted labels created by multiple models line .
we choose the predicted label with the max frequency i.e.
majority voting as the estimated label line which is the basic of the following steps.
step classify top bottom models.
we employ the votedlabels to score the predicted results of dl models on samples one by one if the predicted label equals to the voted label we add one score for the current model line .
after we go through all the samples we obtain an estimated score for this model.
we sort nmodels in descending order by their estimated score line .
according to the classification in we classify nmodels into three classes line topclass containing the top models bottom class containing the bottom models and other class containing other models.
step compute sample discrimination.
we employ difference performance of models in top bottom class to calculate discrimination.
specifically for each sample sj the value of discrimination is the number of models with right prediction in top class minus the number in bottom class line .
intuitively if the number in top class is much larger than the number in bottom class the result of this sample is more identical with the rank i.e.
it would be helpful to estimate the rank of model performance.
finally we normalize and store the sample discrimination line .
step we consider the samples with higher discrimination as the ones which are more helpful to rank multiple dl models.
to eliminate the effects of outlier samples with higher discrimination we introduce random selecting instead of direct selecting from higher discrimination to lower discrimination.
specifically we choose as the cutoff point to construct the subset of samples with higher discrimination since quartering is common for dataset partition in software engineering i.e.
we consider the top samples as the candidates line and randomly select samples from them according to the given labeling effort line .
figure shows an example of sds running on four dl models m1 .
.
.
m4with the testing context containing four samples s1 s2 s4 which are classified into three classes f n and .
four subfigures show the running results of the first four steps3of sds respectively where the entries with a gray background indicates the target information obtained in each step.
next we describe the subfigures one by one.
figure a shows that sds constructs the 4prediction matrix where f n and are the predicted labels.
figure b presents that sds employs majority voting to obtain the estimation of actual labels.
for example for s1 three models predict it as f and one as .
therefore sds adds fas its estimated label.
figure c presents that sds estimates the scores of models based on estimated labels e.g.
since m3have three right and one wrong prediction m3is scored and sds classifies m1into the top class and m2into the bottom class.
figure d shows sds counts the number of models with right prediction in top class minus the number in bottom class e.g.
for s2 both m1andm2predict right the discrimination of s2is1 .
3as step is easy to understand we omit its running here.
388predictionsno.
s1s2s3s4 m1fn f ?
m2 nff ?
m3fn n ?
m4ff f ?
l ?
?
?
?
d ?
?
?
?
a extract prediction resultspredictionsno.
s1s2s3s4 m1fn f ?
m2 nff ?
m3fn n ?
m4ff f ?
lfn f d ?
?
?
?
b vote for sample labels predictionsno.
s1s2s3s4 m1xxxx t m2 x x b m3xxx m4x xx lfn f d ?
?
?
?
c classify top bottom modelspredictionsno.
s1s2s3s4 m11 t m2 b m3 m4 lfn f d d compute sample discrimination fig.
.
an example of sds running on four dl models m1 .
.
.
m4 with the testing context containing four samples s1 s2 s4 which are classified into three classes f n and .
due to the limited space some abbreviations are used in the subfigures l estimated label s score d discrimination t top class b bottom class .
iv.
experimental setups in this section we present the experimental setup to evaluate the performance of sds.
a. studied dataset and models we introduce three widely used datasets mnist fashion mnist and cifar to conduct our experiments.
mnist is a dataset of handwritten digit images with training samples and testing samples.
samples in mnist are 28pixel grayscale images to denote handwritten digits from to .
fashion mnist is similar with mnist containing training samples and testing samples which are 28pixel grayscale images to describe ten types of clothing.
cifar contains 32pixel color images for training and for testing which are equally distributed into classes e.g.
cat dog ship and truck.
in summary each dataset supports testing samples which are considered as the testing context in the following experiment.
for these three datasets we extract a large amount of models on github models for mnist for fashionmnist and for cifar respectively.
to simulate the different implements of the same tasks we choose these dl models with different stars from a few to tens of thousands on github different model structures and different accuracies.
for each model if the model files e.g.
saved as h5 file are provided in the repository on github we reuse them directly otherwise we employ the code and data provided to train the studied models.
table i presents the detailed description of these studied models with their github repositories parameters of model structure and actual accuracies in the testing context.
as shown in table i some of studied models come from the same repository we put them together and providethe minimum and maximum values of layers parameters and accuracies among them.
b. experimental settings this section will describe some details in the experimental settings in the following aspects.
sampling size.
as mentioned above the labeling effort is the bottle neck i.e.
tester are limited to label only the very small percent of testing samples.
following the experiment design in we focus on results on each sample size from to with intervals of i.e.
.
.
.
which are .
.
samples selected from the whole testing context.
baseline method.
given multiple dl models our goal is to rank the performance of these models by selecting and labeling a discriminative subset.
it s worth pointing out that as comparative testing is a new testing scenario proposed in this paper there are not existing baseline methods.
to clarify the performance of our method we conduct comparative experiments with three baselines two state of the art sample selection methods ces at fse and deepgini at issta in current dl testing and random selection.
ces li et al.
proposed an effective method named ces to select samples for dl testing to assess the accuracy of the single dl model.
we choose ces as a baseline as it also aims to select representative subsets of sample and reduce the labeling costs.
since ces runs based on the single model given ndl models ces may construct n selections of samples for nmodels respectively.
here we introduce the best of nselections i.e.
choose the subset that gets the highest performance of ranking as the result generated by ces which is a stronger baseline to show the advance of our method.
deepgini feng et al.
proposed a technique called deepgini to help prioritize testing dl models which measures the likelihood of misclassification by calculating the set impurity of prediction probabilities for multiple classification.
deepgini supports a deterministic baseline method i.e.
it sorts the test samples according to the calculated likelihood and selects the samples according to sampling size.
as sds and ces are with randomness we combine random selection and deepgini to construct a new baseline in which we perform random sampling in the first the same cutoffs in sds samples according to the rank of .
to differentiate these two baseline methods we call the the former deterministic deepgini ddg and the latter random deepgini rdg .
srs simple random sampling srs is a basic method for subset selection which is used as baseline for many studies .
we randomly select a subset from the testing set and test the ranking performance of this subset.
we implement sds and baseline methods in python .
.
with the frameworks including tensorflow .
.
and keras .
.
.
our experiments are performed on a ubuntu .
server with gpu cores tesla v100 sxm2 32gb .
we provide the replication package including the detailed description 389table i the detailed description of the studied dl models including 28for mnist 25for fashion mnist and 27for cifar .
datasetmodelgithub websitemodel structureactual accuracyno.
layers params mnist1 cnn mnist .
.
.
.
model mnist .
.
keras cnn .
.
.
.
.
.
.
.
.
fashion mnist1 .
.
.
keras fashion mnist .
.
fashion mnist tutorial .
.
.
.
cifar .
.
.
.
.
.
keras .
.
.
.
.
.
cifar .
.
cnnmodel keras cifar10 dataset .
4cifar10 .
.
.
of our proposed methods sds and source code online see section x .
repetition.
as sds and several baseline methods are with randomness we conduct the experiment times and report the average of calculated results.
c. evaluation indicators to evaluate to what extend the estimated rank w.r.t.
selected samples are identical with the actual rank w.r.t.
the whole testing context we introduce two indicators spearman s rank correlation coefficient and jaccard similarity coefficient.
spearman s rank correlation coefficient is a measure of the correlation between two variables xandy .
it can be calculated by the following formula p i xi x yi y pp i xi x 2p i yi y the value of ranges from to the closer it is to the two sets of variables are positively negatively correlated.
besides we introduce jaccard similarity coefficient denoted as jk to evaluate the similarity between the topkmodel sets generated by the estimated rank and actual rank.
for example the estimated rank is m1 m3 m5 and the actual rank is m1 m3 m2 .
the jaccard similarity coefficient between two top model sets m m3 m5 and m m2 m3 is calculated as j3 m m3 m5 m m2 m3 m m3 m5 m m2 m3 .
as we encounter dozens of models in the testing context e.g.
models for mnist we focus on k 10to evaluatethe performance of our method on different cutoff points.
we takek as the representative to report the evaluation under jaccard similarity coefficient in the experimental results.
the other jaccard coefficient when k will be discussed in the discussion part.
d. analysis method first we employ wilcoxon rank sum test to verify the difference of the rank performance between our method and the baselines.
if the p value are less than .
the two sets of data are considered significantly different.
next we introduce cliff s delta which measures the effect size for comparing two ordinal data lists.
we judge the difference between the two sets of data based on the range of negligible if .
small if .
.
medium if .
.
and large if .
.
finally we use w t l to compare the results of our approach and the baseline where w means our approach wins t means the results are tie and l means our approach loses.
reaching the two standards shows that our approach wins a the p value of wilcoxon rank sum test is less than .
p .
which means the results between our approach and baseline are significantly different b the cliff s delta is larger than .
.
which means the difference between the two results are positive and not negligible.
if p .05and .
we consider our approach loses.
otherwise the result of comparision is tie.
e. research questions we are committed to promoting the ranking performance of the multiple models under limited labeling effort in the a spearman coefficient of mnist b spearman coefficient of fashion mnist c spearman coefficient of cifar d jaccard coefficient of mnist e jaccard coefficient of fashion mnist f jaccard coefficient of cifar fig.
.
the results of our approach and the four baselines for ranking model performance under spearman coefficient and jaccard coefficient k .
in each subfigure the x axis indicates the number of samples from to and the y axis represents the values of spearman jaccard coefficient.
these five studied methods are denoted by lines with different colors i.e.
for sds for ces for ddg for rdg and for srs.
comparative testing scenario.
we propose the following two research questions rqs to organize our experiments rq1 effectiveness whether our method sds can sur pass the state of the art methods in ranking multiplemodels?
rq2 efficiency compared with the state of the artmethods is our method sds efficient?
v. experiment results in this section we present the results of the experiments and answer the above two rqs.
a. rq1 effectiveness motivation and approach.
our problem is to obtain effective ranking results of model performance with a very low labeling percent of testing samples for multiple models inthe testing scenario.
we hope to verify whether our proposedapproach sds is more effective than the baseline methodswith limited labelling effort.
to achieve this aim we comparethe ranking performance of sds with the baseline methods inthree testing contexts containing the testing samplesfrom mnist fashion mnist and cifar respectively under the sampling sizes from to with intervals of5 i.e.
.
.
.
.
specifically we employ these fivestudied methods to sample the subset under different samplingsizes and use the ranking result on the subset to estimate therank on the whole testing context.
we repeat running thesemethods times and report the average of calculated results.results.
figure shows the comparison results of our approach and the four baselines for ranking model performance.the three subgraphs in the first row show the comparisonresults of spearman coefficient and the subgraphs in the second row present the results of jaccard coefficient j10 .
in each subfigure the x axis indicates the number of samples from to and the y axis represents the values of spearman jaccard coefficient.
these five studied methods aredenoted by lines with different colors i.e.
for sds for ces for ddg for rdg and for srs.
it can be seen in figure that in all sub graphs our method sds isobviously better than the other baselines under all samplingsizes from to .
besides our approach is very stable on the contrary some baselines have a strong volatility e.g.
ddg has wild gyrations when measuring jaccard coefficientfor minist fashion minist and cifar .
in order to show more details of the experiment results we choose six sampling points and asthe representatives.
table ii presents the detailed results underthese six points with the mean values of spearman coefficientand jaccard coefficient of ranking multiple models obtained by50 repetitions of running the five studied methods.
the bestnumbers are highlighted in bold.
in table ii if our approachwins the baseline method that is to say the pvalue is less than .
and the is greater than .
then we add the 4we take k as the representative to report the evaluation under jaccard similarity coefficient in the experimental results.
the other jaccard coefficient when k will be discussed in the discussion part.
391table ii the results of spearman correlation and jaccard correlation j10 with our method and four baseline methods .
indicator cutoffmnist fashion mnist cifar srs ces rdg ddg sds srs ces rdg ddg sds srs ces rdg ddg sds spearman35 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w t l jaccard35 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w t l gray background to the value of the baseline method.
based on the values of spearman and jaccard coefficients we have added two rows average to calculate the average value of each column and w t l to record the number of times our approach win tie lose other baselines.
from table ii we have the following observations.
a from the average value of each point our approach is higher than all baselines under both spearman coefficient and jaccard coefficient.
b the gray background indicates that our approach wins other baselines at the most of points.
c the results of w t l shows that our approach is not only higher than other baselines in mean but also significantly better.
answer to rq1 in ranking multiple dl models our approach is significantly better than all other baselines in effectiveness.
b. rq2 efficiency motivation and approach.
in rq1 we have observed that our approach sds is significantly better than other baselines under both spearman coefficient and jaccard coefficient in raking multiple dl models.
the process of sample selection may be time consuming.
in this rq we want to check the efficiency of our approach compared with other baselines.
results.
table iii shows the total time consumed when running studied methods with sampling from to .
from table we find that our approach sds takes longer than srs because it contains sample sorting and operations on the prediction matrix.
the time sds consumes is similar to the other three baselines ces rdg and ddg which is around seconds.
answer to rq2 except for srs our approach is similar to other baselines in time consumption.table iii the time second consumed when samples are selected by different approaches .
dataset srs ces rdg ddg sds mnist fashion mnist cifar vi.
d iscussion in this section we further discuss some parameter settings and results in the experiments.
first we analyze the parameter and indicator involved in the experiments.
after that we discuss why our algorithm can effectively help multi model performance ranking and whether our method is effective when the number of models is reduced.
a. the performance under other selection rates in our experiment the random sampling interval is set to the top as shown in step of algorithm .
we want to further discuss the performance under other selection rates by conducting experiments on five different selection rates i.e.
random sampling of the top and intervals .
the results are shown in figure where the performances of different rates are denoted by line with different colors i.e.
for for for for and for respectively.
it can be seen that the performances of different selection rates on different datasets vary a lot.
generally speaking there is no obvious trend in all subfigures.
in addition the sampling interval the green line we set in the experiment obtains the best ranking performance under the most of sampling sizes in the cifar dataset.
as quartering is common for dataset partition in software engineering and easy to implement we still suggest applying the interval in our algorithm.
b. the performance of jaccard coefficient with k we employ jaccard coefficient to measure the similarity between the two top kmodel sets generated by the selected a spearman of mnist b jaccard of mnist c spearman of fashion mnist d jaccard of fashion mnist e spearman of cifar f jaccard of cifar fig.
.
the graph of ranking performance when the random selection rates changes from top top .
the performances of different rates are denoted by line with different colors i.e.
for for for for and for .
subset and the whole testing context respectively.
in the previous experiments when we use the jaccard coefficient we calculate it with k .
in this section we will discuss whether our method has advantages when the values of kare different i.e.
k .
due to space limitation we cannot display all the the former for the three datasets and the latter for k subgraphs we calculate the average of the three datasets in three subgraphs for k 5in figure .
as shown in figure we compare our approach sds the green line with other baselines when k .
figure shows the average values of the jaccard coefficient of the three datasets when the sampling changes.
it can be seen that our approach still has advantages under the most of points which shows that our approach is still superior in ranking models when considering k .
c. analysis and insight of our algorithm in this section we will discuss why our algorithm works.
in order to illustrate this point we conduct a two step analysis.
the first is to measure the precision of the majority voting.
we compare estimated labels obtained by the majority voting with true labels.
figure shows the matched rate of estimated labels with true labels when the majority voting gets different numbers of votes.
it can be seen that as the number of votes a jaccard coefficient j1 k b jaccard coefficient j3 k c jaccard coefficient j5 k fig.
.
the graphs for k 5in measuring jaccard coefficient with the top kmodel sets generated by the selected subset and the whole testing context.
a mnist b fashion mnist c cifar fig.
.
the histogram of matched rate when the votes changes.
the red line represents the matched rate on the entire data set.
obtained increases the matched rate also rises.
in general the average matched rate of majority voting results with the true labels reaches .
for mnist .
for fahionmnist and .
for cifar respectively as shown by the red line in each subfigure.
in other words majority voting is close to the true label which is the key to explain why our method is effective.
this finding leads to an insight for following studies in comparative testing the distribution of predicted labels would be helpful to deal with the lack of actual labels which is a main difficulty in actual testing scenarios due to the limitation of labelling effort .
we encourage following researchers to employ more effective methods to measure the distribution in comparative testing.
in the second step we analyze whether the sample discrimination is positively correlated to the ranking performance i.e.
whether higher discrimination is more helpful for ranking multiple dl models.
we conduct an additional experiment.
after sorting the samples according to the discrimination we randomly select samples in the top the the and the intervals to observe the results of ranking performance.
we take the averages of the three datasets and show them in the figure the blue line represents a spearman b jaccard fig.
.
the graph of four intervals for random sampling the first and to show that the sample discrimination is positively correlated to the ranking performance.
fig.
.
comparison result of ranking performance of sds srs and ces when the number of models is .
the random sampling in the first interval which is the interval used in our experiment.
it can be seen that the model ranking effectiveness of random sampling in the first is significantly better than other intervals.
that indicates higher discrimination is more helpful for ranking multiple dl models.
d. the performance when there are fewer models the previous experiment content is to calculate the ranking performance of the sds method when the number of models is large i.e.
more than models for a given task .
in this section we report the performance of sds on the model ranking when there are few models.
we have selected four models in each data set to compare the ranking effect of sds and other baselines.
we measure the spearman coefficient5 value when the sample size is from to .
the experiment was repeated times and the average results were reported.
figure shows the comparison results of sds srs and ces which are the best three methods when the number of models is large.
figure presents the average ranking performance on the three data sets where the green curve denotes the spearman coefficient of sds.
we observe that sds can still show superior performance when there are fewer models which obviously exceeds srs and ces.
to some extent the above result shows the generalization of the sds method.
5as there are only four models jaccard coefficient k is not applicable here.
we focus on spearman coefficient.
fig.
.
comparison result of ranking performance of sds with ranking performance when majority voting is used as the real label.
e. the ranking performance when choosing majority voting as true labels an intuitive idea is to use the labels obtained by the majority voting as the true labels to measure the accuracy of the models and then get the ranking performance of the models see line of algorithm .
in this section we compare this intuitive method with the results of sds to verify whether the calculation after line in algorithm really plays a role in model ranking.
we show the results of the comparison in figure .
the blue curve in the figure is the average result of the spearman coefficient on the three data sets that vary with the sample size and the red line is the ranking result obtained by using the majority voting results as the true labels which is also the average result of the three data sets.
it can be seen from figure that sds overcomes majority voting when the sample size is larger than which is roughly about one percent i.e.
of the total test set.
besides the curve after this point still shows an upward trend along with the increasing of sampling size.
that is to say the calculation content after line in algorithm is useful for the model ranking.
vii.
thread to v alidity the threat to validity is discussed in the following three aspects for our study.
first the datasets we select may be a threat.
we use three well known graph classification datasets which are widely used in many studies but their complexity is not high.
in the future we will explore on larger and more diverse datasets to validate the effectiveness of our algorithm.
second the selection of models in the experiments could become a threat.
we try to choose a wide range of models on github i.e.
models for mnist for fashion mnist and for cifar respectively which include multiple dl models with different stars from a few to tens of thousands on github different model structures and different accuracies.
however these studied models may not fully cover the real situation.
more models are expected in the following studies to validate our results.
finally it may also be a threat to the implementation of the models.
as discussed earlier if the trained model file is 394provided in the github repository we will use it directly otherwise we will use the provided python code and datasets for training.
due to the difference in the training environment it may cause the reproduced model to be different from the original one.
for new trained models we compare the accuracies announced in the github repository and actual accuracies and find that the difference between them is slight.
viii.
relatedwork in this section we introduce the related work.
in the angel of traditional software testing on the one side testing aims to find more bugs which is called debug testing on the other side testing aims to make reliability assessment of software through conditioning which is called operational testing.
the main body of current dl testing is to focus on debug testing i.e.
the main aim is to find bugs.
pei et al.
proposed a whitebox framework named deepxplore which uses neuron coverage as the standard for dl model testing .
tian et al.
implemented a tool named deeptest to simulate the real world to help find behaviors that may cause accidents for dnn driven vehicles .
zhang et al.
proposed unsupervised framework for dnn named deeproad and utilized gans and metamorphic testing to test the inconsistent behaviors in self driving car .
xie et al.
proposed a coveragebased framework named deephunter which used metamorphic mutation to help find defects for dnns .
sun et al.
presented an approach named transrepair to help machine translation systems test and repair inconsistency bugs .
ma et al.
proposed a set of testing criteria named deepgauge for measuring the testing adequacy of dnns .
ma et al.
proposed deepct which applied the idea of combinatorial testing to dl testing and produced a series of combinatorial testing criteria for dl systems .
tian et al.
developed a technique called deepinspect which can detect the confusion and bias errors based class for image classification .
lee et al.
presented a white box testing approach named adapt which used an adaptive neuron selection strategy to find adversarial inputs .
meanwhile researchers have focused on the other aspects of dl testing.
li et al.
proposed an effective operational testing technique to estimate the accuracy of the dl model by constructing probabilistic models for the distribution of testing contexts .
to evaluate the quality of test data ma et al.
applied the mutation framework to dl systems and proposed a technique named deepmutation .
zhou et al.
proposed a testing approach for the systematic physical world called deepbillboard which is aimed to generate adversarial test more robust .
gerasimou et al.
proposed a systematic testing approach named deepimportance which is mixed with an importance driven idc test adequacy criterion to support more robust dl systems .
ix.
conclusion the boom of dl technology leads to the reuse of dl models which expedites the emergence of a new testing scenario comparative testing where testers may encountermultiple dl models with the same functionality as candidates to accomplish a specific task and testers are expected to rank them to choose the more suitable models in the testing contexts.
due to the limitation of labeling effort this testing scenario brings out a new problem of dl testing ranking multiple dl models under limited labeling efforts .
to tackle this problem we propose a novel algorithm named sample discrimination based selection sds to measure the sample discrimination and select samples with higher discrimination.
we evaluate our approach on three widelyused image datasets and dl models.
our results lead us to conclude that sds is an effective and efficient sample selection method for comparative testing to rank multiple dl models.
finally we would like to emphasize that we do not seek to claim the advantage of our method sds.
instead the key messages are that a a new testing scenario comparative testing is introduced by our paper where the testing aims are much different with the current dl testing i.e.
debug operational testing b the new testing scenario brings out the new testing challenge ranking multiple dl models under limited labeling efforts c our proposed method sds leads to the insight which would be helpful for the following researchers.
x. repeatability we provide all datasets and code used to conduct this study at