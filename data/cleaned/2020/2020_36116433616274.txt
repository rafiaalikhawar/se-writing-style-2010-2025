input driven dynamic program debloating for code reuse attack mitigation xiaoke wang xkernel whu.edu.cn wuhan university wuhan chinatao hui taohui whu.edu.cn wuhan university wuhan chinalei zhao leizhao whu.edu.cn wuhan university wuhan chinayueqiang cheng yueqiang.cheng nio.io nio mountain view usa abstract modern software is bloated especially for libraries.
the unnecessary code not only brings severe vulnerabilities but also assists attackers to construct exploits.
to mitigate the damage of bloated libraries researchers have proposed several debloating techniques to remove or restrict the invocation of unused code in a library.
however existing approaches either statically keep code for all expected inputs which leave unused code for each concrete input or rely on runtime context to dynamically determine the necessary code which could be manipulated by attackers.
in this paper we propose picup a practical approach that dynamically customizes libraries for each input.
based on the observation that the behavior of a program mainly depends on the given input we design picup to predict the necessary library functions immediately after we get the input which erases the unused code before attackers can affect the decision making data.
to achieve an effective prediction we adopt a convolutional neural network cnn with attention mechanism to extract key bytes from the input and map them to library functions.
we evaluate picup on real world benchmarks and popular applications.
the results show that we can predict the necessary library functions with .
accuracy and reduce the code size by .
on average with low overheads.
these results indicate that picup is a practical solution for secure and effective library debloating.
ccs concepts security and privacy software security engineering .
keywords software security code debloating attack mitigation acm reference format xiaoke wang tao hui lei zhao and yueqiang cheng.
.
input driven dynamic program debloating for code reuse attack mitigation.
in proceedings of the 31st acm joint european software engineering conference lei zhao is the corresponding author.
full information of the affiliation key laboratory of aerospace information security and trusted computing ministry of education school of cyber science and engineering wuhan university wuhan china permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction modern software is bloated especially for libraries.
for facilitating software development developers typically import many features from libraries to synthesize new applications easily.
however such one size fits all strategy integrates excessive functionalities into the code space of the program where only a small set of functions are needed.
for example a previous study shows that only of the library functions are used in the ubuntu desktop environment.
bloated libraries bring a detrimental impact on software security.
first the extraneous code may involve vulnerabilities that enable attackers to compromise the system.
for example the x32 abi on linux is rarely used by real world programs but contains a severe bug that grants attackers extra privileges .
second unnecessary code provides fertile ground for code reuse attacks such as returnto libc ret2libc and return oriented programming rop .
for example the gnu c library glibc is linked to almost all applications but it contains many gadgets that attackers can stitch to construct various malicious exploits .
to mitigate the security damage of bloated libraries researchers have proposed several debloating techniques which remove or restrict the invocation of unused code.
based on when the debloating takes effect these techniques can be classified into two categories offline debloating and online debloating.
offline debloating aims to trim libraries before an application runs.
for example piece wise analyzes call dependencies during library compilation and only enables necessary code when an application is loaded .nibbler achieves a similar goal for binaries .
to ensure the normal operations of an application they have to retain all functions that the application may invoke.
that is all remaining code is loaded into the program code space even if some of them are unused for a concrete execution on a specific input.
however a concrete execution on a given input often traverses a small amount of all program paths and only invokes a small set of the remaining functions.
as a result although offline debloating techniques can remove many unnecessary codes the library is still bloated for a concrete execution.
on the other hand online debloating removes unnecessary library functions at runtime aiming to achieve on demand loading.
for example blankit utilizes a decision tree based predictor with the function calling context to predict and selectively load library functions at each call.
therefore context based debloating can further reduce the attack surface.
however such per context debloating techniques are vulnerable to context corruption attacks.
specifically blankit relies on three factors to predict the esec fse december san francisco ca usa xiaoke wang tao hui lei zhao and yueqiang cheng required functions the call site location the arguments and the reverse dominance frontier rdf of arguments all of which can be manipulated through memory corruptions to modify arguments or slight control flow manipulation to choose a proper rdf and call site .
therefore attackers can fabricate the context for any library function and thus revive rop or ret2libc attacks.
in this paper we propose picup an online debloating approach that dynamically customizes libraries for each input.
picup aims to balance code reduction and enforcement reliability.
our insight is that the library demands by an application vary on different inputs while the user supplied input is the original trampoline for attackers.
by predicting the library demands of the received input we can restrict the suspicious program behavior for the lifetime of each input which avoids only considering the entire lifetime of a program and blocks the attacker from manipulations on the debloating result.
thus picup not only reduces more code than offline techniques but also mitigates the potential threats of online techniques from being nullified by context corruption attacks.
to achieve a practical per input library debloating picup should satisfy two design requirements robustness and functionality.
robustness means that the decision making process and result cannot be affected by attackers.
to achieve this property we hook input receiving system calls to capture the input before it reaches the program.
picup predicts necessary library functions at such locations so that attackers have no chance to affect the decisionmaking.
after prediction and debloating picup guarantees the same code size until getting the next input or reaching the end.
even if attackers manipulate program states at runtime they cannot increase the attack surface by invalidating debloating results.
for the functionality requirement picup is designed to predict minimal but adequate library functions for supporting program normal operations as well as reducing the attack surface to the minimum.
considering among any length and diverse input bytes only a few of them contribute to determining the library demands we adopt a convolutional neural network cnn with attention mechanism to identify sensitive bytes from the input with variable length and map the extracted bytes to library functions.
since picup works for per input rather than all library api call sites with different contexts it does not frequently perform predictions during normal internal operations.
besides we only need to build one model for the whole application instead of many instrumented predictors for each api function.
thus this solution is efficient and portable to handle most programs with any input format.
we implement a prototype of picup which first runs the program with given inputs to collect mappings from inputs to used library functions.
then it applies cnn on the mapped data to construct an input based prediction model.
at runtime picup hooks each input receiving system call and predicts the necessary library functions for each input.
to demonstrate the effectiveness of picup we evaluate the prototype on the spec cpu benchmark and several real world applications.
the experimental results show that picup can predict library functions with an average accuracy of .
and reduce the exposed code surface of libraries by .
thereby mitigating the risk of rop gadgets and vulnerable functions.
in addition our protection only introduces .
runtime overhead to spec cpu benchmarks and has an acceptable performance on real world applications.in summary we make the following contributions we propose per input debloating a practical approach that dynamically reduces the library attack surface for each input.
our method balances the code reduction and the enforcement reliability to achieve better security.
we design and implement a system that captures user input and predicts library functions.
to provide an accurate prediction we adopt the neural network method to model and predict the library demand of each input.
we evaluate picup on spec cpu and popular applications.
results show that picup can predict the library functions with .
accuracy and reduce the code size by .
on average with low overheads.
the source code of picup is available at b1nsecwh picup.
motivation .
library debloating to illustrate existing debloating approaches and demonstrate their differences and limitations we borrow a code fragment from previous studies shown in figure .
the code snippet contains twoifconditional statements s3ands9 .
given different inputs this program will execute along with different paths.
to be more specific if the input indicates an administrator the program will execute along with s3 s6 s8 s9 s12 .
otherwise if the input indicates a normal user the program will execute along with s3 s4 s8 s9 s10 .
moreover there is a classical stack based buffer overflow vulnerability ins8 which could be exploited by attackers to change the execution path.
specifically an attacker can construct an illegal input to overwrite stranduser ats8 resulting in an unintended attack at the ifcondition in s9.
once the attack occurs a malicious non privileged user can perform any sensitive operations by invoking the library function system with arbitrary arguments.
offline debloating approaches trim libraries while supporting the program on all legitimate inputs.
in this example the program will execute along with two paths based on different inputs indicated as a red solid line and a black solid line in figure b .
therefore offline debloating approaches prohibit invocations of any library functions except the seven apis used in the code segment.
however a dynamic execution on a specific input only invokes apis along the red path or the black path.
therefore the remaining apis are still bloated for a concrete execution.
the bloated apis enlarge the attack surface and can assist attackers to obtain extra privileges as demonstrated in the aforementioned attack.
online debloating approaches aim to remove library functions dynamically.
a recent work blankit restricts api invocations such that each library function can only be called at corresponding call sites within certain contexts.
to achieve this blankit designs a context based model to predict necessary code and then embeds it into every api call site.
that is based on the execution context blankit predicts and debloats code at each api call to ensure that only part of the library functions is available.
however the context based prediction mechanism itself has limitations.
specifically blankit needs to build the decision tree based model for all api functions which is hardly scalable and can bring an extremely high overhead when there are lots of api calls in 935input driven dynamic program debloating for code reuse attack mitigation esec fse december san francisco ca usa b offline approaches super user flownormal user flow available apistrncmp memcpy strcpy fprintf systemstrncmpsuper user input freenormal user input a code s1 char str user s2 ... s3 if strncmp user admin s4 free ... release data s5 else s6 memcpy ... cache data s7 s8 strcpy str someinput vul point s9 if strncmp user admin s10 fprintf ... str log for normal users s11 else s12 system str sensitive op for admin s13 c online approaches normal user input strncmp memcpy strcpy fprintf systemstrncmpcontextfree debloat pointcandidate available api d our approach block the attacknormal user input strncmp memcpy strcpy fprintf systemstrncmpfree control flow hijack figure debloating an example code with different solutions.
the example code in a has a stack based buffer overflow vulnerability at line s8 enabling various attacks.
offline debloating approaches in b allow all imported apis and therefore the system can be used for the attack online approaches in c though they only activate one api at a time may still allow system to be used by attacks due to attacker controllable contexts.
our approach in d only provides apis along with the execution path of one input so even if the control flow is hijacked the apis on the execution path of other inputs are still not available.
fprintfs8 ...s4 ...s3 ...process memory s6 call blankit predict ... call memcpy.text fake old ebp fake ret addr fake call context eipstrncmp memcpy strcpy systemstrncmp contextfree esp ebpstacks12 ... ... figure attack on blankit via faked context.
an attacker constructs fake calling contexts in the stack which will mislead the prediction model in blankit and then allow the attacker to call any functions in section .text .
one execution.
what is worse such a prediction mechanism can be bypassed by fabricating runtime context.
first because blankit only predicts at each call site it is insensitive to control flow hijacking that occurs outside the call site.
as shown in figure c the approach is also vulnerable to the aforementioned attack to call thesystem function.
moreover as shown in figure the attacker can construct the calling context for any function by overrunning the stack frame.
through hijacking the return address the attacker can jump to the target site and mislead the decision tree model to make a mistaken decision and then copy unintended code back e.g.
memcpy .
with appropriate context corruption the attacker is available to invoke any functions in the code section .text .
consequently despite the excellent code reduction rate the context based approaches cannot always guarantee its enforcement.in summary offline debloating approaches still contain unneeded features for each concrete execution.
by contrast online debloating approaches ensure minimal library size at execution but they are vulnerable to corruption attacks because of the over reliance on context.
in this case we aim to balance code reduction and reliable enforcement to achieve better security.
.
per input online debloating given an execution path only functions on the path are invoked so that the other functions beyond the path are unnecessary.
what s more we observe that the execution path of a program mainly depends on each received input.
for the example in figure the two execution paths are determined by inputs that indicate admin users or not.
if we can predict the apis required on the coming execution path based on the input then the functions e.g.
memcpy andsystem will be blocked for normal users.
meanwhile as attackers cannot interrupt the results until sending the next input the aforementioned attack is not feasible anymore even if attackers can corrupt the stack variable user via current input calling system will not be allowed and finally trigger an execution exception i.e.
segmentation fault with invalid permission.
following the above observation we propose per input debloating which is to dynamically debloat unused code for each input.
by predicting the dynamic api demand just for a specific input at runtime and thus its code reduction rate is higher than existing offline debloating approaches.
in addition per input debloating can guarantee the enforcement reliability because attackers cannot affect the debloating results nor call unexpected apis during the dynamic execution even if attackers can manipulate program context.
.
threat model we assume that the underlying hardware and operating system are trustworthy and thus the prediction model and debloating operations with the necessary data can be protected.
that is the details 936esec fse december san francisco ca usa xiaoke wang tao hui lei zhao and yueqiang cheng of the prediction model can maintain agnostic to users and cannot be steered by them to influence the prediction.
we do not restrict the attacker s knowledge of the memory layout.
the attacker can read write data and code sections of a process and the attacker can hijack control flow by exploiting vulnerabilities such as buffer overflow and use after free.
note that other memory protection mechanisms such as stackguard aslr dep and cfi do not conflict with our approach.
3picup design and implementation in general picup works in two phases the preparation and runtime phase.
in the preparation phase we collect the program execution traces on various inputs and then use them to train the prediction model.
meanwhile we obtain the dependencies between functions to assist in identifying required functions in runtime.
during the runtime phase we capture the program input and utilize the trained model to predict the unnecessary library apis.
finally picup enables required library functions during the dynamic execution and restricts the invocation to unused library functions.
librarylibrary dependencydynamic debloating api demand predictionlibrarycode dependency analysis execution monitoring input extraction inputprocess memory1010... api listmapping applicationdynamic execution on an input figure the overview of picup .
figure shows the overview of picup which consists of four modules execution monitoring and input extraction input driven prediction code dependency analysis and dynamic library debloating.
the first module execution monitoring and input extraction aims to monitor the dynamic execution of programs and captures the input before the program receives it which enables picup to perform the subsequent prediction and library debloating.
given a captured input picup will predict the library demand on the execution path for it.
to do so we design a neural network model to make predictions driven by inputs.
as the demand of a program on libraries is api functions we make the output of the prediction model as a list that indicates which apis are needed.
additionally considering an invoked api function typically calls other functions and such sub functions are also required during execution we make a dependencies analysis to identify the subfunctions for every exported function in the preparation phase.after identifing the required library functions picup will use the component of dynamic debloating to remove unnecessary functions.
to achieve this we restrict the permission to unnecessary functions by assigning corresponding memory pages as non executable.
in the following content of this section the design details of the above four modules will be presented.
.
execution monitoring and input extraction to debloat library functions for each input picup needs to obtain the input and mapping library addresses about the program execution.
besides picup should be automatically triggered to perform operations such as model prediction and dynamic debloating.
therefore we monitor each execution of the program and focus on extracting the received input.
there are several design requirements in execution monitoring and input extraction.
first the execution monitoring should introduce as little impact as possible on the normal running of the program.
besides the input extraction should be reliable and generic for types of interfaces for receiving inputs.
for example a program can receive inputs from the command line e.g.
stdin from the file system and the network interface e.g.
socket .
therefore picup requires a generic approach to identify types of inputs.
in addition the input extraction should be reliable to ensure that our protection cannot be bypassed even if attackers can manipulate the state of the program.
for example when the attack shown in section .
happens we should ensure that attackers do not have any chance to corrupt our protection mechanism.
get inputsys execvekernel application startsys mmap load librarypid lib addr input sys exit exitsys recv argv envpsys read figure execution monitoring and input extraction.
by hooking syscalls picup is awakened when the target program starts execution and sleeps on exit.
the address can be obtained when the library is mapped into memory and the input can be captured before it reaches the program.
in this study we design a lightweight and generic approach for execution monitoring and input extraction by hooking syscalls .
as shown in figure picup works at the kernel mode thus the hooking will not be directly affected by any operation from user space.
meanwhile this approach avoids introducing additional context switching between user mode and kernel mode.
to be more specific picup monitors the running state of dynamic execution by hooking sys execve sys exit .
by hooking sys execve picup is automatically triggered to start working every time the target program starts running.
by hooking sys exit picup is able to identify the exit of the execution and releases resources in time.
besides picup leverages sys open at andsys mmap to identify libraries that are mapped to the process as well as their memory layouts.
picup will use these memory layouts to debloat unnecessary library functions via memory access control.
937input driven dynamic program debloating for code reuse attack mitigation esec fse december san francisco ca usa to capture program inputs via types of interfaces picup hooks system calls related to i o. in detail picup hooks sys execve to obtain the inputs e.g.
argv envp provided before the program runs.
during execution picup captures the inputs e.g.
stdin file from character devices and block devices by hooking sys read and related system calls including sys readv sys pread sys preadv .
similarly picup captures the inputs from the network e.g.
socket by hooking sys recv sys recvfrom sys recvmsg sys recvmmsg .
hooking system calls is generic to identify received inputs.
whenever a program receives an input picup captures the input and further debloats library functions before the program processes it.
more importantly we can ensure the robustness of picup .
as we mentioned before the hooking works at the kernel level so that an attacker does not have any chance to modify the hooked input even if the program in user mode is hijacked.
the module of execution monitoring and input extraction is implemented by using a loadable kernel module with the help of kprobes which enables us to dynamically break into any kernel system call and collect debugging information non disruptively.
.
input driven prediction picup trains the prediction model in the preparation phase.
in the runtime phase our input driven prediction model predicts required library functions for the execution on a given input.
the main challenge in input driven prediction is various inputs make different contributions to predict program execution demand.
in general there are both control bytes and data bytes in the input which are different for predicting program execution.
specifically thecontrol bytes determine the program behaviors which contribute more to the prediction results while the data bytes are merely used to hold the input content.
moreover the control bytes in different inputs are specific to input formats and thus vary a lot from input to input.
without an in depth understanding of input formats it is difficult to distinguish bytes for various inputs.
in recent years artificial intelligence has been developing at a rapid pace.
the advances in deep learning provide us with a chance to build a prediction model for program inputs.
to address the above challenge of extracting control bytes without a deep understanding of the input format we propose to leverage cnn for predicting the library functions demand.
the convolutional neural network cnn performs outstandingly in artificial intelligence tasks such as image recognition especially for feature extraction.
by leveraging cnn our model can automatically learn control bytes that determine program behaviors.
however as demonstrated in previous techniques control bytes often represent a small fraction of all the bytes in program input.
with this impact our cnn based prediction model may decrease the accuracy for inputs with large sizes.
to improve the performance of our prediction model we further design to leverage the attention mechanism as feature refinement to identify the contribution of each byte extracted by cnn.
in detail the attention layer works after the cnn extracted the feature bytes.
it generates a weight map to indicate the importance of each extracted byte and then makes the bytes with high weights play an active role in the model decision.
in this way we can enhance our model by making it pay more attention to control bytes.
.... .... 0apisfully connected layermaxpooling attention layerconvolutional layer binary stream inputs character stream inputs sigmoid .... conv2conv1 attention1 attention2figure the prediction model in picup .
the features are first extracted in the convolutional layer and then refined in the attention layer.
in the fully connected layer the model predicts whether the api will be invoked or not.
to deal with various types of program inputs we divide inputs into two categories according to their forms.
one is the binary stream e.g.
images elf file and the other is the character stream e.g.
argv .
the model applies different types of convolutional kernels to extract control bytes in these two types of inputs.
specifically the binary stream inputs are processed by the kernel same to lenet5 and the character stream inputs are processed by the kernel same to textcnn after each word is encoded into vector.
the last layer of the model contains neurons with the same number as the apis in global offset table got of the target program.
the value of each neuron is normalized into a range denoting the probability of requiring the corresponding api.
after processing with a threshold .
by default the model will output an api list where means the api is required and is not.
in the implementation we build the prediction model based on the torch framework with convolutional block attention module .
for binary stream input we directly convert each byte to a grayscale pixel.
for character stream input to encode all words even if the word has not appeared before we apply fasttext to generate a distributed representation of each word.
.
code dependency analysis for libraries as an api usually not only executes its own code but also calls other functions in the library an api list is insufficient to identify all required code for a certain execution.
to address this problem we perform a code dependency analysis for libraries in the preparation phase to record the functions each api depends on.
the technique we use is similar to control flow analysis but focuses on inter procedural control flow transfer.
specifically we search for all the call instructions of each function and iteratively analyze their destinations.
as a result all functions that possibly be invoked by an api are regarded as dependencies.
to avoid duplicated analysis we maintain a list of analyzed functions with their dependencies.
note that this step is in the preparation phase so it does not bring extra overhead to the running phase.
additionally as the execution traces of inputs are also required to be collected via tracing tools in the preparation phase we 938esec fse december san francisco ca usa xiaoke wang tao hui lei zhao and yueqiang cheng strncmp memcpy fprintfsystemstrcpy freeprediction result process memory strncmp strcpymemcpy fprintfsystem vfprintf free0 non executable function dependencybuffered vfprintf ... figure library dynamic debloating.
according to the prediction result all library code in the process memory is set to non executable except for these apis that will be called and their dependencies.
can also evaluate the addresses recorded by the traces to validate and fix the dependencies in practice.
based on the dependencies of each function picup is able to identify other necessary library functions that will be invoked by the required api functions so that ensuring the required apis work properly in the runtime phase.
in our prototype because the analyzed inter procedural control flow transfer based on binary code in practice is usually neither sound nor complete we implement the code dependency analysis by leveraging both the symbolic analysis in angr and the static analysis in barf to enable the analysis as we demonstrated above.
to avoid missing valid dependencies by required code that may influence normal functionality we conservatively combine the results of angr and barf.
that is the final dependency includes all relationships recorded by angr and barf.
.
dynamic debloating for dynamic debloating we need to know the layout of libraries mapped into the process memory i.e.
where are the libraries in memory so that we can further locate the required code.
to do this as described in section .
picup hooks sys open at and sys mmap during execution.
as sys open at is used to open a file and sys mmap is used to map files into memory hooking these syscalls enables us to identify each loaded library and get the base memory address of each library when the library is loading.
with the memory layout of libraries picup can identify the address of each required library code in the process memory based on the prediction result and the code dependencies for debloating.
as the operating system uses pages to manage process memory picup chooses to change the executable permissions of the library code by pages.
compared to directly modifying the content in memory this way introduces a relatively negligible load.
specifically since there is usually a small portion of the code required by a specific input picup disables the executable permissions for all pages by default so that we do not need to operate on too many pages each time.
afterward for each input and the corresponding prediction result the memory pages occupied by the required code will be identified and set as executable before the process switches to user space i.e.
before the program handles input.it should be noted that the component of dynamic debloating runs in kernel space and works in cycle by determining the start exit and input of the execution so we can strictly maintain the permission during the execution of the process in user space until the next input or exit.
meanwhile picup monitors the system calls related to memory permission control such as sys mprotect sys mmap .
as a result any call that tries to recover the debloated code as executable via the memory permission control related system calls from the user space will be treated as illegal.
with this setting we can prevent the results of debloating from being affected by users to ensure reliable enforcement.
that is the restricted code cannot be invoked even if the calling context is manipulated.
taking figure as an example the apis strncmp strcpy free fprintf that are predicted will be called and their dependent functions buffered vfprintf etc.
are retained as enabled.
except for required functions all other code include system andmemcpy is set to non executable.
if an attacker hijacks the control flow to system by affecting the code branch the relevant code in memory is not executable.
instead this will cause a fault that is caught by picup and then be handled with a specific security policy.
in the prototype of picup we implement the module of dynamic debloating as a part of the loadable kernel module.
with the kernel module picup is automatically triggered to debloat library code whenever the program gets an input.
evaluation we implemented a prototype of picup with lines of c code and lines of python code on the platform of ubuntu .
lts.
to demonstrate the effectiveness of our approach we conduct comprehensive evaluations with four objectives.
code reduction.
how much code can picup reduce from the libraries?
functionality guarantee.
how accurate is our model?
can the target program work properly under picup ?
security.
how much can picup improve the security of the target program?
runtime overhead.
how much is the runtime overhead introduced by picup ?
.
experiment setup baseline techniques.
we select a state of the art online debloating technique blankit as the baseline for comparison.
additionally we also discuss the results of piece wise a representative offline debloating technique.
datasets.
we evaluate picup on spec cpu and several real world applications.
spec cpu is employed for evaluation in the previous study blankit .
to be more specific we evaluate picup on the same c c applications selected by blankit.
besides spec cpu to show that picup can debloat realworld applications with different running statuses by handling various types of input interfaces we employ multiple real world applications for evaluation.
specifically the real world applications include two web server applications nginx and lighttpd two database applications memcached and redis for which the input interface is the network and three gnu binutils programs readelf objdump andnm for which the input interface 939input driven dynamic program debloating for code reuse attack mitigation esec fse december san francisco ca usa is files and command lines.
in terms of running status these realworld applications include both long running and standalone processes.
all of the long running applications are running under the default configuration files and options.
for web server applications we deploy a copy of static html pages from wikipedia .
test cases.
the spec cpu dataset includes three types of test cases for every application denoted as test train and ref respectively.
following the configuration in blankit we use the test cases of both test small and train medium as training samples to train the prediction model.
then we use the test cases of ref large to test the model.
please note that some applications in spec cpu only consist of a small number of input cases which may lead to the over fitting problem in the prediction model.
to alleviate the impact we further leverage afl a representative coverageguided fuzzing tool to randomly generate more input cases for training.
specifically we take the original input cases as initial seeds for fuzzing and then retrieve the newly generated seeds from the working directory of fuzzing as part of the dataset since these new seeds represent inputs that trigger different program states and usually have diverse content.
for real world applications we collect the running logs from the open deployment environment for the web server and database applications and then extract inputs from the logs to construct the set of test cases.
for gnu binutils applications we randomly collect both elf and non elf files to construct the set of test cases.
finally we collect more than inputs for each application and then use ltrace to record the apis invoked by each input.
these datasets are partitioned for training and testing with a ratio of which is a typical ratio in neural network jobs.
.
code reduction to demonstrate the effectiveness of our approach in code surface reduction we leverage the code reduction rate as a metric and the calculation is reductioncode n r t n wheretrefers to the number of total instructions in libraries rrefers to the number of instructions removed by debloating techniques and nrefers to the number of executions during testing.
to compare the performance of online and offline debloating techniques we further calculate the reduction rate of the imported apis.
the calculation is reductionapi n u i n whereirefers to the total number of imported apis for the application urefers to the number of import apis that are removed in dynamic execution.
.
.
code reduction on spec cpu .
table shows the code reduction rates on spec cpu .
note that the data of blankit is directly referenced from the literature .
overall picup achieves .
code reduction on average and we can observe that the reduction rate of picup is less than that of blankit.
the reason is that when calling an api blankit only remains the current api function and its sub functions based on execution contexts while picup remains all the api functions that are predicted to be used by the currently received input.
however the predictions made by blankit are overly dependent on the program context which can be easily forged by attackers.
as discussed in section an attacker can copy back any api code they want in the section .text through context corruption.
thus despite achievingtable code reduction on spec cpu .
benchmark blankit picup .bzip2 .
.
.gcc .
.
.mcf .
.
.milc .
.
.namd .
.
.gobmk .
.
.soplex .
.
.povray .
.
.hmmer .
.
.sjeng .
.
.libquantum .
.
.h264ref .
.
.lbm .
.
.omnetpp .
.
.astar .
.
.sphinx3 .
.
.xalancbmk .
.
average .
.
table code api reduction on real world applications.
application code api nginx .
.
.
.
lighttpd .
.
.
.
redis .
.
.
.
memcached .
.
.
.
objdump .
.
.
readelf .
.
.
nm .
.
.
average .
.
a high code reduction rate the context based online debloating approaches cannot always guarantee its security enforcement.
in addition based on our statistics an average of .
imported api functions of spec cpu programs are invoked by different inputs which means that offline techniques like piece wise need to retain an average of .
unnecessary imported api functions as well as their dependencies.
note that we did not directly make a fair comparison between picup and piece wise on the spec cpu programs because piece wise customizes the compiler and loader to generate elf files and perform linking but its full implementation is not available.
according to the original experiments of piece wise it evaluates of our selected spec cpu programs only with the compiled musl libc i.e.
the other libraries are retained without change .
in such a situation its code reduction rate on musl libc achieves in worst case and in best case both of which are lower than the average reduction rate ofpicup on all the shared libraries that a program depends on.
.
.
code reduction on real world applications.
table shows the code reduction rate on the seven real world applications.
to find out how many imported apis of applications for picup are restricted in execution we also count the reduction rate of the imported api.
as shown in table the average code reduction rate is .
and the average imported api the api in got reduction rate is 940esec fse december san francisco ca usa xiaoke wang tao hui lei zhao and yueqiang cheng .
.
to investigate which code is reduced we took a manual analysis on nginx .
specifically for the .
code removal rate ofnginx all code of several imported libraries e.g.
libz libcrypto libcrypt libssl is even removed during execution based on our manual analysis because these libraries are not often used by normal http requests.
for the .
api removal rate of nginx we also notice that some risk apis e.g.
execve syscall that are rarely invoked by general input are successfully disabled by picup in execution.
in section .
we will further analyze how does the reduced attack surface by our approach improves security.
in short in addition to removing most code our approach specifically restricts many risk apis according to the learning of realistic situations by predictive models.
.
functionality guarantee in this section we evaluate whether picup can guarantee the normal functionalities of executions on different inputs.
if picup accurately retains the required functions then the program can undoubtedly run normally.
therefore we use the prediction accuracy as a metric which is defined as the percentage of apis that are correctly predicted and the calculation is accuracy ia i where iarefers to the api correctly predicted and irefers to the number of imported apis in a binary got.
besides there are two situations for inaccurate predictions false positive and false negative.
a false positive refers to an api that is useless but predicted to be required and a false negative indicates an api that is required but predicted to be useless.
as false negative can influence normal functionalities of the target program we take false negative rate fnr to further show the negative impact of our prediction model on normal executions.
fnr is calculated as fnr if n i whereifnrefers to the api with false negatives and irefers to the number of all invoked apis during executions.
it should be noted in our evaluation once the prediction does not have false negatives the program will work normally as the obtained code dependency has been already revalidated and fixed based on the pre collected traces of the samples.
however it is hard to perform complete and sound code dependency analysis on binary in practice especially for resolving indirect calls and missing code dependency may also influence normal functionality.
if such cases occur we need to trace the execution of the input and confirm that the control transfer is natural then we can fix the code dependency based on the trace to alleviate the negative impact.
in this way the code dependency will continuously be more complete.
.
.
accuracy and fnr on spec cpu .
table shows the prediction accuracy and fnr for the applications in spec cpu .
note that piece wise has no accuracy data as it is an offline approach without prediction.
for blankit we find that it is hard to fairly reproduce its results due to its public implementation and datasets are incomplete so we directly present the accuracy from the original paper .
while blankit does not report the fnr so its fnr is missing in table .
from table we can observe that the accuracy of picup .
is higher than that of blankit .
.
in particular picup outperforms blankit in prediction for out of the applications as underlined in table .
moreover picup also outperforms blankittable prediction accuracy and fnr on spec cpu .
benchmarkaccuracyfnrblankit picup .bzip2 .
.
.gcc .
.
.mcf .
.
.milc .
.
.namd .
.
.gobmk .
.
.soplex .
.
.povray .
.
.hmmer .
.
.sjeng .
.
.libquantum .
.
.h264ref .
.
.lbm .
.
.omnetpp .
.
.astar .
.
.sphinx3 .
.
.xalancbmk .
.
average .
.
.
table accuracy and fnr on real world applications.
application accuracy fnr nginx .
.
.
.
lighttpd .
.
.
.
redis .
.
.
.
memcached .
.
.
.
objdump .
.
.
readelf .
.
.
nm .
.
.
average .
.
even for the worst case.
picup provides .
accuracy on .milc whereas blankit merely achieves accuracy on .libquantum .
meanwhile we can also observe that picup achieves .
fnr for all applications except three applications .gcc .mcf and .gobmk .
specifically the fnr for these applications are .
.
and .
respectively.
our manual analysis shows that the test cases of .mcf are uninformative numbers with very large size.
therefore it is difficult for the model to extract accurate features that determine program behaviors.
the input of .gcc contains some complex syntax information which makes it difficult to predict.
the input of .gobmk a smart game format file consists of semantics rich fields.
it defines some special symbols to represent specific program states.
such semantics rich fields are difficult to extract by our prediction model.
.
.
accuracy and fnr on real world applications.
table shows the accuracy and fnr on seven real world applications.
from table we can observe that the average prediction accuracy of picup is .
and the fnr is .
on seven real world applications.
overall these results indicate that our prediction model can accurately predict the control bytes in inputs that determine program behaviors.
to find out what key bytes are extracted we also made a further analysis on the prediction process.
take nginx as a brief example we found that our model successfully identifies the word gzip contributes to the invocation for the api 941input driven dynamic program debloating for code reuse attack mitigation esec fse december san francisco ca usa table reduction of cve vulnerability functions in glibc on spec cpu .
means the vulnerability function is eliminated bypicup while means the function is not eliminated.
glibc vulnerability benchmark cve id glibc vul.
bzip gcc mcf milc namd gobmk soplex povray hmmer sjeng libquantum h264ref lbm omnetpp astar sphinx3 xalancbmk total .
wordexp .
iconv .
iconv .
.
iconv .
.
glob .28parse reg exp .
realpath .
realpath .
mempcpy .26posix memalign total deflateinit2 and this api actually will be invoked if the received input requests compression through gzip .
furthermore we can also find that the prediction accuracy of two cached database applications is relatively lower than that of other applications.
based on our manual analysis we find that some api calls depend on the environment of the program such as whether the query data exists in memory.
however it is difficult for our model to obtain the environmental information from the input alone.
this reason leads to less accuracy of the model in these applications.
conservatively predicting the required functions by turning down the threshold for neuronal activation is a possible way to mitigate the negative impact brought by false negatives on the normal functionalities of the target program but it may also bring more false positives so that the code reduction rate will be reduced.
in fact it is almost impossible to eliminate all inaccurate predictions on diverse real world programs even if considering every potential factor.
in section we present discussions on the possible ways to handle false predictions.
.
security to find out how much our approach improves the security we evaluated picup from the following three perspectives.
.
.
reduction of rop gadgets.
to show how much of the program s attack surface is reduced we firstly count the rop gadgets that are removed after receiving each input.
rop gadgets are pieces of code that can be run in a certain order to carry out attacks by using return oriented programming.
the reduced gadgets will restrict an attacker s capabilities in practice so that the difficulty and effort required for attacks will also increase even if there are still other security risks in the remaining code.
to measure the reduction of rop gadgets we leverage angrop an rop gadget finder and chain builder.
for each program in the spec cpu we recorded the code that is removed by picup under test case input and then calculated the removed gadgets rate according to the proportion of removed gadgets in all gadgets of each library.
table shows the rop gadgets reduction rate in libraries on spec cpu .
in total an average of .
of rop gadgets were removed by picup when the program is running.
specifically picup removes an average of .
rop gadgets from libc .
and the reduction rate reaches .
for libm .
.table reduction of rop gadgets on spec cpu .
benchmark libc2.
.solibm2.
.solibgcc s .so.1libstdc .so.
.
.25avg.
bzip2 .
.
gcc .
.
mcf .
.
milc .
.
.
namd .
.
.
.
.
gobmk .
.
.
soplex .
.
.
.
.
povray .
.
.
.
.
hmmer .
.
.
sjeng .
.
libquantum .
.
.
h264ref .
.
.
lbm .
.
.
omnetpp .
.
.
.
.
astar .
.
.
.
.
sphinx3 .
.
.
xalancbmk .
.
.
.
.
avg.
.
.
.
.
.
.
.
reduction of glibc vulnerability.
another security benefit ofpicup is the reduction of vulnerable code in the library.
that is the library code containing the vulnerability is removed by picup for specific input during one execution.
it helps the program to avoid related attacks.
to demonstrate this ability we collected a total of cves on glibc gnu c library published in recent years.
we prepared vulnerable libraries linked by the spec cpu benchmark program and checked whether vulnerable functions were effectively removed by the debloating process.
table shows the evaluation result including the cves vulnerability functions on the spec cpu benchmark programs.
in out of cves the vulnerability functions were removed with an effect of no less than .
in particular three of them were removed in all program runs.
the worst result was theposix memalign function in cve which was retained by picup for being a possible dependency for the program.
from the perspective of the programs of the programs also achieved a reduction rate for the glibc vulnerability functions.
.
.
case study real world exploit defense in nginx.
in order to further study the effectiveness of picup under real exploits we 942esec fse december san francisco ca usa xiaoke wang tao hui lei zhao and yueqiang cheng evaluation security .
.
.
.
.
.80normalized running timenative blankit picup figure normalized running time on spec cpu .
table details about the exploits in nginx defended by picup no.
exploit source type blocked key api s i exploit db ret2system system ii github ret2shellcode mprotect iii brop brop strcmp usleep dup2 execve a a normal input b an exploit input figure a normal input and an exploit input.
the difference lies in the size and content of the chunk.
chose nginx the most popular web server in the world as the object of discussion.
cve is a high risk stack overflow vulnerability in the versions .
to .
of nginx.
the remote attackers are allowed to crash or attack nginx via a chunked transfer encoding request with a large chunk size which triggers an integer signedness error and a stack based buffer overflow.
we gathered three exploits from exploit db github and brop that can successfully attack nginx under cve .
the exploit i from exploit db calls the system api in libc by reading the address in the got and adding an offset.
the exploit ii from github first makes the buffer executable by calling themprotect api through the rop chain and then executes the shellcode written to the buffer.
the exploit iii from brop is more complex.
briefly the attack steps include 1find a stop gadget and plt 2find the brop gadget to control the first two arguments to calls 3findstrcmp in the plt to control the first three arguments to calls 4findwrite in the plt to dump the entire binary to find more gadgets 5build a shellcode and exploit the server.
table shows the details of these exploits.
although these exploits differ in their approaches the requests they send to nginx are actually similar in format see figure b .
more specifically in order to ensure that the vulnerability is successfully triggered the header of these requests must be transferencoding chunked the same as the normal requests in figure a .
these exploit requests differ from normal requests only in the oversized chunk size and the constructed payload.
therefore when receiving these exploit requests picup will treat them as normal requests and provide the same apis as normal requests.in general all these attacks can be successfully defended by picup .
in detail exploit i fails since the system is not executable and exploit ii fails to run the shellcode when mprotect does not work.
for exploit iii brop fails in step 3when trying to control the first three arguments since strcmp cannot be executed.
besides at the last step it also cannot redirect the socket to standard input and output because dup2 is disabled cannot write payload because usleep is disabled and cannot execute the shell because execve is disabled.
even if there are other ways to perform attacks we believe the restricted apis will at least increase their difficulty.
.
runtime overhead in this section we evaluate the runtime overhead of picup on spec cpu and real world applications.
besides we also compare the runtime overhead of picup with that of blankit .
.
.
runtime overhead on spec cpu .
first we measure the running time of each benchmark in spec under picup and compare it with the original running time.
all the applications are tested three times and the average normalized running time is shown in figure .
please note that the data in figure about blankit is directly cited from the literature .
from figure we can draw several observations.
first our approach introduces less than .
runtime overhead for out of the applications.
in particular the runtime overhead is only increased by .
and .
for .sjeng and .omnetpp respectively.
second picup introduces lower overhead than blankit for out of all the applications.
the overhead of our approach is only .
whereas the overhead of blankit is .
third we can observe that picup outperforms blankit even for the worst case.
the worst overhead of picup is .
for .gobmk while the worst case of blankit is for .gcc .
the main reason why picup performs better than blankit is that blankit debloats for every api call during execution while picup performs debloating only once when an execution receives an input.
for instance an execution of .sjeng invokes api functions times.
with such a large number of api calls blankit will perform debloat times too and thus introduces overhead.
as a comparison picup only introduce .
overhead.
.
.
runtime overhead on real world applications.
second as shown in table we measure the incremental time brought by picup and blankit on real world applications by calculating the processing time between two inputs and minus the raw time.
due to the lack of partial code of blankit for training the prediction 943input driven dynamic program debloating for code reuse attack mitigation esec fse december san francisco ca usa table runtime overhead on real world applications.
application api calls rate picup ms blankit ms nginx .
.
.
lighttpd .
.
.
redis .
.
.
memcached .
.
.
nm .
.
.
readelf .
.
.
objdump .
.
.
model we skip the lightweight decision tree based predictions and directly estimate the overhead of blankit by applying the simplified rules as follows.
first when an api is called we just copy the api and all its sub functions i.e.
dependencies back.
second we clear all copied back functions before another api is called.
since picup works on every input while blankit works on every api call the key overhead difference between them is related to the ratio of input to api call although the time spent by picup on each input and blankit on each call may vary.
for revealing the above factor we measure the amount of api calls that the program makes after each input which we call as api calls rate .
as shown in table for low api calls rate programs blankit performs better than picup but picup also has an acceptable results.
with the increasing of api calls rate the overhead of blankit can be extremely high.
taking objdump as an example it calls the api on average .
times after each input and blankit generates .
ms of extra time which is 1160x longer than picup .
by contrast the overhead of picup does not depend on api calls rate but can be influenced by the input itself so picup has a similar overhead on the applications that receive the same type of input.
related work software debloating is a scheme to reduce the attack surface by eliminating unused code.
among types of debloating techniques some of them aim to debloat the code in the program space rather than libraries.
trimmer identifies unnecessary functions by user defined configurations and then statically eliminates the code.
damgate is a framework for dynamic feature customization.
it uses static and dynamic analysis to rewrite binaries.
chisel employs a reinforcement learning approach based on user provided test cases to debloat software.
razor uses binary rewriting to produce the program that only supports necessary functionalities.
it does this by collecting the execution code of the software running on a given input and then uses heuristics to infer the non execution code associated with the given input.
there are also several approaches in library debloating.
piecewise introduces a specialized compiler to accomplish program debloating.
it uses static analysis and training based techniques to compute function level dependencies and then removes unneeded functions at load time.
nibbler performs similar library specialization at the binary level.
it creates an application level fcg by extracting the function call graph fcg of the binary and all imported libraries and removes any untouchable code.
in embedded systems ziegler et al.
do this by both static analysis and dynamic tracing and a recent work trimmer explores the offline library debloating for binaries on mips architecture.
theseoffline library debloating techniques directly remove unused code from libraries and are robust during the dynamic execution.
however they retain code for all inputs and cannot remove more code for each concrete execution.
the current online technique blankit is a context sensitive approach for debloating.
it leverages a decision tree to predict subfunctions that will be used by the calling api based on call site arguments and reverse dominance frontier of arguments and then provides only those sub functions.
although blankit removes more code it is vulnerable if an attacker forges proper contexts.
there are also debloating techniques for specific applications.
kasr aims to remove unused code from os kernels.
several studies aim to slim down the containers.
other studies aim to debloat java programs the java virtual machine jvm web applications bluetooth stack and browsers .
discussion error handling.
the prediction model may make false predictions which can further result in exceptions that the required code is prohibited by picup .
therefore it is significant to handle errors and distinguish them from attacks.
as in previous techniques picup can employ similar error handling mechanisms such as a virtual machine with check pointing memory forensics and memory safety check.
for example when an application runs fault due to a page permission error we move the entire process to a secure monitored environment to continue running.
if the process runs without risk operations it is assumed that a prediction error has occurred and the corresponding page permissions will be restored.
conversely an attack is considered to have occurred.
malicious input.
attackers may construct adversarial samples to maliciously misguide our prediction model and further invoke unused code.
we acknowledge that this situation cannot be thoroughly avoided but we argue that there are potential approaches that make such attacks difficult.
first we can employ some negative samples and label them as all zero lists which indicates that no code in libraries is permitted to train the prediction model for robustness.
second with the threat model of our approach the details of the prediction model are agnostic to users.
that is the prediction model is a black box to attackers which increases the difficulty for attackers to implement attacks.
conclusion in this study we aim to balance the code reduction and the enforcement reliability of debloating and propose picup a per input debloating approach that dynamically reduces the library attack surface for each input.
we evaluate picup on real world benchmarks and popular applications.
the experimental results show thatpicup is a practical solution for secure and effective library debloating which can predict the necessary library functions with .
accuracy and reduce the code size by .
on average with low overheads.