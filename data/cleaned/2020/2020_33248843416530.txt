ocor an overlapping aware code retriever qihao zhu key lab of high confidence software technologies ministry of education department of computer science and technology eecs peking university zhuqh pku.edu.cnzeyu sun key lab of high confidence software technologies ministry of education department of computer science and technology eecs peking university szy pku.edu.cnxiran liang key lab of high confidence software technologies ministry of education department of computer science and technology eecs peking university liangxiran11 .com yingfei xiong key lab of high confidence software technologies ministry of education department of computer science and technology eecs peking university xiongyf pku.edu.cnlu zhang key lab of high confidence software technologies ministry of education department of computer science and technology eecs peking university zhanglucs pku.edu.cn abstract code retrieval helps developers reuse code snippets in the opensourceprojects.givenanaturallanguagedescription coderetrieval aimstosearchforthemostrelevantcoderelevantamongasetof code snippets.
existing state of the art approaches apply neural networks to code retrieval.
h owever these approaches still fail to capture an important feature overlaps.
the overlaps betweendifferent names used by different people indicate that two differentnamesmaybepotentiallyrelated e.g.
message and msg and the overlaps between identifiers in code and words in natu ral language descriptions indicate that the code snippet and the description may potentially be related.
to address this problem we propose a novel neural architecture named ocor1 where we introduce two specifically designed components to capture overlaps the first embeds names by characters to capture the overlaps between names and the second introduces anoveloverlapmatrixtorepresentthedegreesofoverlapsbetween each natural language word and each identifier.
the evaluation was conducted on two established datasets.
the experimentalresultsshowthatocorsignificantlyoutperformsthe existing state of the art approaches and achieves .
to .
improvements.moreover wealsoconductedseveralin depthexperiments to help understand the performance of the different components in ocor.
corresponding author 1ocor is short for an o verlapping aware co de retriever.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
concepts information systems novelty in information retrieval software and its engineering computing methodologies information extraction neural networks keywords code retrieval neural network overlap acm reference format qihao zhu zeyu sun xiran liang yingfei xiong and lu zhang.
.
ocor an overlapping aware code retriever.
in 35th ieee acm international conference on automated software engineering ase september melbourne australia.
acm new york ny usa pages.
introduction coderetrievalisanimportantsoftwareengineeringproblem which aims to retrieve the most related code snippet among a set of code snippetsbyagivennaturallanguagedescription.aneffectivecode retrieverhelpsdevelopersreusethecodesnippetsfromtheinternet.
for example if a sql programmer gives an instruction get all the dataintablea acoderetrieverwillhelptheprogrammertosearchfromthelargescaleofcodeontheinternetandfindthetargetcode select from a .
withthedevelopmentofdeeplearningandthecollectionoflarge scale labeled datasets neural networks have been widely used for variousareas .forthetaskofretrieval various approaches have been proposed by using neuralnetworks.theseapproachesmostlyembedthequestionandtheanswerintoahigh dimensionalvectorspaceandtrytofindthe most similar one between the vectors of questions and the vectors ofanswers e.g.
usingcosinesimilarity .whenitisappliedtocode retrieval it takes the natural language description as the question and the target code as the answer .
however these retrieval approaches fail to effectively handle overlaps whichareimportantincoderetrieval.ononehand differ entpeoplemayusedifferentnamestodescribethesimilarmeanings either in codeor in natural languages andsuch names often have overlapped substrings.
for example sort and quicksort has theoverlappedsubstring sort .ontheotherhand identifiersin 35th ieee acm international conference on automated software engineering ase !
!
!
!
.
figure an example from the staqc dataset.
the code retrieval in our approach ranks candidate code snippets withthe scores given by the model.
code are often related to words in the natural language description.
though they may not be fully equal overlapped sub strings often exist.
for example in figure the identifier joint table b is re lated to the words joint and table .
as far as we are aware noexisting neural architecture is specifically designed for handling overlaps.
to address these problems we propose a novel neural architecture ocor acoderetrieverbasedontheoverlapfeatures.werepresent each word by combining the representations of the characters withinit namelyusingthecharacter levelembeddingtocapture the overlap between the names used by different programmers.
furthermore we introduce a novel overlap matrix to represent the degrees of overlaps between each word in the natural language description and each identifier in code.
finally we combine different code retrieval approaches by ensemble to enhance our model.
theexperimentwasconductedonseveralestablisheddatasets forsqlandc coderetrieval followingiyeretal .
yaoetal .
.
the experimental results show that our model significantly outperforms existing approaches by .
to .
improvements and achieve the best performance on all the datasets.
to better understandourmodel wealsoconductedtheexperimentsfocusing ontheeffectivenessofthecomponents andtheresultsshowthat each component contributes to the overall performance.
to summarize this paper makes the following contributions weproposeanovelneuralarchitecture ocor forcoderetrieval.
ocor uses two novel techniques namely characterlevelembeddingandtheoverlapmatrix tocapturetheoverlaps between identifiers in code and words in natural language descriptions.
weconductedextensiveexperimentstoevaluatetheeffectiveness of our approach and the components in our ap proach.
the results show that our approach significantly outperforms existing approaches by .
to .
improvements and all components in our approach are effective.
motivation asmentionedintheintroduction therearetwotypesofoverlapsin thecoderetrievaltask.thefirsttypeofoverlapisthatdifferentpeople may use different names to describe the similar meanings e.g.
thewords innaturallanguageand theidentifiersincode .for example joint table a and joint table b in the first sql query in figure1couldalsobenamedas joint table 1 and joint table 2 .
figure2 examplesofthecomputationofthecharacter level embedding and the overlap matrix.
ifaneuralnetworkistrainedoverthecodeinfigure1 itisdifficultforittoknowthattheidentifiers joint table 1 and joint table 2 are also related to the same query.
to address this challenge existingapproaches forcoderetrievalreplacingvariablenamesand raw strings with the variable types and numbers e.g.
rename the first table variable joint table b in a sql with table 1 .
in this way theneuralnetworkisforcedtoignoretheidentifiernamesbut usesthestructureofthecodeandtheidentifiertypes.however thename of the identifier potentially carries useful information for thecoderetrieval andignoringthemislikelytolowertheperformance.
tosolvethisproblem weproposecharacter levelembeddingtoencode the names.
the character level embedding first encodes each character within each name via one hot encoding and combine theserelativevectorsbyaconvolutionallayer.figure2 a shows the computation of character level embedding of joint table b and joint table c .as shown the combinedvectors ofthese two identifiersarealmostcomputedfromthesamevectorsexceptthe vector of the last character.
thus the final embedding of these identifiers is closed to each other in the high dimensional space.
thesecondtypeofoverlapisthatidentifiersincodeareoften related to some words in natural language description.
in a general perspective this type of overlap is the overlap between questionand answer and is often considered in existing information retrievalapproaches.theseapproachesmeasurethenumberofthe exactly matched tokens between the questions and the answers.
however inthecoderetrievaltask identifiersincodeandwords innaturallanguagedescriptionsareoftennotfullyequal.forexample as shown in figure the identifier joint table b is not fullyequaltoanywordinthenaturallanguagedescription butit is related to two words joint and table .
to address this problem we not only consider exactly matched words but also measure the degree ofoverlaps between partially matchedwords.
we design a representation namedoverlapmatrix torepresentthedegreeof overlap between each word in natural language description and each identifier in code.
in this matrix each row represents a word in the natural language description while each column represents an identifier in code.
each cell is the degree of overlap between the word and the identifier.
the degree of overlap can be measured by differentmetrics andinthispaperweusethelongestcommonsubstring theproportion ofthe longest consecutivesub string pthat appears in both the natural languages word and the code identifier.
figure2 b showsapartialoverlapmatrixofthequestion codepair in figure .
we can see that though identifiers joint table b and figure an example of the computation convolutional layer.
word joint are not exact match their degree of overlap is still higher than most other pairs.
finally our model takes the overlapmatrixasinput utilizingthedetailedoverlapinformationfor identifying the most related code snippet.
background .
convolutional layer the convolutional layer which is the main building layer of a convolutional neural network cnn has been widely used in variousareas .thislayercanberegardedasaregularizedversionofafully connectedlayer.thefully connectedlayer usuallyconsistsofseveralneurons andeachneuroninonelayer is connected to all neurons in the next layer.
different from such a fully connected layer the connection in the convolutional layer is connected from each neuron in one layer to several corresponding neurons in the next layer.
such connections depend on pre defined convolutional kernels.
innaturallanguageprocessing theconvolutionallayerisused toextractthefeaturesinthecontents.givenaninputvector which represents the words in natural language the convolutional layer uses kernels to extract the features in each vector and its neighboringvectorsandoutputsanewvector.forexampleinfigure3 foran inputvectorofasentence thisisacoderetriever withkernelsize the layer outputs a new vector of is by a weighted summation ofthevectorsof is withitsneighbors this and a .thishelps capturethefeaturesofthecontentsintheinputnaturallanguage descriptionandcodeofocor.thus weapplyitinourapproach.
the details of using this layer will be introduced in section .
.
attention inthebasicencoder decoderframework themodelalways suffers from the the long dependency problem with long sequences.toalleviatethisproblem bahdanauetal .
proposedthe attentionmechanism whichaimstolettheneuralmodelinspect the relevance between each pair of tokens in two long sequences.
recently tobetteralleviatethelong dependencyproblem vaswani etal.
proposedawidelyusedattentionmechanismcalledmultihead attention.
the overview of such mechanism is shown in 2the encoder decoder framework which is also known as sequence to sequence framework isawidelyusedapproachinneuralmachinetranslation.inthisframework the neural network is divided into two parts encoder decoder.
the encoder encodes the sequence into a vector and the decoder decodes the vector to a sequence in target language.
figure the detail of multi head attention.
figure a .
this mechanism takes three vectors mostly the representation of the words in the input sequence as inputs and maps a query vector and a set of key value vector pairs to the output.
the main computation of this attention is called dot product attention layers where the input of each layer consists of query vectors q key vectors k and value vectors v as shown in figure b .
the weightsofthevaluevectorsarecalculatedbythequeryvectorsand thecorrespondingkeyvectors.finally theoutputiscomputedasaweightedsumofthevalues wherethequerydetermineswhich values to focus on.
self attentionisanattentionmechanismforasinglesequence to extract the complex comprehension of itself formally q k v .
this technique is often used to capture the long dependency information in sequences and has good performance in various tasks .thedetailedcomputationoftheattentionmechanism will be introduced in the following section.
proposed model .
problem definition we follow the existing studies and use the same definition forcoderetrieval.givennaturallanguagedescription qandaset of candidate code snippets c our task is to retrieve a relevant code snippetcr cthat specified by q. asshowninfigure1 toretrieveacodesnippet wefirstcompute the relevance score between each code snippet c cand the input naturallanguagedescription q.then werankthecodesnippets in the set of candidate code snippets c. finally the code crwith the highestscore is selectedas theoutput of ourapproach which is computed as cr argmax c cr q c whererdenotes the computation of the relevance score.
in our approach the relevance score is a real number between and .
.
overview figure shows the overview of ocor.
we adopt the traditional overallarchitectureusedininformationretrievalfuetal .
where we encode the question the natural language description and the !
!
!
!
!
!
!
!
figure the overview of the ocor.
the a code nl anda nl code are the overlap matrices.
answer the code snippet respectively and combine the outputs via attention layers for further predicting the target relevant score.
based on this architecture we design two encoders with the same structureforthequestionandtheanswer.eachencodertakesthe overlap matrix and the question answer as input and turns this input into a set of vectors.
furthermore as the evaluation will show later our model complements existing approaches on code retrieval.
to achieve even better performance we use an an additional ensemble component to combine previous code retrieval models with ocor.
in the rest of this section we will describe the components of our architecture one by one.
besides .
input of model the inputs of ocor are divided into three parts the natural language description the code snippet the overlaps between the natural language description and the candidate code snippet where the last one is computed from the first two.
.
.
preprocessing.
for the first two parts we first process these inputstomakethemsuitabletobefedtotheneuralnetwork.for the input natural language description we first tokenize this input by the tool in the nltk toolkit and convert its characters withinthetokenstolowercase.asfortheinputcodesnippet we keeptheoriginalvariablenamesandtherawstrings tokeepthe semanticinformationinthenames.then wetokenizethecodeand alsoconvertthecharacterswithinthenamestolowercase.thus we get the preprocessed inputs for the neural network.
.
.
overlapmatrix.
asmentionedbefore weusetheoverlapmatrix to represent the degrees of overlaps between natural language words and code identifiers.
the overlap matrix is a real valued matrixa t1 t2 rl t1 l t2 which containstheoverlapscores between a token sequence t1and another token sequence t2.
each cellaij t1 t2 in this matrix denotes the overlap score between thei th token t1 i in thet1sequence and the j th token t2 j inthet2.
such score in ocor is computed by aij t1 t2 len s t1 i t2 j len t2 j wherelen t2 i denotesthelengthoftheword t1 i s t1 i t2 j denotes the longest common sub string of t1 t2.
in particular thecomputationoftheoverlapmatrix aisnotcommutative i.e.
a t1 t2 a t2 t1 .inourapproach weconsiderboththeoverlap scorebetweenthenaturallanguagedescription nlandthecode codeaswellastheoverlapscorebetweenthecodeandthenatural languagedescription namelyboth a n c anda c n respectively.
these two metrics are further fed to the encoder layer to extract features for code retrieval.
.
encoder in ocor there are two encoders for both the natural language description and the code.
each encoder takes the overlap matrix andthe naturallanguagedescription codeasinputs.
theseinputs are encoded into vectors in a high dimensional space for further similarity computations.
to better encode the input information inspired by vaswani et al.
we design the encoder with a stack of nmechanisms.
eachmechanismcontainsthreesub layers aselfattentionlayer agatinglayer aconvolutionallayer.aftereachmechanism the resnet 3and the layer normalization 4are used.
for the firstmechanism ittakestheoverlapmatrix namely a nl code a code nl asinputandfurthercombinesthenaturallanguage description the code.
for the rest of n mechanisms they take theoutputofthepreviousmechanismasinputandalsocombine thefeaturesofhiddenlayersinthenaturallanguagedescription the code.
wewill first describe how we feedthe overlap matrix to the first mechanism.
3resnet is residual learning framework to ease the training of networks.
4layer normalization normalizes the values of the neurons into a suitable distribution which eases the training.
886input overlap matrix.
the overlap matrix5for our approach is a real valued matrix where each cell denotes the overlap scores between the natural language words and the identifiers in code.
to take this matrix as input we first reduce the matrix a nl code into an overlap vector a nl where the i th element ai nl in thisvectordenotesanewoverlapscorecomputedfromthe i throw in thea nl code the cells representing the i th token in the naturallanguageandeachidentifierinthecode viamax pooling.
max pooling has been shown to be an effective way to reduce the matrix into a vector in various areas .
following max pooling we select the maximum value for each column as the element of a nl which is computed by ai n len c max j 1aij n c this vector contains the maximum overlap scores.
for example the fixed size vector of the overlap matrix in figure b is .
to ease neural processing of the overlap scores we further embed the scores with one hot encoding.
please note that the scores are the real values between and .
we partition the scores with the interval .
and use a one hot vector of length to encode the scores.
.
.
selfattentionlayer.
thefirstsub layerintheencodermechanismistheselfattentionlayer.asweknow sequentialinformation is important in both the natural language description and the code.
to handle such information effectively we apply the self attention mechanism whichisproposedbyvaswanietal .
andhasshown to be an effective way to encode this information as the first sub layer in the encoder.
theselfattentionlayertakesthepreviousoutputvectors o1 o2 ol as input where ldenotes the length of the input natural language description code.
this layer consists of two parts the position embedding layer multi head attention layer.
position embedding layer.
a position embedding layer is a standard layer in transformer architecture to provide the indexesofthewordsintheinputmatrix.forexample thelayershould know the word joint is the th token in the natural language description in figure .
if we directly use an attention layer for the input vectors the position information will not be considered and this is why we need the position embedding layer.
in this layer the vector of the i th position is represented as a real valued vector which is computed by p i 2j sin pos 100002j d p i 2j cos pos 100002j d wherepos i step jdenotestheelementoftheinputvectorand stepdenotestheembeddingsize.afterwegetthevectorofeach position we directly add this vector to the corresponding input vector where ei oi pi.
multi head attention layer.
the second part of the self attentionlayeristhemulti head6attentionlayer.asintroducedin thebackgroundsection anattentionmechanismmapsaquery a 5we takea nl code as example in this section.
6themulti headmechanismconsistsofseveralheads eachofwhichisanattention layer separately.
the outputs of these heads are further jointed together by a fullyconnected layer.keyandavaluetoanoutput.inthislayer thequery thekey the value and output are all vectors.
following the definition of vaswani et al .
we divide the multi head mechanism into hheads.
each head is an attention layer which maps the query q the key kand the value vto an output namely the output of each head head.
the computation of thes th head is represented as heads softmax qkt radicalbig dk v wheredk d hdenotesthelengthofeachextractedfeaturevector andq kandvarecomputedbyafully connectedlayerfrom q k v. in the encoder the vectors q kandvare all the outputs of the positionembeddinglayer e1 e2 el.theoutputsoftheseheads are furtherjointed togetherwith afully connected layer which is computed by att wh wherewhdenotestheweightsinfully connectedlayerandtheoutputvectors att arethehigh levelvectors which combinethesequentialinformationandtheoriginalinformation together.
however these vectorsstill fail to encode the semantic informationofeachwordeffectivelyatleastinthefirstmechanism in the encoder.
thus we will then describe how we address this issue via a gating layer .
.
.
gatinglayer.
thesecondsub layerintheencodermechanism is a gating layer.
this layer takes the outputs of the previous layer and the input natural language description code as input.
existing state of the art approaches use the word2vec mechanismtoutilizethesemanticinformationoftheinput.however it may be not suitable for code retrieval where the similar identifierscanbenameddifferentlybydifferentprogrammersbut withoverlappedcharacters e.g.
itmayhaveaverysimilarmeaning fortwowords dataid and data id whichmayleadtoalargenumber of vocabulary for neural networks to learn.
to address this issue we propose to use the character level semantics to catch the overlaps between identifiers in code retrieval.
we combine the outputsofthepreviouslayerwiththecharacter levelembedding approach for each word.
character embedding .to implement the character embedding wefirstpadeachtoken boththewordinthenaturallanguage descriptionandtheidentifierincode toafixedlength clwitha special character.
in particular if the length of the token is more thancl wetruncatetheendofthistokenandmakeita cl length token.
then we represent each character in the token as a realvaluevector namely embedding .asweknow atokenconsistsof severalcharacters.tocatchthesemanticinformationofeachtoken we adopt a set of convolutional layers to integrate the vectors of the characters within the token.
the extracted semantic vector for thei th token tiis computed by t i n w c n wherewcare the convolutional weights and ndenotes the n th layeroftheconvolutionallayers.inparticular t k ck where ck denotesthecharacterembeddingvectorofthe k thcharacterwithin thei thtoken.inourapproach wehavethreeconvolutionallayers 887for this character embedding layer.
for the first two convolutional layers we use the zero padding and the sizes of the convolutional kernel are set to and respectively.
gatingmechanism .toincorporatethe semanticinformation of each token with the previous outputs we use a mechanism namedgatingmechanism .thismechanismincorporatesan inputsemanticvector tiwithagivencontrolvector7withthemultihead mechanism.
in this paper we use the previous output vectors namely ai as the control vector.
the computation of the gating layer in our model can be represented as o i exp qt iko i d c i exp qt ikc i d hi o i vo i c i vc i o i c i heads where qi ko i vo iareallcomputedbyafully connectedlayeroverthe control vector ai kc i vc iare computed by another fully connected layer over the semantic vector ti.
after this computation we enhancethevectorswiththesemanticinformation andtheextracted new features are denoted as c com c com c com l. .
.
convolutionallayer.
thefinalsub layerintheencodermechanism is a set of convolutional layers.
we follow the design of the encoderproposed byvaswaniet al .
and adopt aset ofconvolutional layers to extract the local features around each token.
the computation of the convolution layer can be represented as yl i wl yl i w yl i w whereldenotesthe l thconvolutionallayerintheset wlarethe convolutional weights w k andkdenotes the window size.
in particular yl iis the output of the previous gating layer c com i. we use two convolutional layers in this sub layer and add the activation function gelu between these convolutional sub layers.
in particular we use the zero padding in these layers.
.
max pooling afterallthese nmechanismsintheencoder weadoptanadditional convolutional layer as equation which is padded with a special vector during convolution.
then we get the final features of each token.
the features denote the high level information of the input natural language description code.
however these features have the same shape as the input.
to facilitate further prediction for code retrieval we need to aggregate such features into a fixed size vector which is regardless of the input size.
max pooling has shown the power in aggregating features thus weapplythemaxpoolingapproachovertheoutputsoftheencoder and extract the fixed size vector for each encoder.
7thecontrolvectoristhespecialvectorsgiveninourapproach.thisvectordecides the weights of different vectors.
.
attention layer theencodersencodetheinformationoftheinputnaturallanguage description and the input code separately.
however it still lacks the relations between two inputs even if we have used the prior knowledge of the overlaps.
to help the neural network learn such relations between the two inputs we adopt two attention layers after the encoder.
as described in the previous section the outputs of the encoder combine the overlap information with semantic information character embedding .
thus we also apply the multi head attentionlayer to the outputs of two encoders to extract the relations.
as for the description and the code we design two separate attention layers for them.
the computation of the attention mechanism is similartothe selfattention withdifferentinputs.onelayertreats the encodingof the descriptionas query q andthe other treats theencodingofthecodeasquery q .thisdesignallowsthemodel toextracttheweightedsumoftheoutputsoftwoencodersbasedoneachother.aftertheattention twoconvolutionallayersanda max pooling layer are followed to integrate the features.
.
prediction after all the computation of the attention layer we concatenate all features.theyarefurtherfedtoatwo layerperceptronfollowed by asoftmaxactivation.
the output of these computation is the classification probability of two classes.
the first class denotes that theinputnaturallanguagedescriptionandtheinputcodeisrelated whereasthesecondclassdenotesthattheinputnaturallanguage description andthe input code isnot related.
in ourapproach the predicted classification probabilityof the first class isthe relevancescore between the input natural language description and the code where the relevance score is computed by r q c exp h1 summationtext.
j 1exp hj wherehiis the input logit of softmax.
.
training our model is trained by minimizing cross entropy loss against the groundtruth.specifically foreachtrainingdata q c a where qisthedescription cisthecode adenotesthegroundtruthclass.
the cross entropy loss is computed by loss summationdisplay.
i 1g i log i wheregdenotesthegroundtruthclass istheclassificationresult predicted by the neural network.
.
model combination onthe basisof ourbasic modelintroduced above wealsoconsider an additional method that combines different models on code retrievaltasktogetherbyensemble.inspiredbyyaoetal .
where theproposedcoacorcombinescoderetrievalwithcodeannotation forbetterperformance.weconsidertocombinedifferentmodels byintegratetherelevancescorescomputedbydifferentmodelsand outputthefinalrelevancescoreforocor.thescoreiscomputed 888by a linear combination as r q c s1 s2 wheres1denotes the relevance score computed by ocor s2is the scorecomputedbythecombinedmodel isarealnumberbetween and .
experiment setup in this section we present the experimented setup.
we will first introduce the research questions8.
.
research question our evaluation aims to answer the following research questions rq1 what is the performance of ocor?
to answer this question we conducted an experiment on several established datasets and compared the performance of ocor with the existing state of the art approaches.
rq2 what is the contribution of each component in ocor?
toanswerrq2 westartfromthefullmodelofocor and in turn removed each component to understand the con tribution of it.
then we also replaced the metrics used inmeasuring the overlap score with longest common prefix lcp and word embedding based similarity to better understandthecontributionofthecomponentoftheoverlap matrix.
rq3 why does the model combination work?
in fact the result of rq1 will suggest that the model combinationplacesasignificantroleintheoverallperformance.
to understand why different models can be combined we analyzed the distribution of the result predicted by different models on the sql dataset.
more specifically we selected someexamplesfrom thesedatasetstoshow thedifferences of the models.
.
dataset ourexperimentisbasedontwoestablishedbenchmarks thestaqc benchmark and the c benchmark used in iyer et al .
.
the staqc benchmark contains question code pairs writtenin the sql.
these pairs are collected from stack overflow makingitselfthelargest to dateinsqldomain.wefollowedthe original train dev test split in staqc namely staqc train staqcval and staqc test.
for better evaluation we used two additional test dataset namely dev and eval for sql are collected byiyer et al .
.
these datasets contain and code written in sql respectively.
for every snippet they use three differentreferences written by humans as the additional test cases.
the second benchmark contains question code pairs written in c collectedfromstackoverflow.wesplitthedatasetintoc train c val andc testasiyeretal .
thedetailedstatisticsof these datasets are listed in table .
for the staqc benchmark we took the training set of staqctrainasthetrainingset tookthedevsetasthedevelopmentset and took other three datasets staqc test staqc val eval asthe test set.
for the c benchmark we also followed the same 8the code of our experiment is available at the development set in our experiment.
notethattotesttheperformanceofacoderetrievalapproach we not only need the desirable question code pair the positive answer but also other code snippets as negative answers.
we call such a case containing a question and a set of code snippets as aretrieval case.
we used the same retrieval cases used in existing works where the counts of these cases are show in the number of cases row in table .
each retrieval case contains positive code snippet and negative code snippets.
.
metrics to measure the performance of our approach we followed yao etal.
andusedastandardmetricscalledmeanreciprocalrank mrr in this paper.
the mrr metrics is computed over the entire dataset d q1 c1 q2 c2 qn cn mrr summationtext.1n i ri d whereridenotes the ranking of ciin thei th query qi.
in this metrics thehighervaluedenotesthebetterperformanceofcode retrieval.
.
implementation details weimplementedourapproachbasedontensorflow .wesetthe n whichdenotesthateachencoderinourexperimentcontains astackof3mechanisms.wesettheembeddingsizeforboththe characters and the overlap scores to .
all hidden sizes were set to except that was used for both the first layer of the convolutional layer and the first layer in the mlp.
during training the dropout wasusedtoavoidoverfitting wherethedropratewas set to .
.
our model was optimizedby adam optimizer with learning rate .
.
for the model combination we set the hyperparameter to0.
.thesehyper parametersandparametersforour model were chosen based on the development set dev is used which followed the existing state of the art work .
specifically for each query natural language description in the training corpus we randomly sampled code snippets as the negative examples for eachtrainingepoch.inocor thenaturallanguagedescriptionand the code snippet shared the same embedding weights.
.
baselines inour experiment we usedthe existingstate of art coderetrieval approaches as the baselines for comparison.
deepcodesearch dcs .dcsjointlyembedstheinput codesnippetsandtheinputnaturallanguagedescriptioninto a high dimensional vector space with an rnn based neural network.inthisway acodesnippetanditscorresponding naturallanguagedescriptionhavesimilarvectors whichare then used for computing the similarity between two inputs by cosine similarity.
code nn .
the core component of code nn is an lstm based rnn with attention.
this attention mechanismcomputestheprobabilityofannaturallanguagedescription givenacodesnippet.forcoderetrieval givenan 889statistics staqc train staqc val staqc test dev eval c train c dev c test number of qc pairs number of cases avg.
tokens in description max.
tokens in description avg.
tokens in code max.
tokens in code table1 statisticsofthedatasetsweused.
numberofqc pairs denotesthetotalamountofquestion codepairsinthespecific dataset.
number of cases is the number of the retrieve cases for evaluation.
inputnaturallanguagedescription code nncomputesthe probability of the input for each code.
after the computation code nn ranks the given code snippets based on the probability.
coacor .coacorusedareinforcementlearning based frameworktocombinecoderetrievalandcodeannotation together for enhancing the code retrieval.
they also combine the code retrieval approaches together by ensemble to improvetheperformance.inparticular thebasicmodelof coacor is denoted as qn rlmrr whereas the combined models the best performance are denoted as qn rlmrr code nn.
results in this section we report the results of our experiment and answer the research questions.
.
performance of ocor rq1 the results of rq1 are presented in table .
we first compare the original ocor with the original models ori.intable2 .asshown amongthethreestate of the artmodels ocorachievesthebestperformanceonalldatasets.ocorishigher than the existing best results by .
to .
improvements.
forthemodelcombination wefirstcombineocorwiththeoriginal model qn rlmrr denoted as ocor qn rlmrr .
we select the state of the art models proposed by yao et al .
qn rlmrr code nn asthebaselines.asshownintable2 ourapproach significantly outperforms existing state of the art models by pointsimprovementonaverage.inparticular weachieved13 .
to .
improvementsonalldatasets whichshowseffectivenessof our approach.
then we combine ocor with the model combined byy aoetal .
qn rlmrr code nn wherewealsoachievedthe bestresults ocor qn rlmrr code nn amongalldatasets.
the results suggests that our approach is more effective than existing state of the art models on different datasets and different program languages.
answerto rq1 ocor hasagoodperformance .
to .
improvements compared with the existing state ofthe art approaches on all datasetscovering two programming languages.
.
the contribution of each component rq2 to answer rq2 we first conducted the ablation test on the sql dataset to figure out the contribution of each component.
in this subsection weonlyconductedtheexperimentbasedontheoriginal ocor which aims to understand the contribution of each component more clearly.
the model in the ablation test had the same setting with the original ocor except we removed each component in turn.
the resultsarepresentedintable3.wefirstremovedtheinputoverlapscoresfromocor.toremovethis insteadofusingoverlapmatrices as the inputs of the first mechanism in the encoder we replacedit with the input tokens in natural language description code.
furthermore we followed the previous joint model and the model was trained by minimizing the cosine similarity betweenthe natural language description and code as the .
by applying suchsettings theperformancewasclosedtothepreviousapproach based on word2vec which shows the effect of the overlap matrix.
to better understand the contribution of the metrics for computingoverlapscores namelyequation2 wefurtherconducted theexperimentonthesamedatasetsbutwithadifferentmetrics of overlapscores.
thebaseline metriccompared withthe original metric is wordsimilarity based on word embedding.we first used theglove topretrainthewordembeddingvectorbasedonthe training set with bpe .
then we use the cosine similarity of w1 w2astheoverlapscore.theresultsarepresentedintable3.the character level metric achieves better performance on all datasets.
thisresultshowsourmetricsaremoresuitablethanthetraditional similarity matrix used in document retrieval for code retrieval.
we also replaced the original overlap metric with longest commonprefix lcp .thelongestcommonprefixofapairofstrings aandbis the longest string pwhich is the prefix of both strings.
we uselen p len a len p len b as the overlap scores of two strings respectively.inparticular wealsocomputethescorebased onthelongestcommonsuffix andfinallyselectthebiggeroneas the overlap score.
the performance of this metric is slightly lower.
theablationtestalsoindicatesthatthecharacter levelinformation is important to the code retrieval task.
answerto rq2 eachcomponentcontributestotheoverall performance of ocor.
890model eval staqc val staqc test c ori.dcs .
.
.
.
code nn .
.
.
.
qn rlmrr0.
.
.
.
ocor .
.
.
.682com.qn rlmrr code nn .
.
.
.
ocor qn rlmrr0.
.
.
.
ocor qn rlmrr code nn .
.
.
.
table the results show the mrr of code retrieval among examples.
in this table we divide the existing model into two differentcategories.thefirstcategory ori.
wheretheoriginalmodelisused isinthe2to5rows.thesecondcategory com.
where different models are combined together by ensemble is in the to rows.
model eval staqc val staqc test ocor .
.
.
overlap score .
.
.
character level overlap wordsimilarity .
.
.
overlap lcp .
.
.
table3 theablationtestonocor whereocordenotesthe original model of our approach.
figure6 theoverlapsoftheprefectrankingofdifferentapproaches the among three datasets.
.
model combination analysis rq3 to answer rq3 we try to figure out the reason why the model combination works.
we first implemented the existing approaches coacor qn rlmrr and code nn.
then we studied the predictionoverlapsoftheperfectrankingamongthedatasets.foragiven naturallanguagedescriptionandasetofcodesnippetswithonepositive code snippet the perfect ranking in this paper means that the positive code snippet is ranked in the top .
theresultsarepresentedinfigure .asshown onthesethree datasets .
perfectrankingcasesonaveragecanbesolvedby allthreeapproaches whereas26 .
.
.
.
inall perfect rankingcasesonaveragecanonlysolvedbytheapproachocor coacor and code nn respectively.
such .
cases show the potentialimprovementsonmodelcombination andthisiswhythe model combination works well in our approach.
tohelpunderstandthemodelcombination wealsoconductedan additionalcasestudy.inthiscasestudy weanalyzeocorwiththe existing state of the art model coacor qn rlmrr code nn .
case study.
table shows three examples that are ranked perfectlybyocorbutnotcoacor.asshown therearemanyoverlaps between the input natural language description and the code insql e.g.
the word select and data in the first example row2 the word table and time in the second example .
in these examples theinformationofoverlapscoresisimportant wherea human can utilize this information and retrieved the target code easily andocorcatchessuchinformationsuccessfully.existing approaches like coacor do not utilize the information of overlapscoresproperly wherethecoacorapproachdirectlyusesthe token level embedding for the neural network and replaces identifiernameswithnumberedplaceholdertokens e.g.
thesqlcode in the second example is turned to select col0 col1 from tab1 in table4 .thus ocorhasagoodperformanceontheseexamples while the coacor does not work well which is the strength of ocor.
to understand the weakness of ocor we also conducted another case study on examples where ocor does not work wellcompared with the coacor.
these examples are presented in table .
as shown in these examples there are few overlaps between theinputnaturallanguagedescriptionandthecodeinsql.insuch a situation ocor can hardly measure the overlap scores in thematrix which makes it difficult to utilize the key information ofoverlap scores in ocor.
however coacor which combines the annotation generation and code retrieval together replaces identifiernameswithnumberedplaceholders andextractsthehigh level information of these situations.
this is the reason why coacorhas a good performance on these examples.
the cases show the 891type example description selecta formatted daterange from values in a table column sql code selectdate format start date m date format start date d date format end date d date format startdate y from yourtable description sql insertmultiple values where1 value comes fromaselectquery sql code insertinto table2 telnumber adress select applicatieid fromapplicatie wherename piet description quick way to spacefill column 256chars sql server sql code select space table the examples that ranked perfectly by ocor but not coacor.
type example description how to use max and top in sql query in oracle?
sql code select id item quantity date from select id item quantity date from your table order by quantity desc date desc where rownum description find level deep hierarchical relationship be tween columns of a table for one of the top level values sql code select t2.cat id t2.subcat id t2.name from test t1 join test t2 on t1.cat id t2.cat id where t1.subcat id and t2.subcat id table the examples that ranked perfectly by coacor but not ocor.
weaknessofocor whichdoesnothaveagoodperformancewhen theoverlapscoresarehardtomeasure.itisprobablyagoodway tocombinedifferentapproachestogetherandutilizethestrength of each approach.
thus we use the model combination to combine the strengths of different approaches.
answerto rq3 thethreetechniques complementseach other allowing the model combinations to produce better results.
threats to validity threatstointernalvalidity.
athreattointernalvalidityisthe potentialfaultsintheimplementationofourexperiments.toreduce this threat for the performance of original models we directlyuse their reported performances and for the performance of combined models we directly used their published code .
furthermore the implementation of our model was based on a published model to avoid faults in re implementation.
threatstoexternalvalidity.
our model was evaluated on the staqc and the c benchmarks which are widely used in previous code retrieval approaches .
in these two benchmarks allof the programs are collected from stack overflow which givesa threat to external validity.
however we find that the overlap relations we used also widely exist in datasets collected from other sources such as github .
for example in codesearchchallenge corpus whichisanothercoderetrievalbenchmarkcollected from github repositories .
.
.
and .
of the instanceshaveatleastoneoverlapforpython java javascript and go respectively while only .
in the staqc dataset used in ourexperiment.suchwidelyexistedrelationsshowthepotential value of our approach when applied to the github benchmarks.
meanwhile sinceourapproachwasonlytestedonsqlandjava programs collected from stack overflow further studies are alsoneeded to apply our model to other programming languages collected from github.
related work code retrieval.
coderetrievalin softwaredevelopmenthelps developersreusetherelevancecodesnippetsamongalargescale open sourceprojects.earlystudiesmostlyfocusonapplyingthe informationretrievalmethodstocoderetrievaltask .
with the development of deep learning more and more works try to use neural networks to code retrieval .
gu et al.
first proposed an lstm based rnn for code retrieval wheretheyencodetheinputnaturallanguagedescriptionandcode intoavectorspaceandmeasurethecosinesimilaritybetweenthem.basedonacodeannotationworkproposedbyiyeretal .
where they use a sequence to sequence model to generate the specificannotation by a given code yao et al .
proposed coacor for code retrieval where they combinethe code annotationapproach ofiyeretal .andthecoderetrievalapproachofguetal .together bya reinforcement learningframework.
differentfromtheseap proaches we focus on unitizing the overlap scores between the natural language description and code.
based on this we proposed a novel neural architecture for code retrieval.
overlap information .many works focus on using neural networks combined with overlap information in sentence pairs matching .
huet al .
firstproposed to usea neural network.
they adopted a stack of convolution layers to infer the relation betweenthequestionandthegivenanswer.qiuandhuang introducedatransformationlayertousetheinteractionbetween 892thequestionandtheanswer.theytriedtoutilizehiddenunitsto extracttheoverlapsbasedonhiddenstates.fuetal .
proposed a kind of overlap features and combined it with a convolutional network.
such overlap features compute the similarity betweentwo words via whether they are the same.
it cannot utilize the overlapbetweenwords e.g.
theoverlapbetween joint table a and table which is important in encoding identifiers in code.
the atomic value of these overlap features cannot represent the relation between identifiers and corresponding words.
thus wedesign the overlap matrix based on longest common sub string tomeasurethedegreeoftheoverlap.wealsoadoptsomespecial neural components for this representation.
conclusion and future work inthispaper weproposedanoveloverlap awareneuralarchitecture ocor for code retrieval.
our approach makes use of theoverlap score between the natural language description and the code by using the overlap matrix and the character embedding.
we evaluated our approach on several datasets.
the experimental results show that ocor achieves significant improvement compared with existing state of the art approaches.
the further evaluation shows that each component in our approach is important.
futurework.
ourapproachisbuiltmainlyonthebasisofoverlap scores between two inputs especially for the natural language description and code.
the experimental results show that the overlapscorecanboosttheperformanceofthemodel.itisinterestingtostudyasthefurtherworktotrymoremetricstomeasurethedegreeoftheoverlapsbetweentwostrings e.g.
editdistanceandlongest commonsub string .ourexperimentalsoshowsthepotentiality oftheensemble namelymodelcombination whichmayalsobe aneffectivewaytousesuchtechniquetocombinedifferentmetrics toimprovetheperformance.furthermore ocor canbedirectly applied to other programming languages e.g.
java python andc .
our model is designed for the general code retrieval task andsomespecificfeaturesmaybeaddedtoitbygating.itisalso interesting to study as the further work.