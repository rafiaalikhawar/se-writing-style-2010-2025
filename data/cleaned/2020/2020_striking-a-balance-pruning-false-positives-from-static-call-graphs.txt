striking a balance pruning false positives from static call graphs akshay utture university of california los angeles u.s.a. akshayutture ucla.edushuyang liu university of california los angeles u.s.a. sliu44 cs.ucla.edu christian gram kalhauge dtu denmark chrg dtu.dkjens palsberg university of california los angeles u.s.a. palsberg ucla.edu abstract researchershave reportedthat staticanalysis toolsrarely achieve a false positive rate that would make them attractive to developers.
weovercomethisproblembyatechniquethatleadstoreporting fewer bugs but also much fewer false positives.
our technique prunes the static call graph that sits at the core of many staticanalyses.
specifically static call graph construction proceeds as usual after which a call graph pruner removes many false positive edges but few true edges.
the challenge is to strike a balance be tween being aggressive in removing false positive edges but notso aggressive that no true edges remain.
we achieve this goal by automatically producing a call graph pruner through an automatic ahead of timelearningprocess.weaddedsuchacall graphprunertoasoftwaretoolfornull pointeranalysisandfoundthatthefalsepositive rate decreased from to .
this improvement makes the tool more useful to developers.
ccs concepts softwareanditsengineering automatedstaticanalysis computing methodologies supervisedlearning byclassification.
acm reference format akshayutture shuyangliu christiangramkalhauge andjenspalsberg.
.strikingabalance pruningfalse positivesfromstaticcallgraphs.
in44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
https introduction the problem.
christakis and bird interviewed developers about program analysis tools and they concluded programanalysisdesignshouldaimforafalsepositive rate no higher than .
this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
.until now this goal has been particularly hard to achieve for static analyses which are tools that analyze programs without executing them.
as a motivating experiment we tried wala which is one of thebesttoolsforstaticanalysisofjavabytecode onasubsetofthe njr benchmark suite .
for each benchmark we compared theedgesinthestaticcallgraphwiththeedgesfoundbyexecuting the benchmark.
with a context insensitive analysis wala has a false positive rate of while with a better but also much slower context sensitive analysis the false positive rate is .
those results are disappointing though we must emphasize that call graphs are usually fed to client tools rather than directly to developers.
so we did a second experiment to see how the high false positive rate of call graphs affects client tools.
specifically we implemented a versionofastaticanalysisforwarningaboutnull pointerproblems that is a client of the context insensitive call graphs produced bywala.weranthistoolonthesamesubsetofnjr 1andagain haddisappointingresults 60bugsamong223warnings onaverage so a false positive rate of .
we can easily imagine how a developerwilltireofinvestigatingwarningsthatinnearlythree ofeveryfourcasesarefalsealarms.thefalsealarmshaveseveral causes but an important cause is the high false positive rate in the underlyingstaticcallgraph.hence wecanalsoseeaglimmerof hope if we can reduce the false positive rate of static call graph constructors wemaybeabletomoveclienttoolsclosertothegoal of a false positive rate of .
ouridea.
ourapproachstemsfromanotherconclusionbychristakis and bird who reported a preference of developers when forced to choose between more bugs orfewer false positives they typically choose the latter.
thisquoteinspiredourideaforho wtoimprove thefalse positive rate we will report fewerbugs but also much fewer false positives.
indirectsupportforthisideacomesfrompreviousworkthatshowedthatpracticalstaticanalysesaren ttotallysound andtherefore may miss bugs.
thus developers expect bug reports to be incomplete so reporting fewer bugs seems acceptable.
we want to reduce the false positive rate in a modular way that leavesexistingcall graphconstructorsunchanged.thisbringsus to our idea of a call graph pruner that statically post processes a ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa akshay utture shuyang liu christian gram kalhauge and jens palsberg static call graph by removing manyfalse positive edges but few true edges.
the challenge is to strike a balance between being aggressive in removing false positive edges but no so aggressive that no true edges remain.
additionally we have to do better than removingedgesatrandombecauserandomremovalswillleavethe false positive rate unchanged.
how can we design a call graph pruner?
ourapproach.
weexecuteanautomatic ahead of timelearning process on results from both a static and a dynamic call graph constructor.
the outcome is a call graph pruner that works as follows.thecall graphprunerdeterminestheprobabilitythatan edge in the call graph is a false positive and if this probability is above a threshold then the call graph pruner removes the edge.
we can vary this threshold and thereby tune the call graph pruner.
in contrast to previous work on using a dynamic analysis to improve a static analysis we use the dynamic call graph constructor only in an ahead of time training phase and only on a trainingsetofprograms.oncethetrainingphasehasproduceda call graph pruner the combination of the call graph constructor and the call graph pruner is itself a static analysis as illustrated in figure .
our contributions and the rest of the paper.
we begin with an example of how a call graph pruner works section and then we detail our contributions we present the design section and implementation section of a tool that produces call graph pruners.
weshowexperimentally section5 thataddingacall graph pruner to a client tool can significantly decrease the falsepositiverate inonecasefrom73 to23 .specifically we added a call graph pruner to the tool for warning about null pointer problems after which we got bugs among warnings on average.
thus we reported fewer bugs but also fewer false positives.
we show experimentally section that the overhead of adding a call graph pruner is of the original call graph analysis time.
weendwithadiscussionofrelatedwork section6 andourconclusion section .
significance.
call graphprunersimprovestaticcall graphssignificantly and thereby make client tools more useful to developers.
example now we give an example of a call graph pruner how it works on a example call graph and how it affects a client analysis for warning about null pointer problems.
our example program in figure shown in full in the appendix has three classes a b c each of which has a method foo and a main method that contains amethodcall x.foo x.f .thecallto getobjc returnsanobject of type c which is then assigned to the variable x. on the next line the access x.fhappens but the field a.fmay be uninitialized hencenull.thusthecall x.foo x.f maypassnullasanargument toc.foo which in turn at the call c.tostring may throw a nullpointerexception.theprogramhastwoadditionalmethods including getobjc that we omitted from figure .program call graph construction tool call graph prunercall graph false positive rate true edges missed call graph false positive rate true edges missed balanced call graph construction toolnew figure overview of our technique ... a x getobjc x.foo x.f class a a f foo a a a.tostring class b extends a foo a b b.tostring class c extends b foo a c c.tostring decision tree dest node in deg .
src node out deg .
static analysis call graph t tf f dest node in deg .
ft40 figure example call graph and call graph pruner null pointer warnings.
as we mentioned in section we implemented a version of a static analysis for warning about nullpointer problems.
this analysis finds null pointer problems that stemfromuninitializedfields liketheproblemwith c.tostring thatiscausedbytheuninitializedfield a.f.ifwerunthistoolon theexampleprogram weget threewarnings oneforeachcallof tostring inthe foomethods.oneofthemisatruewarningbut the other two are false alarms.
let us investigate how that could happen and what a call graph pruner can do about it.
call graph.
the null pointer tool uses a static call graph constructor that built the call graph shown in figure .
in a call graph eachnodeisamethod andeachedgeisadirectededgefromone method to another.
such an edge represents a call that may happen during the execution of the program.
the call graph constructor uses a data flow analaysis to analyze the entire program including the methods that we omitted from figure2.weskipthedetailsofhowthisworksandinsteadwefocus on the constructed call graph.
specifically in figure we focus on thefournodesforthemainmethod a.foo b.foo and c.foo.the call graph has an edge from the main method to each of a.foo b.foo and c.foo as well as an edge some other method to b.foo andacoupleofedgesfromsomeothermethodsto a.foo.theedge from main to c.foois a true edge while the edges from main to a.fooand from main to b.fooare false positives.
thefalsecall graphedgesfrommaintoeachof a.fooandb.foo can arise from difficult to analyze methods one of which is part of the full example program in the appendix.
2044striking a balance pruning false positives from static call graphsicse may pittsburgh pa usa thenull pointeranalysisinmoredetail.
basedonthecallgraph in figure the null pointer analysis derives that x.foo x.f may callanyof a.foo b.foo and c.foo.thenthenull pointeranalysis uses the rule that if a field is not initialized by the end of a constructor it is marked as uninitialized and if an uninitialized field is dereferenced the analysis gives a null pointer warning.
thus the analysis concludes that each of the foomethods may be passed null as an argument and thus it issues a warning for every one of those methods.
call graph pruner.
the goal of a call graph pruner is to remove edges from the call graph preferably many false positive edges andfewtrueedges.thekeycomponentofacall graphpruneris aclassifierthatcomputestheprobabilitythatacall graphedgeis atrue positive.basedonthatprobability acall graphprunerwill decide whether to keep or to remove the edge.
figure shows a classifierthatisrepresentedasadecisiontree.eachinternalnodeof the decision tree asks a true false question about a call graph edge.
the recursive decision process begins in the root of the decision tree iftheanswertothequestionattherootisfalse wemoveto the left subtree while if the answer is true we move to the right subtree.
when we reach a leaf we find the probability that the call graphedgeisatrue positive.theprobabilitiescomputedfor eachcall graphedgeinthisfashionaremarkedonthecallgraph in figure .
based on these probabilities we will decide whether to keep or remove the call graph edge.
thedecisiontreeinfigure2hasthreeinternalnodesthatarelabeledwithquestionsabout dest node in deg whichisthein degree of the destination node of the edge and about src node out deg whichistheout degreeofthesourcenodeoftheedge.forexample theedgefrommainto c.foohasdestination nodein degree 1andsource nodeout degree3.thisgivesusthepathfalse truefalse which assigns the edge the probability .
similarly the edges from main to a.fooandb.fooget probabilities and respectively.
the call graph in figure shows those three probabilities.
let us set a threshold of for when we deem an edge to be a false positive ifthe probability of being a true positiveis below we removethe edge.
then the call graphpruner will remove theedgesfrommainto a.fooandb.foo.hence thenull pointer analysiswillissuejustasinglewarning andindeedatruewarning namely for the call of tostring inc.foo.
call graph pruners nowwedescribehowweusemachinelearningtoproduceacallgraph pruner.
.
overview we will use program to denote the set of java bytecode programs.
acallgraph g callgraph isamulti graphinwhicheachnode represents a method and each edge represents a potential transfer of control at a method call.
two nodes can have multiple edges betweenthembecauseofmultiplemethodcalls.eachedgehasa label that identifies the method call site.we distinguish between two kinds of call graph constructors that have the same type staticcallgraphconstructor program callgraph dynamiccallgraphconstructor program callgraph here anelement of staticcallgraphconstructor constructs acall graph without running the program while in contrast an element ofdynamiccallgraphconstructor runsaninstrumentedversion of the program on one or more inputs and examines the output from the instrumentation.
thekeycomponentofeachcall graphprunerisaclassifier.a classifier c classifier is afunction that maps avector of feature valuesforanedgetoaprobabilitythattheedgeisatrue positive.
in our case such a vector has elements that we will define in section .
.
our tool for generating classifiers implements a function of this type classifier generator staticcallgraphconstructor dynamiccallgraphconstructor set classifier ourclassifiergenerator executesanautomatic ahead of timelearning process on results from running both a static and a dynamic call graph constructor on a training set of programs.
the dynamic call graphs serve as ground truth for the learning process.
we will detail this learning process in section .
.
oncewehaveaclassifier wecanuseitinacall graphprunerof this type call graph pruner callgraph classifier threshold callgraph algorithm shows how a call graph pruner works.
intuitively a call graph pruner uses a classifier to determine the probability that anedgeinastaticcallgraphisatrue positive.ifthatprobability is below a given threshold t threshold the call graph pruner removes the edge.
algorithm call graph pruner 1inputs callgraph g classifier c threshold t 2letg primebeac o p yo f g 3forevery edge eingdo 4v the feature values for e 5ifc v tthen removeefromg prime 7outputg prime thethresholdparameterenablesustoexploredifferentlevelsof aggressiveness in removing edges.
for our example in figure we discussedathresholdof50 insection2 whichledtotheremovalof twoedges.wecouldalsousealowerthresholdof20 whichwould lead to the removal of a single edge namely the one from main toa.foo.
the challenge is to strike a balance between removing manyfalse positiveedgesandkeepingmanytrue positiveedges.in 2045icse may pittsburgh pa usa akshay utture shuyang liu christian gram kalhauge and jens palsberg edge f1... fklabel e1 ... .
e2 ... .
... ... ... ... ...training programs program program n compute features concatenate into single training set trained classifiertrain using classification algorithmstatic call graphdynamic call graph edge f1... fklabel e3 ... .
e4 ... .
... ... ... ... ...compute featuresstatic call graphdynamic call graph figure classifier generator workflow section we will show results from an experimental investigation of how to choose a good threshold.
noticethatweuseastaticcallgraphconstructor adynamiccall graph constructor and the training set of programs for the sole purpose of generating a classifier while those items are no longer needed when we use the call graph pruner.
.
our classifier generator wecasttheedge pruningproblemasaclassificationproblemfor which learning a classifier can be done with machine learning.
we proceed in three steps.
inthefirststep werunexistingstaticanddynamiccall graph constructor tools on every program in the training set the dataset of programs is described insection .
theresult isa set ofpairs of call graphs each pair consists of a static call graph and a dynamic callgraph.
weuse thedynamiccall graphasan approximationof the ground truth if a static call graph edge is also present in the dynamic call graph we view it as a true positive and otherwise as a false positive.
in the second step for each program we construct a table in which each row represents a static call graph edge.
figure illustrates this table.
the last column in each row titled labelin figure contains a label of or based on whether the edge exists in the dynamic call graph.
the remaining columns titled f1tofk representthesetof featuresofthestaticcall graphedge.
the example in figure uses two features dest node in deg and src node out deg wewilldiscussotherfeaturesbelow.wecanview each row in the table as a vector of feature values.
concatenating thetablesofeachindividualprogramgivesusasinglelargetraining dataset of call graph edges with ground truth labels.
this training dataset consists of a large number of pairs xe ye wherexeis a vectoroffeaturevaluescorrespondingtoastaticcall graphedge andyeisapredictionofwhetheritisafalse positiveornot.our problem is now expressed in a format where it can be cast as a machine learning classification problem .
inthethirdstepwerunanoff the shelfmachine learningtool on the table constructed in second step.
the result is a classifier that for any edge assigns a probability that it is a true positive.
we picked random forests ensembles of decision trees .
one might try other approaches which we leave to future work.
our goalwiththisstepistoshowthatanoff the shelfmachine learning tool is sufficient to get good results.
ourclassifiergeneratorcantakeanystaticcall graphconstructor as input.
for example we have used the call graph constructors wala doop and petablox as inputs and generated a call graph pruner for each one.
the complexity of generating a classifier based on a training set withnedges iso nlogn .
.
our feature set now we describe how we designed the feature set that both our classifier generator and our generated call graph pruners use.
afeatureis information about a static call graph edge that may help predict whether the edge is a true positive.
we would like our feature set to capture important context and semantic information about a call graph edge.
encoding important semantic information asfeaturesisacommonmachinelearningpracticeforincorporating domain knowledge into the learning process.
for example since dynamic dispatch is likely to affect the false positive probability of a call graph edge we should add features that capture information aboutthetargetsofamethodcall.usingthecontextinformationof agraphedgehasbeenusefulfortherelatedtaskofselectivecontext andheapsensitivityinpointer analysis andweconsiderita goodcriteriaforpickingfeatures.contextinformationcanbelocal bydescribingtheneighborhoodoftheedge orglobalbydescribing the call graph that the edge is a part of.
in addition to capturing context and semantic features we identify three criteria that we want our feature set to satisfy linear time computation complexity interpretable and generalizable and black box.
the time complexity guideline is particularly important given that someofourbenchmarkscanhaveseveralhundredthousandcallgraphedges.interpretabilitygivesusanunderstandingofwhich call graph edges are being dropped and generalizability ensures thatwhatislearnedforthetrainingedgesalsoappliestocall graph edgesofunseenprograms.theblack boxcriterionimpliesthatthe features should only be designed on the output call graph and not onsomeinternalstateorrepresentationofatool.thisallowsus to post process the results without being specific to a particular algorithm or tool.
using these criteria we arrived at the following features for an edge.
figure4presentsourfeaturesetforanedgeinastaticcallgraph g where the edge is from a caller method callerto a callee method callee.
the node for the main method in gismain.
the first seven features describe local information while the last four describe globalinformation.notethatthel fanoutofanedgeisthenumber ofoutgoingedgesatthecall siteofthatparticularedge whereas 2046striking a balance pruning false positives from static call graphsicse may pittsburgh pa usa feature description src node in deg number of edges ending in caller src node out deg number of edges out of caller dest node in deg number of edges ending in callee dest node out deg number of edges out of callee depth length of shortest path from maintocaller repeated edges number of edges from callertocallee l fanout number of edges from the same call site node count number of nodes in g edge count number of edges in g avg degree average src node out deg in g avg l fanout average l fanout value in g figure our feature set src node out deg is the number of outgoing edges from all the call sites of an entire source method.
our selection process started with a much longer list of features that allsatisfy thethree criterialisted above.
wepicked fromthat list the ones that helped the most with removing false positives.
ourprocessusedthetrainingsetascasestudiestofindthemain reasons why tools give false positives.
the result was the eleven features in figure .
implementation and dataset static call graph constructors.
we used the static call graph constructors wala doop and petablox .
in each case we used the default setting which implements cfa for methodsthatareestimatedtobereachablefromthemainmethodand without any special handling of reflection.
those tools produce significantly different call graphs and so we generate a separate call graph pruner for each tool.
reflection.
inpreliminaryexperiments wefoundthatenabling specialhandling ofreflection inthe staticcall graph constructors introduces many false positive edges in the call graphs.
our generated classifiers tend to assign each of those edges a low probability of beinga true positive andtherefore our call graphpruners will correctlyremovemostofthem.therefore specialhandlingofreflectionpresentsnoadditionalchallengeforcall graphpruningand wedecidedtogowiththedefaultsettingofeachstaticcall graph constructor.
dynamic call graph constructor.
we used the open source tool wiretap toinstrumentthejavabytecodeandtherebyenable dynamic call graph construction.
next we ran the instrumented bytecodeandcollecteddataabouttherun particularlyaboutthe method calls.
standard library.
the java standard library is large and has the potential to dominate the measurements for every benchmark whichiscounterproductive.so whenwedoourmeasurementsand training we omit nodes from the standard library as well as edges between standard library nodes.
we preserve aspects of the edges to and from the standard library in the following way.
for every path of the form v angbracketleft... standard library nodes ... angbracketright wfigure5 histogramofedge countsinthe100trainingprograms.
wherev warenodesoutsidethestandardlibrary wecreateasingle edge from vtow.
randomforestclassifier.
ourclassifiergeneratorusestherandom forest algorithm implemented with the scikit learn library v0.
.
.
the random forest algorithm works as follows it trains several decision trees using bagging and makes predictions by a majority vote across the decision trees.
the training took minutes.
we tuned the hyper parameters using random hyper parameter search with fold cross validation on the training set.
we list the chosen hyper parameters in the appendix.
dataset.our dataset consists of programs from the njr benchmark suite of which we used programs for generatingthreecall graph prunersandtheremaining41programs for ourevaluation.weselectedthose141programsfromthe293njr programs according to the following criteria consistsatleast1 000methodsandatleast2 000staticcallgraph edges according to wala executes at least distinct methods at runtime and has high coverage executes a large percentage of the methodsthatarereachablefromthemainmethodaccordingto wala for our benchmarks the coverage is on average.
each program consists of lines of code on average not countingthestandardlibrary .inmoredetail eachprogramconsists of the main application which is lines of code on average in addition to third party libraries which account for an estimated lines of code on average.
the total number of static call graph edges not counting the standard library that are reachable from the main methods of the programs is .
million.
for our classifier generator each edge from of those programs is a data point which is edges.
note that manual creation of ground truth about those edges infeasible.
large benchmarks.
the histogram in figure gives the distributionoftheedgecountsinthetrainingprograms.thex axisis plottedonalogarithmicscaleduetotheskewinthedistribution.
amongthe100trainingprograms 7ofthemhaveaverylargenumber of call graph edges .
this gives them the potential to dominate how the classifiers work.
to overcome this we randomly sample20 000edgesfromtheedge setsofthese7programs.notice that this sampling is done only during generation of call graph 2047icse may pittsburgh pa usa akshay utture shuyang liu christian gram kalhauge and jens palsberg pruners whileweusealltheedgesfromthe41programsthatwe use for evaluation.
analysis time.
running the three static call graph constructors andthedynamiccall graphconstructoronalltheprogramstakes four days of compute time.
precisionandrecall.
weestimatethequalityofastaticcallgraph using the standard notions of precisionandrecall.
in our setting if sis the edge set produced by a static call graph constructor and wis the edge set produced by wiretap then precision s w s recall s w w therateoffalse positivesis precision .wecomputetheaverage precision and recall values for the entire test set by taking the arithmetic mean over the precision and recall values of individual programs.
figure6showsahistogramoftheoriginalprecisionandrecall scoresforwalaonthe41individualprogramsofthetestset.note thattheprecisionvaluesvarysignificantly butalmostallprograms getbelow40 precision.hence thereisalotofscopeforimproving the precision.
the recall is close to for most programs but low for some due to heavy use of reflection dynamic class loading or native code.
experimental results in this section we discuss our experimental results that validate the following claims.
our generated call graph pruners for wala doop and petablox produce call graphs with balanced precision and recall.
forprecision sensitiveclients ourgeneratedcall graphpruners are significantly better at boosting precision than contextsensitive analyses and have a much smaller overhead.
theprecision improvementisconsistentacross thetestset.
thecall graphprunerenablesamonomorphiccall siteclient tobalanceitsskewed52 precisionand93 recalltoamore balanced precision and recall.
the call graphpruner enables anull pointer analysis toreduceitsaveragewarningcountfrom223to20 whileincreasing precision from to .
all experiments are run on a separate test set of programs whichwerenotusedduringtraining.theexperimentswerecarried outonamachinewith24intel r xeon r silver4116cpucores at .10ghz and gb ram.
a minimum ram size of 32gb is essential for ensuring that the static analyses run in reasonable time.
the artifact for the paper is available here and the njr dataset can be downloaded from .
.
main result figure gives the main result of the paper a call graph pruner can be successfully used to boost precision and to balance the goals of precision and recall for the cfa call graph analysis of wala doop and petablox.
the plot is used to represent the precision andrecallvaluesofvarioustools whereinallprecisionandrecall values are reported as averages over the test set programs.
theblack triangle marks the wala cfa analysis .
precision .
recall the green triangle marks the doop cfa analysis .
precision .
recall and the blue triangle marks the petablox cfa analysis .
precision .
recall .
they all haveclosetoperfectrecall butpoorprecision.theredplussign marks the wala cfa analysis .
.
.
.
the black curve representstheprecision recalltrade offpointsobtainedwhenacallgraphprunerisappliedtothewala0 cfaoutput.theoriginal wala 0cfa output is a single point on the precision recall graph but the call graph pruner gives a curve instead.
this is because the call graphprunergivesaprobabilityscoreforeachedgebeingin the ground truth call graph and by setting different thresholds i.e.
cutoffsbelowwhichanedgeisremoved wecanobtaindifferent pointsontheprecision recallcurve.joiningallthesedifferentpoints gives us the black curve in the figure.
setting a low probability thresholdforacceptinganedge givesuspointsneartheleftend ofthe blackcurve because weaccept alargepercentage ofedges thereby giving us higher recall but lower precision.
setting a highprobabilitythresholdgivesuspointsneartherightendofthecurve because we accept only very few edges which are very likely to be intheground truthcall graph andthisgivesushigh precisionand lowrecall.thegreenandbluecurvesrepresenttheprecision recall trade offobtainedbyapplyingthecall graphprunertothedoop andpetabloxcall graphsrespectively andthecaseisverysimilar to the black wala curve.
thesecurveswhichtrade offrecallforprecisionshowthatthe classifier has assigned probabilities meaningfully.
in contrast a tool that randomly assigns probabilities to edges would result in acurvethatgoesstraightdowntozerorecallwithoutimproving any precision.
this is because it results in a random removal of edges which keeps the ratio of true positives i.e.
precision the same.
boosting precision requires the ratio of false positive edges in the removed edge set to be higher than the rest of the edges.
there are particularly interesting points on the black wala curve in figure .
the first is the one marked by the black wala square .
precision .
recall which represents the point with balancedprecision and recall.such a pointwill beuseful to a precision sensitiveclient analysis.ascompared tothe original wala cfa black triangle this point has over of the edges fromtheoriginalcall graphremoved andoutoftheremovededges lessthan10 aretruepositives.thispointisata0.45probability threshold.similarpointsfordoopandpetablox markedbyagreen square hidden behind the black square and blue square also hiddenbehindtheblacksquare respectively areat .
precision .
recall and .
precision .
recall respectively.
a second interesting point is the right most point on the curve after which recall starts dropping faster represented by a black circle precision recall .
such a point would be useful for a client analysis that needs to increase a little precision without losingmuch recall.similar pointsfor doopandpetablox aremarked by the green circle precision recall and blue circle precision recall respectively.
both these points give larger precision boosts than the cfa analysis.
however in general the best precision recall trade off pointisdecidedbytheneedsoftheclientofthecallgraph.precisionsensitive clients would benefit more from our call graph pruner 2048striking a balance pruning false positives from static call graphsicse may pittsburgh pa usa figure precision and recall for test programs.figure precision and recall after call graph pruning.
figure main result for the wala doop and petablox staticanalysistools.thebaselineprecision recallvaluesfor the3tools alongwiththeprecision recallcurveobtainedafter applying a call graph pruner averaged over all test programs since it gives a larger precision boost but clients that need high recall may prefer the cfa call graph.our call graph pruner adds an overhead of to the wala cfa analysis whereas moving to a cfa analysis adds overhead.priorresearchalsofindsthatcontext sensitivityincreases analysis time by many folds .
for completeness we also ran this experiment for wala s rta implementation and it gets similar results that we show in the supplementarymaterial .sincethethreetoolsshowsimilarcharacteristics we only present numbers for the wala cfa call graph in the rest of this section.
the corresponding graphs for doop and petablox are available in the supplementary material.
picking a cutoff value.
we picked the balanced precision recall point because it gave good results for a null pointer analysis client but different precision recall trade off points may be suitable for differentclientanalyses.figure9helpsauserpicktherighttrade off point for their client.
it plots the probability cutoff values on the xaxis and the precision recall and f score on the y axis.
the graph shows what values each of these metrics takes at every probability cutoff value as well as what the expected cutoff would be for a giventargetprecision recallorf score.forexample bylookingat the figure we can say that to obtain an expected precision of we can set a cutoff value of .
.
at this point we would get a recall ofapproximately75 andf scoreofaround65 .thisgraphalso shows that the balanced precision recall point is also very close to the point with maximum f score.
featureimportance.
figure10givestheimpurity basedimportance foreachfeatureusedintherandom forestindescending order.the l fanoutanddest node in deg arethemostimportantfeaturesandthefourglobalfeaturesaretheleastimportant.dropping 2049icse may pittsburgh pa usa akshay utture shuyang liu christian gram kalhauge and jens palsberg figure probability cutoff plotted vs precision recall and f score curves for wala feature importance l fanout .
dest node in deg .
src node out deg .
repeated edges .
src node out deg .
depth .
dest node out deg .
node count .
edge count .
avg l fanout .
avg degree .
figure10 importanceofeachfeatureintherandomforest classifier in descending order.
thefourglobalfeaturesdecreasestheareaundertheprecision recall curve from figure by .
human interpretable explanation of the classifiers.
we can give a human interpretable explanation of the main aspects of the random forest classifiers that were learned in the experiment.
in each case the top level decisions center around the following generic classifier if l fanout m dest node in deg n then else theaboveexpressionsaysthatifanedgehas l fanoutgreaterthan manddestination nodein degree greaterthan n thentheprobabilityfigure historgram of percentage improvement in precision scores for individual programs.
that it is a true edge is and otherwise .
for each of the static call graph constructors we can identify the constants mandn wala if l fanout .
dest node in deg .
then else doop if l fanout .
dest node in deg .
then else petablox if l fanout .
dest node in deg .
then else the orange cross precision recall in figure gives theprecision recalltrade offwhenusingthegenericclassifierfor wala.
this generic classifier has a slightly worse trade off and is much less tunable than the black line wala with call graph pruner .
however its pruning rules are also much simpler and easily understandable.
the use of l fanoutanddest node in deg in the generic classifier aligns with the fact that these are the most important features according to figure .
.
distribution of precision and recall for individual programs figure gives a histogram of the precision and recall scores of individual programs when a call graph pruner is used to prune the wala call graph at the balanced precision recall point marked by the black square in figure .
most of the programs get at least precision andaseveralevenreachthe70 precisiongoal.contrast thistotheprecisioninfigure6wherealmostallprogramsfailto cross the precision point.
asexpected therecallscoresfromfigure7droppedascompared to figure .
however most programs still get at least recall implyingthattheyretainagoodportionoftheirtrueedges.note that it is impossible to improve recall using a call graph pruner since it cannot find new edges that wala did not find.
thehistogramfromfigure11illustratesthepercentageimprovementinprecisionscores.thex axisisplottedonalogarithmicscale.
by using a call graph pruner out of the programs have their precision score boosted by at least times their original precision score.allbut2programshavetheirprecisionscoreboostedbyat least20 .nobenchmarkgetsaworseprecision.thus asignificant majority of the individual programs consistently get a large precisionimprovementwithoutloosingtoomuchrecall andachieve a better precision recall balance.
the doop and petablox graphs havesimilarcharacteristicsandareshowninthesupplementary material.
2050striking a balance pruning false positives from static call graphsicse may pittsburgh pa usa call graph tool precision recall wala cfa .
.
wala cfa call graph pruner .
.
figure impact of improved call graph precision on a monomorphic call sites client id warnings true positives in a sample of before after before after b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 average .
.
figure13 totalwarningcountsandamanualclassification of a sample of warnings for the null pointer analysis before and after applying a call graph pruner .
effect on client analyses next we look at the effect of improved call graph precision on the monomorphic call site detection and null pointer analysis clients.
monomorphic call site client.
this client is based on the walagenerated0 cfacallgraph anditusesthedynamicanalysisasthe ground truth.figure12givetheprecisionandrecallofamonomorphic call site client with and without the call graph pruner.
the call graph pruner helps the client boost precision from to and balance its goals of precision and recall.
applications of the monomorphic call sites client include devirtualizationandinlining.sincethecall graphanalysisisneversound in practice these applications require some safety checks resulting in overheads.
for example if devirtualization is used for optimization run timechecksneedtobeinsertedtoensurecorrectness .higherprecisionforthemonomorphiccall sitesclient implies that more of the call sites declared monomorphic by the static analysis actually turn out monomorphic in the ground truth.
thisinturnimpliesthatwheneverweincurtheoverheadofinlining or devirtualization we are also more likely to realize its benefits.
null pointer analysis.
this analysis is based on the paper by hubertetal.
.itisimplementedinwala andisusedtofindnullpointer errors originating from uninitialized instance fields.
the analysis is context insensitive field insensitive and flow sensitive.
it only reports potential null pointer dereferences in application code and not for the standard library.
the original wala call graph gives us on average null pointerwarningsperprogram.thehighvolumeofwarningsmakes itcumbersomefordeveloperstomanuallyinspectandinpractice thisresultsindevelopersignoringthetooloutputentirely .using the call graphs produced after pruning gives us much fewer on average per program warnings.
two of the authors manually inspected a random sample of null pointer warnings from of the test programs when usedwithandwithoutthecall graphpruner.the10programswere chosenwiththecriteriathattheyhadatleast10warningsbothwith and without the call graph pruner and the ratio of warnings with and without the call graph pruner was close to .
figure givesthetotalwarningcountsaswellasthetrue positivecounts from a sample of warnings for each of these programs.
the useofacall graphprunerhelpedthenull pointeranalysisimprove its precision from to thecriteriaformarkingawarningasatrue positivewasthat the author could trace the backward slice of a dereference to an instance fieldwhich wasuninitialized bythe endof a constructor.
warningsthateithercouldnotbeverifiedin10minutes raninto another exception before triggering the null exception or otherwiseunverifiablebytheauthors wereconsideredasfalse positives.
reachability from the main method was not considered because it is hard to verify manually.
we leave to future work to try other clients including other approaches to null pointer analysis such as nullaway .
.
threats to validity the first threat is the use of a dynamic analysis as a proxy for thecall graphgroundtruth.itassumesgoodcoverageofthetrue ground truthcall graphandaffectstheprecision recallcalculations.
ifthedynamicanalysishadhighercoverage moreofthestaticanalysisedgeswouldbeinthedynamiccall graph.asaconsequence both the baseline precision scores as well as the pruned call graph precision scores would be higher.
in contrast we expect the recall scorestoremainsimilar.however improvingdynamicanalysiscoverage is a non trivial and orthogonal problem and any techniques improving coverage will automatically improve our technique and evaluation.symbolicexecution isoneoptiontoimprovecoverage butitdoesn tscaletothesizeofourprograms.instead weusea subsetofthenjr 1benchmarksetwhichgetsgoodcoverage.note that this threat does not affect the evaluation of the null pointer analysis.
thesecondthreatisthemanualinspectionofthenull pointer warnings whicharevulnerabletohumanerrors.theauthorsinspectingtheerrorshavealimitedfamiliaritywiththecode bases of the examined program.
this could lead to misclassification of both true and false errors and affect the precision score accordingly.further the precisionscoresarereportedfor asampleof10 programs.
thethirdthreattovalidityisthegeneralizabilityoftheresults to programs outside the njr dataset.
our assumption is that our learningandevaluationresultsgeneralizetootherprogramsoutside the dataset.
the fourth threat to validity is that programs in the training set and evaluation set share some third party libraries.
on average geometricmean .6percentofthemethodsofaprograminthe evaluationsetalsooccurinsometrainingprogram.webelievethat thisoverlapislowenoughtonotsignificantlyaffecttheconclusions of our evaluation.
2051icse may pittsburgh pa usa akshay utture shuyang liu christian gram kalhauge and jens palsberg related work ourtechniqueisthefirsttoapplymachinelearningtoboostcallgraphprecision.inourdiscussionofrelatedwork wefocusonthree areas combiningstaticanddynamicanalyses applyingmachine learningtoremovestatic analysisfalse positives andimproving the precision of call graph construction.
combining static and dynamic analysis.
prior research has used a dynamic analysis to improve the precision of a static analysis.
grechet.al generatedynamicheapinformationandusethis as a drop in replacement for the heap modeling part in an existing static analysis tool to improve its precision.
artzi et.
al use a dynamic analysis to confirm the mutability information computed by a static analysis.chen et.
al use the informationfrom testexecutionstoprioritizethealarmsgivenbyastaticanalysis.the main drawback that these tools face is that they need the dynamic analysis to be run every single time the tool is run.
in contrast our technique needs the dynamic analysis only for generating a callgraph pruner.
after that a call graph pruner is purely a static tool andhencedoesnotsufferfromtheusualdrawbacksofadynamic analysis like long execution times or finding good inputs.
applyingmachinelearningtoimprovestatic analysisbyremovingfalse positives.
thetechniqueoffilteringstatic analysisfalsepositives by casting it to a classification problem with hand picked featureshasbeenusedforstaticbug analysistools .eachof theseworksfollowsthe sameworkflow collectstatic analysiserror reports getaprogrammertolabelthemastrueor false positives design a feature set for the error reports and then train a classifier on these labeled error reports to identify falsepositives.however theyhaveminordifferencesamongthemselves in terms of the feature set chosen the bug reporting tool used and the benchmarks used forthe training data.
ruthruff et.
al use the findbugs bug reporting tool and the set of java programs at google as their dataset.
heckman and williams also use findbugs reported bugs on open source java projects.
yuksel andsozer classifybug alertsforadigitaltvsoftware.flynn etal.
combinethebug alertsfrommultipletools inaddition to using the hand picked features.
tripp et.
al work with a javascriptsecuritychecker swarningsfrompopularwebsitesas its dataset.
our work differs in three ways it uses an estimate of groundtruthproducedbydynamicanalysis ithasageneralizableapproach to picking afeature set and ithas a tunable precision recalltradeoff as we discuss next.
the key bottleneck faced by each of these prior works was that theyreliedonthecollectionofhuman labeledground truth which does not scale.
this restricted their dataset to a handful of projects and a couple of thousand data points bug reports at best.
in fact foreachtypeoferror thereistypicallylessthanafewhundredbugs in each of the datasets.
in contrast our technique uses an estimate of ground truth produced by dynamic analysis which allows it to scale to a much larger number of programs with a million data points call graph edges .
thesecondmajordifferenceisinthechoiceofthefeature set.
this is partly a consequence of the fact that the previous work focuses on static analysis error report data which is different fromthegraphoutputgeneratedbycall graphconstructiontools.hence some of the common features used in these works are the bugpriority level file modification frequency coding style metrics and lexical features like method or package names .
these features though appropriate violate generalizability and black box guidingprincipleslistedinsection3.
.non black boxfeatureslike bug priority level will not generalize across different tools or algorithms andnon generalizablefeatureslikelexicalfeaturesare unlikely to generalize to programs outside the dataset.
in contrast we use a systematic approach to selecting features as described in section .
and as a consequence our approach generalizes easily across multiple programs and multiple call graph construction tools.
the third difference is that these prior works except for provide a single precision recall point.
provide eight differentprecision recallpoints byvaryingtheclassifierused.instead our approach has a tunable precision recall trade off by predicting edge probabilities and pruning edges with probability lower thanathreshold.further weonlyuseasingleclassifier random forests sinceitachieves superiorprecision recall trade offsthan the classifiers used in .
another areathat uses machine learningfor filtering falsepositive is the work by raghothaman et al.
.
they predict the probabilitiesofstatic analysisalarmsusingbayesianinferenceand updatetheseastheuserresolvesalarmsastrueorfalsepositives.
thisparadigmofonlinelearning wherethemodelislearnedand improved as the user gives feedback is quite different from our fully automatedofflinelearningparadigm wherewedoaone time trainingonalargedatasetofstaticanddynamicanalysisoutputs and require no user input.
recently data driven techniques have also been used to selectively apply context and flow sensitivity to methods that willbenefititthemost.thesetechniquescanpotentiallyprovide the precision improvement of a cfa at a lower overhead but as seen in figure this improvement is still much lower than what is achieved by our call graph pruner.
improving the precision of call graph construction.
lhotak designed an interactive tool to qualitatively understand the root cause of differences between different static and dynamic analysis tools.
this is then used in a case study to understand the main cause of imprecision in a static analysis tool as compared to its correspondingdynamicanalysisoutput.incontrast ourclassifier generator is fully automated using machine learning and doesn t require a skilled programmer to use an interactive tool to figure out the cause of the imprecision.
sawin and rountev propose certain heuristics to deal with dynamic features like reflection dynamic class loading and native method callsin java which helpsto improvecall graphprecision of the cha algorithm without sacrificing much recall.
similarly a call graph pruner trades of a little recall for a large boost in precision butitachievesthisthroughautomatedmachinelearning on a dataset of call graphs instead and is able to boost precision by a much larger amount.
additionally we work with a cfa baseline with no handling of dynamic features like reflection which already has a large precision gain over a cha algorithm with reflection handling.
2052striking a balance pruning false positives from static call graphsicse may pittsburgh pa usa zhangandryder createpreciseapplication onlycallgraphs byidentifyingwhichedgesfromthestandardlibrarytotheapplicationarereallyfalse positive.thisissimilartotheprecisionboost we gain for the edges that go via the standard library.
however wegenerateaclassifierthatlearnsthisonitsownfromdata and we use the classifier in a call graph pruner that is able to boost precision even further.
the patent by reif et.
al uses probabilities to quantify analysis imprecision.
each analysis constraint is assigned a probability heuristically or via user configuration and the probabilities for call graphedges arederived fromthese usingatype propagation graph.
in contrast our call graph pruner learns all its edge probabilities from data about static and dynamic call graphs.
further while their technique calls for a new static analysis our call graph pruner works as a black box post processor for existing call graph construction tools.
moredistantlyrelatedistheworkbyblackshearet.al which prunescontrol flowedgesrepresentinginterleavingsbetweenevents in an event driven system.
this pruning task is different from our task which focuses on pruning call graphs edges for sequential code.
there has alsobeen prior work that usesa dynamic analysis to evaluate call graph related static analysis tools .
our tool additionally uses the dynamic analysis results as training labels to prune the result from a static call graph construction tool.
conclusion and future work ourapproachtogeneratingahigh precisioncallgraphfirstruns anexistingblack boxcall graphconstructorandthenprunesthe resulting call graph.
a call graph pruner uses a classifier which is trainedonalargenumberofstaticanddynamiccallgraphs topredicttheprobabilityofanedgebeingatrue positive.usingdifferent thresholds for the edge probabilities we can tune the precisionrecall trade off of the call graph.
we empirically showed how a call graphprunercanbeusedtoboostprecisionandbalancethe recallandprecisionofcallgraphsproducedbywala doopand petablox which are otherwise skewed towards high recall and low precision.
we also ran a null pointer analysis and a monomorphic call sites analysis with these pruned call graphs and we showed that they got much closer to the high precision expectations of their users.
futureworkincludesautomaticallylearningafeature setforuse byourprunergeneratorandourgeneratedcall graphpruners.a particularly promising avenue for future work is to explore graph neuralnetworksforautomaticfeature learning.recentworkhas used graph neural networks for program analysis tasks like program similarity variable misuse prediction variable nameprediction andmethodnameprediction .thefeatures thatarediscoveredinthosepapersarenotfeaturesofcallgraphs and hence this remains an open problem.
a second futuredirection could be to replace dynamic analysis ground truth labeling with developer labeling for call graph edges.
thechallengehereisthatthecumulativenumberofedgesinthe training dataset is nearly a million and developer labeling doesn t scale to such a large dataset.athirdfuturedirectioncouldbetoadaptourtechniquetoheapreachability queries .