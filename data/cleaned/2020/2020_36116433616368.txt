deminify neural variable name recovery and type inference yi li new jersey inst.
of technology new jersey usa yl622 njit.eduaashish yadavally university of texas at dallas texas usa aashish.yadavally utdallas.edujiaxing zhang new jersey inst.
of technology new jersey usa jz48 njit.edu shaohua wang new jersey inst.
of technology new jersey usa davidshwang ieee.orgtien n. nguyen university of texas at dallas texas usa tien.n.nguyen utdallas.edu abstract to avoid the exposure of original source code the variable names deployed in the wild are often replaced by short meaningless names thus making the code difficult to understand and be analyzed.
we introduce deminify a deep learning dl based approach that formulates such recovery problem as the prediction of missing features in a graph convolutional network missing features.
the graph represents both the relations among the variables and the relations among their types in which the names or types of some nodes are missing.
moreover deminify leverages dual task learning to propagate the mutual impact between the learning of the variable names and that of their types.
we conducted experiments to evaluate deminify in both name recovery and type prediction on a python dataset with 180k methods and a javascript js dataset with 322k files.
for variable name prediction in .
and .
of the cases in python and js code respectively deminify can predict correctly the variables names with a single suggested name.
deminify relatively improves .
.
and .
.
in top accuracy over the state of the art variable name recovery approaches for python and js code respectively.
it also relatively improves .
.
in top accuracy over the existing type prediction approaches.
our experimental results showed that learning of data types helps improve variable name recovery and vice versa.
ccs concepts computing methodologies neural networks software and its engineering language types .
keywords type inference name recovery deep learning minified code acm reference format yi li aashish yadavally jiaxing zhang shaohua wang and tien n. nguyen.
.
deminify neural variable name recovery and type inference.
in corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction code minification is the process in which source code is minified such that the variable names are replaced with short opaque and meaningless names.
it is useful in software development as it could improve rendering time due to payload size.
e.g.
in web technology aws cloudformation templates may have lambda function source code in python embedded in them but only if the function is less than 4kb.
in other cases it is a code protection scheme that slows down those who have bad intentions since program is a valuable asset to companies.
variable name minification hides the business logics from the readers while maintaining the essence of the code.
for better code readability and understandability especially when the original source code is unavailable there is a natural need to automatically recover the minified code with meaningful variable names.
with the recovered names and types the source code will be accessible for code review analysis enhancement reuse etc.
several automated approaches have been proposed to automatically recover the names of the variables in the minified source code.
the approaches can be broadly classified into three directions information retrieval statistical learning and machine learning .
jsneat follows an information retrieval ir approach to recover names by searching for them in a large corpus of open source js code.
jsneat integrates three types of contexts to match a variable in given minified code against the corpus including the properties and roles of the variable its relations with other variables under recovery and the task of the function to which the variable contributes.
despite its successes due to the inherent limitation of the information retrieval direction jsneat cannot generate a new variable name that was not encountered in the corpus.
jsnice following a statistical direction is an automatic variable name recovery approach that represents the program properties and relations among program entities in javascript js code as dependence graphs.
jsnice uses a probabilistic model with the dependency network including variables and surrounding program entities.
it formulates the problem of variable name recovery as the structured prediction via conditional random fields crfs .
unfortunately it still has low accuracy.
in contrast jsnaughty formulates that problem as a statistical machine translation smt from minified code to the recovered code.
however its phrase based esec fse december san francisco ca usa yi li aashish yadavally jiaxing zhang shaohua wang and tien n. nguyen translation approach cannot capture well the relations among the variables to be recovered leading to low effectiveness.
in this work we present deminify a deep learning dl based variable name recovery and type inference approach.
we address both tasks as parts of the dual task learning between a variable name prediction model and a type prediction model.
correct learning of one model can benefit for the learning of the other and vice versa due to the naturalness of names in source code .
except for a few un important variables e.g.
running variables in a loop or temporary variables the majority of the variables carry some contextual meaning toward achieving the task intended in the current function.
the names chosen for such a variable in the original code should be natural unsurprising with respect to its type .
for example for easy comprehension a variable of the type offset might have a name relevant to the notion of offset or its abbreviations e.g.
startoffset endoffset etc.
similarly if a model learns the name of a variable its type should be in accordance with the name.
for example if a model recovers the name of a variable as count or index its type might likely be of intorinteger .
exploring this duality can provide useful constraints to predict both the variable names and their types.
to build the variable name prediction model and the type prediction model we leverage graph convolution network missing features gcnmf to model different kinds of relations dependencies among the variables and among the types .
we formulate the name recovery problem as the predicting the missing features in gcnmf.
with the philosophy that tell me your friends i ll tell you who you are deminify decides a variable s name by learning at once the names of the variables connecting to one another.
our dl based model is expected to have better predictive power than crf in jsnice and smt in jsnaughty especially in predicting the missing features when considering both relations among variables and types.
to propagate the mutual impact of type learning and name learning we apply a dual task learning mechanism between the two models.
we have conducted experiments to evaluate deminify in both name recovery and type prediction on a python dataset with 180k methods and a javascript js dataset with 322k files.
for variable name prediction in .
and .
of the cases in python and js code respectively.
deminify can predict correctly the variables names with a single suggested name.
in and .
of the cases in python and js code the correct names of local variables are in the top candidate suggested lists.
for python code deminify relatively improves .
.
and .
top accuracy over the state of the art variable name recovery approaches jsnice jsnaughty and jsneat respectively.
for js code the relative improvements over those baselines are .
.
and .
respectively.
for variable type prediction in python in of the cases deminify can predict correctly the types with a single predicted type.
top accuracy for type prediction is .
it relatively improves .
.
in top accuracy and .
.
in top5 accuracy over the state of the art type prediction approaches hityper type4py typilus and typewriter .
in brief the contributions of this paper includes .deminify a deep learning dl based approach to recover variable names with type inference for minified code in dual task learning.
deminify s type inference can be used for regular code.
.
an extensive evaluation and analysis on deminify s accuracy.
def exportselection self root doc if not root return null selection doc.getselection if selection.rangecount range selection.getrangeat preselectionrange range .clonerange preselectionrange.selectnodecontents root preselectionrange.setend range .startcontainer range .startoffset start len str preselectionrange selectionstate start start end start len str range if self.doesrangestartwithimages range doc selectionstate.startswithimage true trailingimagecount self.gettrailingimagecount root selectionstate range .endcontainer range .endoffset if trailingimagecount selectionstate.trailingimagecount trailingimagecount if start !
emptyblocksindex self.getindexrelativetoadjacentemptyblocks doc root range .startcontainer range .startoffset if emptyblocksindex !
selectionstate.emptyblocksindex emptyblocksindex ... figure an original code from a project in github def exportselection self w b if not w return null q b.getselection if q.rangecount r q.getrangeat d r.clonerange d.selectnodecontents w d.setend r.startcontainer r.startoffset m len str d p start m end m len str r if self.doesrangestartwithimages r b p.startswithimage true a self.gettrailingimagecount w p r.endcontainer r.endoffset if a p.trailingimagecount a if m !
y self.getindexrelativetoadjacentemptyblocks b w r.startcontainer r.startoffset if y !
p.emptyblocksindex y ... figure the minified code for the code in figure .
a novel formulation of variable name recovery and type inference in minified code as a missing feature graph based neural network in a dual task learning framework to benefit both tasks.
motivating example let us start with a real world example to motivate our approach.
figures and show the original and minified versions of the function exportselection in python.
the function is aimed to export retrieve the selection from a document.
in the minified code all local variables were randomly renamed by a minification tool with short and meaningless names e.g.
rootbecomes w docbecomes b etc.
this makes the code difficult to comprehend.
we aim to recover the names of the variables in the minified code.
such process is not trivial and affected by multiple factors.
let us explain the following observations then to motivate our solution 759deminify neural variable name recovery and type inference esec fse december san francisco ca usa observation .
the mutual impact between variable name learning and variable type learning.
if a model learns correctly the type of a variable it would help learn better the name of the variable and vice versa.
let us consider the name emptyblocksindex for the variable y at line in figure if y !
.
from that comparison a model could learn that yis of the type int.
knowing that it is an integer a model could combine that with the knowledge learned from line y self.getindexrelativetoadjacentemptyblocks ... and predict the name for ycould be index or similar.
on the other hand correct learning of a variable name can also benefit for learning of its type.
at line of figures if the name trailingimagecount is recovered for the variable a its type is likely to be intif the model could make sense of the sub token count in that name trailingimagecount .
key idea .
dual task learning between name prediction and type prediction while aiming to recover variable names in minified code we leverage the duality between the learning to predict the variable names and the learning to predict the variable types.
in the original code the name of a variable should be natural unsurprising with respect to the type of that variable.
we build a name prediction model and a type prediction one and we apply a dual task learning mechanism connecting the two models.
observation .
the name and type of a variable are affected by the names and types of the surrounding variables in a function.
intuitively because multiple variables are used together to achieve the task in the function their names are often consistent with one another.
for example at line the choice of the name range in the recovery process could be derived by the choice of the variable selection and the call to getrangeat as it is made on that variable as in the statement range selection.getrangeat .
the choice of the name preselectionrange could be affected by the choice of the variable range due to the statement preselectionrange range .clonerange .
moreover the type system in a programming language always requires the concordance between the types of variables.
key idea .
a variable name or type are influenced by the names or the properties of the other variables having the relations with that variable in the surrounding context.
we treat the problem of variable name generation as predicting the missing features in a graph neural network by leveraging graph convolutional network missing features gcnmf .
we also use edge enhanced graph convolutional network ee gcn to model different kinds of relations among the variables and types in the function method.
observation .
the actual variable name must also be in accordance with the names of the accessed fields and called methods .
for example in the original code in figure the variable name range makes sense inrange .startoffset and range .endoffset because a range could have a starting offset and an ending offset.
the rationale is that in the original code for easy comprehension developers tend to follow naming conventions and use meaningful names with respect to the surrounding variable names in the code.
that is the predicted name of a variable and the names of its properties fields and methods are in accordance.
as an example preselectionrange and setend are in accordance with each other in preselectionrange.setend setting the end of the selected range .
in fact pradel et al.
explore the concordance between the method s name and the names of its arguments to detect name based bugs in a program.
figure neural variable name recovery and type inference observation .
the fields and methods of a variable are kept intact after minification.
if the names of the fields and methods were minified the corresponding field accesses and method calls would not be valid anymore.
for example clonerange inrange .clonerange at line and startcontainer inrange .startcontainer at line in figure are unchanged in figure .
thus a model can rely on the names of those properties of a variable to predict the variable s name.
key idea .
the name of a variable is in accordance with its own properties including the names of its fields in field accesses and the names of its methods in the method calls on that variable.
moreover the names of fields and methods are kept intact after minification thus a model can rely on those names to predict the variables names.
for example a model can learn from the variables that have the field accesses to startcontainer endcontainer startoffset and endoffset and have the method calls to clonerange .
it can use that knowledge to predict the name range .
in brief to recover the name and type of a variable a model could examine the properties of the variable its relations with other variables and their properties and the relations among their types .
approach overview we propose deminify that accepts minified code and at the same time recovers the variable names and derives the types for variables expressions.
it also can take regular code and derive the types.
figure illustrates the overall process.
the input is the minified code with all the original variables names and types during training and without them during the prediction.
the process contains the following key steps.
first the minified code is parsed and two feature graphs are extracted the type dependency graph representing the relations among the types of the variables in a function method according to type inference rules and the relation graph represent the relations among the variables including the ones via field accesses and method calls section .
the two graphs can always be extracted for minified code in both training or prediction.
they will be merged into a representation graph.
we have two models dedicated to the two tasks variable name generation vng and variable type generation vtg .
deminify first extracts the features in a representation graph and converts them into the input vectors for the vng and vtg models.
the vng model processes them as follows.
for the nodes that represent the variables with the minified names we mask the node features and regard them as the missing features and feed the 760esec fse december san francisco ca usa yi li aashish yadavally jiaxing zhang shaohua wang and tien n. nguyen figure relation graph for the code in figure graph into a graph convolutional network missing features gcnmf .
the actual variables names in the input minified code are used as the labels during training.
for name prediction the same process is used except that the variables names are predicted by the trained vng model.
to generate names we also use program rules to ensure scoping and valid and consistent names.
the vtg model leverages edge enhanced graph convolutional network ee gcn with the support of an embedding model as well as a gate recurrent unit gru .
the actual variables types in the input minified code are used as the labels during training.
for prediction the same process is used except that the types are predicted using the trained model.
to generate the types we use type inference rules to eliminate the impossible candidates.
to propagate the impact between vtg and vng we apply a dual task learning scheme between them.
we use the uncertainty weighted multi task loss as the loss function and use the maximum of the top accuracy scores from two tasks as the training target.
for non minified code to infer the types the representation graph is extracted as explained and then fed to deminify whose vtg will produce the types for variables expressions.
the full variable names will help vng improve vtg s type inference task.
important concepts in this section we present the important concepts used in deminify .
to identify the name of a variable first deminify examines its ownattributes and behaviors via field accesses and method calls then the relations of the variables to learn the concordance among the variables names.
at the same time it examines the relations among the types of the variables .
definition .
the fields and methods of the object represented by a variable are referred to as the attributes and behaviors respectively.
the names for those fields and methods of a variable are intact after code minification.
in figure at lines we explore the field accesses and method calls of the variable rin the method calls and field accesses made tor e.g.
r.clonerange r.startoffset and r.startcontainer .
we denote an instance of field access and method call as a triple v p t wherevis the variable pis the name of the field or method and tis either fieldaccess ormethodcall .
the examples are r clonerange methodcall and r startoffset fieldaccess .
type u none type elementarytype int float str bool bytes generictype a list tuple dict set callable generator union b builtintype b u userdefinedtype u allclassesandnamedtuples o overloadinguserdeftype o allclasseswithoperator overloadingincode figure types in python definition .
a variablevis said to have an argument relation with a method mif it is used as an argument of a call to that method as in o.m ... v ... .
definition .
a variablevis said to have an assignment relation with a method mor a fieldfif it is used as a left hand side in an assignment from a method call or a field access as inv o.m ... orv o.f.
the idea is that the name of the minified variable vin the original code is often in accordance with the names of the method or the field in such an assignment or an argument.
for example selectnodecontents and root are in accordance with each other in preselectionrange.selectnodecontents root or range and getrangeat are in accordance with each other in range selection.getrangeat .
we will use the triple notations v m argument v m assignment and v f assignment to denote those three cases where vis a variable mis a method name and fis a field name.
definition .
a relation graph rg is a directed graph in which each node of the rg represents a variable.
the connected nodes represent the methods fields in method calls or field accesses respectively and are labeled with their names.
edges represent relations among nodes and are labeled with relation types.
figure shows the relation graph for the variables in the code in figure .
for example there are an assign edge from the variable rto the node getrangeat and amethodcall edge from qtogetrangeat because we have r q.getrangeat at line .
regarding type inference figure shows the type system in python .
to represent the dependencies among the types we adopt type dependency graph tdg which aims to capture the type inference rules for variables expressions.
definition .
a type dependency graph is a graph g n e in whichnis the set of nodes representing all the variables and expresions and eis the set of edges fromni njindicating that the type of njcan be derived from the type ofniby the type inference rules in the type system.
in figure let us consider line q b.getselection .
the tdg will contain a node for the expression b.getselection connecting to a node for the variable qbecause its type can be derived from the return type of the method call getselection .
we also have a node for the variable bconnecting to a node of the method getselection since the type of getselection can be derived from that of variable b. b.getselection q b getselection 761deminify neural variable name recovery and type inference esec fse december san francisco ca usa figure type dependency graph for the code in figure connecting all the dependencies among the types of variables and expressions we have the type dependency graph for a function method.
note that both rg and tdg can be built for either the original or minified code.
the merging of rg and tdg is straightforward.
the nodes of the same variables method calls field accesses in two graphs are unified.
the other nodes in two graphs are kept.
all edges are combined including the type dependency edges.
figure displays the tdg for the running example.
the type of the variable mat line figure can be derived by the function call len.
the subscript is used to denote the same variable at different locations.
at line figure the type of the variable pis derived from the type of the left hand side of line dict .
thus we have an edge from dict write top.
similarly the type of the variable rat line can be derived from the call to getrangeat .
the types at line and line can be derived from the type at line .
variable name generation model this section presents the variable name generation model vng .
during training the input is the minified code with all the original variables names and types and during predicting the input does not have names and types.
first we build tdg and rg for the given code.
the two graphs are combined into a representation graphg.
for each node in g we tokenize the names in the corresponding code sequence of the node.
we consider each of them as a sentence and use an embedding model e.g.
glove to build the representation vector for each node in g. next for training we process the graph gwith the node vectors as follows.
for the node nthat represents a variable with minified name we perform masking the node feature with a special mask token for the variable name and consider it as a missing feature.
in the vng model we leverage an advanced neural network called graph convolutional network missing features gcnmf .
the actual names are used as the ground truth labels to train the gcnmf model.
the key characteristic of gcnmf is its ability to deal with incomplete and missing features in a gcn.
it represents the missing data by gaussian mixture model gmm and calculates the expected activation of neurons in the first hidden layer of gcn while keeping the other gcn layers unchanged.
the gmm parameters and gcnmf weight parameters are learned within the samearchitecture enabling the learning of missing features.
the gcnmf model is trained with the masks for the minified variable names.
for prediction applying on the minified code without names the gcnmf model outputs the vectors for the nodes in the graph gand the vectors for the missing features.
the vectors representing the missing features i.e.
the missing variables names in the input graph gare used next.
we leverage an gate recurrent unit gru as a decoder.
the decoder accepts those vectors for missing names as input and generates the names for the variable nodes during training the name labels are known and used .
finally we apply the semantic checkers to make sure that the variables names are valid in the scope and the same variable is assigned with a consistent name.
let us use figure to illustrate the benefit of modeling the variable name generation recovery problem as predicting the missing features using gcnmf .
a variable with a minified name is modeled as a nodel in our graph e.g.
r q andd.
the prior works based on machine translation e.g.
jsnaughty aiming to translate the minified code to the orginal code face an issue of different naming schemes used by different minification tools.
for example the variable range might become r1 instead ofr by a different minification tool or by alpha renaming.
in deminify the minified names themselves do not play a crucial role in deciding the original names as in prior work.
they help mainly in recognizing the occurrences of the same variable.
in our graph g deminify considers a node for a minified variable as a placeholder with a missing feature that it aims to fill in.
in prediction gcnmf will create the vectors vr vq andvdfor the nodes.
during the convolution process those vectors are automatically updated based on the neighboring node features and the relations.
after convolution the final vectors will be fed into the gru decoder for variable name prediction.
next let us present the variable type generation model and then the dual task learning scheme between vng and vtg models.
variable type generation model this section presents the variable type generation model vtg .
the input is the minified code with all the original variables names and types during training and without the types during prediction.
similar to vng model the combined graph gis processed in which the names in the code sequence of each node nis tokenized and an embedding model is applied to build the vector for nconsidering each sequence of sub tokens for nas a sentence.
next we feed the graph gwith those vectors to edge enhanced graph convolutional network ee gcn .
ee gcn could accept both node and edge features.
we use the above vectors as node features and the edge types in the graph g built from tdg and rg as the edge features.
the rationale for choosing ee gcn is its capability producing the embeddings emphasizing on the edges ing which represent the relations dependencies among the data types.
a key characteristic of ee gcn is that it has an edge aware node update module and a node aware edge update module and two modules work in a mutual way by updating each other iteratively.
specifically for each layer the edge aware node update module is first performed for aggregating information from neighbors of each node through specific edges.
then a node aware edge update module is used to dynamically refine the edge representation with 762esec fse december san francisco ca usa yi li aashish yadavally jiaxing zhang shaohua wang and tien n. nguyen figure variables name generation model vng figure variable type generation model vtg its connected node representations making the edge representation more informative .
the output of the ee gcn model includes the list of the representation vectors vnfor all the nodes in g. to further propagate the impact from variable name learning to type learning we combine the above vectors vnwith the vectors obtained from the gcnmf in the variable name generation model.
specifically we use the cross product between the two vectors to produce the final vectors vffor the type prediction for all the nodes.
next we leverage a gate recurrent unit gru as a decoder which accepts the vectors vfs as input and generates the type for the node.
during training the type labels are used .
deminify handles the primitive and non primitive types in the same way as the decoder generates them as texts for the type names.
finally we also apply the rule based filter which performs type checking to eliminate the candidates that violate the type inference rules.
in the current implementation we used a small subset of the static type inference rules for python .
for example we check the types of the lefthand side and the right hand side of an assignment the type of the condition in an ifstatement the type of a comparison operation the simple sub typing rules for primitive types and for lists tuples and dictionaries etc.
the final result contains the types for all the nodes including the the nodes for the variables.
7dual task learning for vng and vtg indeminify to propagate the mutual learning between variable name learning and variable type learning we leverage a dual task learning framework to train both vng and vtg models together.
as for these two tasks deminify regards them as the regression problem and multi task loss to learn both of them at the same time.
specifically for each regression deminify uses a smooth l1 loss function to estimate the accuracy of output as follows l l1 l2 ... ln t ln .
f x n yn f x n yn f x n yn .5otherwise wheref x nis the output for a variable nin a regression task f yn is the ground truth.
to get the joint loss function for the dual task learning with uncertainty weighting following kendall et al.
s we have li w lw lw ... lw n t lw n .
f x wn yn f x wn yn f x wn yn .5otherwise l w 2 i1 2 2 ili w log 2 i wherewis the weight adding to the input iis theithnoise scalar andwand iare both trainable parameters in the model.
by combining all the loss functions into one as in formula deminify trains the variable name model and the type prediction model together.
we choose the smallest loss results to get the most suitable model parameters for deminify .
empirical evaluation .
research questions and datasets to evaluate deminify we seek to answer the following questions rq1.
comparative study on variable name prediction.
how well does deminify perform in comparison with the state of theart variable name prediction approaches on the python dataset?
rq2.
comparative study on variable name prediction.
how well does deminify perform in comparison with the state of theart variable name prediction approaches on the javascript dataset?
rq3.
comparative study on variable type prediction.
how well does deminify perform in comparison with the state of theart variable type prediction approaches on the python dataset?
rq4.
ablation study.
how do the key features in deminify affect its overall performance?
dataset.
we have conducted our experiments to evaluate deminify on the well established python dataset manytypes4py provided in the work by mir et al.
.
the dataset includes 180k methods 763deminify neural variable name recovery and type inference esec fse december san francisco ca usa from python projects with different variable types.
we applied pyminifier a minification tool for python to minify the variable names in the source files.
we also used a dataset in javascript js provided in a prior work jsneat .
the js dataset includes 320k methods from projects with 176k unique variable names.
we minified them with the minifying tool uglifyjs .
.
experimental methodology .
.
comparison on variable name prediction in python rq1 .
baselines.
we compared deminify against the state of the art variable name recovery approaches for minified code including jsneat jsnice and jsnaughty .
procedure.
we took all the methods in the python dataset and used pyminifier to produce the minified code with variable name mininification.
we randomly split all the methods into in which of the methods as the training dataset of the methods as the tuning dataset and of the methods as the testing dataset for all the baselines and deminify .
parameter tuning.
we tuned deminify with automl for the following key hyper parameters to have the best performance epoch size batch size learning rate .
.
.
vector length of feature embeddings and its output number of gcn layers .
.
.
comparison on variable name prediction in js rq2 .
we build the js representation graph and use the same model.
baselines.
we compared deminify against the state of the art approaches jsneat jsnice and jsnaughty .
procedure and tuning.
we used uglifyjs for code minification in the js dataset.
we used the same splitting and tuning as in rq1.
.
.
comparison on variable type prediction rq3 .
baselines we compared deminify against the state of the art variable type prediction approaches ivanov et al.
typilus typewriter type4py and hityper .
we did not compare our approach with xu et al.
deeptyper nl2type lambdanet opttyper and typebert because in its paper type4py has been shown to perform better than those tools.
we did not compare on js code because js variables must not always have type declarations.
procedure.
with all variable type information summarized in the manytypes4py dataset we directly used the types for the variables as the ground truth for all the approaches.
we used the same data splitting for training tuning and testing as in rq1.
we ran the baselines on the original source code with the parameters in their documentation.
we fed to deminify the minified code but with the original names and no types i.e.
the original code and obtained the resulting types from the variable type generation vtg model for all the variables expressions in the code.
parameter tuning.
tuning was done via automl as in rq1.
.
.
ablation study rq4 .
we built several variants of deminify to evaluate different factors in its components by measuring their accuracies and making comparisons.
first we evaluated our hypothesis that mutual impact exists between name learning and type learning via dual task learning.
we built two variants vng and vtg separately without connecting them via dual task learning.
second the graph representation for source code is important in the figure top accuracy on name prediction python rq1 name and type prediction.
thus we kept the same architecture for deminify and fed to it different graph representations for source code.
this allows us to evaluate the impact of our chosen graph representations for this problem.
third the key technical solution in deminify is the formulation in which predicting names and types for variables is considered as predicting the missing features of connected program elements.
we currently used the ee gcn model to capture the relations and dependencies among program elements.
we replaced it with the label gcn and made a comparison.
evaluation metrics.
for the variable name prediction we follow a prior work to calculate the prediction accuracy on local variables and all variables.
we compared the resulting names from a tool against the original names.
a tool is considered to correctly recover the name of a variable vif the recovered name is matched exactly with its original name.
for v if matching we count it as a hit otherwise it is a miss.
accuracy is measured by the ratio between the total number of hits over the total number of cases.
top kaccuracy is measured similarly however a hit is achieved when the correct name is in the top kcandidate list from a tool.
for the variable type prediction we use two metrics exact match and parametric match with the top kaccuracy as in hityper .
exact match occurs when the resulting type matches exactly with the human annotations.
parametric match occurs if the result matches with the correct type but those of the parameters might not.
for example dict is parametric matched with dict .
experimental results .
comparison on name prediction rq1 as seen in figure for all variables in minified python code deminify achieves high top accuracy of .
i.e.
in .
of the cases it can recover the correct variable names with a single prediction.
the relative improvements in top accuracy for all variables over jsnice jsnaughty and jsneat are .
.
and .
respectively.
the absolute improvements in top accuracy over those state of the art approaches are from .
.
.
considering only local variables in .
of them deminify correctly predicts their original names with a single result.
the relative improvements in top accuracy in recovering local variables names over jsnice jsnaughty and jsneat are .
.
and .
respectively.
the absolute improvements in top accuracy over those state of the art approaches are from .
.
.
764esec fse december san francisco ca usa yi li aashish yadavally jiaxing zhang shaohua wang and tien n. nguyen table comparison on variable name prediction rq1 top top top local all local all local all jsnice .
.
.
.
.
.
jsnaughty .
.
.
.
.
.
jsneat .
.
.
.
.
.
deminify .
.
.
.
.
.
as seen in table the result is also consistent for top and top accuracies for deminify .
the relative improvements are with same trends in comparison with the baselines.
we examined the predicted names from all the baselines.
compared to jsneat an information retrieval approach we found that it often failed in the following cases.
it has not seen the names before in the database.
deminify can generate a new name with its decoder.
due to its explicitly setting of similarity thresholds jsneat faces two other issues.
the correct name was not returned since the relations and contexts are not similar enough with pre defined thresholds.
if two variables in the same function are assigned with the same name via the similarity measure jsneat cannot decide one turning to a random selection.
avoiding feature matching and explicit similarity threshold deminify with its neural network can implicitly do so without any pre defined threshold.
compared to jsnaughty with statistical machine translation from minified code to original code we found that it relies much on the minified names in minified code.
we examine the phrase mapping table a byproduct of their machine translation model which contains the knowledge it learned from training.
we reported that jsnaughty learns inconsistent mappings between the minified names and original ones.
that is there are several mappings for the same minified names with different weights depending on their occurrences in the corpus.
for example b doc b book etc.
in contrast deminify does not rely on the minified names themselves by modeling them as the nodes with missing features i.e.
missing names and types in the graph based gcnmf model .
deminify is similar in spirit to jsnice in which both formulates the problem as predicting the features attributes of the nodes in a graph.
the conditional random field in jsnice uses probabilistic name prediction graph.
in comparison deminify leverages more advanced neural network in gcnmf as well as a more specialized graph with the help of the type prediction model.
.
comparison on name prediction in js rq2 as seen in figure for all variables in minified js code deminify achieves high top accuracy of .
i.e.
in out cases it can recover the correct variable names with a single prediction.
the relative improvements in top accuracy for all variables over jsnice jsnaughty and jsneat are .
.
and7.
respectively.
the absolute improvements in top accuracy over those state of the art approaches are from .
.
.
considering only local variables in .
of them deminify correctly predicts their original names with a single result.
the relative improvements in top accuracy in recovering local variables names over jsnice jsnaughty and jsneat are .
.
and .
respectively.
as seen in table the comparison result is also consistent for top top and top accuracies.
figure top accuracy on name prediction in js rq2 table comparison on name prediction in js rq2 top top top local all local all local all jsnice .
.
.
.
.
.
jsnaughty .
.
.
.
.
.
jsneat .
.
.
.
.
.
deminify .
.
.
.
.
.
as seen in figures and deminify improves over the baselines in both languages.
however its relative improvement over the top baseline jsneat for python is higher than that for js.
the reason is that js is a weakly typed language.
in js code the types of variables do not need to be specified and can be changed that is type information is not always available .
deminify is less effective in those cases while jsneat an ir approach is still effective since the variable names might be seen in the js dataset.
.
comparison on type prediction rq3 while we focus on variable name prediction the result of variable type prediction is also useful since the types in the minified code are exactly the same for the original code.
in this study we evaluatedeminify s accuracy and compare it with the state of the art approaches in variable type prediction.
as seen in figure for all variables deminify achieves high top accuracy of for exactmatches of the types and for the parametric matches.
that is in of the cases it can recover the correct variable types with a single prediction.
the relative improvements in top accuracy in exact matching over ivanov et al.
typewriter typilus type4py and hityper are51.
.
.
.
and .
respectively.
the absolute improvements in top accuracy over those type prediction approaches are from .
regarding the parametric matches disregarding the types of the parameters deminify achieves higher top accuracy.
in of the cases it can recover the correct variable types regardless of parameters types with a single prediction.
the relative improvements in top accuracy in parametric matching over ivanov et al.
typewriter typilus type4py and hityper are .
.
.
.
and .
respectively.
the absolute improvements in top accuracy with parametric matching over those state of the art approaches are from .
moreover deminify also achieves high top k k accuracies as seen in table in of the variables the correct types 765deminify neural variable name recovery and type inference esec fse december san francisco ca usa figure top accuracy on type prediction rq3 table comparison on type prediction rq3 top top top em pm em pm em pm ivanov et al.
typewriter typilus type4py hityper deminify em exact match pm parametric match of the variables are in the list of five candidate types.
the relative improvements in top accuracy in parametric matching over ivanov et al.
typewriter typilus type4py and hityper are .
.
.
and .
respectively.
we examined the cases that deminify predicted correctly and the baselines missed.
compared with hityper hityper did not perform well for the isolated groups of a couple variables.
that is the groups are isolated i.e.
not type dependent to other variables and hityper did not correctly detect any of those variable types.
in those cases deminify could rely on the concordance between the names and types of a variable.
for example the type of a variable named index orcount will likely be int.
type4py uses a hierarchical neural network model to learn to distinguish between similar dissimilar types in a vector space.
the candidate types are predicted via nearest neighbor search.
in contrast deminify also encodes the information on variable names into the embeddings in the high dimensional space via two mechanisms dual task learning and cross product of vectors.
thus deminify could perform better in the cases in which the embeddings for types are not sufficiently distinguishable from one another which poses challenges for type4py.
regarding the used neural networks it uses recurrent neural network rnn operating on the code token embeddings built from the ast.
in contrast we use gcnmf that captures better the dependencies with different types of edges.
compared to typilus deminify relatively improves .
in top accuracy.
similar to type4py typilus builds a vector space for type embeddings.
it uses a graph neural network to learn to map variables parameters and function returns to a type embedding space using deep similarity learning.
for type inference using the type map it accepts unannotated code computes type embeddings with the trained gnn and finds the concrete knearest neighboring figure dual task learning on name prediction rq4 figure dual task learning on type prediction rq4 types as the candidates.
there are two key limitations in typilus thatdeminify overcomes.
first their graph representation does not directly encode the type dependencies.
it encodes among the code tokens the next lexical token relation structural syntactical relations and data dependencies.
second it does not have the assistance of the learning of variable names via dual task learning as explained in the comparative analysis with hityper.
typewriter builds token embeddings and uses two rnns for identifiers and for code to merge them to form type vectors.
it then uses feedback directed search on the results from a static type checker to search for consistent types.
the key limitation is that the type dependencies are not encoded during the learning.
instead typewriter leverages an external static type checker and relies on the feedback directed searching for the right types.
if the rnn models do not produce the right types at the first place searching via a static type checker will not result in any better output.
for ivanov et al.
deminify has the same advantages as the comparison to the above approaches with graph embeddings.
.
ablation study rq4 .
.
impact of dual task learning.
figure shows the top accuracy in variable name prediction when we removed the dual task learning scheme and measured only the accuracy of the variable name generation vng model.
as seen without the impact from variable type generation vtg via dual task learning vng still performs better than the best baseline jsneat .
versus .
.
the drop in top accuracy from deminify is .
from .
down to .
.
similarly as seen in figure without the impact from vng due to the removal of the dual task learning scheme vtg performs slightly worse than the best baseline hityper.
the drop in top accuracy from deminify is .
.
these results indicate the positive contribution to deminify from the dual task learning for the mutual impact of vng and vtg key idea .
.
.
impact of different types of program graphs.
as in any approach code representation is important and affects the performance.
in this study we kept the same neural network architecture however we changed different input graphs extracted from source code.
in addition to the graphs used in deminify relation graph 766esec fse december san francisco ca usa yi li aashish yadavally jiaxing zhang shaohua wang and tien n. nguyen figure impact of input graphs on name prediction rq4 cpg code property graph tdg type dependency graph rg relation graph.
figure impact of input graphs on type prediction rq4 table impact of ee gcn on top accuracy rq4 accuracy name prediction type prediction label gcn gcnmf .
.
ee gcn gcnmf deminify .
label gcn label graph convolutional network ee gcn edge enhanced graph convolutional network gcnmf graph convolutional network missing features rg type dependency graph tdg we experimented with code property graph cpg since it has been used in several machine learning approaches for code .
we did not experiment with program dependence graph pdg because it works at the statement level which does not help with variable name prediction.
as seen in figure and figure for variable name prediction rg helps the model more than tdg and for variable type prediction tdg helps the model more than rg.
this is expected because each of them is designed toward capturing the key features for its problem.
for the dual tasks the combined graph rg tdg in deminify yields the highest accuracies in both name prediction and type prediction.
in contrast cpg capturing the dependencies among program elements are not specifically designed for handling variable names and types thus did not yield high accuracy as tdg rg and cg.
an interesting observation is that cpg which contains lexical syntax and dependency information did not help the model as much as others.
it seems that cpg contains too much irrelevant information for variable name and type prediction.
.
.
impact of graph models.
table shows the accuracy of the variant model as we replaced the graph neural network ee gcn with the label graph convolutional network label gcn .
we did not replace gcnmf because it is the core component in our solution which formulates the name type recovery as the prediction of missing features.
as seen edge enhanced gcn helps improveaccuracy more than label gcn.
ee gcn enables the modeling of different types of relations because it handles different edge types in different channels while label gcn just integrates the edge information as simple labels.
this is crucial for our problem in which the type dependencies and the variable relations are well captured.
.
limitations examples threats to validity .
.
limitations.
first deminify does not predict names well for the original code with the short and meaningless names e.g.
i j etc.
in this case the variable type could not help because the name and type are not in accordance.
the relations among the variables in the context do not help either because there does not exist the naturallanguage semantic connections among their names.
the following example shows such a case the variable rat line was minified into eat line .
deminify failed this case and jsneat predicted correctly as it has seen the code with the same names.
1if word.indexof g word.indexof m 2r find smoothie modal groups group return includes group.modes word ... minified code 5if s .indexof g s .indexof m 6e find smoothie modal groups t return includes t .modes s ... second deminify does not produce well long variable names with several sub tokens.
in the next example the variable resetdocumentpropertyisset line was minified into t line .
deminify predicted an incorrect name resetdocumentownproperty with its current decoder.
a solution could be to replace the gru decoder with a better model that can predict the length of the name and the name itself.
1function shouldresetdoc config 2var resetdocumentpropertyisset config .hasownproperty ... 3return config .resetdocument !resetdocumentpropertyisset ... minified code 5function shouldresetdoc e 6var t e.hasownproperty ... 7return e.resetdocument !
t ... third for the type prediction deminify works not so well for the user defined types with long names.
fourth in some cases generic types e.g.
string did not help learn variable names.
finally we currently implemented deminify for two languages python and js.
however the approach is general for any language with a weak strong type system in which it works better for a strong one.
to expand for a new such language one just needs to have a parser and a representation graph building module for that language.
the graphs will be fed to the same architecture model.
.
.
examples.
figure shows an example of correct predictions from deminify .
at line minified code our model can derive the type boolforpdue to the ifstatement.
at line phas a relation via an assignment with check request limit then our model can produce the name is limited instead of limit for p. at line the variable qhas a relation with extract ip from deminify can use the name relevant to ipforq e.g.
client ip .
at line with a minus operation deminify can derive the type of the variable masint.
it also has a relation with birdnamedatabase.
objects.count thus our model can derive the name count form.
at line nis used as the index of an array type.
moreover at line our model can derive its type of intdue to the relation 767deminify neural variable name recovery and type inference esec fse december san francisco ca usa def get self request client ip self.extract ip from request is limited self.check request limit client ip if is limited return response status rest framework.status.http 403 forbidden count birdnamedatabase.objects.count index randint count bn birdnamedatabase.objects.all serialized birdnameserializer bn many false self.save general statistics client ip bn return response serialized.data minified code def get self r q self.extract ip from r p self.check request limit q if p return response status rest framework.status.http 403 forbidden m birdnamedatabase.objects.count n randint m t birdnamedatabase.objects.all s birdnameserializer t many false self.save general statistics q t return response s.data figure a correct prediction example by deminify def post self event subscribable none event class type event if event class not in self.
listeners return for listener in self.
listeners listener event minified code def post self b subscribable none c type b if c not in self.
listeners return for d in self.
listeners d b figure another correct prediction example by deminify with randint .
thus our model can assign the name index for n. moreover due to the not so much meaningful name bnat line our model did not produce the name for tat line .
figure shows another example in the project eventbus in our dataset.
at line our model can recognize the variable dused in a forloop with an array of listeners thus can assign for dthe name listener .
from line our model can learn that bis of the type subcribable and at line and it knows that bis related to d listener .
thus from subcribable andlistener it could derive the name for b asevent .
at line if our model can learn that typeis to return the class for b it can assign event class forcdue to the assignment.
.
.
threats to validity.
we evaluated in one dataset which might not be representative.
however the dataset has been verified and used in prior work.
deminify currently works for python and js.
other programming languages could be supported as explained above.
because some of the baselines for variable name recovery were designed for js we re implemented for python based on their source code and documentation.
we kept all the parameters in their tools as in their documentation or original source code.
related work variable name recovery approaches.
the approaches follow the three categories information retrieval ir statistical learning andmachine learning ml .
jsneat follows an ir approach to search for the names in large code corpus but is not effective for un seen names.
jsnice infers the variable names via structuredprediction with conditional random fields crfs .
we showed that deriving missing features via a neural network yields higher accuracy than predicting program properties with statistical learning.
deminify is computationally heavier than those approaches.
jsnaughty uses a statistical machine translation from the minified code to recovered code.
first it faces the issue of relying on minified names as explained.
second it uses a phrase based translation model which enforces a strict order between the recovered variable names in a function.
the other methods for code deobfuscation mainly leverage static dynamic analyses .
ml based type inference approaches.
hityper is a hybrid approach between static inference and deep learning.
it uses tdg to encode type inference rules to conduct type rejection to inspect the output predictions.
it iteratively conducts static inference and dl based prediction until the tdg is fully inferred.
as shown deminify also improves over hityper due to the propagation of vng to vtg.
type4py is a deep similarity learning based hierarchical neural network model.
it learns to discriminate between similar and dissimilar types in a high dimensional space.
deminify avoids finding similar types via embeddings.
typilus proposes typespace containing the embeddings with type properties of a symbol.
typewriter learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming languages.
deminify does not use natural language properties.
ivanov et al.
show that graph based embeddings could improve type prediction.
we use gcnmf to predict the missing features.
statistical nlp approaches have been used for name and code style suggestions .
other applications include code suggestion code convention name suggestion api suggestions code mining type resolution pattern mining code generation e.g.
swim deepapi anycode .
conclusion we introduce deminify a deep learning dl based approach that formulates name recovery problem as the predicting the missing features in graph convolution network missing features.
the learning of types and names are mutual to support both tasks of name and type recovery.
the graph represents both the relations among the variables and those among their types.
deminify also leverages dual task learning to propagate the mutual impact between the learning of the variable names and that of their types.
our empirical evaluation on real world data shows that deminify relatively improves from .
.
in top accuracy over the existing variable name recovery approaches.
it relatively improves .
.
in top accuracy over the existing type prediction approaches.
we plan to explore the gcn missing features in other problems in traditional languages e.g.
java in which the attributes of a node could possess the domain logic or other properties e.g.
values.
data availability data and code is available in a website .