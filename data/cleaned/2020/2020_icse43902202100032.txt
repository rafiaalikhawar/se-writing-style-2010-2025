distribution aware testing of neural networks using generative models swaroopa dola department of computer engineering university of virginia charlottesville usa sd4tx virginia.edumatthew b. dwyer department of computer science university of virginia charlottesville usa matthewbdwyer virginia.edumary lou soffa department of computer science university of virginia charlottesville usa soffa virginia.edu abstract the reliability of software that has a deep neural network dnn as a component is urgently important today given the increasing number of critical applications being deployed with dnns.
the need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems.
in the last few years there have been a number of research efforts focused on testing dnns.
however the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid and thus invalid inputs are produced.
to illustrate this situation we explored three recent dnn testing techniques.
using deep generative model based input validation we show that all the three techniques generate significant number of invalid test inputs.
we further analyzed the test coverage achieved by the test inputs generated by the dnn testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics.
to overcome the inclusion of invalid inputs in testing we propose a technique to incorporate the valid input space of the dnn model under test in the test generation process.
our technique uses a deep generative model based algorithm to generate only valid inputs.
results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.
index terms deep neural networks deep learning input validation test generation test coverage i. i ntroduction deep neural networks dnn components are increasingly being deployed in mission and safety critical systems e.g.
.
similar to traditional programmed software components these learned dnn components require significant testing to ensure that they are reliable and thus fit for deployment.
yet dnns differ from programmed software components in a variety of ways.
they generally do not have welldefined specifications and instead rely on a set of examples that represent intended component behavior.
these examples are used to train the parameters of a fixed implementation architecture resulting in implementation behavior encoded as values of the learned parameters.
the training process continues until the learned function is an accurate approximation of the intended behavior.
finally the accuracy of the learned function is intended to generalize to the set of valid inputs comprised of the data distribution of which the training examples are representative.the above characteristics of dnns present challenges for applying existing software testing methods to dnns.
for example the lack of specifications makes it most challenging to develop a rich test oracle as well as the fact that parameter values encode behavior which renders traditional structural code coverage ineffective.
the growing body of research on dnn testing has begun to address some of these characteristics.
while structural code coverage metrics are ineffective for dnns methods that cover combinations of computed dnn neuron values have been developed to assess and drive dnn testing .
also variations of metamorphic testing have been developed to check critical continuity properties across the learned function approximations helping to fill the oracle gap .
in this paper we focus on the challenges that dnn generalization presents to testing and in particular how current dnn testing techniques treat valid and invalid inputs.
to understand these challenges consider the implementation of a traditional software component c which is developed to meet a specification s rn!rm e where edenotes the error behavior intended for invalid inputs.
in this setting the input domain rnis partitioned into valid inputs v and invalid inputs v rn v which should yield e. the testing ofcselects a test set t rnand assesses whether8t2t c t s t .
as sketched in fig.
1a typicallycis comprised of input validation which determines if an input value lies in vand then executes either functional logic which realizes the behavior of sonv or error processing for invalid data.
developers have come to rely on the several intuitions about such software.
first input c i if valid i return logic i else return error i a coden i f .
.
.
.
.
.. .
.
return g b dnn fig.
structure of code and dnn components candn.
ieee acm 43rd international conference on software engineering icse .
ieee v .
v .
.
v .
v .
.
fig.
cumulative neuron coverage of lenet on the first valid and invalid inputs generated by dlfuzz top and deepxplore bottom coverage vectors left and ratios right for each set are shown along with the cumulative ratio in parentheses validation logic is distinct from functional logic demanding testing approaches that exploit its properties to effectively support it .
second test suites that achieve higher coverage are better in that they exercise more of the validation functional and error logic.
now consider a dnn n rn!rm which is trained to accurately approximate the possibly unavailable specification s. as sketched in fig.
1b nis comprised of layers of neurons that are cross coupled by connections labeled with learned parameters.
when the learned parameters for nare such thatpr n i s i ji2v for a desired error the network is expected to generalize to the valid input distribution v. even ifnwere trained to detect invalid data and respond appropriately its structure does not force a distinction between input validation functional logic or error processing.
in practice this distinction is uncommon and in this casendoes not even have an analog for ein its output domain.
because of the lack of this distinction whether an input lies in vorv the computation performed by noverlaps to a large degree e.g.
common sets of neurons are activated.
not distinguishing between valid and invalid input can be problematic for dnn testing in at least three ways.
testing techniques that generate invalid inputs increase cost with little value added for testing the functional logic of n. fig.
depicts valid test inputs and selected invalid test inputs from two recently proposed dnn test generation techniques .
as we show inxiv across a range of testing approaches for dnns on average of the generated tests are invalid and in the worst case all generated tests by a given technique are invalid.
when a test case fails developer time is required to triage the failure.
with high numbers of invalid test inputs developers may be forced to look through large numbers of test inputs similar to those depicted in fig.
to make judgements about test validity.
the high rate of invalid inputs runs the risk that developers will avoid the use of these techniques thereby negating their purported value.
whereas for traditional software the coverage produced by invalid inputs is confined to the validation and error logic for dnns an analogous separation of coverage is not guaranteed.
as depicted at the top of fig.
the cumulative coverage from valid and invalid test sets can be almost identical differing by as few as of neurons.
worse yet as depicted in the bottom of fig.
invalid tests can artificially boost coverage significantly beyond what is achieved by valid tests from .
to .
.
this increase in coverage suggests that unlike for traditional software dnn test suites that achieve higher coverage are not necessarily better !
fig.
valid tests vs invalid tests.
top row valid tests from mnist training dataset.
middle row invalid tests from deepconcolic.
bottom row invalid tests from deepxplore in this paper we study the effects of dnn test generation techniques not distinguishing between valid and invalid data and characterize the potential impact of the issues identified above.
our approach is to leverage a growing body of research from the machine learning ml community that learns models of the training distribution v from which the training data is drawn .
while there are many such models in this paper we employ the variational auto encoder v ae leaving the study of alternative models to future work.
leveraging v ae models allows us to study techniques representative of the current state of dnn testing research and to make two important observations.
first we demonstrate that existing dnn testing techniques such as deepxplore dlfuzz and deepconcolic produce large numbers of test cases with invalid inputs which increases test cost without a clear benefit.
second we demonstrate that existing dnn test coverage metrics e.g.
are unable to distinguish valid and invalid test cases which risks biasing test suites toward including more invalid inputs in pursuit of higher coverage.
building on these observations we present a novel approach that combines a v ae model with existing test generation techniques to produce test cases with only valid inputs.
more specifically we formulate the joint optimization of probability density of valid inputs and the objective of existing dnn test generation techniques and use gradient ascent to generate valid tests.
an experimental analysis on datasets used in the dnn testing literature shows the cost effectiveness 227of the proposed approach.
the primary contributions of this work lie in a the identification of limitations in existing dnn test generation and coverage criteria in their treatment of invalid input data b the development of a technique for incorporating an explicit model of the valid input space of a dnn into test generation to address those limitations and c experimental evaluation that demonstrates the extent of the limitations and the effectiveness of our technique in mitigating them.
the remainder of this article is organized as follows.
the following section xii describes the concepts that are used in this paper and related work.
our approach is detailed in xiii.
experimental strategy and results are described in xiv.
xv discusses the threats to validity of our study and xvi concludes.
ii.
b ackground and related research a. deep neural networks deep neural networks dnns are a class of machine learning models that can extract high level features from raw input.
similar to the human brain dnns contain a large number of inter connected elements called neurons.
dnns have multiple layers and each layer contains a number of neurons.
a typical dnn consists of an input layer one or more hidden layers followed by an output layer.
connections between neurons are called edges and their associated weights are referred to as the model parameters.
a neuron receives its input as a weighted sum over outputs of neurons from the previous layer.
the neuron then applies a non linear activation function on this input to generate its output.
overall a dnn is a mathematical function over the model parameters for transforming inputs into outputs.
the model learns its parameters by training on known input data called the training data.
the objective of dnn training is to learn the model parameters in order to make accurate predictions on unseen data during deployment.
b. dnn testing techniques dnn testing is an active research area with a number of testing techniques developed to address the challenges of testing these systems in terms of test coverage criteria test generation and test oracles.
after training dnn testing techniques use either natural inputs or adversarial inputs for testing.
adversarial inputs are test inputs that are generated by applying tiny perturbations on the original inputs which cause the model to make false predictions .
there is another line of research that focuses on generating adversarial examples for exposing vulnerabilities of dnn models without addressing test adequacy.
however our work differs by focusing on coverage guided dnn testing techniques from the software engineering literature.
coverage criteria in traditional software testing coverage criteria are used to measure how thoroughly software is tested.
most practical coverage criteria e.g.
use the structure of the software system to make this assessment e.g.
the percentage of statements or branch outcomes covered by a test suite.
similar to structural software coverage criteria coverage criteria for dnns have been proposed by various research efforts as follows.
pei et al.
proposed neuron coverage nc as a test coverage criteria.
for a given test suite neuron coverage is measured as the ratio of the number of unique neurons whose output exceeds a specified threshold value to the total number of neurons present in the dnn.
ma et al.
proposed a range of coverage criteria including k multisection neuron coverage kmnc neuron boundary coverage nbc and strong neuron activation coverage snac .
these coverage criteria can be used to determine whether a test case falls in the major functional region or corner case region of a dnn.
activation traces of all neurons are captured for the training data and lower and upper bounds of activations are measured for each of the neurons.
k multisection coverage is calculated by dividing the interval between lower and upper bounds into k bins and measuring the number of bins activated by the test inputs.
for a test suite k multisection coverage is the ratio of the uniquely covered bins to the total number of bins in the model.
neuron activations above the upper bound or below the lower bound are considered to be in corner case regions.
neuron boundary coverage is measured as a ratio of the number of covered upper and lower corner case regions to the total number of corner case regions of the model.
strong neuron activation coverage is the ratio of the number of covered upper corner case regions to the total number of upper corner case regions in the dnn.
top k neuron coverage and top k neuron patterns are based on top hyper activate neurons and their combinations.
modified condition decision coverage variants for dnns are proposed by sun et al .
these metrics are based on sign and value change of a neuron s activation to capture the causal changes in the test inputs.
ma et al.
proposed combinatorial test coverage to measure the combinations of neuron activations and deactivations covered by a test suite.
in our work we focus on the nc kmnc nbc and snac criteria and we show that these metrics cannot differentiate between valid and invalid test inputs generated by existing dnn test generation techniques.
we leave the analysis for other coverage metrics for future work.
dnn test generation research on dnn test generation is largely inspired by traditional software testing techniques such as metamorphic testing fuzz based testing and symbolic execution.
below we discuss the state of dnn test generation research.
deepxplore is a white box differential test generation technique that uses domain specific constraints on inputs.
this technique requires multiple dnn models trained on the same dataset as cross referencing oracles.
the objective of deepxplore is a joint optimization of neuron coverage and differences in the predictions of dnn models.
maximizing the objective generates tests that achieve high neuron coverage while simultaneously achieving erroneous predictions by the 228dnn model.
deepxplore uses gradient ascent to solve the joint optimization.
deeptest is another testing technique that generates test inputs by applying domain specific constraints on seed inputs.
the major focus of deeptest is to generate test inputs for testing autonomous vehicles.
it uses greedy search driven by neuron coverage criteria.
fuzzing is another traditional software testing technique that has been adapted for dnn test generation including dlfuzz and tensorfuzz .
dlfuzz is an adversarial input test generation technique.
it uses neuron coverage driven test generation similar to deepxplore.
however unlike deepxplore it does not require multiple dnn models.
it also uses a constraint to keep the newly generated test inputs close to the original inputs.
tensorfuzz is a coverage guided testing method for finding numerical issues in trained neural networks and disagreements between neural networks and their quantized versions.
deepconcolic uses the concolic testing approach for generating adversarial test inputs for dnn testing.
concolic execution is a coverage guided testing technique that combines symbolic execution and path information from concrete execution for generating tests satisfying a coverage criteria.
deepconcolic supports neuron coverage and mc dc variants for dnns.
none of these dnn testing techniques check whether the test inputs they are generating follow the training distribution.
they generate a significant number of invalid inputs that are outside the model s training distribution as shown in our evaluation section iv.
c. out of distribution input detection out of distribution input detection ood also referred to as outlier or anomaly detection is a well studied problem in ml field .
a recent survey describes the state of deep learning based outlier detection research and classifies deep learning based outlier detection techniques into supervised semi supervised unsupervised categories.
unsupervised models are preferred as labeling is expensive.
we use an unsupervised generative model based approach for our work.
a generative model learns the distribution of the data and can predict how likely a test input is with respect to training distribution.
this prediction can be used to identify invalid test inputs.
a dnn classifier learns the conditional distribution of target variables with respect to observable variables.
even though such a classifier has high accuracy on data sampled from the training distribution its accuracy on samples outside the training distribution cannot be guaranteed .
by training a generative model with the same data its density predictions can be used to reject inputs with low densities.
when a test input has low density it implies that the dnn classifier did not have enough samples around test input region in the training dataset.
examples of generative models are autoencoders variational autoencoders generative adversarial networks gan and autoregressive models such as pixelcnn fig.
technique for identifying invalid test inputs and pixelcnn .
we primarily use the variational autoencoder based out of distribution detection technique in our work.
also we repeat our experiments to identify invalid inputs generated by test generation techniques using a pixelcnn based validation approach.
the study is described in section iv b to show how sensitive invalid input identification is with respect to the out of distribution detection mechanism used.
d. variational autoencoder a variational autoencoder is a generative model that represents latent space as a probability distribution.
it has an encoder code layer and a decoder .
the encoder is responsible for mapping inputs to a lower dimensional latent space and the decoder generates new inputs by sampling from the latent space.
latent space is modeled by a code layer and it is generated from a prior distribution e.g.
a normal gaussian distribution.
the encoder s objective is to learn the posterior distribution and decoder s objective is to learn the likelihood of the original input reconstructed by the decoder.
a v ae model is trained by minimizing the difference between posterior and latent prior distributions and maximizing the likelihood estimation of the input.
a trained v ae model will generate high probability density estimates for data belonging to the training data distribution when compared to out ofdistribution inputs.
this key insight is used for validating test inputs generated by dnn test generation techniques in our research.
iii.
a pproach in this section we describe our approach to identifying limitations of existing dnn test generation techniques and generating valid test inputs for testing dnns.
a. analysis of existing dnn test generation techniques the methodology for analysing test inputs generated by existing test generation techniques is depicted in fig.
.
dnn s under test and the deep generative model are trained on the same dataset.
test inputs generated by existing dnn test generation techniques for the dnn s under test are passed as inputs to the deep generative model which estimates their densities.
these densities are used by the decision logic to classify inputs as valid or invalid.
for our experiments we use a v ae for expressing the deep generative model logic and in particular the model proposed by an and cho where the decoder of a v ae outputs distribution parameters for the samples generated by the encoder.
the probability of generating the original test input from a latent variable is calculated using these distribution parameters.
this probability is referred to as reconstruction 229probability.
valid inputs have higher reconstruction probability when compared to invalid inputs.
for a dataset under test which we call the valid dataset we identify another dataset which has a different distribution.
the inputs from this dataset are considered as invalid inputs.
invalid dataset selection is guided by two factors the dataset should have same input dimensions as the valid dataset and invalid and valid datasets should model disjoint data categories.
after identifying an invalid dataset we compute the reconstruction probability threshold for identifying invalid inputs.
reconstruction probabilities are calculated for inputs from both valid and invalid datasets.
we generate a range of thresholds from the combined reconstruction probability values of valid and invalid inputs.
we compute the f measure which is a measure of a test s accuracy for these threshold values.
the f measure is the harmonic mean of precision and recall.
a good f measure balances precision and recall and results in a fewer number of both false positives and false negatives.
in our case this means fewer valid inputs are falsely classified as invalid and fewer invalid inputs are falsely classified as valid.
the threshold value with the highest f measure is selected for our experiments.
when classifying test inputs generated by dnn test generation techniques test inputs with reconstruction probability less than the selected threshold are classified as invalid by the v ae classifier.
we measure the percentage of invalid inputs generated by multiple test generation techniques and the coverage of both valid and invalid tests.
the results of the experiments are used to answer the research questions related to the limitations of existing techniques presented in section iv.
b. our test generation technique we present a technique to generate valid test inputs in this section.
our workflow is described in fig.
.
our approach leverages existing gradient ascent based test generation technique s objective formulation.
the objective of existing test generation techniques is modeled to increase test coverage and produce inputs that cause the model to make incorrect predictions.
we augment this objective with probability density estimated by a generative model.
gradient ascent is used to solve the joint optimization.
maximizing the joint optimization will result in inputs that follow the distribution of the training data of the dnn under test along with satisfying objective of the baseline testing technique.
we provide a detailed description of our test generation algorithm using a v ae as the generative model in algorithm .
the decoder of the v ae outputs the distribution parameters x x for the samples generated by the encoder as per the ood detection algorithm proposed in .
the algorithm requires a dnn under test an objective function of a baseline gradient ascent based test generation technique obj1 a probabilistic encoder and decoder as inputs and produces both a test suite of valid inputs and their test coverage as output.
for every input of the seed set the probabilistic encoder generates parameters in latent space as shown in line of the fig.
technique for generating valid test inputs algorithm .
in lines a sample from the latent space is used by the decoder to calculate the reconstruction probability of the input.
the objective is modeled as a weighted sum of obj1and reconstruction probability in line .
lines show the gradient ascent.
the gradient is calculated for the objective and at this stage domain constraints if any are applied to the gradient and a new test input is generated.
in lines the generated test is tested for validity.
if this test input causes the model to mispredict and has a reconstruction probability higher than the threshold then on lines the coverage is updated and input is added to the generated test suite.
the procedure continues until all seeds are processed.
we evaluate this technique using deepxplore as a baseline test generation technique in section iv.
algorithm valid test input generation using v ae input x seed inputs dnn dnn under test obj1 objective function of test generation technique s step size for gradient ascent max iterations maximum iterations for gradient ascent f g trained probabilistic encoder and decoder hyperparameter for balancing two goals reconstruction probability threshold output set of test inputs coverage gen test fg forx in x do fori 1tomax iterations do z z f zjx draw sample from z n z z x x g xjz obj2 p xj x x obj obj1 obj2 gradient obj x gradient constraints gradient x x s gradient p reconstruction probability x f g ifcounter example dnn x and p then gen test.add x update coverage break end if end for end for 230dataset name architecture accuracy source l n p mnistmni mni mni mni 4lenet lenet lenet custom .
.
.
.
svhnsvh svh svh svh 4all cnn a all cnn b all cnn c vgg19 .2m .3m .3m 38m96 .
.
.
table i models used in our studies with number of layers l neurons n parameters p and test accuracy m denotes millions of parameters.
iv.
e valuation the design and evaluation of experiments for studying existing techniques and demonstrating effectiveness of our approach are described in this section.
we answer the following research questions rq1 do existing test generation techniques produce invalid inputs?
rq2 existing test generation techniques are guided by test coverage criteria.
how do invalid inputs affect test coverage metrics?
rq3 v ae based input validation can be incorporated into test generation techniques.
how effective is this technique in generating valid inputs and what is the overhead?
rq4 is the determination of invalid inputs sensitive to the generative model used?
a. evaluation setup all experiments are conducted on servers with one intel r xeon r cpu e5 v4 .10ghz processor with cores 62gb of memory and nvidia titan xp gpus.
the software that supports our evaluation as well as all of the data described below is available at distributionawarednntesting.
test generation frameworks we study three state of the art test generation techniques deepxplore dlfuzz and deepconcolic to demonstrate the limitations of existing techniques in terms of generating valid test inputs and satisfying test coverage criteria.
the choice of these frameworks is guided by the categorization of test input generation techniques presented in a recent survey and the availability of open source code.
the survey categorizes test generation frameworks into three algorithmic families we choose one technique from each family.
deepxplore is selected from domain specific test input synthesis dlfuzz from fuzz and search based test input generation and deepconcolic from symbolic execution based test input generation categories.
test coverage criteria deepxplore and dlfuzz use neuron coverage as the test adequacy criteria whereas deepconcolic can be used with neuron coverage neuron boundary coverage and mc dc coverage criteria for dnns .
we use neuron coverage as the test adequacy criteria for generating tests using all three frameworks.
resulting test inputs from test generation are analyzed usingneuron coverage and extended neuron coverage metrics i.e kmultisection neuron coverage neuron boundary coverage and strong neuron activation coverage.
we leave the remaining coverage criteria discussed in these works for future study.
datasets and dnn models we use two popular datasets mnist and svhn for the experiments.
generative models can assign higher densities to datasets whose distributions are different from their training datasets in some cases .
for example a v ae trained on cifar10 can assign higher densities to inputs from svhn dataset.
when such a model is used for invalid input identification it might result in high densities being assigned to invalid inputs which will result in false negatives.
also selecting the threshold density for deciding invalid inputs becomes challenging in such scenarios.
this problem is actively being addressed by ml research community .
generative models trained on mnist and svhn do not have this issue so we selected these two datasets for our research.
mnist is a collection of grayscale images of handwritten digits with training images and test images.
all three frameworks that we are studying support test generation for mnist dataset.
similar to deepxplore we use lenet lenet and lenet networks from lenet family and a custom architecture used in the deepconcolic work for mnist classification.
all the four models are convolutional networks with max pooling layers and the number of layers ranging from to .
svhn contains color images of digits in natural scenes and the dataset has training images and test images.
we implemented svhn support for all three frameworks.
we trained svhn classification models with the allcnn a all cnn b and all cnn c network architectures proposed in and vgg19 for our experiments.
these models are convolutional networks with dropout and either global average pooling or max pooling layers and the number of layers range from to .
the models are summarized in table i where we report measures of their architecture and test accuracy.
vae models for mnist we trained the v ae that outputs distribution parameters using the model architecture described in .
the fashionmnist dataset is similar to mnist and contains 28x28 grey scale images.
however the distribution is different from that of mnist as fashionmnist contains clothing images.
we use the fashionmnist as the invalid input space for calculating the reconstruction probability threshold.
since the v ae is not trained on fashionmnist distribution and fashionmnist clothing inputs are semantically unrelated to mnist digit inputs the v ae should output lower reconstruction probabilities for test inputs from the fashionmnist dataset.
we experimented with different variations of the generator architecture used in for selecting a v ae network for the svhn dataset.
for each of the variants the encoder is created by transposing the generator network as suggested in .
the network that achieved the highest f measure for identifying 231dataset mnist svhn valid mnist test svhn test invalid fashionmnist test cifar10 test f measure .
.
false positives .
.
false negatives .
.
table ii f measure and percentage of false positives and false negatives for v ae based input validation model dnn testing technique valid invalid total mni 1deepxplore .
.
.
dlfuzz deepconcolic .
.
mni 2deepxplore .
dlfuzz .
.
.
deepconcolic .
.
mni 3deepxplore .
.
.
dlfuzz .
deepconcolic .
.
mni 4deepxplore .
.
.
dlfuzz .
.
.
deepconcolic .
.
table iii neuron coverage of test inputs generated by deepxplore dlfuzz and deepconcolic for mnist classifiers invalid inputs is selected for our experiments.
cifar10 is used as the invalid input dataset for calculating reconstruction probability threshold of v ae trained on svhn.
f measure values and percentage of false positives for mnist and svhn test datasets are given in table ii.
b. results and research questions in this section we present results of our experiments we used to answer the research questions.
rq1.
do existing test generation techniques produce invalid inputs?
we generated test inputs for mnist and svhn classifiers using the deepxplore dlfuzz and deepconcolic techniques.
the deepxplore framework supports three types of input transformations lightening occlusion and blackout.
we generated tests for all three transformations to answer rq1.
we randomly sampled seed inputs from each mnist and svhn test dataset for deepxplore and dlfuzz.
deepxplore and dlfuzz use gradient ascent for test generation and we used the hyperparameters reported in their respective works for our study.
similarly we selected the neuron coverage threshold of .
as it is commonly used in deepxplore and dlfuzz experiments in their original work.
the deepconcolic tool uses a single seed input for test generation for neuron coverage and a timeout of hours is used for test generation in the primary work .
we used the same strategy and the framework is run with the global optimisation approach.
generated tests are classified as valid or invalid by using the reconstruction probability metric of v ae.
the top row of fig.
shows the percentage of invalid test inputs generated by these frameworks for mnist and svhn dnn models.
the percentage of tests generated by deepxplore varies depending on the constraint used.
for all the four mnist classifiers occlusion constraint produced a high percentage ofdnn testing technique valid invalid total svh 1deepxplore .
.
.
dlfuzz .
.
.
deepconcolic .
.
svh 2deepxplore .
.
.
dlfuzz .
.
.
deepconcolic .
.
svh 3deepxplore .
.
.
dlfuzz .
.
.
deepconcolic .
.
svh 4deepxplore .
.
dlfuzz .
.
.
deepconcolic .
.
table iv neuron coverage of test inputs generated by deepxplore dlfuzz and deepconcolic for svhn classifiers invalid test inputs i.e.
greater than while blackout constraint generated less than invalid inputs.
the lightening constraint generated and invalid inputs for models mni and mni and less than for other two.
dlfuzz generated invalid inputs in the range to for mni mni and mni classifiers while less than for mni .
for svhn classifiers the occlusion and blackout constraints generated a higher number of invalid tests when compared to lightening constraints on an average.
dlfuzz generated invalid inputs are in the range to for svhn classifiers.
all the test inputs generated by the deepconcolic framework for both mnist and svhn classifiers are classified as invalid by the v ae model.
result for rq1 all three testing techniques studied produced significant numbers of invalid tests on average and ranging from in the worst case.
rq2.
existing test generation techniques are guided by test coverage criteria.
how do invalid inputs effect test coverage metrics?
we measured neuron coverage nc multi granularity coverage criteria i.e.
k multisection neuron coverage kmnc neuron boundary coverage nbc and strong neuron activation coverage snac of both valid and invalid tests generated by the three frameworks.
the k value of is used for measuring kmnc coverage.
we also measured the cumulative neuron coverage of valid and invalid test inputs.
results are presented in tables iii and iv for neuron coverage metric and tables v and vi have data for multi granularity coverage criteria.
across dnns test generation techniques and coverage criteria of the time invalid tests achieved coverage greater than or equal to that achieved by valid tests.
the entries in tables iii iv v and vi corresponding to this insight are highlighted in bold.
of the time invalid tests outperform valid for coverage and of the time invalid coverage boosts overall coverage by more then .
result for rq2 invalid inputs yield high coverage for a variety of coverage criterion when compared to valid inputs and they frequently increase coverage beyond that which would be achieved with valid inputs alone.
232fig.
percentage of invalid test inputs identified by v ae top pair and pixelcnn bottom pair input validation techniques.
rq3.
v ae based input validation can be incorporated into the test generation techniques.
how effective is this technique in generating valid inputs and what is the overhead?
to answer this question we generated test inputs by using v ae based input validation along with a gradient ascent based test generation technique as described in algorithm .
we selected deepxplore as the baseline test generation technique and density estimated by v ae is incorporated as a goal into its objective to formulate a joint optimization.
result of a joint optimization is sensitive to the weights of different goals used in the objective function.
to address this we fixed the weights of the goals of the baseline s objective and performed a sweep over a range of density weights to find the best configuration.
we used gradient ascent to generate test inputs for mnist and svhn models.
we randomly identified seed inputs from each of the two datasets and used the same seed set and gradient ascent parameters i.e.
step size and maximum iterations for baseline and our technique.
the experiments are repeated three times and average results are presented in this section.
we measured the number of valid tests generated along with their neuron coverage for our technique and the baseline to demonstrate the effectiveness of our technique.
the validity of the inputs is measured with respect to the ood detectionalgorithm used i.e.
the v ae in this case.
our technique generates only valid test inputs.
since baseline generates both valid and invalid test inputs we added the input validation module to the baseline to capture only the valid test inputs.
neuron coverage achieved by the baseline technique and our technique are presented in figures and for mnist and svhn classifiers respectively.
the plots show the coverage over a range of seed inputs.
our technique achieved neuron coverage greater than or equivalent to that of deepxplore baseline for all the dnn models.
for the scenarios where baseline is able achieve neuron coverage comparable to ours our technique outperformed the baseline in terms of the number of valid inputs generated.
fig.
contains a comparison of the number of valid inputs generated by the baseline and our technique for mnist and svhn classifiers.
the total valid inputs generated by our technique for the mnist models are .
times the valid inputs generated by the baseline.
for svhn dataset our technique generated .
times more valid inputs when compared to the baseline.
hence having v ae in the test objective guides gradient ascent effectively in searching for valid inputs.
table vii shows the performance data of deepxplore v ae and deepxplore algorithms for seed inputs.
every iteration of these algorithms has two components gradient ascent and input validation.
for each seed input gradient 233dnntesting techniquecoveragevalid invalid total mni 1deepxplorekmnc .
.
nbc .
.
snac .
.
dlfuzzkmnc .
.
.
nbc snac deepconcolickmnc .
.
nbc snac mni 2deepxplorekmnc .
.
nbc .
.
snac .
.
dlfuzzkmnc .
.
.
nbc snac deepconcolickmnc .
.
nbc .
.
snac .
.
mni 3deepxplorekmnc .
.
.
nbc .
.
.
snac .
.
.
dlfuzzkmnc .
.
.
nbc .
.
snac deepconcolickmnc .
.
nbc .
.
snac .
.
mni 4deepxplorekmnc .
.
.
nbc .
.
snac .
.
dlfuzzkmnc .
.
.
nbc .
.
snac .
.
deepconcolickmnc .
.
nbc .
.
snac .
.
table v multi granularity neuron coverage of test inputs generated by deepxplore dlfuzz and deepconcolic for mnist classifiers ascent is performed until it finds a valid test input or for a maximum of iterations whichever happens first.
input validation is performed only when the differential oracle fails the generated test input in that iteration.
in all the cases deepxplore v ae ran for fewer iterations and input validations when compared to the baseline.
for the scenarios where the difference between deepxplore v ae and baseline s number of iterations and input validations is high deepxplore v ae is faster because the baseline is spending more time on generating invalid inputs which are then rejected by the input validation module.
when this difference is small baseline has better overall run time but deepxplore v ae generates more valid inputs and has lower cost per valid input when compared to the baseline.
we note that due to deepxplore v ae s improved effectiveness in generating valid tests it improves on the baseline s time to produce a valid test reducing it from .
to .
minutes on average measured across three runs.
result for rq3 incorporating a v ae into test generation eliminates the generation of invalid test inputs significantly increases the generation of valid inputs reduces the time to generate valid tests and increases coverage achieved on generated valid tests.dnntesting techniquecoveragevalid invalid total svh 1deepxplorekmnc .
.
.
nbc .
.
.
snac .
.
.
dlfuzzkmnc .
.
nbc .
.
.
snac .
.
.
deepconcolickmnc nbc snac svh 2deepxplorekmnc .
.
.
nbc .
.
.
snac .
.
.
dlfuzzkmnc .
.
nbc .
.
.
snac .
deepconcolickmnc .
.
nbc .
.
snac .
.
svh 3deepxplorekmnc .
.
.
nbc .
.
.
snac .
.
.
dlfuzzkmnc .
.
nbc .
.
.
snac .
.
.
deepconcolickmnc .
.
nbc .
.
snac .
.
svh 4deepxplorekmnc .
.
.
nbc .
snac .
.
.
dlfuzzkmnc .
.
.
nbc .
.
snac .
.
.
deepconcolickmnc .
.
nbc .
.
snac .
.
table vi multi granularity neuron coverage of test inputs generated by deepxplore dlfuzz and deepconcolic for svhn classifiers rq4.
is the determination of invalid inputs sensitive to the generative model used?
to answer rq4 we use a pixelcnn based input validation technique.
pixelcnn is an autoregressive deep generative model .
the advantage of using this model for out of distribution detection is that the model outputs the probability density explicitly.
we trained pixelcnn models for mnist and svhn datasets.
for each dataset we find the threshold for identifying invalid inputs by using an invalid dataset and f measure analysis similar to v ae based detection technique described in section iii a. the f measure precision and recall of the selected thresholds for both the datasets are presented in table viii.
the percentage of test inputs generated by deepxplore dlfuzz and deepconcolic for the mnist and svhn classification models that are classified as invalid by pixelcnn based input classifier are presented on the bottom row of fig.
.
pixelcnn for the mnist models classified a high percentage of test inputs generated by deepxplore s light and occlusion constraints as invalid and classified all test inputs as valid for blackout constraint.
for the svhn classifiers occlusion and blackout constraints result in higher number of 234fig.
neuron coverage of valid inputs generated by deepxplore and deepxplore extended with v ae for mnist models fig.
neuron coverage of valid inputs generated by deepxplore and deepxplore extended with v ae for svhn models dnndeepxplore v ae deepxplore iterations deepxplore v ae deepxplore input validations deepxplore v ae deepxplore run time in minsvalid inputsiterationsinput validationsrun time in minsvalid inputsiterationsinput validations mni .
.
mni .
mni .
.
mni .
.
svh .
.
svh .
.
svh .
.
svh .
.
table vii run time analysis of test generation algorithms of deepxplore v ae and deepxplore for mnist and svhn classifiers fig.
number of valid inputs generated by deepxplore and deepxplore extended with v ae for mnist and svhn models invalid inputs when compared to the light constraint.
the pixelcnn classified all test inputs generated by dlfuzz as invalid for mnist models and more than test inputs as invalid for svhn models.
all inputs generated by deepconcolic are identified as invalid for both the models.
the results follow the same trend as observed by v ae based classifier.
however the percentage of test inputs classified as invalid by pixelcnn is less when compared to that of v aedataset mnist svhn valid mnist test svhn test invalid fashionmnist test cifar10 test f measure .
.
false positives .
false negatives .
.
table viii f measure and percentage of false positives and false negatives for pixelcnn based input validation model for deepxplore generated tests.
for dlfuzz the pixelcnn approach resulted in more invalid tests when compared to the v ae based classifier.
both the v ae and pixelcnn based techniques classified all test inputs generated by deepconcolic as invalid.
result for rq4 test generators are judged to produce invalid tests with different ood techniques but the number of invalid tests is sensitive to the deep generative model architecture used.
v. t hreats to validity we designed our study to provide a degree of generalizability by spanning all of the algorithmic families of dnn 235test generation approaches that have been developed to date as well as datasets models coverage criteria and approaches to out of distribution detection.
moreover the datasets and models that we have chosen are those that have been used in prior research which was both a convenience choice and a means of promoting comparison among methods e.g.
against baselines.
despite these measures our findings may be dependent on these choices.
further study especially with additional ood techniques beyond v ae and pixelcnn is warranted to understand the generalizability of our findings as relates to the rate at which invalid inputs are generated and the degree of coverage achieved by those inputs.
our study on adapting test generation with ood is more limited using a single model a v ae and a single test generation approach deepxplore which is a representative of the class of optimization based test generation approaches.
it is not a simple matter to extend this study to other families of test generation methods but that will be necessary to understand the extent to which the benefit of integrating ood methods with dnn test generation techniques broadly generalizes.
we ran all of our experiments multiple times and crosschecked them with prior work e.g.
that we achieved the same level of coverage for baseline techniques as was reported in prior work.
we took these measures to assure the quality of the data reported here and we made the code available in github for transparency and replicability.
vi.
c onclusions this paper demonstrates that existing dnn test generation and test coverage techniques do not consider the valid input space which can have several deleterious effects.
it can lead dnn test methods to generate large numbers of invalid inputs those that lie off the training distribution as judged by state of the art techniques thereby reducing the efficiency of the test generation process and even worse producing large numbers of tests that might be rejected as invalid during faulttriage processes.
it can lead test coverage techniques to value invalid tests inappropriately by achieving or improving on coverage from valid tests this has the potential to bias test generation results.
ndefensive i if !ood i returnn i else return error i fig.
defensive dnnwe demonstrate that existing out of distribution detection techniques can be coupled with test generation algorithms to address this problem.
in this work we focused on v ae based ood detection and incorporating such models into optimization based test generation.
our study shows this to be effective in significantly boosting the number of valid test inputs generated and in eliminating invalid tests.
while promising more work is needed to explore the potential for other ood models to inform test generation and to incorporate such models into constraintbased and fuzzing test generators.
finally we plan to explore how the well understood concept of defensive programming for traditional programs as sketched in fig.
1a can be adapted to dnns.
fig.
sketches a possibility suggested by the findings in this paper where the role of input validation is played by an ood detector.
in such an architecture testing of nshould be restricted to inputs that are not out of distribution but testing of the ood itself must be conducted over a broader input space as is the case with prior work on input validation testing .
with such an architecture dnn test suites that achieve higher coverage of ood and narebetter thereby reestablishing the long held intuitions about test coverage for traditional software.