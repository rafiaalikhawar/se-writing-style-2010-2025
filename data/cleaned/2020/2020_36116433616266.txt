distxplore distribution guided testing for evaluating and enhancing deep learning systems longtianwang xi anjiaotonguniversity chinaxiaofeixie singaporemanagementuniversity singaporexiaoning du monashuniversity australia meng tian singaporemanagementuniversity singaporeqing guo ihpc andcfar agency for science technology andresearch singaporezheng yang tte lab huawei china chaoshen xi anjiaotonguniversity china abstract deep learning dl models are trained on sampled data where the distribution of training data differs from that of real world data i.e.
the distribution shift which reduces the model s robustness.
various testing techniques have been proposed including distribution unaware and distribution aware methods.
however distribution unawaretestinglackseffectivenessbynotexplicitly considering the distribution of test cases and may generate redundanterrors withinsame distribution .distribution awaretesting techniques primarily focus on generating test cases that follow the trainingdistribution missingout of distributiondatathatmayalso be validandshouldbe consideredin the testingprocess.
inthispaper weproposeanoveldistribution guidedapproach for generating validtest cases with diversedistributions which can better evaluate the model s robustness i.e.
generating hard todetecterrors andenhancethemodel srobustness i.e.
enriching trainingdata .unlike existingtestingtechniquesthat optimize individualtestcases distxplore optimizestestsuitesthatrepresent speci f ic distributions.
to evaluate and enhance the model s robustness we design two metrics distribution difference which maximizesthesimilarityindistributionbetweentwodifferentclasses of data to generate hard to detect errors and distribution diversity which increase the distribution diversity ofgeneratedtestcases for enhancing the model s robustness.
to evaluate the effectiveness ofdistxplore inmodelevaluationandenhancement wecompare distxplore with14state of the artbaselineson10modelsacross datasets.
the evaluation results show that distxplore not only detects a larger number of errors e.g.
on average but also identi f ies more hard to detect errors e.g.
.
on average furthermore distxplore achievesahigherimprovementinempirical corresponding authors esec fse december sanfrancisco ca usa copyright held bytheowner author s .
acm isbn979 .
e.g.
.
moreaccuracyimprovementthanthebaselines on average .
ccsconcepts softwareanditsengineering softwaretestinganddebugging computing methodologies neural networks .
keywords deep learning software testing distribution diversity model enhancement acm referenceformat longtian wang xiaofei xie xiaoning du meng tian qing guo zheng yang and chao shen.
.
distxplore distribution guided testing for evaluating and enhancing deep learning systems.
in proceedings of the 31stacmjoint europeansoftwareengineeringconferenceand symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.https introduction deep learning dl has achieved great success in many applications such as autonomous driving healthcare face recognition and speech recognition .
it is widely known that dl models suffer from the issue of poor robustness making them vulnerabletoadversarialattacks.therefore itiscrucialtosystematicallytestdlsystemsbeforedeployment especiallyinsafety critical scenarios.
machine learning ml involves the process of learning a model fromsampleddata i.e.
trainingdata tomakedecisionsonaspeci f ic task.
the general steps of ml tasks include data collection model training model evaluation and model deployment.due to the huge input space it is impossible to collect all data for training thus high quality data that follows a certain distribution is collected for training.
as shown in fig.
for a speci f ic task e.g.
digit classi f ication there is a vast amount of task relevant data for digits i.e.
thevaliddata shown in the dashed rectangle in the whole input space i.e.
alldatashowninthe solid rectangle .
the task irrelevant data e.g.
noisy data and non digit data is referred to asinvaliddata e.g.
the dataset fin fig.
with respect to the thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa longtianwang xiaofeixie xiaoningdu meng tian qingguo zheng yang andchao shen ... ... b in distribution data out of distribution data valid invalid data all input valid input in distribution input a c d e f figure1 data sampling and an illustrativeexampleof dl system giventask.asmallsubsetofthevaliddata e.g.
thedataset aandb in fig.
is collected for training the model.
however the training distribution is often different from the distribution of valid data due to the distribution shift which greatly affects the model s robustness.
a fundamental assumption is that the model is intended to handle the in distribution data id that follows the distribution of training data but it is hard to correctly predict data e.g.
the dataset c d andein fig.
that does not follow the training distribution i.e.
out of distributiondata ood which highlights the needfor testingbefore deployment.
dltestingaimstogeneratetestcasesthat evaluatetherobustness of dl systems i.e.
discover the data that is valid but cannot be predicted correctly e.g.
the dataset dandein fig.
andenhance the robustness i.e.
retraining model by including test cases data withdiversedistribution dataset c d andeinfig.
.manystudies havebeenconductedfortestingdlsystems wherevalidityanddistribution aretwoimportantpropertiesoftest cases.acommonapproachtoguarantee validityistoconstrainthe degree of the mutation e.g.
the distance between the new test and theoriginalseedisconstrainedwithina u1d43f u1d45dball .however existing methods e.g.
deeptest deephunter andtensorfuzz often ignore the distribution which limits their effectiveness inevaluationandenhancement e.g.
redundanterrorswithinthe similar distribution are generated .
recently some studies haveattemptedtoaddressthisbyincorporatingdistributionaware testing which characterizes the training distribution via variationalauto encoder vae orgenerativeadversarialnetwork gan .
however these methods only generate id data while ood data is considered as invalid .
we argue thatthe ood datais just datathatdoesnotfollowthedistributionofthecollectedtraining data but could still be valid and should be handled properly in realworlddeploymentenvironment.forexample asshowninfig.
for each dataset the input on the right side in a row is mutated from its left side sample the inputs on the right are considered as invalid databyexistingdistribution awaretesting .these data could still be visually valid even though they are identi f ied as invalid databyvae .forexample althoughthedistributionof theimagesin out of distributiondata valid infig.1isdifferent from the distribution of the training data e.g.
the digits written inverydifferentways theycouldstillbethepotentialinputsto the deployed dl systems.
therefore it is crucial to test both indistribution id andout of distribution ood datathatarevalid before deployingthe dl system.
the quality of test cases depends on the testing goals i.e.
what kindofdataismoreusefulinrobustnessevaluationandenhancement in this paper.
for evaluating model s robustness although ood data is likely to trigger incorrect decisions of the model theysvhn mnist fashion mnist figure2 examplesofooddatathatareconsideredas invalidby .
left original inputs right generatedinputs couldalsobeeasilydetectedbyooddetectionmethods.forexample state of the arttestingtechniquescan easilygeneratealarge number of errors e.g.
thousands of errors in but most of them tend tobe weak errors thatcan be detected or f ilteredbyexisting defense techniques e.g.
adversarial example detection .
it is similar to traditional software testing where defenses such as parsersandexceptionhandlingcan f ilteroutweakerrors.thus for dltesting itisimportantandchallengingtodiscoverstrongerrors that can evade the state of the art defenses.
for model enhancement thegeneralgoalistoreducethedistributionshiftbetween the training data and real world data.
hence how to generate tests withdiversedistributions e.g.
covering u1d450 u1d451 u1d452 isanotherchallenge.
these diverse tests can be added to the training data for improving the modelgeneralizabilityandrobustness.
tothisend inthispaper weproposeanoveldistribution guided testing framework named distxplore for better evaluating and enhancing dlsystems i.e.
togenerate hard to detect anddiverse errors.distxplore adoptsthesearch basedapproachtoadaptively generatetestcaseswiththeguidanceofdistribution.unlikeexisting techniques that optimize test cases individuality the optimization ofdistxplore is performed on a test suite that represents a speci f ic distribution.speci f ically weleveragemaximummeandiscrepancy mmd tomeasuretheclosenessbetweentwodistributions.for model evaluation distxplore maximizes the distribution closeness betweenthedataintwodifferentclassesforgeneratingstatistically indistinguishable errors which are difficult to defend.
to enhance the model s robustness we proposea metric to measure the distribution diversity of the test cases guiding distxplore to generate testsuiteswithvariousdistributions.thetestcaseswithdiverse distributions are more likely to cover a wider range of unseen data andimprove the model srobustness.
we conduct a comprehensive evaluation to demonstrate the usefulness and the effectiveness of distxplore in evaluating and enhancingthemodel srobustness.speci f ically weselect10modelson4datasets andcompare distxplore with14state of the art tools covering different types of techniques i.e.
adversarial attacks distribution unaware testing distribution aware testing and robustness oriented testing .
the results demonstrate that the statistically indistinguishable errors generated by distxplore are harder to detect by two state of the art defense techniques e.g.
the attack as defense can only detect errors generated bydistxplore but almost errors from adversarial attacks and distribution aware testing.
distxplore is more efficient in detecting errors e.g.
on average it detects errors compared to the best baseline.
the test cases generated by distxplore are more usefulinimprovingthemodel srobustness e.g.
.
moreaccuracy improvement thanthe baselinesonaverage.
tosummarize this paper makes the following contributions we f irst discuss the limitation of existing distribution aware and distribution unaware testing techniques in terms of validity and 69distxplore distribution guidedtestingforevaluating andenhancingdeep learning systems esec fse december3 san francisco ca usa invalid dataid datatest suite test suite test suite test suite 4training data in class 0distribution guided generationtraining data in other classes .... all input valid input id input generated test suite...... ood data valid .... .. .. .. seed suite figure3 illustrationof testsuitegeneration distribution.thenweproposeanoveldistribution guidedtesting technique for generatinghard to detect errors and diverse data covering a wider range of unseen data.
to the best of our knowledge thisisthe f irstdistribution guidedtestingforgenerating test suites withdiversedistributions.
technically we design two distribution based metrics i.e.
distribution difference and distribution diversity to guide the testing forgeneratingstatisticallyindistinguishableerrorsandtestcases withdiversedistributions respectively.
wedemonstratetheusefulnessof distxplore indiscoveringstrong errorsandenhancingmodel srobustnessbycomparingitwith state of the artmethods.
preliminaryand overview .
preliminary .
.
deepneuralnetwork.
adeepneuralnetwork dnn can berepresentedasafunction u1d453 u1d44b u1d44cthatmapsan u1d45b dimensional input u1d465 u1d44btoan u1d45a dimensionaloutput u1d466.alt u1d44c.adnnusuallyis thecompositionoflayersdenotedas u1d453 u1d4590 u1d4591 ... u1d459 u1d458.weuse u1d453 u1d456 u1d465 to representthe outputofthe u1d456 u1d461 uni210elayer where u1d4530 u1d465 u1d465and u1d453 u1d458 u1d465 u1d466.alt.forexample theoutput u1d466.altinclassi f icationisaprobability vector for u1d45apossible classes e.g.
classesincifar .
.
.
data validity.
let u1d44bbe the whole input space with u1d45bdimensions i.e.
r u1d45b .
we use u1d44dtodenote allpossibleinputsthat are relevant to the given task e.g.
all images of digits .
u1d44dis consideredasvaliddatawithrespecttothetaskastheycouldbethe potential inputs when the trained model is deployed in real world.
theinputs u1d44b u1d44d u1d465 u1d465 u1d44b u1d465 u1d44d areinvaliddata e.g.
thedata ofothertasksandlow qualitydata.itisdifficulttopreciselyde f ine the validity of the data.
in practice the u1d43f u1d45dnorm is usually usedtoguarantee thevalidityofthe generateddataby theexisting dl testing and adversarial attack techniques.
speci f ically given a valid input u1d465 the new test case u1d465 generated by adding some perturbationson u1d465isconsideredasvalidif u1d465 u1d465 u1d45d u1d451 where u1d451 isasafe radius.
.
.
datadistribution.
sincevalidinputs u1d44dcanbein f inite itis notpossibletocollectallofthemfortraining.inpractice adnn u1d453isusuallytrainedfromcollecteddata u1d447 i.e.
trainingdata that follows a distribution d u1d447 called in distribution id data.
some generativemodelssuchas variational autoencoders vae and generativeadversarialnetworks gan areusedtoapproximate the id data distribution .
there is often a distribution shift between d u1d44dandd u1d447 i.e.
the trainingdatacannotrepresentthereal worlddata makingthatthemodelunderperformsontheout of distribution ood data.hence testcaseswith diversedistributionsaremorelikelytorevealthe weaknessesofthemodel.ontheotherhand theoodtestcasescan enrich the trainingdata such that the distribution of newtraining dataset iscloser to the distributionof training data.
note that the validity and the out of distribution of the data are different in this paper.
the valid data is anypotential inputs of the modelwithrespecttothetask andisusuallyofhighquality.the out of distributiondatareferstothedatathatdoesnotfollowthe distribution of speci f ic training data.
the valid data can be id or ood depending on the training data collected.
the ood data can also be valid or invalid depending on the relevance and quality of the data.
to measure the validity we adopt the widely used measurement i.e.
u1d43f u1d45dnorm.
to measure the distribution difference we adopt the metric maximum meandiscrepancy de f inedbelow.
.
.
maximum mean discrepancy.
maximum mean discrepancy mmd isacommonteststatistictomeasuretheclosenessbetween two sets of samples drawn from two distributions.
assume we have two sets of samples u1d44b u1d4651 ... u1d465 u1d45a and u1d44c u1d466.alt1 ... u1d466.alt u1d45b drawn from two distributions d u1d44bandd u1d44c mmd calculates the distancebetweenthetwosetsofsamplesinauniversalreproducing kernelhilbertspace rkhs .theempiricalestimationofmmd between the two distributions in rkhs denoted as u1d440 u1d440 u1d437 u1d44b u1d44c can be calculatedas u1d45a2 u1d45a summationdisplay.
u1d456 u1d457 u1d458 u1d465 u1d456 u1d465 u1d457 u1d45a u1d45b u1d45a u1d45b summationdisplay.
u1d456 u1d457 u1d458 u1d465 u1d456 u1d466.alt u1d457 u1d45b2 u1d45b summationdisplay.
u1d456 u1d457 u1d458 u1d466.alt u1d456 u1d466.alt u1d457 wherekisameasurableandboundedkernelofarkhs mmdis zero if and only if d u1d44b d u1d44c.
as mentioned in that gaussianandlaplacekernelsareuniversal weusegasussiankernelto calculatemmd.more details aboutmmd can refer to .
.
overviewof distxplore fig.3shows the main idea of our approach.
we mainly consider classi f icationtaskinthispaper.speci f ically distxplore considers thedatadistributionineachclassseparately i.e.
togeneratetest cases with diverse distributions for each class.
to measure the distribution diversity of the test cases we calculate the distribution difference i.e.
mmd betweenthetestsuitefromaclassandthe training data in each of other classes and then measure the diversityofthesedistributiondifferences.weconsiderdistribution differencesbetweentestcasesandthedataindifferentclasses since eachinputmaybeclassi f iedintoanyclassbyamodel representing the different decision behaviors of the model.
therefore we aim to generate diverse test cases by considering the diversity of distribution differences between the generated test cases and training data ofdifferentclasses.
as shown infig.
given the initial test suitesampled from the trainingdataofaclass whichrepresentsthetrainingdistributionof the class the goal is to generate new test suites that have different distributiondistanceswiththetrainingdatainotherclasses e.g.
class1 .thedistributioncurveofthetestsuites i.e.
redcurve shiftsfromtheoriginaldistribution i.e.
bluecurve tothetarget distribution i.e.
green or orange curve thus distxplore generates test suites that are more likely to be predicted incorrectly.
for robustnessevaluation thegoalistogenerateerrorsthatarehardto 70esec fse december3 san francisco ca usa longtianwang xiaofeixie xiaoningdu meng tian qingguo zheng yang andchao shen real world t rain pgd deephunter distxplore vae figure4 diversity of data distribution on mnist detect.
the more similar the distribution of the test suite e.g.
class is tothe distribution of the training data in the target class e.g.
class the harder it is to detect the errors because the errors are statistically indistinguishable from the target class.
for robustness enhancement distxplore isusedtogeneratetestsuiteswithdiverse distributions insteadofonlyhard to defenderrors thatcanenrich thetrainingdatabyaddingunseendata thusimprovingthemodel s robustness.
distribution guided testing .
testinggoals in this paper we mainly focus on two objectives model evaluation andenhancement.wedesigntheobjectivefunctionsthatguidethe test casegeneration i.e.
optimize test suites.
.
.
modelevaluation.
toevaluatethemodel srobustness we aimtogeneratetheerroneousinputsthatarehardtobedetected by existing defense techniques.
speci f ically just as any dataset can follow aspeci f icdistribution thedatawithinindividualclassesin classi f icationtasksalsopossesstheirowndistribution.duetothe differences between these different classes their data distributions arealsoverydifferent e.g.
dogsandbirds .awell trainedmodelis capable of accurately distinguishing the differences between these classes.infig.
theblueareasrepresentthedifferentdistributions ofdatafordifferentclassesinthemnistdataset.
conversely ifthe data distributions between two classes are very similar the model maystruggle tomakeaccuratepredictions.thus distxplore aims to generate test cases in a class that are statistically similar to the training data inotherclasses.
formally given a dnn u1d453and a test suite u1d446 u1d450belonging to a sourceclass u1d450 we de f ineitsdistributiondifferencewith respectto the training data u1d447 u1d450 inanothertarget class u1d450 as u1d437 u1d439 u1d453 u1d446 u1d450 u1d450 u1d440 u1d440 u1d437 u1d453 u1d459 u1d446 u1d450 u1d453 u1d459 u1d447 u1d450 where u1d453 u1d459refersto the outputofthe layer u1d459and u1d450 u1d450.
the distribution difference is measured on a speci f ic layer of the dnn.
in this paper we select the logits layer i.e.
the layer before the softmax layer which is frequently used in previous works .intuitively thesmallerthevalue u1d437 u1d439 u1d453 u1d446 u1d450 u1d450 the more difficult it is for the model u1d453to distinguish u1d446 u1d450and u1d447 u1d450 .
hence itismorelikelytogenerateundetectableerrorsbyminimizingtheir distributiondifference.
.
.
model enhancement.
the model s robustness can be improved if the distribution of training data u1d447 is closer to the distributionofreal worldvaliddata u1d44d i.e.
toaddmoreunseenvalid datatotrainingdata.however itisimpossibletodirectlycollectall real world data.
therefore we could adjust the objective to generate data that is as diverse as possible aiming to make the distribution of the generated data more closely resemble that of real worldvaliddata u1d44d .toprovideaeasyunderstandingofthe fundamental concept behind generating diverse data to enhance model s robustness we conducted a qualitative analysis as depicted in fig.
.
in this visualization we show the distribution of training data represented in blue and the distributions of speci f ic errors generated by different types of tools adversarial attack tool pgd distribution unawaretestingtool deephunter distribution awaretestingtool vae anddistxplore .additionally we include some real world data examples which represent a wide range ofpossible data samples.
theresultsofthisanalysishighlighttwokeyobservations the modelisnotrobustduetothedistributionshiftbetweenthetraining dataandreal worlddata.byutilizingvarioustools wecangenerate valid ood data that helps reduce the distribution shift and further enhancetherobustnessbyincorporatingpreviouslyunknowndata intothetrainingset.
theerroneousinputsgeneratedbyexisting tools exhibit limited diversity while distxplore aims to generate testcaseswithdiversedistributions suchthatthedistributionof the generated data could be closer to real world data distribution.
we propose a metric to measure the distribution diversity of testsuites whichcanguidethegeneratonofdiversedata.given a dnn u1d453that performs the classi f ication on u1d45aclasses denoted as u1d436 u1d453 andasetoftestsuites u1d447 u1d446 u1d450inaclass u1d450 thedistributiondiversity isde f inedas u1d437 u1d456 u1d463 u1d447 u1d446 u1d450 summationtext.
u1d450 u1d436 u1d453 u1d450 b u1d437 u1d439 u1d453 u1d446 u1d450 u1d446 u1d447 u1d446 u1d450 u1d436 u1d453 u1d450 u1d458 where u1d436 u1d453 u1d450representstheotherclassesexcept u1d450 bisaninterval abstractionfunctionthatmapsaconcretemmdvaluetoaninterval and u1d458isthenumberofintervalsbetween u1d450andeachofotherclasses.
the basic idea is to measure the diversity of distribution differences between the current test suites and the training data of otherclasses.sincethedifferencebetweentwodistributions i.e.
mmd isacontinuousvariable weadopttheintervalabstractionto spilt its values into u1d458intervals i.e.
u1d458distributions .
the numerator and the denominator represent the number of intervals covered andthetotalnumberofintervalsbetweenthecurrentclass u1d450and other classes respectively.
as shown in fig.
the distribution adequacyismeasuredfromtwoperspectives distributiondifference diversity for a given target class u1d450 multiple intervals between the testsuites and the trainingdata of u1d450 canbe covered.
target class diversity multiple classes i.e.
u1d436 u1d453 u1d450 are used to guide the testgeneration whichallowstoconsidertherelationshipsbetween every twoclasses.
intuitively the test suites in multiple intervals have different distributions.
to enhance the model s robustness the training data should cover the distributions as many as possible i.e.
to increase thedistributiondiversity.notethat onlyusingthestrongerrors i.e.
undetectable isnotsufficienttoimprovethewholerobustness asitcannothandleerrorswithdifferentdistributions seetheresults in section .infig.
thegenerated test suites i.e.
testsuite1 havediversedistributions i.e.
differentredcurves andare addedintothe training data for retraining.
71distxplore distribution guidedtestingforevaluating andenhancingdeep learning systems esec fse december3 san francisco ca usa we select the classes in the same task as targets because the classi f icationisbasedontheirrelationships i.e.
tochoosearelativelysuitableclass withhigherprobability .thesetargetsmaybe incompleteintermsofcharacterizingthedistributiondiversity.we canalsoselectothertargetstoguidethetestgenerationsuchasthe classesinothertasksaslongasthegeneratedtestsarevalid.for example wecanselecttheclassesincifar 10orromannumerals as the targets of mnist task.
we plan to evaluate the effects of more differenttargets inthe future work.
.
distribution guidedtestgeneration toachievebothtestinggoals weuseageneticalgorithm ga to solvetheproblem.withoutlossofgenerality theobjectivefunction canbede f inedas u1d437 u1d439 u1d453 u1d446 u1d450 u1d450 u1d463 i.e.
todecrease u1d437 u1d439 u1d453 u1d446 u1d450 u1d450 until it is close to a small value u1d463 where u1d446 u1d450is the test suite belonging to u1d450 u1d450 is a target class and u1d463is a constant value.
for the goal of model evaluation u1d463is set as i.e.
to generate u1d446 u1d450that is statistically indistinguishable from the training data in u1d450 .
for the goal ofmodelenhancement distxplore generatestestsuites thatcover morediverseintervals.consideratargetinterval thatwe aim to cover the objective function is de f ined as u1d437 u1d439 u1d453 u1d446 u1d450 u1d450 u1d463 where u1d463 canbeanyvaluewithintherange.thegeneral objective functioncan be argmin u1d446 u1d450 u1d437 u1d439 u1d453 u1d446 u1d450 u1d450 u1d463 algorithm 1shows thesearch based method to solve theobjective function.
the inputs include the dnn u1d453 a seed test suite u1d446 u1d450 fromclass u1d450 atargetclass u1d450 u1d450 u1d450 andthetargetdistributiondifference u1d463.
the output is the new test suite that can reach the target distribution difference.
the seed test suite can be collected from trainingdatasetortestingdataset.we f irstconstructa population thatcontains u1d45atestsuites line bymutatingtheseedtestsuite u1d45atimes.notethatthechromosomeisatestsuite includingmultipleinputs insteadofasingleinput.itrepeatedlyoptimizesthe population line forminimizingthedistributiondifference.in eachiteration we f irstcalculatethe f itnessesoftheupdatedpopulation line .thenweupdatethenewpopulationwiththestandard crossoverandmutation.ifthebestchromosome u1d446inthepopulation satis f ies the objective or timeout then the optimization process terminates line .thedistributiondifferencedecreasesduring optimization until itis less than a pre de f inedvalue u1d716.
forexample u1d716 0indicates that u1d437 u1d439 u1d453 u1d446 u1d450 is equal to u1d463.
note that u1d437 u1d439 u1d453 u1d446 u1d450 is decreasing for the two test goals because the distribution of the initial test suite is often far from the distribution of the training data inthe target class u1d450 .
we keep the chromosomethat has the best f itness unchanged i.e.
nocrossoverormutation toensurethattheoptimizationdoes notgetworse line .forothers we f irstselecttwochromosomes based on the tournament strategy line13 .
a uniform crossover is performed between the selected two chromosomes in the input level i.e.
genes in a chromosome are inputs of the model u1d453 line15 .
each gene in the chromosome u1d446can be selected to mutatewithaselection probability u1d45f line16 .
in this paper we mainly focus on image classi f ication tasks.
distxplorecanbeeasilyextendedtootherdomains.weselectthediverse image transformations e.g.
translation rotation brightness algorithm1 testgeneration input u1d453 the targetdnn u1d446 u1d450 aseed test suite from class u1d450 u1d450 the targetclass u1d463 targetdistribution difference output u1d446 u1d450 the newtest suite const u1d45a population size u1d461 tournamentsize u1d45f mutation rate u1d443 u1d45c u1d45d 2for u1d456 u1d45a do u1d443 u1d45c u1d45d u1d443 u1d45c u1d45d uniontext.
u1d45a u1d462 u1d461 u1d44e u1d461 u1d452 u1d452 u1d44e u1d450 uni210e u1d446 u1d450 4whiletruedo 5for u1d446 u1d443 u1d45c u1d45ddo u1d453 u1d456 u1d461 u1d446 u1d437 u1d439 u1d453 u1d446 u1d450 u1d463 7for u1d446 u1d443 u1d45c u1d45ddo if u1d442 u1d443 u1d45c u1d45d.
u1d453 u1d456 u1d461 u1d446 u1d453 u1d456 u1d461 u1d442then if u1d453 u1d456 u1d461 u1d446 u1d716ortimeout then returns continue else u1d4461 u1d461 u1d45c u1d462 u1d45f u1d460 u1d452 u1d459 u1d452 u1d450 u1d461 u1d443 u1d45c u1d45d u1d461 u1d4462 u1d461 u1d45c u1d462 u1d45f u1d460 u1d452 u1d459 u1d452 u1d450 u1d461 u1d443 u1d45c u1d45d u1d461 u1d446 u1d450 u1d45f u1d45c u1d460 u1d460 u1d45c u1d463 u1d452 u1d45f u1d4461 u1d4462 u1d446 u1d45a u1d462 u1d461 u1d44e u1d461 u1d452 u1d45d u1d45f u1d45c u1d44f u1d446 u1d45f usedindeeptest anddeephunter .foreachselectedgene themutation randomly selects a transformation function to mutate it.toguarantee thevalidityofthegeneratedinputs weadoptthe conservative strategy that constrains the transformation with both u1d43f0and u1d43f .
evaluation we have implemented distxplore in python .
based on dl framework keras ver.
.
.
with tensor f low ver.
.
.
.
to evaluate the effectivenessof distxplore inthemodelevaluationandmodelenhancement we aim to answer the following research questions rqs where rq1 and rq2 are to demonstrate the effectiveness in modelevaluation rq3andrq4aretoevaluatethemodelenhancement andrq5 isto study the generalization of distxplore .
rq1 howeffectiveis distxplore indetectingerrors1thatcan bypassthe defensemethods?
rq2 howefficient is distxplore for discoveringvalid errors?
rq3 how effective is distxplore in improving the robustness of the dl modelundertesting?
rq4 how useful are distribution difference diversity and target class diversityinimprovingrobustness?
rq5 candistxplore be generalizedto otherdomains?
.
setup .
.
datasetsanddnnmodels.
weselectfourdatasets i.e.
mnist fashion mnist cifar and svhn and six dnns i.e.
lenet lenet vgg16 resnet inception v3 andinception resnet v2 thatarecommonlyusedinexistingworks .
.
.
baselines.
to evaluate the effectiveness of distxplore we select 4typesof approaches including state of the art baselines for the comparisons adversarial attacks distribution unaware 1the errorin the paper refersto the erroneous inputsthat aremissclassi f ied.
72esec fse december3 san francisco ca usa longtianwang xiaofeixie xiaoningdu meng tian qingguo zheng yang andchao shen testingtechniques 3distribution awaretestingtechniquesand1 robustness orientedtesting.
adversarial attack .
we select adversarial attack techniques including3classicalones i.e.
bim pgd andc w and 3newones i.e.
di fgsm d2f si ni fgsm snf and ti fgsm tif to generate adversarial examples and compare themwiththe errorsgeneratedby distxplore .
distribution unawaretesting .weselectdeephunter neuron path coverage npc and combinatorial testing ct as the baselines.
deephunter is con f igured with two different coverageguidance i.e.
k multisectionneuroncoverage kmnc neuronboundarycoverage nbc .kmncandnbcaredesigned totestthemajorfunctionregionandthecorner caseregion npcis con f iguredwithstructure basedneuronpathcoverage snpc whichisdesignedtotestthedecisionlogic cttakesthe relationships between neurons in adjacent layers into considerationwhen testingdnn models.
distribution aware testing .
we select three recent distributionawaretestingtechniques asbaselines.in thetest selectioncriteriaareproposedtomeasurethesurpriseadequacy sa oftestcases.weselectthelikelihood basedsa lsa that measuresthetrainingdistributionwithkerneldensityestimation as a baseline.
in a variational auto encoder vae is usedtospeci f icallygeneratein distribution test cases.in a hierarchical distribution aware hda testing is proposed based ontheglobaldistributionandlocaldistribution.wedenotethese twobaselinesas vae andhda respectively.
robustness oriented testing.
to evaluate the robustness enhancement we selectthestate of the art robustness orientedtesting techniquerobot as our baseline.
.
.
defense methods.
to evaluate the strengths of generated errorsbydifferenttechniques weselecttwostate of the artdefense methodsthat detectadversarialexamples as follows dissector which dissects the outputs of intermediate layers andcalculatesascoreforthegiveninput.thescoreshowsthe degreeofsimilaritybetweentheinputandbenigndata.forlenet4andlenet weselectthefullyconnectedlayers.forefficiency f iveintermediatelayersareselectedforlargermodel.thedetails are providedonour website .
attackas defense a2d which detects adversarialsamples basedontheobservationthatadversarialsamplesarelessrobust than benign ones.
it measures the robustness of the given inputs withexistingadversarialattacks.weusejsma thatisdifferentfrombaselineadversarialattacks tocalculatetheattack costofeachinputfor detecting whether itisabnormal input.
.
.
experimentsetup.
seedselection.
foreachtask werandomlyselect100seedinputs foreachclassfromtrainingdataset.totally weselect1 000seeds thatareusedbyallbaselines.notethatthehdaapproachproposes adistribution awarestrategytoselectseeds hencewecon f igure hdawithtwoinitialseedconstructionstrategies usingthesame 000seedinputsasusedfor otherbaselinesfor afaircomparison denoted as u1d43b u1d437 u1d434 and using the hda s own seed selection to select1 initialseedinputs denotedas u1d43b u1d437 u1d434 u1d45c .con f igurationofdistxplore.
weusethe100initialseedsselected ineachclassasaseedtestsuite.foreachclass u1d450 werundistxplore multiple times i.e.
by setting different target classes u1d450 with algo.
.
finally for each model we run distxplore times i.e.
source classes target classes .
we set the f itness function as minimizing the distribution difference i.e.
the values of u1d463and u1d716in algo.1arecon f iguredas0 .notethat tocalculatethedifferenceef f iciently werandomlyselectanother100samplesfromthetraining datainclass u1d450 insteadofallofthem.wefoundthatthedistribution distance between the selected samples and the corresponding class oftrainingdatais closetozero mmd whichindicates that theselectedtrainingsamplescan representthedistributionofthe wholetraining data.
foreachrunof distxplore welimitthetotalnumberofiterations in ga as .
we empirically con f igured the population size the tournament size and the mutation rate as as and .
respectively.duetothelimitofthespace theexperimentsabout the impact of the parameters are put on our website .
for the robustnessenhancement wedonotexplicitlygeneratetestcases foreachinterval see u1d437 u1d456 u1d463 u1d447 u1d446 u1d450 insection .
.
.instead wemap thedistributiondifferenceineachiteration i.e.
the f itnessvalue to an interval.
during the optimization process the distribution distance is decreasing in multiple iterations covering different intervals.toensurethevalidityofthegeneratedtestcases weadopt amoreconservativecon f igurationcomparedtodeephunter to constrain the mutation.
con f iguration of baselines.
for the three classic adversarial attacks weperformthetargetattackforeachseedinputbyselecting otherclassesasthetargets i.e.
wegenerate9adversarialexamples foreachseedinput.forthethreenewadversarialattacks asthey arenotdesignedfortargetattacks weperformuntargetattackwith the defaultcon f igurationsprovided.
note that lsa is a test selection metric instead of a testing tool.
toperformthecomparison wedevelopanewtestingtoolbasedon deephunter i.e.
using lsa asthe guidance to generatetestcases.
forothers wefollowtheirdefaultcon f igurationstorundeephunter ct npc hda vae androbot.speci f ically eachmodelis testedfor5 iterationsby deephunter kmnc and nbc ct andnpc.eachseedisoptimizedwith50 and30iterationsby hda vae androbot respectively.
more detailedsettingscanbe foundonour website .
rq setup.
to demonstrate the capability of distxplore in generating strong errors for model evaluation rq1 we collect the test suite in the last iteration for every pair u1d450 u1d450 i.e.
the best chromosome returns from algo .
for each model we collect a total number of chromosomes over pairs which are used to evaluate the strength of these errors.
the strength of errors is measured by the success rate of bypassing defenses.
in addition wealsoevaluatetheefficiencyof distxplore fordiscoveringvalid errors rq2 .
to evaluate the efficiency we count all the errors generatedduringthe 30iterations.
speci f ically we select two metricsforthecomparisons thetotalnumberoferrorsandthesuccess rate of generating errors for each seed.
to evaluate the validity of generated errors we perform a human study to manually check the validity ofthe discoverederrors.
73distxplore distribution guidedtestingforevaluating andenhancingdeep learning systems esec fse december3 san francisco ca usa table1 resultsof bypassingthedefense techniqueson datasetsmnist m fashion mnist fm cifar c and svhn s and dnns lenet l lenet l vgg16 v resnet r inception resnet v2 ir v2 and inception v3 i v3 .
ds model defense distx bim pgd c w d2f snf tia kmnc nbc ct npc lsa hda u1d43b u1d437 u1d434 u1d45cvae ml 4dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l 5dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fml 4dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
l 5dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
cv 16dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
r 20dissector .
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
ir v2dissector .
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
i v3dissector .
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
sv 16dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
r 20dissector .
.
.
.
.
.
.
.
.
.
.
.
.
.
a2d .
.
.
.
.
.
.
.
.
.
.
.
.
.
todemonstratethecapabilityinenhancingrobustness weselect test suites with diverse distributions i.e.
distribution difference diversityandtargetclassdiversity .foreachpair u1d450 u1d450 wesplitthe distribution difference into intervals where u1d437 u1d439 u1d45b represents the best f itness value in the u1d45b u1d461 uni210eiteration.
note that the f itness values in multiple iterations may fall into the same interval.
to achieve the distribution difference diversity we randomly select aniterationfromeachintervalandcollectitsbestchromosome i.e.
10chromosomesforeachpair .toachievethe targetclassdiversity weconsiderallofotherclassesasthetargets i.e.
9targetsforeach source .finally wecollect900testsuites 10intervals 90pairs for f ine tuningin rq3 e.g.
testsuite1 ...infig.
.toconduct a fair comparison we collect the same number of test cases by eachbaselineforretraining.speci f ically foradversarialattacks we con f igure different parameters such that we can generate multiple adversarial examples for each seed input.
for testing tools we f irst generate a large number of errors and then randomly select the same number ofinputsfor retraining.
forrq4 we evaluate the usefulness of distribution difference diversityandtargetclassdiversity inrobustnessenhancement.
we collect two sets for retraining we only consider the distribution difference diversity and ignore the target class diversity .
we randomly select one target class and collect multiple chromosomes fromeachinterval denotedas distxplore u1d451 u1d453 e.g.
testsuite1 in fig.
.
weselectalltargetclassesforthe targetclassdiversity but restrict their intervals.
for each target class we randomly select some chromosomes from only one interval denoted as distxplore u1d461.
e.g.
testsuite1 infig.
.notethat tomakeafaircomparison with the results in rq3 we control the number of test cases in distxplore u1d451 u1d453anddistxplore u1d461by collecting multiplechromosomes fromaninterval suchthattheyhavethesamesizewiththedata using inrq3 i.e.
900test suites .
fortherobustnessmeasurementin rq3andrq4 weselectthe empirical robustness thatiscommonlyused in previousworks .
theempirical robustness is measured by the accuracy on avalidationdataset.togeneratesuchavalidationdataset weselecta new set of initial seeds that differs from the seeds in testing.
thenwerun distxplore andotherbaselinestogenerateerrorsbased on new seeds.
these errors found by different tools form a new testsetforevaluatingempiricalrobustness.consideringthatthe transformationstrategiesaredifferentindifferenttypesoftools we try to construct a balanced dataset for a fair comparison including 000errorsfromeachtypeoftool i.e.
adversarialattacks 000for each ofbim pgd and c w distribution unaware testing for each con f iguration of deephunter distribution aware testing for each of lsa vae and hda and distribution guided testing for eachsource target pair .
forrq5 we evaluate the generalization ability of distxplore by adaptingittotwonlpclassi f icationtasks i.e.
sentimentanalysison imdb andnewsclassi f icationonag snews .we f ine tune the pre trainedmodelbert onthe twodatasets respectively.
duetotheintrinsicdifferencesbetweenimagesandtextualdata we develop the text speci f ic mutation strategies.
the details about thetextmutationcanbefoundonthewebsite .asothertesting toolsaremainlyusedinimagedomain weselecttwonlpadversarialattacks i.e.
pwws andtextfooler asthebaselines.
additionally we select the state of the art method wdr as the defense technique as dissector and a2d are not suitable for bertpre trainedmodels.
we follow the existing work and repeat each experiment times to reduce the effect of the randomness during the test generation.
.
results .
.1rq1 strength of errors.
we evaluated our method using three metrics the unique number of errors thesuccess rate and the strength of errors .
theunique number oferrors represents the total number of erroneous inputs generated within a given time budget.
this metric is widely used in existing dl testing works andprovidesameasureoftheeffectivenessof 74esec fse december3 san francisco ca usa longtianwang xiaofeixie xiaoningdu meng tian qingguo zheng yang andchao shen dltesting.the successrate measuresthepercentageofseedinputs from which the testing tools can generate at least one erroneous input.thismetrichasbeenemployedindltestingandadversarial attacktools .ahighersuccessrateindicatesthat ourmethodiscapableofgeneratingerrorsforalargerproportionof seed inputs.
the strength of errors quanti f ies the severity or impact oftheerroneousinputsgenerated.weemphasizetheimportance ofgeneratingstrongerrors asweakererrorscanbeeasilydetected byexisting defensetools.
table1shows the results on the strength of generated errors by different methods.
for dissector we use auroc to indicate the capabilityon detectingerrors.
for attack as defense we show the proportionof errors thatcan be detected.the symbol incolumn npcindicatesthatnpccannotbeusedtotestthesednnssince thecriticalpathscannotbeextracted.thesymbol incolumnvae indicates thatthe vae methoddoes not work well onthe selected taskas mentionedin .
the overall results show that distxplore columndistx can generate more strong errors that are difficult to be detected by defense techniques compared with baselines.
speci f ically all errors generatedbyadversarialattacksunderperform distxplore which maybebecausethattheyonlyaddminorperturbations.wealso found that the new advrsarial attacks outperform the classic adversarial attacks i.e.
bim pgd c w .
compared with testing techniques we can see that distxplore performs better in most cases.
comparing the results between distribution unaware testing i.e.
kmnc nbc ct and npc and distribution aware testing i.e.
hda u1d43b u1d437 u1d434 u1d45cand vae we found that distribution unaware testing tends to perform better because it generates some ood data indicating that id errors from distribution aware testing are easier to detect.
distxplore explicitly considers the distribution difference which guides to generate statistically indistinguishable errorsthat are more difficult to detect.
compared to other distribution aware testing i.e.
hda u1d43b u1d437 u1d434 u1d45c and vae we found that the errors generated by lsa are harder to detect because lsa can also generate ood data based on the surprise guidance.
distxplore performs better than lsa since it considers the distribution difference between each two classes and optimizes each test suite making the discovered errors statistically indistinguishablecomparedwithotherclasses.
answerstorq1 comparedwith adversarialattacks and existing dl testing techniques distxplore is more effective in generating hard to detect errors.
existing distribution aware testing techniques mainly focus on generating in distribution data that could be easier to detect.
fig.5showstherelationshipbetweenthedistributiondifference and the strength of errors.
due to the space limit other results are put on our website .
for each pair u1d450 u1d450 we collect the best chromosome u1d446aftereachiterationandcalculate mmd target thedistributiondifferencebetween u1d446andthetrainingdataoftarget class u1d450 mmd source the distribution difference between u1d446and the training data of source class u1d450 error rate the proportion of errors in u1d446 error target rate the proportion of errors in u1d446 predictedasthetargetclass and dissector anda2d theresults0 iteration0.
.
.
.
.
.0mnist iteration0.
.
.
.
.
.0fashion mnist mmd target mmd sourceerror rate error target ratedissector a2d figure the average results during the optimizationof distxplore model lenet table2 resultsof efficiency on four datasets ds mod metric distx kmnc nbc lsa hda vae ml 4time s .
.
.
.
.
.
error21 .
.
.
.
.
.
succ.r .
.
.
.
.
.
l 5time s .
.
.
.
.
.
error .
.
.
.
.
.
succ.r .
.
.
.
.
.
fml 4time s .
.
.
.
.
error21 .
.
.
.
.
succ.r .
.
.
.
.
l 5time s .
.
.
.
.
error18 .
.
.
.
.
succ.r .
.
.
.
.
cv 16time s .
.
.
.
.
error26 .
.
.
.
.
succ.r .
.
.
.
.
r 20time s .
.
.
.
.
error30 .
.
.
.
.
succ.r .
.
.
.
.
sv 16time s .
.
.
.
.
.
error29 .
.
.
.
.
.
succ.r .
.
.
.
.
.
r 20time s .
.
.
.
.
.
error29 .
.
.
.
.
.
succ.r .
.
.
.
.
.
detectedbythedifferentdefensetechniques.weaveragetheresults fromallpairs andnormalizetheresultsfrom0to1exceptfor error rateanderror target rate for easier comparison.
the results show that during the optimization the distribution of u1d446is getting closer to the training distribution of the target class seemmd target andgettingfartherawayfromthesourceclass seemmd source .
meanwhile error rate anderror target rate are increasing indicatingthatmoreerrorsaregeneratedandgradually become statistically indistinguishable between the original class u1d450andtarget class u1d450 .theeffectofindistinguishabilitycan befurther con f irmed by the detection results i.e.
dissector anda2d errors become indistinguishable and difficult to detect while the mmd target decreases.
answers to rq1 the distribution difference is useful in guidingthegenerationofstatisticallyindistinguishableerrors makingthemmore difficultto detect.comparedwithothers distxplore generatesmore diverseerrors.
75distxplore distribution guidedtestingforevaluating andenhancingdeep learning systems esec fse december3 san francisco ca usa .
.2rq2 efficiency of distxplore.
we further study the efficiency of distxplore in discovering errors as shown in table .
note that we do not set the same time to run all tools as different tools have different con f iguration methods.
we emphasize that this papermainly focuses ongenerating high quality i.e.
hard todetect errors rather than merely comparing the total number of errorswithinasettime asmanyweakerrorscanbeeasilydetected bydefensemethods see rq1 results .
in table2 we show the time used for each tool under its con f iguration time s andthetotalnumberoferrors error .dueto the space limit other results are put on our website .
we do not showtheresultsofadversarialattacksherebecausetheydifferfrom the settings of testing tools i.e.
they generate an adversarial example for each seed.
overall we can observe that distxplore column distx generatesmoreerrorswhileusestheshortesttime.wecould alsoobservethattheexistingdistribution awaretestingtendsto be slower due to time consuming distribution measurements such asthekernel density estimationand vae.table 2alsoshows the success rate of generating errors for each seed.
the results show thatdistxplore hasahighersuccessratethanotherbaselines.we also notice a exception that lsa achieves higher success rate on mnistlenet .weconjecturethatitisduetotheoptimizationobjectiveof distxplore thatminimizesthedistributiondistance rather thanspeci f icallyguidingmisclassi f icationforindividualsamples.
in somespeci f icdatasets theoptimization may not requireerrors for certainseeds.
in order to evaluate the validity of the generated inputs we conducted a manual investigation by randomly selecting erroneous inputs from the testing outputs of each model and calculatingthe averagevalidity ratio.the validityratios werefound to be .
.
.
and .
for mnist fashion mnist cifar and svhn datasets respectively.
the results demonstrate that distxplore is capable of generating valid inputs with high proportions.
more details are providedonour website .
answers to rq2 compared to other dl testing tools distxploreachievesthehighestefficiencyintermsofthenumberof errors generated per second and success rates.
moreover distxploreis more effective in terms of generating valid samples.
.
.3rq3 robustnessenhancement.
foreachtool we f ine tune the original model epochs following previous works by adding the new data generated from each tool and evaluate the empirical robustness of the new model on the validation dataset we created.
note that all data in validation dataset is predicted incorrectly by the original model.
table 3shows the accuracy of f ine tuned models on the validation dataset.
as expected distxploreoutperformstheadversarialattacks distribution awaretesting distribution unaware testing and robustness oriented testing.
the overall results demonstrated the effectiveness of distxplore in improvingrobustness.inaddition lsaachievesthesecondbestresults which outperform the results of other baselines because lsa can generate some ood test cases increasing the diversity.
the three modern adversarial attack techniques perform worse than the three classictechniques because these techniquesare designed for untargetattack whichdecreasethe distributiondiversity.d g adv d u d adistxplore bim pgd c w kmnc nbc lsa hda robot0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.52cifar .
.
.
.
d g adv d u d adistxplore bim pgd c w kmnc nbc lsa hda vae robot0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.71svhn .
.
.
.
.
.
.
.
figure6 accuracy on differenttypesof dataset model vgg16 answerstorq3 overall distxplore iseffectiveinimproving robustness by generating data with different distributions.
distribution awaretestingtechniquesonlyconsideriddata makingitperform poorlyonerrorsgeneratedbyothertools.
to further interpret the results we analyze the accuracy on differentkindsofvalidationdataset whichisshowninfig.
.otherresultsareshowninthewebsite .recallthatourvalidationdataset includes errors from distribution guided testing distxplore i.e.
d g errors from adversarial attacks i.e.
adv errorsfromdistribution awaretestinglsa hda andvae i.e.
d a and errors from distribution unaware testing i.e.
d u .
note thatthedataset d gandd ucovermorediversetransformations e.g.
rotationandtranslation whilethedataset advandd aare mainly created by the noise based transformation.
speci f ically the image transformation directly determines the distribution of the generated test cases that further affects the accuracy evaluation.
takinginto account thatthese toolsusedifferenttransformations we build such a balanced validationdataset for a fairer comparison.
not surprisingly each tool usuallyachieves betteraccuracy on thevalidationdatageneratedbythesametypeoftools becausethey have similardistribution whilethedata from othertypes oftools are more likely tobe ood.for example bim pgd and c w get muchhigheraccuracyon advdatasetsincetheaddedtrainingdata and theadvdata are very similar i.e.
adding minor perturbation .
however the tools with only noise based perturbation i.e.
bim pgd c w hda vae and robot achieve much lower accuracy on the data d gandd uthat use very different transformation.
their accuracy on d g .
is relatively lower than that on d u .
indicating some errors generated by distxplore are harder to predict.
comparing the results between deephunter and distxplore which use the same transformations we found that deephunter achieves lower accuracy than distxplore ond gdata because distxploregeneratestestcaseswith diversedistributions whichmay be ood for deephunter.
as for the data d ugenerated by deephunter the accuracy of distxplore is slightly higher than that of deephunter which indicates that the errors from distxplore couldcoversomedistributionofthedatageneratedbydeephunter.
considering the distribution aware testing hda and vae as they onlygenerate iddata theyperformmuchworseon otherdataset.
consider the results of distribution aware testing hda and vae adversarial attacks bim pgd and c w and robustnessorientedtesting whichusethesametransformation wefoundthat hdaandvaeachieveloweraccuracy seetable indicatingthat 76esec fse december3 san francisco ca usa longtianwang xiaofeixie xiaoningdu meng tian qingguo zheng yang andchao shen table3 resultsof robustnessenhancement usingthetestcases generatedby differenttools on four datasets d m distxplroe bim pgd c w d2f snf tif kmnc nbc ct npc lsa hda u1d43b u1d437 u1d434 u1d45cvae robot ml .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fml .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cv .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
.
.
ir .
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
sv .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table4 resultsof robustnesswithdifferentdistribution diversity dataset model distxplore distxplore u1d451 u1d453distxplore u1d461 mnistlenet .
.
.
lenet .
.
.
fmnistlenet .
.
.
lenet .
.
.
cifar 10vgg16 .
.
.
resnet .
.
.
ir v2 .
.
.
i v3 .
.
.
svhnvgg16 .
.
.
resnet .
.
.
onlyconsideringidislesseffectiveinimprovingtherobustness especiallyonood data.
the data generated by different testing tools may have different distributions depending on their transformation and guidance strategies.allthesedatacouldbethepotentialinputsintherealworlddeployment andtestcasesgeneratedbyatoolmaynotcover alldistributions.forexample although distxplore isdesignedto increasethedistributiondiversity itdoesnotalwayscoverthedata distribution from other tools.
in general it can cover more unseen distributionsif we gradually increasethe distributiondiversity.
answerstorq3 distxplore cangeneratetestcaseswith diversedistributions whichcanidentifymoreunseendatafor further robustnessimprovement.
.
.
rq4 usefulness of distribution diversity.
table4shows the results about the usefulness of the distribution difference diversity and target diversity.
distxplore distxplore u1d451 u1d453 anddistxplore u1d461representstheaccuracyofmodels f ine tunedwithdifferentdata see more con f iguration details in section .
.
.
note that the number of data used in distxplore u1d451 u1d453 distxplore u1d461 anddistxplore are the same.comparedtotheresults distxplore wefoundthat theaccuracydrops ifonlyconsidering thedistributiondifference diversity distxplore u1d451 u1d453 ortargetdiversity distxplore u1d461 whichindicatesthe usefulnessofboth kindsofdiversityinimprovingthe robustness.
answers to rq4 bothdistribution difference diversity and targetclass diversity are useful inimprovingthe robustness.
.
.
rq5 generalizationability.
table5showstheresultsonthe strength of generated errors by different methods i.e.
the percentage of errors that can be detected by existing detection methods.theoverallresultsshowthat distxplore canstillgeneratestrong errorsthantheselectedbaselines.moreover theresultsalsodemonstrate the generalizabilityof distxplore to otherdomains.
discussion on application scope.
this paper primarily focuses on the classi f ication task which is one of the most popular and important machinelearning tasks and hasbeen widelystudied in theresearchareaofdltesting .
while there is much less work on testing generation tasksin theliteratureduetothechallengeof de f iningtestoracles i.e.
how to de f ine the errors.
recently researchers proposed a fewmetamorphicrelations formachinetranslationtasks to overcome the problem.
it is noteworthy that the challenge of test oracle is orthogonal to the problem we aim to solve in the paper.consideringthatnoneoftheexistingworkslookintodata distribution webelievethat distxplore couldalsoplayanimportant role in generating test cases with better diversity for generation tasks in view that data distribution is a fundamental concept for generallearningtasks.
speci f ically distxplore canbeextendedtogenerationtasksby modifying the feedback of distribution differences.
currently in classi f icationtasks weselectotherclassesastargetstoguidethe generation of test suites for achieving diverse distributions due to the classi f ication characteristics.
for generation tasks that do not have classes suppose there is a generation model that can generatehumanfacesfollowingaspeci f icdistribution basedonthe trainingsamples wecanselectotherdatasets suchasimagenet cifar or other image datasets as the targets to guide the generation of test suites such that the test suites can also have diversedistributions.however howtoselectthetargetdistribution and how effectively they can helpwith the testing require further explorationandevaluation.weleavetheextensiontogeneration tasksas our future work.
answers to rq5 distxplore is also useful in testing nlp models.
threats to validity there aresomethreatsthat could affectthevalidityoftheresults.
the selected models and datasets are threats to the validity.
we mitigatethesethreatsbyselectingthepopulardatasetsandmodels that are used by existing dl testing works.
the randomness could be a threat which is mitigated by generating a large number of test cases over a relatively long time and running each tool timesinourexperiments.inaddition wemakeourexperimental 77distxplore distribution guidedtestingforevaluating andenhancingdeep learning systems esec fse december3 san francisco ca usa table5 resultsof bypassingthedefense techniquesfor nlp tasks dataset defense distxplore pwws textfooler imdb wdr .
.
.
ag s news wdr .
.
.
resultspubliclyavailable.theselectionoftheseedinputsisathreat.
we mitigate it by selecting a large number seeds that are usedbyallbaselines.thelayerselectedforcalculatingthemmd could be a threat to affect the results.
we mitigate thisproblem by selectingthecommonlyusedlayer i.e.
logitslayer.inthefuture we plan to evaluate distxplore by selecting different layers and their combinations.
another threat is that the empirical robustness depends on the validation dataset and the transformations used in selected tools are different which could be a threat to affect theresults.tomitigatethisproblem wetryourbesttoassemble a balanced validation dataset comprised of data generated from differenttypesoftestingtools 000inputsgeneratedbyeachtype of tool .
moreover we choose a new set of seeds to generate the validation dataset in order to avoid the overlapping between the newtraining dataset andvalidation dataset.
related work .
distribution unawaretesting duetothedifferencesbetweentraditionalsoftwareanddeepneural networks some coverage criteria have been proposed.
the general ideaistode f ine metrics formeasuring thebehaviorsofthetarget dnns while the distribution is not explicitly considered.
the neuron coverage isthe f irst dl coverage criterion that measures the percentage of neurons activated by the given inputs.
ma et al.
thenextendedtheneuroncoverageandproposedasetof f ine grainedcoveragecriteriasuchask multisectionneuroncoverage kmnc neuronboundarycoverage nbc andtop kneuron coverage tknc .althoughthedistributionisnotexplicitlyconsidered there could be some implicit relationship between them.
forexample nbcde f inesthecoveredupperandlowercornercase regions which is more related to ood data.
npc proposes twopath basedcoveragecriteriatomeasurethecoverageonthe decisionlogic.apathrepresentsapossibledecisionlogic.basedon thecoveragecriteria someautomatedtestingtechniqueshavebeen developed such as deepxplore dlfuzz deeptest deephunter deepstellar andtensorfuzz althoughthesetechniquescouldalsogeneratetestcaseswith different distributions none of them explicitly considers the distribution.forexample alotoferrorsaregeneratedbuttheymay followthesimilar distributions.in addition the existingworksdo not consider the strength of generated errors.
differently distxploregenerates strong errors that are statistically indistinguishable andenhancesrobustnesswithdifferentdistributions.
.
distribution awaretesting recently some testing works start to discuss the effect of distributionfortesting whichisbasedonthefactthatadlmodelistrained onsampledtrainingdatafollowingaspeci f icdistribution.berend et al.
conductedanempiricalstudyontherelationshipsbetween data distribution and existing testing techniques.
they call for the attention of data distribution awareness when designing testingmethods.
zhou et al.
study the robustness of dnns with distributionawareness.hu etal.
studythedistribution awareseed selectionmethodsfordnns.dola etal.
developthedistributionawaretestingtechniquethatbasicallygeneratesthein distribution databythevariationalautoencoders vaes .toledo etal.
proposedthedistribution awareveri f ication.itusesagenerativemodel to represent the data distribution of the trained model and then changes the original model such that all the inputs to the dnn followthelearneddistribution.themostrecentwork proposeda hierarchical distribution aware testing method that measures both ofglobaldistributionandlocal distribution.
besides kim et al.
propose lsa and dsa to measure the surprise adequacy sa of the test cases i.e.
the surprise degree of asingletestcasecomparedwiththetrainingdata.althoughboth distxplore andsaconsiderthedistancebetweentestcase s and training data there are some key differences distxplore measures the distribution difference between two setsof data while sa measures the surprise of a singletest case in addition considering the distance calculation distxplore is more efficient.
distxplore ismore f ine grainedandconsidersintra classandinter classdistribution shifts while sa mainly considers the distance between a test case and all training data.
the goals are not totally the same.saisatestselectionmethodthatmainlyselectssurprising data.
however it is not clear whether the surprising data from sa is effective in generating hard to detect errors or enhancing model srobustness whichisourmainfocus.theevaluationresults demonstratethat distxplore ismore effective.
conclusion in this paper we propose a distribution guided testing approach to evaluate and enhance dl models.
to the best of our knowledge thisisthe f irstworkthatexplicitlygeneratestestcaseswithdiverse distributions.wediscussedtherelationshipbetweenvalidityand distribution where valid out of distribution data is ignored by existing distribution aware testing.
we evaluated the effectiveness of distxplore on10modelsandcompareditwith14state of the art tools.
the results demonstrate that distxplore is efficient and effectiveindiscoveringhard to defenderrorsandimprovingrobustness.
data availability we provide the source code and data on