source code summarization in the era of large language models weisong sun1 yun miao2 yuekang li3 hongyu zhang4 chunrong fang2 yi liu1 gelei deng1 yang liu1 zhenyu chen2 1college of computing and data science nanyang technological university singapore singapore 2state key laboratory for novel software technology nanjing university nanjing china 3school of computer science and engineering university of new south wales sidney australia 4school of big data and software engineering chongqing university chongqing china weisong.sun ntu.edu.sg miaoyun001my gmail.com yuekang.li unsw.edu.au hyzhang cqu.edu.cn fangchunrong nju.edu.cn yi009 e.ntu.edu.sg gelei.deng ntu.edu.sg yangliu ntu.edu.sg zychen nju.edu.cn abstract to support software developers in understanding and maintaining programs various automatic source code summarization techniques have been proposed to generate a concise natural language summary i.e.
comment for a given code snippet.
recently the emergence of large language models llms has led to a great boost in the performance of coderelated tasks.
in this paper we undertake a systematic and comprehensive study on code summarization in the era of llms which covers multiple aspects involved in the workflow of llmbased code summarization.
specifically we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by llms and find that the results of the gpt evaluation method are most closely aligned with human evaluation.
then we explore the effectiveness of five prompting techniques zero shot few shot chain of thought critique and expert in adapting llms to code summarization tasks.
contrary to expectations advanced prompting techniques may not outperform simple zero shot prompting.
next we investigate the impact of llms model settings including top p and temperature parameters on the quality of generated summaries.
we find the impact of the two parameters on summary quality varies by the base llm and programming language but their impacts are similar.
moreover we canvass llms abilities to summarize code snippets in distinct types of programming languages.
the results reveal that llms perform suboptimally when summarizing code written in logic programming languages compared to other language types e.g.
procedural and object oriented programming languages .
finally we unexpectedly find that codellamainstruct with 7b parameters can outperform advanced gpt in generating summaries describing code design rationale and asserting code properties.
we hope that our findings can provide a comprehensive understanding of code summarization in the era of llms.
index terms large language model source code summarization prompt engineering i. i ntroduction code comments are vital for enhancing program comprehension and facilitating software maintenance .
while it is considered good programming practice to write highquality comments the process is often labor intensive and time consuming .
as a result high quality comments are frequently absent mismatched or outdated during software evolution posing a common problem in the software industry .
automatic code summarization or simply llm b summary evaluation codesnippet summarycategoryprompting techniques modelsetting llm generatedsummary prompt referencesummarysummary summarysimilarity codesnippet summary codesimilarity promptgenerator a summary generationfig.
general workflow of llm based code summarization and its effectiveness evaluation code summarization a hot research topic addresses this challenge by developing advanced techniques models for automatically generating natural language summaries i.e.
comments for code snippets such as java methods or python functions provided by developers.
recently with the success of large language models llms in natural language processing nlp an increasing number of software engineering se researchers have started integrating them into the resolution process of various se tasks .
in this study we focus on the application of llms on the code summarization tasks.
figure shows the general workflow of llm based code summarization and its effectiveness evaluation methods.
in the summary generation process the input consists of a code snippet and the expected summary category.
the input is passed to a prompt generator equipped with various prompt engineering techniques referred to as prompting technique which constructs a prompt based on input.
this prompt is then used to instruct llms to generate a summary of the expected type for the input code snippet.
in the summary evaluation process a common method used to automatically assess the quality of llm generated summaries is to compute the text or semantic similarity between the llmgenerated summaries and the reference summaries.
there have been several recent studies investigating the effectiveness of llms in code summarization tasks .
these studies can help subsequent researchers rapidly understand the aspects of code summarization garnering attention in the era of llms but they still have some limitations.
first most of them only focus on one prompting technique while 1arxiv .07959v2 aug 2025some advanced prompting techniques have not been investigated and compared e.g.
chain of thought prompting .
for example sun et al.
solely focus on zero shot prompting while several other studies only focus on few shot prompting.
second they overlook the impact of the model settings i.e.
parameter configuration of llms on their code summarization capabilities.
there is no empirical evidence showing llms remain well in all model settings.
last but not least these studies follow prior code summarization studies to evaluate the quality of summaries generated by llms through computing text similarity e.g.
bleu meteor and rouge l or semantic similarity e.g.
sentencebert based cosine similarity between the llm generated summaries and the reference summaries detailed in section iv a. however prior research by sun et al.
has shown that compared to traditional code summarization models the summaries generated by llms significantly differ from reference summaries in expression and tend to describe more details.
consequently whether these traditional evaluation methods are suitable for assessing the quality of llm generated summaries remains unknown.
to address these issues in this paper we conduct a systematic study on code summarization in the era of llms which covers various aspects involved in the llm based code summarization workflow.
considering that the choice of evaluation methods directly impacts the accuracy and reliability of the evaluation results we first systematically investigate the suitability of existing automated evaluation methods for assessing the quality of summaries generated by llms including codellama instruct starchat gpt .
and gpt4 .
specifically we compare multiple automated evaluation methods including methods based on summary summary text similarity summary summary semantic similarity and summary code semantic similarity with human evaluation to reveal their correlation.
inspired by the work in nlp we also explore the possibility of using the llms themselves as evaluation methods.
the experimental results show that among all automated evaluation methods the gpt based evaluation method overall has the strongest correlation with human evaluation .
second we conduct comprehensive experiments on three widely used programming languages java python and c datasets to explore the effectiveness of five prompting techniques including zero shot few shot chain of thought critique and expert in adapting llms to code summarization tasks.
the experimental results show that the optimal choice of prompting techniques varies for different llms and programming languages.
surprisingly the more advanced prompting techniques expected to perform better may not necessarily outperform simple zero shot prompting.
for instance when the base llm is gpt .
zero shot prompting outperforms the other four more advanced prompting techniques overall on three datasets.
then we investigate the impact of two key model settings parameters including top p and temperature on llms code summarization performance.
these two parameters may affect the randomness of generated summaries.
the results demonstratethatthe effect of top p and temperature on summary quality varies depending on the base llm and programming language .
as alternative parameters they exhibit a similar impact on the quality of llm generated summaries.
furthermore unlike existing studies that simply experimented with multiple programming languages we reveal the differences in the code summarization capabilities of llms across five types including procedural object oriented scripting functional and logic programming languages encompassing ten programming languages java python c ruby php javascript go erlang haskell and prolog.
the erlang haskell and prolog datasets are built by ourselves and we make them public to the community.
we find that across all five types of programming languages llms consistently perform the worst in summarizing code written in logic programming languages .
finally we investigate the ability of llms to generate summaries of different categories including what why how to use it how it is done property andothers .
the results reveal that the four llms perform well in generating distinct categories of summaries.
for example codellama instruct excels in generating why and property summaries while gpt is good at generating what how it is done and how to use summaries .
our comprehensive research findings will assist subsequent researchers in quickly and deeply understanding the various aspects involved in the workflow of code summarization based on llms as well as in designing advanced llm based code summarization techniques for specific fields.
in summary we make the following contributions.
to the best of our knowledge we conduct the first investigation into the feasibility of applying llms as evaluators to assess the quality of llm generated summaries.
we conduct a thorough study of code summarization in the era of llms covering multiple aspects of the llm based code summarization workflow and come up with several novel and unexpected findings and insights.
these findings and insights can benefit future research and practical usage of llm based code summarization.
we make our dataset and source code publicly accessible to facilitate the replication of our study and its application in extensive contexts.
ii.
b ackground and related work code summarization is the task of automatically generating natural language summaries also called comments for code snippets.
such summaries serve various purposes including but not limited to explaining the functionality of code snippets .
the research on code summarization can be traced back to as early as when sonia haiduc et al.
introduced automated text summarization technology to summarize source code.
later on following the significant success of neural machine translation nmt research in the field of nlp a large number of researchers migrate its underlying encoder decoder architecture to code summarization tasks .
in the past two years research on llm based code summarization has mushroomed.
2fried et al.
introduce an llm called incoder and try zero shot training on the codexglue python dataset.
incoder achieves impressive results but fine tuned small plms like codet5 can still outperform the zero shot setting.
ahmed et al.
investigate the effectiveness of few shot prompting in adapting llms to code summarization and find that it can make codex significantly outperform fine tuned small plms e.g.
codet5 .
given the concern of potential code asset leakage when using commercial llms e.g.
gpt3.
su et al.
utilize knowledge distillation technology to distill small models from llms e.g.
gpt .
.
their experimental findings reveal that the distilled small models can achieve comparable code summarization performance to llms.
gao et al.
investigate the optimal settings for fewshot learning including few shot example selection methods few shot example order and the number of few shot examples.
geng et al.
investigate llms ability to address multiintent comment generation.
ahmed et al.
propose to enhance few shot samples with semantic facts automatically extracted from the source code.
sun et al.
design several heuristic questions to collect the feedback of chatgpt thereby finding an appropriate prompt to guide chatgpt to generate in distribution code summaries.
rukmono et al.
address the unreliability of llms in performing reasoning by applying a chain of thought prompting strategy.
recently some studies have also investigated the applicability of parameter efficient fine tuning peft techniques in code summarization tasks.
in this paper we focus on uncovering the effectiveness of various prompting techniques in adapting llms to code summarization without fine tuning.
iii.
s tudy design a. research questions this study aims to answer the following research questions rq1 what evaluation methods are suitable for assessing the quality of summaries generated by llms?
existing research on llm based code summarization widely follow earlier studies and employ automated evaluation metrics e.g.
bleu to evaluate the quality of llm generated summaries.
however recent studies have shown that llm generated summaries surpass reference summaries in quality.
therefore evaluating llm generated summaries based on their text or semantic similarity to reference summaries may not be appropriate.
this rq aims to discover a suitable method for automated assessment of the quality of llm generated summaries.
rq2 how effective are different prompting techniques in adapting llms to the code summarization task?
this rq aims to unveil the effectiveness of several popular prompting techniques e.g.
few shot and chain of thought in adapting llms to code summarization tasks.
rq3 how do different model settings affect llms code summarization performance?
to better meet diverse user needs llms typically offer configurable parameters i.e.
model settings that allow users to control the randomness of model behaviour.
in this rq we adjust the randomness of thegenerated summaries by modifying llms parameters and see the impact of different model settings on the performance of llms in generating code summaries.
rq4 how do llms perform in summarizing code snippets written in different types of programming languages?
programming languages are diverse in types e.g.
object oriented and functional programming languages with their implementations of the same functional requirements being similar or entirely different.
the scale of programs implemented with them in internet open source repositories also varies which may result in differences in the mastery of knowledge of these languages by llms.
hence this rq aims to reveal the differences in llms capabilities to summarize code snippets across diverse programming language types.
rq5 how do llms perform on different categories of summaries?
previous research has shown that summaries can be classified into various categories according to developers intentions including what why how to use it how it is done property and others.
therefore in this rq we aim to explore the ability of llms to generate summaries of different categories.
b. experimental llms we select four llms as experimental representatives.
codellama instruct.
code llama is a family of llms for code based on llama .
it provides multiple flavors to cover a wide range of applications foundation models python specializations code llama python and instruction following models code llama instruct with 7b 13b 34b and 70b parameters.
in this study we evaluate the summaries generated by codellama instruct 7b.
we also verify the ability of codellama instruct 70b to act as an evaluator in rq1.
starchat .starchat is an llm with 16b parameters fine tuned on starcoderplus .
compared with starcoderplus starchat excels in chat based coding assistance.
gpt .
.
gpt .
is an llm provided by openai.
it is trained with massive texts and codes.
it can understand and generate natural language or code.
gpt .
gpt is an improved version of gpt .
which can solve difficult problems with greater accuracy.
openai has not disclosed the specific parameter scale of gpt .
and gpt .
our study uses gpt .
turbo and gpt preview.
model settings.
apart from rq3 where we investigate the impact of model settings we uniformly set the temperature to .
to minimize the randomness of llm s responses and highlight the impact of evaluation methods prompting techniques programming language types summary categories.
note that the llm generated summaries are relatively long and usually consist of several sentences.
by observing some examples we find that the first sentences are suitable as the final summaries and the following sentences elaborate on some details and supplementary explanations.
therefore we extract the first sentences as the final summaries.
3c.
prompting techniques we compare five commonly used prompting techniques below.
zero shot.
zero shot prompting adapts llms to downstream tasks using simple instructions.
in our scenario the input to llms consists of a simple instruction and a code snippet to be summarized.
we expect llms to output a natural language summary of the code snippet.
therefore we follow and adopt the input format please generate a short comment in one sentence for the following function code .
few shot.
few shot prompting also known as in context learning provides not only straightforward instruction but also some examples when adapting llms to downstream tasks.
the examples serve as conditioning for subsequent examples where we would like llms to generate a response.
in our scenario the examples are pairs of code snippet summary .
according to the findings of gao et al.
we set the number of examples to to achieve a balance between llms performance and the cost of calling the openai api.
chain of thought.
chain of thought prompting adapts llms to downstream tasks by providing intermediate reasoning steps .
these steps enable llms to possess complex reasoning capabilities.
in this study we follow wang et al.
and apply chain of thought prompting to the code summarization task through the following four steps instruction input the code snippet and five questions about the code in the format code n code question n q1 n q2 n q3 n q4 n q5 n get llms response to instruction i.e.
response .
instruction let s integrate the above information and generate a short comment in one sentence for the function.
get llms response to instruction i.e.
response .
response contains the comment generated by llms for the code snippet.
when asking instruction instruction and response are paired as history prompts and answers and input into the llm.
critique.
critique prompting improves the quality of llms answers by asking llms to find errors in the answers and correct them.
we follow kim et al.
and perform critique prompting on the code summarization task through the six steps below instruction similar to zero shot prompting input the instruction and the code snippet in the format please generate a short comment in one sentence for the following function n code get llms response to instruction i.e.
response .
response contains the temporary comment generated by llms for the code snippet.
instruction review your previous answer and find problems with your answer.
get llms response to instruction i.e.
response .
instruction based on the problems you found improve your answer.
get llms response to instruction i.e.
response .
response contains the modified comment which is the final comment of the code snippet.
when prompting each instruction previous instructions and responses are fed into the llms as pairs of history prompts and answers.
expert.
expert prompting first asks llms to generate a description of an expert who can complete the instruction e.g.
through few shot prompting and then the description serves as the system prompt for zero shot prompting.
we use the few shot examples provided by xu et al.
and employ few shot prompting to let llms generate a description of an expert who can generate a short comment in one sentence for a function.
this description will replace the default system prompt of llms.
by default we use the system prompt of codellama instruct for all llms to ensure fairness in comparison.
then we utilize the same steps as zero shot prompting to adapt llms to generate summaries.
d. experimental datasets the sources of the datasets utilized in our experiments include codesearchnet csn .
the codesearchnet corpus is a vast collection of methods accompanied by their respective comments written in go java javascript php python and ruby.
this corpus has been widely used in studying code summarization .
we use the clean version of the csn corpus provided by lu et al.
in codexglue.
we randomly select samples for each programming language from the test set of this corpus for experiments.
ccsd.
the ccsd dataset is provided by liu et al.
.
they crawl data from projects such as linux and redis.
the dataset contains function summary pairs.
similarly we randomly select samples from the final dataset for experiments.
in addition to the above two sources we construct three new language datasets to evaluate llm s code summarization capabilities across more programming language types.
erlang haskell and prolog datasets.
erlang and haskell are functional programming languages fp and prolog belongs to logic programming languages lp .
to construct the three datasets we sort the github repositories whose main language is erlang haskell prolog according to the number of stars and crawl data from the top repositories.
following husian et al.
we remove any projects that do not have a license or whose license does not explicitly permit the re distribution of parts of the project.
we consider the first sentence in the comment as the function summary.
we remove data where functions are shorter than three lines or comments containing less than tokens.
we remove functions whose names contain the substring test .
we remove duplicates by comparing the jaccard similarities of the functions following allamanis et al.
.
finally we get pairs of function summary pairs.
for each language we randomly select samples for experiments.
all in all our experiments involve programming languages across types.
note that considering that experiments 4table i datasets.
pp procedural programming languages oop object oriented programming languages sp scripting programming languages fp functional programming languages lp logic programming languages.
language source type usage java csn oop rq1 rq2 rq3 rq4 rq5 python csn sp rq1 rq2 rq3 rq4 c ccsd pp rq1 rq2 rq3 rq4 ruby csn sp rq4 php csn sp rq4 go csn pp rq4 javascript csn sp rq4 erlang by us fp rq4 haskell by us fp rq4 prolog by us lp rq4 with llms are resource intensive especially those involving gpt which are quite costly not all experiments are conducted on all programming language datasets.
specifically we first conduct experiments associated with rq1 and rq2 on commonly used programming languages including java python and c. analyzing the results of these two rqs helps find a suitable automated evaluation method and a suitable prompting technique.
subsequent experiments for other rqs can be built upon these findings thereby significantly reducing experimental costs.
we use all programming languages in the experiments for rq4.
in the experiments for rq5 we only use the java dataset because other programming languages lack readily available comment classifiers.
while training such classifiers would be valuable it falls outside the scope of this paper and is left for future exploration.
iv.
r esults and findings a. rq1 what evaluation methods are suitable for assessing the quality of summaries generated by llms?
experimental setup.
comparison evaluation methods.
existing automated evaluation methods for code summarization can be divided into the following three categories.
i. methods based on summary summary text similarity assess the quality of the generated summary by calculating the text similarity between the generated summary and the reference summary.
this category of methods is the most widely used in existing code summarization research .
the text similarity metrics involved include bleu meteor and rouge l which compare the count of n grams in the generated summary against the reference summary.
the scores of bleu meteor and rouge l are in the range of .
the higher the score the closer the generated summary approximates the reference summary indicating superior code summarization performance.
all scores are computed by the same implementation provided by .
ii.
methods based on summary summary semantic similarity evaluate the quality of the generated summary by computing the semantic similarity between the generated summary and the reference summary.
existing research demonstrates that semantic similarity based methods can effectively alleviate the issues of word overlap based metrics where not allwords in a sentence have the same importance and many words have synonyms.
in this study we compare four such methods including bertscore sentencebert with cosine similarity sbcs sentencebert with euclidean distance sbed and universal sentence encoder with cosine similarity usecs .
they are commonly used in code summarization studies .
bertscore uses a variant of bert we use the default roberta large to embed every token in the summaries and computes the pairwise inner product between tokens in the reference summary and generated summary.
then it matches every token in the reference summary and the generated summary to compute the precision recall and f1measure.
in our experiment we report thef1measure of bertscore.
the other three methods use a pre trained sentence encoder sentencebert or universal sentence encoder to produce vector representations of two summary sentences and then compute the cosine similarity or euclidean distance of the vector representations.
sbcs sbed and usecs range within .
higher values of sbcs and usecs represent greater similarity while lower values of sbed indicate greater similarity.
iii.
methods based on summary code semantic similarity assess the quality of the generated summary by computing the semantic similarity between the generated summary and the code snippet to be summarized.
unlike the first two methods this type of evaluation method does not rely on reference summaries and can effectively avoid issues related to lowquality and outdated reference summaries.
side proposed by mastropaolo et al.
is a representative of this type of method.
it is based on contrastive learning and has been trained to assess the relevance of a given textual summary for a java method.
note that it has not been trained on other language datasets.
hence in our experiment side is only used to evaluate the java dataset.
side provides a continuous score ranging within where a higher value represents greater similarity.
we present the scores reported by the above similarity based evaluation methods in percentage.
human evaluation.
we conduct human evaluations as a reference for automated evaluation methods.
comparing the correlation between the results of automated evaluation methods and human evaluation can facilitate achieving the goal of this rq which is to find a suitable automated method for assessing the quality of llm generated summaries.
to do so we invite volunteers including phd candidate masters and undergraduates with more than years of software development experience and excellent english ability to carry out the evaluation.
for each sample we provide volunteers with the code snippet the reference summary and summaries generated by four llms where the reference summary and the summaries generated by four llms are mixed and out of order.
in other words for each sample volunteers do not know whether it is a reference or a summary generated by a certain llm.
we follow shi et al.
and ask volunteers to rate the summaries from to based on their quality where a higher score represents a higher quality.
the final score of the summaries is the average of scores rated by volunteers.
5fig.
an example of using an llm as an evaluator.
llm based evaluation methods.
inspired by recent work in nlp we also investigate the feasibility of employing llms as evaluators.
its advantage is that it does not rely on the quality of reference summaries and the evaluation steps can be the same as human evaluation.
specifically similar to human evaluation when using llms as evaluators for each sample we input the code snippet to be summarized the reference summary and llm generated summaries and ask llms to rate each summary from to where a higher score represents a higher quality of the summary.
the specific prompt when using llms as evaluators is shown in figure .
datasets and prompting techniques.
in this rq to reduce the workload of human evaluation volunteers we randomly select samples from the java python and c datasets respectively which means samples in total.
we employ few shot prompting to adapt the four llms to generate summaries for code snippets as recent studies have demonstrated the effectiveness of this prompting technique on code summarization tasks.
experimental results.
human evaluation results.
table ii shows the human evaluation scores for reference summaries and summaries generated by the four llms.
observe that the scores of reference summaries in the three datasets are between and .
points suggesting that the quality of the reference summaries is not very high.
therefore evaluation methods based on summarysummary similarity may not accurately assess the quality of llm generated summaries.
among the four llms gpt has the highest scores on the java and c datasets and gpt .
attains the highest score on the python dataset.
this suggests that the quality of summaries generated by gpt .
and gpt is relatively high.
finding according to human evaluation the quality of reference summaries in the existing datasets is not particularly high.
summaries from general purpose llms e.g.
gpt .
excel over those from specialized code llms e.g.
codellama instruct in quality.
automated evaluation results.
table iii displays the scores of the llm generated summaries reported by three categories of automated evaluation methods and llm based evaluation methods.
observed that among the three methods based on summary summary text similarity the bleu basedtable ii human evaluation scores for reference and llmgenerated summaries.
the value in parentheses represents the percentage increase or decrease relative to the score of the corresponding reference summary.
summary fromhuman evaluation score java python c reference .
.
.
codellama instruct .
.
.
.
.
.
starchat .
.
.
.
.
.
gpt .
.
.
.
.
.
.
gpt .
.
.
.
.
.
and rouge l based methods give starchat the highest scores on all three datasets the meteor based method gives starchat the highest score i.e.
.
on the java dataset while gives codellama instruct the highest scores i.e.
.
and .
on the python and c datasets.
among the four methods based on summary summary semantic similarity bertscore sbcs and sbed give the best scores to starchat and usecs gives the best score of .
to codellama instruct on the java dataset.
on the python and c datasets the four methods consistently give the best scores to codellama instruct and starchat respectively.
the summary code semantic similarity based method side gives the highest score i.e.
.
to starchat on the java dataset.
on the java dataset the five llm based methods consistently give the highest scores to gpt .
while on the python dataset they consistently award the highest scores to gpt .
.
on the c dataset starchat gives the highest score to codellama instruct while other llms give the highest score to gpt .
finding according to automated evaluation overall methods based on summary summary text semantic similarity tend to give higher scores to specialized code llms starchat and codellama instruct while llmbased evaluators tend to give higher scores to generalpurpose llms gpt .
and gpt .
the summary code semantic similarity based method tends to give higher scores to starchat on the java dataset.
correlation between automated evaluation and human evaluation.
from table iii it can be observed that the average scores of reference summaries evaluated by the four llmbased methods are mostly below points.
it means that similar to human evaluation llm based evaluation methods also believe that the quality of the reference summaries is not very high.
besides llm based evaluation methods are inclined to give higher scores to general purpose llms gpt3.
and gpt which is the same as human evaluation.
based on the above observations we can reasonably speculate that compared to methods based on summary summary text semantic similarity and summary code semantic similarity llm based evaluation methods may be more suitable for evaluating the quality of summaries generated by llms.
therefore we follow and calculate spearman s 6table iii automated evaluation scores for reference and llm generated summaries.
s s tex.sim.
methods based on summary summary text similarity s s sem.sim.
methods based on summary summary semantic similarity s c sem.sim.
methods based on summary code semantic similarity.
codellama i codellama instruct.
we bold the best score in each column.
languagesummary froms s tex.sim.
s s sem.sim.
s c sem.sim.
llm based evaluation methodhuman bleu meteor rouge l bertscore sbcs sbed usecs side codellama i 7b codellama i 70b starchat gpt .
gpt javareference .
.
.
.
.
.
.
codellama i .
.
.
.
.
.
.
.
.
.
.
.
.
.
starchat .
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
pythonreference .
.
.
.
.
.
codellama i .
.
.
.
.
.
.
.
.
.
.
.
.
starchat .
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
creference .
.
.
.
.
.
codellama i .
.
.
.
.
.
.
.
.
.
.
.
.
starchat .
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
correlation coefficient with the p value between the results of each automated evaluation method and human evaluation providing more convincing evidence for this speculation.
the spearman s correlation coefficient is suitable for judging the correlation between two sequences of discrete ordinal continuous data with a higher value representing a stronger correlation .
and respectively indicate the presence of negative correlation no correlation and positive correlation .
the p value helps determine whether the observed correlation is statistically significant or simply due to random chance.
by comparing the pvalue to a predefined significance level typically .
we can decide whether to reject the null hypothesis and conclude that the correlation is statistically significant.
due to the page limit we present the statistical results of andp value in .
the results demonstrate that among all automated evaluation methods there is a significant positive correlation between the gpt4 based evaluation method and human evaluation in scoring the quality of summaries generated by most llms followed by the gpt .
based evaluation method.
for other automated evaluation methods in most cases their correlation with human evaluation is negative or weakly positive.
in a shot existing metrics are inadequate for llm generated summaries.
the main reason for this phenomenon lies in the low quality of reference summaries.
the reference summaries are short and detail lacking while llms can generate detailed useful summaries not fully covered by the reference summaries.
therefore the similarity with the reference summary cannot accurately reflect the quality of the llm generated summaries.
in addition bleu penalizes long summaries which also affects the correlation between bleu and human evaluation.
although side does not rely on reference summaries it is trained on reference summaries and is therefore also affected.
re training side on llm generated summaries requires highquality and extensive training data while collecting largescale llm generated summaries especially from llms like gpt is costly.
numerous factors influence llm generated summary quality complicating the collection of high quality 1private void setpeople final email emailwithdata final mimemessage msgtoset throws messagingexception 2msgtoset.setfrom emailwithdata.from .tointernetaddress 3msgtoset.setreplyto emailaddress.convert emailwithdata.replyto 4setrecipients emailwithdata msgtoset a a java code snippet c1 summary fromscoresummaryreference2.8sets from reply to and recipients.codellama i3.8set the from and reply to fields of the email message.starchat 3.4sets the to cc and bcc recipients for an email .gpt .
.0sets the sender reply to and recipients of the email message.gpt .4sets the sender reply to address and recipients on a mimemessage using data from an email object.
b reference summary and llm generated summaries for c1 fig.
an example of code the reference summary and llmgenerated summaries data.
based on the above observations we draw the conclusion that compared with other automated evaluation methods the gpt based method is more suitable for evaluating the quality of summaries generated by llms.
in the subsequent rqs we uniformly employ the gpt based method to assess the quality of llm generated summaries.
we set the temperature to default value when using gpt as the evaluator.
summary among all automated evaluation methods the gpt based method overall has the strongest correlation with human evaluation.
therefore it is recommended to adopt the gpt based method to evaluate the quality of llm generated summaries.
why do llms give such a score?
to investigate the scoring criteria of llms we ask them to give explanations for their evaluation scores.
the prompt we use is here is a piece of code with corresponding comments and your previous rate for each comment on a scale from to where a higher score indicates better quality.
please explain why you scored this way.
figure shows llms explanations of evaluation scores for the reference summary and generated summaries of the example in figure .
due to the page limit we only show the 7reference3this comment is conciseand provides a briefsummary of the code s purpose.
however it does not accuratelydescribe the code s functionality as it does not mentionthe recipients part.codellama i2this comment is not accurateand does not provide a clear descriptionof the code s purpose.
it does not mention the recipients part which is also set by the code.starchat 2this comment is not accurateand does not provide a clear descriptionof the code s purpose.
it does not mention the from and reply to fields which are also set by the code.gpt .52this comment is not accurateand does not provide a clear descriptionof the code s purpose.
it does not mention the from and reply to fields which are also set by the code.gpt 44this comment is accurateand provides aclear descriptionof the code s purpose.
it mentions all the important aspectsof the code s functionality.
a codellama instruct 70b s explanations reference3this comment is not very descriptiveand does not provide any contextabout what the code is it is also not clearwhat the from reply to part refers to.codellama i4this comment provides a clear descriptionof what the code is and provides some context.
however it could be more specificand include information about the type of email message that is being handled.starchat 3this comment is not very descriptiveand does not provide any contextabout what the code is it is also not clearwhat the to cc and bcc part refers to.gpt .54this comment provides a clearand concisedescription of what the code is however it could be more specificand include information about the type of email message that is being handled.gpt 44this comment provides a clear description of what the code is and provides some context.
however it could be more specificand include information about the email object that is being used.
b starchat s explanations reference2this comment is not very clearand seems to have some typos or missing words.
it could be improved by providing more context and clarifying the purpose of the code.codellama i1this comment is not accurateas it only mentions setting the from and reply to fields while the code also sets other recipients.
it could be improved by providing a more comprehensive descriptionof the method s functionality.starchat 3this comment is clearabout what the code does but lacks informationabout setting the sender and reply to fields.
it could be improved by mentioningall recipients being set in the method.gpt .54this comment is clearand provides a good overviewof the functionality of the code.
it could be slightly improved by specifying thatit is specifically for an email message.gpt 45this comment is clear concise and accuratelydescribes the purpose of the code.
it provides relevant information about what the method does and how it relates to the email object.
c gpt .
s explanations reference3this comment is somewhat vagueand lacks proper formatting and clarity e.g.
from reply to is not a standard way to refer to email fields .
it does not mentionthat the method is setting these fields based on an email object nor does it specifythat it s for a mimemessage .
however it does correctly identifythe three main actions of the method hence the mid range score.codellama i2this comment correctly identifiestwo of the three actions performed by the method setting the from and reply to fields but it omits the fact thatthe method also sets recipients.
additionally it does not specifythat the settings are for a mimemessage or that they are based on an email object.
the omission and lack of detail result in a lower rating.starchat 1this comment is misleadingbecause it incorrectly states the method s functionality.
the method sets the from and reply to fields not the to cc and bcc recipients.
this inaccuracy justifies the lowest rating.gpt .54this comment is conciseand accuratelydescribes the main actions of the method.
however it does not mentionthat the settings are for a mimemessage or that they are based on an email object which makes it slightly less informativethan comment resulting in a slightly lower rating.gpt 45this comment is clear informative and accuratelydescribes the method s functionality.
it specifies that the method sets the sender reply to and recipients and it also clarifies that these are being set for a mimemessage based on a provided email object.
this level of detail and clarity justifies the highest rating.
d gpt s explanations fig.
llms explanations of evaluation scores for the reference summary and llm generated summaries of c1 results of codellama instruct 70b which performs better instead of codellama instruct 7b .
observe that llm s scoring criteria include accuracy clarity informativeness and conciseness.
accuracy focuses on whether there is misleading information.
clarity focuses on whether the expression of a sentence is easy to understand.
for example the reference summary is evaluated by gpt as not clear because from reply to is not a standard way to refer to email fields .
informativeness focuses on whether the summary contains enough details.
as for conciseness since the summaries generated by gpt .
and gpt are also evaluated as concise we can infer that llms definition of concise is not equal to being as brief as the reference summary and the llmgenerated summaries are also considered concise.
therefore the main factors affecting llms scoring are accuracy clarity table iv effectiveness of different prompting techniques model prompting technique java python c codellama instructzero shot .
.
.
few shot .
.
.
chain of thought .
.
.
critique .
.
.
expert .
.
.
starchat zero shot .
.
.
few shot .
.
.
chain of thought .
.
.
critique .
.
.
expert .
.
.
gpt .5zero shot .
.
.
few shot .
.
.
chain of thought .
.
.
critique .
.
.
expert .
.
.
gpt 4zero shot .
.
.
few shot .
.
.
chain of thought .
.
.
critique .
.
.
expert .
.
.
and informativeness.
that is whether there is misleading information whether the expression is easy to understand and whether the summary contains enough details.
this also explains why llms do not give high scores to reference summaries.
reference summaries are generally short do not cover every detail in the code snippet and may contain terms in the code snippet which increases the difficulty of understanding.
however although the scoring criteria of llms are similar the scoring results are still different.
this is because llms capabilities to understand code and summaries are different.
as the sota llm gpt achieves the best performance.
it can accurately identify whether each summary contains all the details while other llms may miss some of the details according to their explanation.
in addition gpt does a better job in judging whether the summary is clear as it points out that the phrase to cc and bcc is not easy to understand while gpt .
and codellama instruct 70b does not point out this problem.
finding the main factors affecting llms scoring are accuracy clarity and informativeness.
gpt evaluation results have stronger correlation to human evaluation than other llms owing to its better understanding of code and summaries.
b. rq2 how effective are different prompting techniques in adapting llms to the code summarization task?
experimental setup.
the experimental dataset comprises samples from java python and c datasets collectively.
experimental results.
table iv presents the scores reported by the gpt evaluation method for summaries generated by four llms using five prompting techniques.
observe that when the base model is codellama instruct few shot prompting consistently performs best on all three datasets.
when the base model is starchat chain of thought prompting performs best on all the java and c datasets while expert prompting excels on the python dataset.
when selecting gpt3.
as the base model the simplest zero shot prompting 8surprisingly achieves the highest scores on the java and c datasets and is only slightly worse than few shot prompting on the python dataset.
when using gpt as the base model chain of thought prompting overall performs best.
for the specific llm and programming language there is no guarantee that intuitively more advanced prompting techniques will surpass simple zero shot prompting.
for example on the java dataset when selecting any of starchat gpt .
and gpt as the base model few shot prompting yields lower scores than zero shot prompting.
contrary to the findings of previous studies the gpt based evaluation method does not consider that few shot prompting will improve the quality of generated summaries.
this discrepancy may arise because previous studies evaluated the quality of llm generated summaries using bleu meteor and rouge l which primarily assess text semantic similarity with reference summaries.
however as we mentioned in section iv a reference summaries contain low quality noisy data that undermines their reliability.
therefore achieving greater similarity with reference summaries does not necessarily imply that the human gpt based evaluation method will perceive the summary to be of higher quality.
summary the more advanced prompting techniques expected to perform better may not necessarily outperform simple zero shot prompting.
in practice selecting the appropriate prompting technique requires considering the base llm and the programming language.
c. rq3 how do different model settings affect llms code summarization performance?
experimental setup.
there are three key model settings parameters including top k top p and temperature that allow the user to control the randomness of text code summary in our scenario generated by llms.
considering that gpt .
and gpt do not support the top k setting we only conduct experiments with the top p and temperature.
top p in each round of token generation llms sort tokens by probability from high to low and keep tokens whose probability adds up to no more than top p. for example topp .1means only the tokens comprising the top probability mass are considered.
the larger the top p is the more tokens are sampled.
thus tokens with low probabilities have a greater chance of being selected so the summary generated by llms is more random.
temperature temperature adjusts the probability of tokens after top p sampling.
the higher the temperature the less the difference between the adjusted token probabilities.
therefore the token with a low probability has a greater chance of being selected so the generated summary is more random.
if the temperature is set to the generated summary is the same every time.
top p and temperature are alternatives and one should only modify one of the two parameters at a time .
therefore the questions we want to answer are does top p temperature impact the quality of llm generated summaries?
as alter table v influence of different model settings.
we bold the scores of the best setting combinations on each dataset.
model top p temperature java python c codellama instruct0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
starchat 0. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
native parameters that both control the randomness of llms do top p and temperature have a difference in the degree of influence on the quality of llm generated summaries?
drawing from a review of related work see section ii we find that existing llm based code summarization studies pay more attention to few shot prompting.
since no prompting technique outperforms others on all llms we uniformly employ few shot prompting in rq3 rq4 and rq5 to facilitate comparing our findings with prior studies.
experimental results.
table v shows the scores evaluated by the gpt evaluation method for the summaries generated by llms under different top p and temperature settings.
it is observed that the impact of top p and temperature on the quality of llm generated summaries is specific to the base llm and programming language.
for example when topp .
as temperature increases the quality of gpt4 generated summaries for python code snippets increases while those for c code snippets decrease.
another example is that when top p .
as the temperature rises the quality of gpt generated java comments first increases and then decreases whereas codellama instruct is exactly the opposite first decreases and then increases.
regarding the difference in 9table vi effectiveness of llms in summarizing code snippets written in different types of programming languages.
codellama i codellama instruct.
modeloop pp sp fp lp java c go python ruby php javascript erlang haskell prolog codellama i .
.
.
.
.
.
.
.
.
.
starchat 2. .
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
influence between top p and temperature it is observed that in most cases the influence of the two parameters is similar.
for example for c code snippets when one parameter top p or temperature is fixed as the other parameter temperature or top p grows the quality of gpt .
generated summaries first decreases and then increases.
summary the impact of top p and temperature on the quality of generated summaries is specific to the base llm and programming language.
as alternative parameters top p and temperature have similar influence on the quality of llm generated summaries.
the impact of top p and temperature on gpt is small.
d. rq4 how do llms perform in summarizing code snippets written in different types of programming languages?
experimental setup.
we conduct experiments on all programming language datasets.
as in rq3 we uniformly employ few shot prompting to adapt llms.
experimental results.
table vi shows the performance evaluated by the gpt evaluation method for the four llms on five types of programming languages.
it is observed that for oop i.e.
java gpt performs best followed by codellama instruct gpt .
and starchat .
for pp gpt4 performs best on both c and go while starchat performs worst on both.
the smallest llm codellama instruct outperforms gpt .
on c .
vs. .
but vice versa on go .
vs. .
.
additionally except for codellamainstruct which performs slightly worse on go than on c .
vs. .
the other three llms perform better on go than on c. for sp gpt consistently performs best on all four languages.
surprisingly codellama instruct outperforms gpt .
on both ruby and javascript.
all four llms perform better on php than on python.
for fp the performance of two specialized code llms i.e.
codellama instruct and starchat is better on haskell than on erlang while the opposite is true for the two general purpose llms i.e.
gpt3.
and gpt .
for lp gpt still performs best followed by gpt .
codellama instruct and starchat .
across all five types of languages the four llms consistently perform the worst on lp which indicates that summarizing logic programming language code is the most challenging.
one possible reason is that fewer prolog datasets are available for training these llms compared to other programming languages.
the scale of the prolog dataset we collected can support this reason.table vii statistics of six sub datasets divided from the csn java test dataset according to comment intention summary category number of samples sample ratio what .
why .
how it is done .
property .
how to use .
others .
summary gpt surpasses the other three llms on all five types of programming languages.
for pp llms overall perform better on go than on c. for sp all four llms perform better on php than on python.
for fp specialized code llms e.g.
starchat perform better on haskell than on erlang whereas the reverse is true for general purpose llms e.g.
gpt .
all four llms perform worse in summarizing lp code snippets.
e. rq5 how do llms perform on different categories of summaries?
experimental setup.
following we classify code summaries into the following six categories.
what describes the functionality of the code snippet.
it helps developers to understand the main functionality of the code without diving into implementation details.
an example is pushes an item onto the top of this stack .
why explains the reason why the code snippet is written or the design rationale of the code snippet.
it is useful when methods objective is masked by complex implementation.
an application scenario of why summaries is to explain the design rationale of overloaded functions.
how it is done describes the implementation details of the code snippet.
such information is critical for developers to understand the subject especially when the code complexity is high.
for instance shifts any subsequent elements to the left.
is a how it is done comment.
property asserts properties of the code snippet e.g.
function s pre conditions post conditions.
this method is not a constant time operation.
is a property summary.
how to use describes the expected set up of using the code snippet such as platforms and compatible versions.
for example this method can be called only once per call to next .
is ahow to use summary.
others comments that do not fall into the above five categories are classified as others summaries such as the implementation is awesome.
.
following mu et al.
we consider the code summary pairs with others comments as noisy data and remove them if identified.
we employ the comment classifier coin provided by mu et al.
to classify the csn java dataset according to the comment intention type.
the test dataset is divided into six sub datasets as shown in table vii.
to facilitate comparison between different categories we randomly select samples from each sub dataset.
as in rq4 we uniformly employ fewshot prompting to adapt llms.
for each sub dataset with 10table viii effectiveness of llms in generating different categories of summaries model what why how it is done property how to use codellama instruct .
.
.
.
.
starchat .
.
.
.
.
gpt .
.
.
.
.
.
gpt .
.
.
.
.
different intention types the few shot example is of the same intention type from the training dataset.
experimental results.
table viii presents the results evaluated by the gpt evaluation method for the four llms in generating five categories of summaries.
observe that codellama instruct performs worse in generating how it is done summaries than generating the other four categories of summaries.
starchat gets the lowest score of .
in generating how to use summaries.
both gpt .
and gpt are not as good at generating property summaries compared to generating other categories of summaries.
surprisingly the smallest llm codellama instruct slightly outperforms the advanced gpt in generating why .
vs. .
and property .
vs. .
summaries.
additionally compared with gpt .
codellama instruct achieves higher scores in generating what why and property summaries.
certainly it is undeniable that the reason for this phenomenon is that the optimal prompting technique for gpt .
and gpt4 is not few shot prompting.
this phenomenon is also exciting because it implies that most ordinary developers or teams who lack sufficient resources e.g.
gpus have the opportunity to utilize open source and small scale llms to achieve code summarization capabilities close to or even surpass those of commercial gigantic llms.
summary the four llms excel in generating different categories of summaries.
the smallest codellamainstruct slightly outperforms the advanced gpt in generating why andproperty summaries.
starchat is not proficient at generating how to use summaries.
gpt3.
and gpt perform worse in generating property summaries than other categories of summaries.
v. t hreats to validity our empirical study may contain several threats to validity that we have attempted to relieve.
threats to external validity.
the threats to external validity lie in the generalizability of our findings.
one threat to the validity of our study is that llms usually generate varied responses for identical input across multiple requests due to their inherent randomness while conclusions drawn from random results may be misleading.
to mitigate this threat considering that starchat and codellama instruct do not support setting the temperature to we uniformly set it to .
to reduce randomness except for rq3.
in rq2 rq5 to make the evaluation scores more deterministic we set the temperature to when using gpt as the evaluator.
addi tionally for other rqs we conduct experiments on multiple programming languages to support our findings.
threats to internal validity.
a major threat to internal validity is the potential mistakes in the implementation of metrics and models.
to mitigate this threat we use the publicly available code from previous studies for bleu meteor rouge l and side.
for coin bertscore sentencebert universal sentence encoder starchat and codellama instruct and gpt .
gpt we use the script provided along with the model to run.
another threat lies in the processing of llm s responses.
usually the output of llms is a paragraph not a sentence of code summary code comment that we want.
the real code summary may be the first sentence in the llms response or it may be returned in the comment before the code such as code summary etc.
therefore we designed a series of heuristic rules to extract the code summary.
we have made our script for extracting code summaries from llms responses public for the community to review.
vi.
c onclusion in this paper we provide a comprehensive study covering multiple aspects of code summarization in the era of llms.
our interesting and significant findings include but are not limited to the following aspects.
compared with existing automated evaluation methods the gpt based evaluation method is more fitting for assessing the quality of llmgenerated summaries.
the advanced prompting techniques anticipated to yield superior performance may not invariably surpass the efficacy of straightforward zero shot prompting.
the two alternative model settings have a similar impact on the quality of llm generated summaries and this impact varies by the base llm and programming language.
llms exhibit inferior performance in summarizing lp code snippets.
codellama instruct with 7b parameters demonstrates superior performance over the advanced gpt in generating why and property summaries.
our comprehensive research findings will aid subsequent researchers in swiftly grasping the various facets of llm based code summarization thereby promoting the development of this field.
acknowledgment the authors would like to thank the anonymous reviewers for their insightful comments.
this work is supported by the national research foundation singapore and dso national laboratories under the ai singapore programme aisg award no aisg2 gc the national research foundation singapore and the cyber security agency under its national cybersecurity r d programme ncrp25p04 taicen and the national natural science foundation of china .
any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not reflect the views of the national research foundation singapore and cyber security agency of singapore.
chunrong fang is the corresponding author.
11references s. n. woodfield h. e. dunsmore and v .
y .
shen the effect of modularization and comments on program comprehension in proceedings of the 5th international conference on software engineering .
san diego california usa ieee computer society march pp.
.
s. c. b. de souza n. anquetil and k. m. de oliveira a study of the documentation essential to software maintenance in proceedings of the 23rd annual international conference on design of communication documenting designing for pervasive information .
coventry uk acm september pp.
.
j. zhai x. xu y .
shi g. tao m. pan s. ma l. xu w. zhang l. tan and x. zhang cpc automatically classifying and propagating natural language comments via program analysis in proceedings of the 42nd international conference on software engineering .
seoul south korea acm june july pp.
.
w. sun c. fang y .
chen q. zhang g. tao y .
you t. han y .
ge y .
hu b. luo and z. chen an extractive and abstractive framework for source code summarization acm transactions on software engineering and methodology vol.
just accepted no.
pp.
.
m. l. v asquez b. li c. vendome and d. poshyvanyk how do developers document database usages in source code?
n in proceedings of the 30th international conference on automated software engineering .
lincoln ne usa ieee computer society november pp.
.
f. wen c. nagy g. bavota and m. lanza a large scale empirical study on code comment inconsistencies in proceedings of the 27th international conference on program comprehension .
montreal qc canada ieee acm may pp.
.
x. hu x. xia d. lo z. wan q. chen and t. zimmermann practitioners expectations on automated code comment generation inproceedings of the 44th international conference on software engineering .
pittsburgh pa usa acm may pp.
.
e. shi y .
wang l. du j. chen s. han h. zhang d. zhang and h. sun on the evaluation of neural code summarization in proceedings of the 44th international conference on software engineering .
pittsburgh usa ieee may pp.
.
a. mastropaolo m. ciniselli m. di penta and g. bavota evaluating code summarization techniques a new metric and an empirical characterization arxiv e prints pp.
arxiv .
w. sun c. fang y .
you y .
chen y .
liu c. wang j. zhang q. zhang h. qian w. zhao et al.
a prompt learning framework for source code summarization arxiv preprint arxiv .
.
m. du f. he n. zou d. tao and x. hu shortcut learning of large language models in natural language understanding communications of the acm vol.
no.
pp.
.
a. fan b. gokkaya m. harman m. lyubarskiy s. sengupta s. yoo and j. m. zhang large language models for software engineering survey and open problems arxiv preprint arxiv .
.
x. hou y .
zhao y .
liu z. yang k. wang l. li x. luo d. lo j. grundy and h. wang large language models for software engineering a systematic literature review arxiv preprint arxiv .
.
t. ahmed and p. t. devanbu few shot training llms for projectspecific code summarization in proceedings of the 37th international conference on automated software engineering .
rochester mi usa acm october pp.
.
c. wang y .
yang c. gao y .
peng h. zhang and m. r. lyu no more fine tuning?
an experimental evaluation of prompt tuning in code intelligence in proceedings of the 30th joint european software engineering conference and symposium on the foundations of software engineering .
singapore singapore acm november pp.
.
w. sun c. fang y .
you y .
miao y .
liu y .
li g. deng s. huang y .
chen q. zhang h. qian y .
liu and z. chen automatic code summarization via chatgpt how far are we?
corr vol.
abs .
pp.
.
m. geng s. wang d. dong h. wang g. li z. jin x. mao and x. liao large language models are few shot summarizers multi intent comment generation via in context learning in proceedings of the 46th international conference on software engineering .
lisbon portugal acm april pp.
.
s. gao w. mao c. gao l. li x. hu x. xia and m. r. lyu learning in the wild towards leveraging unlabeled data for effectively tuning pre trained code models in proceedings of the 46th international conference on software engineering .
lisbon portugal acm april pp.
.
s. gao x. wen c. gao w. wang h. zhang and m. r. lyu what makes good in context demonstrations for code intelligence tasks with llms?
in proceedings of the 38th international conference on automated software engineering .
luxembourg ieee september pp.
.
h. wu h. zhao and m. zhang code summarization with structureinduced transformer in proceedings of the findings of the 59th annual meeting of the association for computational linguistics .
online event association for computational linguistics august pp.
.
x. hu g. li x. xia d. lo s. lu and z. jin summarizing source code with transferred api knowledge in proceedings of the 27th international joint conference on artificial intelligence .
stockholm sweden ijcai.org july pp.
.
k. papineni s. roukos t. ward and w. zhu bleu a method for automatic evaluation of machine translation in proceedings of the 40th annual meeting of the association for computational linguistics .
philadelphia pa usa acl july pp.
.
s. banerjee and a. lavie meteor an automatic metric for mt evaluation with improved correlation with human judgments in proceedings of the workshop on intrinsic and extrinsic evaluation measures for machine translation and or summarization .
ann arbor michigan usa association for computational linguistics june pp.
.
c. y .
lin rouge a package for automatic evaluation of summaries inproceedings of the 42nd annual meeting of the association for computational linguistics workshop on text summarization branches out.
barcelona spain association for computational linguistics july pp.
.
s. haque z. eberhart a. bansal and c. mcmillan semantic similarity metrics for evaluating source code summarization in proceedings of the 30th international conference on program comprehension .
virtual event acm may pp.
.
j. wang y .
liang f. meng h. shi z. li j. xu j. qu and j. zhou is chatgpt a good nlg evaluator?
a preliminary study corr vol.
abs .
no.
pp.
.
i. vykopal m. pikuliak i. srba r. moro d. macko and m. bielikova disinformation capabilities of large language models arxiv preprint arxiv .
.
y .
liu d. iter y .
xu s. wang r. xu and c. zhu g eval nlg evaluation using gpt with better human alignment in proceedings of the 28th conference on empirical methods in natural language processing .
singapore association for computational linguistics december pp.
.
w. sun y .
miao y .
li h. zhang c. fang y .
liu g. deng y .
liu and z. chen artifacts of this study site llm4codesummarization accessed .
x. hu g. li x. xia d. lo and z. jin deep code comment generation with hybrid lexical and syntactical information empirical software engineering vol.
no.
pp.
.
s. haiduc j. aponte and a. marcus supporting program comprehension with source code summarization in proceedings of the 32nd international conference on software engineering .
cape town south africa acm may pp.
.
d. bahdanau k. cho and y .
bengio neural machine translation by jointly learning to align and translate in proceedings of the 3rd international conference on learning representations .
san diego ca usa openreview.net may pp.
.
k. cho b. van merrienboer d. bahdanau and y .
bengio on the properties of neural machine translation encoder decoder approaches inproceedings of eighth workshop on syntax semantics and structure in statistical translation .
doha qatar association for computational linguistics october pp.
.
w. u. ahmad s. chakraborty b. ray and k. chang a transformerbased approach for source code summarization in proceedings of the 58th annual meeting of the association for computational linguistics .
online association for computational linguistics july pp.
.
d. gros h. sezhiyan p. devanbu and z. yu code to comment translation data metrics baselining evaluation in proceedings of the 35th international conference on automated software engineering .
melbourne australia ieee september pp.
.
j. zhang x. wang h. zhang h. sun and x. liu retrieval based neural source code summarization in proceedings of the 42nd international conference on software engineering .
seoul south korea acm june july pp.
.
d. fried a. aghajanyan j. lin s. wang e. wallace f. shi r. zhong s. yih l. zettlemoyer and m. lewis incoder a generative model for code infilling and synthesis in proceedings of the 11th international conference on learning representations .
kigali rwanda openreview.net may pp.
.
s. lu d. guo s. ren j. huang a. svyatkovskiy a. blanco c. b. clement d. drain d. jiang d. tang g. li l. zhou l. shou l. zhou m. tufano m. gong m. zhou n. duan n. sundaresan s. k. deng s. fu and s. liu codexglue a machine learning benchmark dataset for code understanding and generation in proceedings of the neural information processing systems track on datasets and benchmarks virtual december pp.
.
c. su and c. mcmillan distilled gpt for source code summarization automated software engineering vol.
no.
p. .
t. a. andkunal suresh pai p. devanbu and e. t. barr automatic semantic augmentation of language model prompts for code summarization in proceedings of the 46th international conference on software engineering .
lisbon portugal acm april pp.
.
s. a. rukmono l. ochoa and m. r. chaudron achieving high level software component summarization via hierarchical chain of thought prompting and static code analysis in proceedings of the international conference on data and software engineering .
toba indonesia ieee september pp.
.
y .
choi and j. lee codeprompt task agnostic prefix tuning for program and language generation in proceedings of the findings of the 61st association for computational linguistics .
toronto canada association for computational linguistics july pp.
.
q. chen x. xia h. hu d. lo and s. li why my code summarization model does not work code comment improvement with category prediction acm transactions on software engineering and methodology vol.
no.
pp.
.
f. mu x. chen l. shi s. wang and q. wang developer intent driven code comment generation in proceedings of the 45th international conference on software engineering .
melbourne australia ieee may pp.
.
b. roziere j. gehring f. gloeckle s. sootla i. gat x. e. tan y .
adi j. liu t. remez j. rapin et al.
code llama open foundation models for code arxiv preprint arxiv .
.
h. touvron l. martin k. stone p. albert a. almahairi y .
babaei n. bashlykov s. batra p. bhargava s. bhosale et al.
llama open foundation and fine tuned chat models arxiv preprint arxiv .
.
l. tunstall n. lambert n. rajani e. beeching t. le scao l. von werra s. han p. schmid and a. rush creating a coding assistant with starcoder hugging face blog bigcode starcoderplus hugging face blog openai openai api site accessed .
j. wei x. wang d. schuurmans m. bosma b. ichter f. xia e. h. chi q. v .
le and d. zhou chain of thought prompting elicits reasoning in large language models in proceedings of the 36th annual conference on neural information processing systems .
new orleans la usa curran associates inc. november december pp.
.
y .
wang z. zhang and r. wang element aware summarization with large language models expert aligned evaluation and chain of thought method in proceedings of the 61st annual meeting of the association for computational linguistics .
toronto canada association for computational linguistics july pp.
.
g. kim p. baldi and s. mcaleer language models can solve computer tasks in proceedings of the 37th annual conference on neural information processing systems vol.
.
new orleans la usa curran associates inc. december pp.
.
b. xu a. yang j. lin q. wang c. zhou y .
zhang and z. mao expertprompting instructing large language models to be distinguished experts corr vol.
abs .
no.
pp.
.
codellama application of codellama site co spaces codellama codellama 13b chat blob main app.py accessed .
h. husain h. wu t. gazit m. allamanis and m. brockschmidt codesearchnet challenge evaluating the state of semantic code search corr vol.
abs .
.
d. guo s. lu n. duan y .
wang m. zhou and j. yin unixcoder unified cross modal pre training for code representation in proceedings of the 60th annual meeting of the association for computational linguistics .
dublin ireland association for computational linguistics may pp.
.
d. wang b. chen s. li w. luo s. peng w. dong and x. liao one adapter for all programming languages?
adapter tuning for code search and summarization in proceedings of the 45th international conference on software engineering .
melbourne australia ieee may pp.
.
s. liu y .
chen x. xie j. k. siow and y .
liu retrieval augmented generation for code summarization via hybrid gnn in proceedings of the 9th international conference on learning representations .
virtual event austria openreview.net may pp.
.
m. allamanis the adverse effects of code duplication in machine learning models of code in proceedings of the acm sigplan international symposium on new ideas new paradigms and reflections on programming and software pp.
.
t. zhang v .
kishore f. wu k. q. weinberger and y .
artzi bertscore evaluating text generation with bert in proceedings of the 8th international conference on learning representations .
addis ababa ethiopia openreview.net april pp.
.
d. cer y .
yang s. y. kong n. hua n. limtiaco r. s. john n. constant m. guajardo cespedes s. yuan c. tar et al.
universal sentence encoder arxiv preprint arxiv .
.
d. roy s. fakhoury and v .
arnaoudova reassessing automatic evaluation metrics for code summarization tasks in proceedings of the 29th joint european software engineering conference and symposium on the foundations of software engineering .
athens greece acm august pp.
.
y .
zhang y .
liu x. fan and y .
lu retcom information retrievalenhanced automatic source code summarization in proceedings of the 22nd international conference on software quality reliability and security .
guangzhou china ieee december pp.
.
j. devlin m. chang k. lee and k. toutanova bert pre training of deep bidirectional transformers for language understanding in proceedings of the 23th conference of the north american chapter of the association for computational linguistics human language technologies .
minneapolis mn usa association for computational linguistics june pp.
.
n. reimers and i. gurevych sentence bert sentence embeddings using siamese bert networks in proceedings of the the 9th international joint conference on natural language processing .
hong kong china association for computational linguistics november pp.
.
w. j. conover practical nonparametric statistics .
john wiley sons vol.
.
c. p. dancey and j. reidy statistics without maths for psychology .
pearson education .
openai create chat completion site docs api reference chat create accessed .
l. tunstall n. lambert n. rajani e. beeching t. le scao l. von werra s. han p. schmid and a. rush starchat beta site https huggingface.co huggingfaceh4 starchat beta accessed .
pcuenq usage of codellama site codellama codellama 13b chat blob main app.py accessed .
openai get up and running with the openai api site openai.com docs quickstart?context python accessed .