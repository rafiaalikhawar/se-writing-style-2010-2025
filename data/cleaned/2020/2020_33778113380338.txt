on the efficiency of test suite based program repair a systematic assessment of automated repair systems for java programs kui liu brucekuiliu gmail.com nanjing university of aeronautics and astronautics chinashangwen wang wangshangwen13 nudt.edu.cn national university of defense technology chinaanil koyuncu kisub kim anil.koyuncu kisub.kim uni.lu university of luxembourg luxembourg tegawend f. bissyand tegawende.bissyande uni.lu university of luxembourg luxembourgdongsun kim darkrsw furiosa.ai furiosa.ai republic of koreapeng wu wupeng15 nudt.edu.cn national university of defense technology china jacques klein jacques.klein uni.lu university of luxembourg luxembourgxiaoguang mao xgmao nudt.edu.cn national university of defense technology chinayves le traon yves.letraon uni.lu university of luxembourg luxembourg abstract test based automated program repair has been a prolific field of research in software engineering in the last decade.
many approaches have indeed been proposed which leverage test suites as a weak but affordable approximation to program specifications.
although the literature regularly sets new records on the number of benchmark bugs that can be fixed several studies increasingly raise concerns about the limitations and biases of state of the art approaches.
for example the correctness of generated patches has been questioned in a number of studies while other researchers pointed out that evaluation schemes may be misleading with respect to the processing of fault localization results.
nevertheless there is little work addressing the efficiency of patch generation with regard to the practicality of program repair.
in this paper we fill this gap in the literature by providing an extensive review on the efficiency of test suite based program repair.
our objective is to assess the number of generated patch candidates since this information is correlated to the strategy to traverse the search space efficiently in order to select sensical repair attempts the strategy to minimize the test effort for identifying a plausible patch as well as the strategy to prioritize the generation of a correct patch.
to that end we perform also with university of luxembourg.
co first author and corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .. .
.
large scale empirical study on the efficiency in terms of quantity of generated patch candidates of the open source repair tools for java programs.
the experiments are carefully conducted under the same fault localization configurations to limit biases.
eventually among other findings we note that many irrelevant patch candidates are generated by changing wrong code locations however if the search space is carefully triaged fault localization noise has little impact on patch generation efficiency yet current template based repair systems which are known to be most effective in fixing a large number of bugs are actually least efficient as they tend to generate majoritarily irrelevant patch candidates.
ccs concepts software and its engineering software verification and validation software defect analysis software testing and debugging.
keywords patch generation program repair efficiency empirical assessment.
acm reference format kui liu shangwen wang anil koyuncu kisub kim tegawend f. bissyand dongsun kim peng wu jacques klein xiaoguang mao and yves le traon.
.
on the efficiency of test suite based program repair a systematic assessment of automated repair systems for java programs.
in42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa 13pages.
introduction in the last decade automated program repair apr has extensively grown as a prominent research topic in the software engineering community.
figure 1overviews the research activities of this topic.
the associated literature includes a broad ieee acm 42nd international conference on software engineering icse range of techniques that use heuristics e.g.
via random mutation operations constraints solving e.g.
via symbolic execution or machine learning e.g.
via building a code transformation model to drive patch generation.
a living review of automated program repair research appears in which shows that the research in this field has been revived with the seminal work ten years ago of weimer et al.
on generate and validate approaches.
patches are generated to be applied on a buggy program until the patched program meets the desired behaviour.
in the absence of formal specifications of the desired behaviour test suites are leveraged as affordable partial specifications for validating generated patches.
over the years the community has incrementally advanced the state of the art with numerous test based approaches that have been shown effective in generating valid patches for a significant fraction of defects within well established benchmarks .
publications e.g.
genprog autofi x afix axis par rsrepair kali astor acs simfix tbar figure apr research publications since .
several studies have revisited the constraints and performance of program repair systems and have thus contributed to shaping research directions towards improving the state of the art.
for example qi et al.
have early shown that repair tools generate mostly overfitting patches i.e.
patches that pass the incomplete test suites but are actually incorrect.
their study led to assessment results being now carefully presented in a way that highlights the capability of new approaches to correctly repair programs.
motwani et al.
then questioned whether state of the art approaches can deal with hard and important bugs.
liu et al.
recently revealed significant biases with fault localization configurations in apr system evaluations.
more recently durieux et al.
have shown that state of the art tools may actually be overfitting the associated study benchmarks.
performance measurement of repair systems has evolved to progressively consider the number of correctly fixed bugs or the diversity of benchmark bugs that are fixed.
another performance aspect that deserves investigation is the efficiency of the patch generation system.
it is however mentioned in only a few assessment reports .
yet efficiency is a key property for bringing program repair into general use within practitioners settings.
indeed apr aims to alleviate the manual effort involved in resolving software bugs and holds this promise in two scenarios in production it is expected to drastically reduce the time to fix delays and minimize downtime in a development cycle apr can help suggest changes to accelerate debugging.
yet until now literature approaches have mainly focused on highlighting the increased performance on eventually fixing more and more benchmark bugs.
in recent work ghanbari et al.
raised the efficiency issue and built on the time cost criterion to demonstrate the efficiency of their prapr tool which does not require re compiling 1data are extracted from monperrus s living review on apr .source code .
this criterion which was already mentioned in a few of the previous work however has limitations with respect to generalizability cf.
section execution time is dependent on many variables that are unrelated to the approach implemented in the repair system and is generally unstable.
we postulate that the efficiency of test based program repair should be assessed along with the following question how many attempts does the repair system make before catching a valid patch?
in previous work qi et al.
have formulated this question into a metric that served to assess the effectiveness of fault localization techniques in a platform agnostic manner.
to the best of our knowledge little attention has been paid to measuring repair efficiency by estimating the number of validated patch candidates.
in this paper we report on the results of a large scale empirical study on the efficiency of test based program repair systems.
our study considers apr systems targeting java programs and performs a systematic assessment under identical and controlled fault localization configurations.
the objective of this work is to contribute a comprehensive analysis of repair efficiency to the literature with respect to generated patches for a large spectrum of apr systems.
eventually we gather insights on how the strategies of approaches in the literature affect repair efficiency.
overall we mainly find that f0 so far efficiency is not a widely valued performance target.
we found that state of the art apr tools are the least efficient.
this calls for an industry investigation of the impact of efficiency on adoption or lack thereof .
f1 across time repair tools subsume each other in terms of which benchmark bugs can be fixed.
unfortunately effectiveness i.e.
how many bugs are eventually fixed is increased at the expense of efficiency i.e.
how many repair attempts are made before a given bug is fixed .
f2 template based repair systems are generally inefficient as they produce too many patch candidates.
however when the templates are mined from clean datasets or are specialized to specific bugs efficiency can be substantially improved.
f3 literature approaches develop a few strategies such as constraint solving or donor code search which contribute to drastically reducing the nonsensical or in plausible patches.
f4 apr systems that implement random search over the repair search space require large sets of patch candidates to increase the likelihood of hitting a correct patch.
f5 implementation details can diversely influence the repair efficiency of an apr approach.
background and motivation test suite based program repair systems commonly implement a three step pipeline as illustrated in figure fault localization which produces a ranked list of suspicious code locations that should be modified to fix the bug patch generation which implements the change operators that are applied on the buggy code locations and patch validation which executes the test cases to check that the patched program meets the behaviour approximatively specified by the test suite.
if a patch candidate can pass all the given test cases both previouslypassing and previously failing test cases on the buggy version it is 616fixed program fault localizationbuggy program test suite im p o r t ja v a .m a th .b ig in te g e r im p o r t ja v a .u til.r a n d o m p u b lic c la s s pr i m e ex pri nt test pri nt test pri nt test syst em .
out .
.o u t.p r in tln is p r im e expect true syst em .
o ut .
pri nt l n is p r im e expect true i nt num p ri m es s t o p wa t c h s. n e w s t o p wa t c h s. start f or i nt i i i i f is p r im e i .o u t.p r in tln num pri m es s s. start bool ean p rim es get pri m es i nt np f or bool ean b pri m es i f b np s. stop syst em .
out .
pr i nt l n n p s sy stem .
out.
pri ntl n new bi gi nt e ge r new r andom pub lic s ta tic bool ean get pri m es i nt m ax bool ean resu lt n ew bool ean ma x f o r i n t i i resul t. l ength i r e s u l t tru e fin al d o u b le l im it ma t h .
s q r t m ax fo r in t i i l i m i t i i f r e s u l t cro ss o u t all m u ltip les in t in d ex i w h ile in d e x resul t. l ength resul t f al se i ndex i re tu rn re s u lt publ i c mi t ma t h .
s q r t num bool ean is p r im e n u m ?
t r u e n um !
i nt di v w hi l e di v li m i t is p r im e is p r im e num di v !
im p o r t ja v a .m a th .b ig in te g e r im p o r t ja v a .u til.r a n d o m p u b lic c la s s pr i m e ex h s. n e w s t o p wa t c h s. start f or i nt i i i i f is p r im e i .o u t.p r in tln num pri m es s s. start bool ean p rim es get pri m es i nt np f or bool ean b pri m es i f b np s. stop syst em .
out .
pr i nt l n n p s sydsf sf sdf sdf sf stem .
out.
pri ntl n new bi gi nt e ge r new r andom pub lic s ta tic bool ean get pri m es i nt m ax bool ean resu lt n ew bool ean ma x f o r i n t i i resul t. l ength i r e s u l t tru e fin al d o u b le l im it ma t h .
s q r t m ax fo r in t i i l i m i t i i f r e s u l t cro ss o u t all m u ltip les in t in d ex i w h ile in d e x resul t. l ength resul t f al se i ndex i re tu rn re s u lt publ i c mi t ma t h .
s q r t num bool ean is p r im e n u m ?
tru e d sfsd fsd fsd fsd fsd fsd f sd f sf n um !
i nt di v w hi l e di v li m i t is p r im e is p r im e num di v !
im p o r t ja v a .m a th .b ig in te g e r im p o r t ja v a .u til.r a n d o m p u b lic c la s s pr i m e ex param args publ i c s t a t i c v oi d m a i n st r i ng args pri nt test pri nt test pri nt test pri nt test pri nt test syst em .
out .
pr i nt l n is p r im e expect f al se syst em .o u t.p r in tln is p r im e expect true syst em .
o ut .
pri nt l n is p r im e expect true i nt num p ri m es s t o p wa t c h s. n e w s t o p wa t c h s. start f or i nt i i i if is p r im e i .o u t.p r in tln num pri m es s s. start bool ean p rim es get pri m es i nt np f or bool ean b pri m es i f b np s. stop syst em .
out .
pr i nt l n np s sy stem .
out.
pri ntl n new bi gi nt e ge r new r andom pub lic s ta tic bool ean get pri m es i nt m ax bool ean resu lt n ew bool ean ma x f o r i n t i i resul t. l ength i r e s u l t tru e fin al doubl e li m i t ma t h .
s q r t m ax fo r in t i i l i c mi t n u m d i v !
im p o r t ja v a .m a th .b ig in te g e r im p o r t ja v a .u til.r a n d o m p u b lic c la s s pr i m e ex param args publ i c s t a t i c v oi d m a i n st r i ng args pri nt test pri nt test pri nt test pri nt test pri nt test syst em .
out .
pr i nt l n is p r im e expect f al se syst em .o u t.p r in tln is p r im e expect true syst em .
o ut .
pri nt l n is p r im e expect true i nt num p ri m es s t o p wa t c h s. n e w s t o p wa t c h s. start f or i nt i i i i f is p r im e i .o u t.p r in tln num pri m es s s. start bool ean p rim es get pri m es i nt np f or bool ean b pri m es i f b np s. stop syst em .
out .
pr i nt l n n p s sy stem .
out.
pri ntl n new bi gi nt e ge r new r andom pub lic s ta tic bool ean get pri m es i nt m ax bool ean resu lt n ew bool ean ma x f o r i n t i i resul t. l ength i r e s u l t tru e fin al d o u b le l im it ma t h .
s q r t m ax fo r in t i i l i m i t i i f r e s u l t cro ss o u t all m u ltip les in t in d ex i w h ile in d e x resul t. l ength resul t f al se i ndex i re tu rn re s u lt publ i c mi t ma t h .
s q r t num bool ean is p r im e n u m ?
t r u e n um !
i nt di v w hi l e di v li m i t is p r im e is p r im e num di v !
patch generationpatch validation patch candidates figure standard steps in a pipeline of automated program repair.
regarded as a valid patch.
this criterion was first used by weimer et al.
in their seminal work on genprog and has become the de facto metric of repair performance .
nevertheless as later studies have revealed even if a generated patch can pass all test cases it might break a necessary behaviour or introduce other faults which are not covered by the given test suite .
besides a developer may not accept the patch due to several reasons such as coding convention .
all such valid patches in terms of the test suite are therefore now referred to as plausible since they require further investigations to ensure that they are correct i.e.
acceptable to developers.
in the literature correctness is generally assessed manually by comparing the apr generated patch against the developer provided patch available in the benchmark.
studies in the literature such as the recent work of durieux et al.
on benchmark overfitting generally focus on information about plausible patches given that correctness is hard to assess.
our work is the first to explore artifacts from the literature where researchers provide correctness labels of their generated patches in order to extract and categorize implicit rules used by the community to define correctness.
we expect that these rules will be studied and augmented by the community to enable systematic assessment of correctness.
efficiency of apr tools has been assessed in the literature via measuring the time to generate and validate patches.
table 1presents the time cost of the prapr state of the art repair tool on defects4j program samples.
on average for each closure bug prapr generated and validated more than thousand patches approximately times more than the average number of patches that are generated and validated for each chart bug.
yet the time cost for closure bugs is times more than the time cost for chart bugs.
this suggests that it is challenging to define a generically suitable time budget for repairing bugs.
we further note that correlation tests did not reveal any linear correlation between the time cost of repairing a bug and benchmark properties such as the number of test cases or program sizes.
consequently time cost may not be a reliable metric for efficiency.
table average prapr time cost s patches per bug .
subjects validated patches time cost s chart .
.
closure .
.
to further highlight the biases that execution time may carry we refer to literature settings of time budgets for running apr systems acs and simfix are evaluated with repair time budgets of minutes and hours respectively.
furthermore in assessment comparison between acs and simfix does not consider the bias related to the difference between the execution platforms.
a comparison of performance in terms of how many bugs each tool can fix may therefore be misleading a given bug may havebeen fixed by one tool because the time budget is sufficient while it cannot be fixed by the other due to lack of time.
with two simple experimental runs of compiling and testing defects4j samples we confirm our concerns time budgets could introduce biases for different bugs.
indeed as revealed in figure different machine configurations may lead to drastically divergent compiling and testing time irrespectively of projects.
the mann whitney wilcoxon tests confirm that the first machine consumes statistically significantly more cpu time than the second machine either for compilation or for testing defects4j buggy programs.
these results definitively suggest that time cost is not a reliable metric to enable reproducible and comparable experiments on the efficiency of program repair.
chart closure lang math mockito timetime cost s chart cloure lang math mockito timetime cost s defects4j compile defects4j test chart closure lang math mockito time chart closure lang math mockito timemachine2machine1time cost s figure distribution cpu times for compiling and testing defects4j programs.
machine runs os x el capitan .
.
with .
ghz intel core i7 16gb 1600mhz ddr3 ram.
machine runs macos mojave .
.
with .
ghz intel core i9 gb 2400mhz ddr4 ram.
instead we propose to rely on the metric of number of generated patch candidates which should be intrinsic to the approach and agnostic of machine configuration variabilities.
study design this section presents the design details of this empirical study.
.
research questions overall our investigation into the efficiency of test based apr systems seeks answers for the following research questions rqs rq1.
repairability across time we first revisit the classical performance criterion of apr systems which is about the repairability i.e.
effectiveness how many bugs can be fixed by test suite based repair approaches?
our investigation goes beyond previous studies in the literature by i systematically assessing a large range of repair systems under the same configurations see section .
.
and ii exploring not only plausibility but also the correctness of patches see section .
.
.
eventually we investigate the evolution across time of effectiveness to better discuss the need for revisiting efficiency as an important complementary performance criterion.
rq2.
patch generation efficiency based on the experimental outputs of benchmarking repair systems in rq1 we can investigate the efficiency of test based repair how many patch candidates are generated and checked before fixing a given bug?
although program repair is often regarded as a background offline task efficiency remains critical since resource budgets are limited.
therefore efficiency may have adverse effects on the adoption of the repair system and even on its effectiveness.
in this rq we extensively review two cases of invalid patches 617whose generation may undermine efficiency nonsensical and in plausible patches see section .
.
rq3.
fault localization noise impact on efficiency finally given that fault localization is known to provide noisy inputs to repair we investigate its impact on efficiency to highlight repair directions for mitigations.
mainly we question whether some repair strategies are more or less resilient to repair attempts on wrong code locations.
our study differs from recent work in the literature which explores the bias of fault localization on repairability with only one repair system.
.
subject selection our study focuses on apr systems targeting java programs.
java is indeed today the most targeted language in the community of program repair.
furthermore a well formed dataset of real world java program bugs is available with the necessary tool support to readily compile and execute programs.
although we initially planned to consider all repair approaches proposed in the last decade we were limited by the fact that many apr tools are not open source or even publicly available.
in the end apr systems considered for our study are systematically selected based on the following criteria availability our study involves the execution of apr tools thus apr approaches without publicly available tools are excluded.
executability some apr approaches provide publicly available tools which however cannot be executed as is for diverse issues e.g.
ssfix failed to execute because of an online connection to a private search engine fails .
we exclude such approaches from the study.
configurability to limit biases we need to configure the different tools to use the same input information e.g.
fault localization details .
we therefore exclude apr approaches whose tools cannot be readily configured.
for example hdrepair implementation is tied to an assumption that exact information on the faulty method is first available.
standalone finally our selection ensures that we focus on apr approaches where the tools can be run if provided with java program source code and the available test suite.
therefore any tool that would require extra data is excluded e.g.
lsrepair requires run time code search over github repositories .
we consider two sources of information to identify java apr tools the community led program repair.org website and the living review of apr by monperrus .
as of july apr tools were targeting java programs listed in the literature.
after systematically examining these tools are found to satisfy our criteria and are therefore finally selected.
table 2enumerates all java based apr tools and provides arguments for rejection consideration.
we categorize them into three main categories heuristic based constraint based and template based repair approaches.
heuristic based repair approaches.
these approaches construct and iterate over a search space of syntactic program modifications .
associated tools include jgenprog genprog a arja rsrepair a simfix jkali kali a and jmutrepair .
jgenprog and genprog a are java implementations of genprog which generates patches by searching donor code from existing code with the genetic programming method.table included and excluded apr tools for our study.
selected reason apr tools for java programs no not publicpar xpar jfix s3 elixir hercules sofix capgen prapr .
nofaulty method requiredhdrepair jaid sketchfix .
no other lsrepair ssfix deeprepair npefix .
yesopen source workingjgenprog jkali jmutrepair cardumen dynamoth nopol acs simfix kpar fixminer avatar tbar arja genprog a kali a rsrepair a .
prapr was not available before august .
lsrepair relies on the data from the run time github repositories and needs a private deep learning model and an online code search engine to search syntactically or semantically similar code which would be biased to assess its repair efficiency.
ssfix fails to execute as it relies on a private code search engine that is failed to connect.
deeprepair is not working thus it is not selected.
npefix is not selected as it does not use any fault localization technique.
arja is also a genetic programming approach to optimizing the exploration of the search space by combining three different approaches.
rsrepair a is a java implementation of rsrepair a random search based repair tool which tries to repair faulty programs with the same mutation operations as genprog but uses random search rather than genetic programming to guide the patch generation process.
simfix utilizes code change operations from existing patches and similar code to build two search spaces of which intersection is further used to search fix ingredients for repairing bugs.
jkali and kali a are java implementations of kali that fixes bugs with three actions removal of statements modification of if conditions to true false and insertion of return statements.
jmutrepair implements the mutation based repair approach for java programs with three kinds of mutation operators i.e.
relational logical and unary to fix buggy if condition statements.
constraint based repair approaches.
these approaches generally focus on fixing a single conditional expression that is more prone to defects than other types of program elements.
nopol dynamoth acs and cardumen are dedicated to repairing buggy ifconditions and to adding missing ifpreconditions.
nopol relies on an smt solver to solve the condition synthesis problem.
dynamoth leverages the runtime context which is a collection of variable and method calls to synthesize conditional expressions.
acs is proposed to refine the ranking of ingredients for condition synthesis.
cardumen repairs bugs by synthesizing patch candidates at the level of expressions with its mined templates from the program under repair to replace the buggy expression.
template based repair approaches.
these approaches are also often referred to as pattern based and include kpar avatar fixminer and tbar .
kpar is the java implementation of par that repairs bugs with fix patterns manually summarized from human written patches.
fixminer automatically mines fix patterns from the code repository for patch generation.
avatar relies on the fix patterns for static analysis violations.
tbar combines diverse fix patterns collected from the literature.
note that technically template based repair approaches can be viewed as heuristics based approaches.
in this study however we separate them in their category to highlight their specificity.
finally there exist some repair approaches that are enhanced by machine learning techniques.
le goues et al.
refer to them as learningbased repair approaches.
one example of such approaches is the prophet tool by long and rinard it learns from a corpus of code a model of correct code which indicates how likely a given piece of code is w.r.t.
the code corpus.
our criteria of subject selection 618however excluded all learning based repair as they are generally not standalone .
our study considers the most diverse set of repair tools in the literature for a systematic assessment of apr.
notably we cover different categories of repair approaches while the previous record for a large scale study which is held by durieux et al.
on apr benchmark overfitting did not consider the most widespread template based tools.
furthermore their study did not include acs and simfix from the current state of the art in java apr.
.
experiment settings we now overview the inputs i.e.
buggy programs and fault localization information and the validation process used in our study.
.
.
defect benchmark.
the apr literature includes several benchmarks .
in recent work durieux et al.
showed that apr system may overfit the study benchmarks in terms of repairability.
since our objective is on efficiency we focus on a single commonly used benchmark in the literature.
we consider defects4j as it has been widely employed to assess approaches or to conduct various apr studies as well as other software engineering research .
defects4j consists of bugs across six java open source projects.
its dissection information shows that the dataset contains a diversity of bug types.
our experiments thus consist of running each selected apr tool to generate patches in an attempt to fix each defects4j bug.
overall our experiments led to repair attempts each attempt requiring program compilation and testing against the test suite .
.
.
fault localization.
as reported by liu et al.
repair performance of apr tools could be biased by fault localization settings.
to minimize such potential bias we take on the challenge and implementation effort to re configure all apr tools so that they are using the same fault localization information for each defects4j bug.
in our experiments we employ the latest release of gzoltar v1.
.
an on hand test automation framework.
note that early versions of this tool were widely used in the apr community .
however liu et al.
revealed that the new version yields better results in the context of program repair .
for sorting suspicious statements we use the ochiai ranking metric.
eventually apr tools are fed with a ranked list of suspicious source code statements that should be changed within the buggy program to repair it.
.
.
patch validation.
patch validation is performed by apr systems based on the execution outcome of regression and bug triggering test cases i.e.
test cases that are passed by the buggy program and those that because they are not passed reveal the existence of a bug.
if a patch candidate can make the revised buggy program pass the entire test suite successfully it is considered as a valid patch.
such a patch however could be incorrect if it is just overfitting the test suite .
thus the community has adopted the terminology ofplausible patches to refer to patches that pass all test cases.
in recent literature following the criticism on overfitting researchers are shifting towards investigating correctness .
so far this has been a manual effort based on a recurrent criterion a plausible patch is considered as correct when it is semantically similar to the developer s patch in the benchmark.
unfortunately the scope of semantics for apr is not explicitly defined as it is subjective.table example rules that the community applies to confirm semantic similarity between tool generated and developer provided patches.
rule id rule description illustrations r1differ entfields with the same value or alias return cavailablelocaleset.contains locale return availablelocalelist .contains locale e.g.
avatar chart return cavailablelocalelist.contains locale r2same exception but different messages throw new numberformatexception str is not a valid number.
e.g.
acs time throw new numberformatexception r3variable initialization with newrather than a default value if str null str e.g.
tbar lang if str null str new string r4ifstatement instead classes array null ?
null array .getclass ofaternary operator if array null continue e.g.
tbar lang classes array .getclass r5unrolling a method this.elitismrate elitismrate setelitismrate elitismrate if elitismrate double .
throw ... e.g.
acs math if elitismrate double .
throw ... r6replacing avalue without a side effect int g int value this.lowerbound this.upperbound int g int v this.lowerbound this.upperbound e.g.
fixminer chart v math.min v this.upperbound value math.min v this.upperbound r7enumerating if fa fb .
if fa fb .
e.g.
acs math 85 if fa fb .
!
fa fb .
r8unne cessar y code uncleaned boolean waswhite false for int i i value.length i if character.iswhitespace c ...... waswhite false e.g.
avatar lang if character.iswhitespace c ...... waswhite false r9return earlier instead of a packaged return return founddigit !hasexp return founddigit !hasexp !hasdecpoint e.g.
acs lang if hasdecpoint true return false r10 more null checks if searchlist null replacementlist null continue e.g.
simfix lang if nomorematchesforreplindex searchlist null searchlist .length replacementlist null continue we applied these rules to determine whether a plausible patch is a correct one when it is syntactically different from the patch that a developer wrote.
in the second column tool name bugid denotes that the patch generated by the tool is identified as correct.
the patches in the grey background are generated by apr tools while the patches in the white background are patches written by the developers.
we propose in this work to provide a first attempt of explicitly determining semantic similarity among patches.
our objective is to reduce the threat of subjectivity and enable reproducible experiments.
to that end we call on the community and consider labels of patches within apr research artifacts.
we manually revisit patches that are generated by apr tools and which researchers have considered as correct in the literature.
the objective is to unveil the implicit rules that researchers use to make the decisions on correctness.
we find that there are broadly two scenarios when comparing a generated patch against the developer provided patch identical patches in this case the two patches are exactly identical excluding variations in whitespace layout and comments.
semantically similar patches in this case the patches are not identical although developers regard that they have the same effect on the program behavior.
in table 3we summarize a taxonomy of correctness decision based on our study of patches labeled as correct by the research community.
this taxonomy is based on the patches generated by acs simfix avatar fixminer kpar and tbar whose authors investigated correctness and provided their manually labeled patches as research artifacts.
in the remainder of this paper for the experiments with the apr tools we will systematically build on the rules of table 32to label plausible patches as correct.
thus unless a generated patch is 2we enumerated only rules in this paper due to space limitation.
please visit https github.com serval dtf apr efficiency for more rules and detailed descriptions.
619identical to the developer patch it must fall under rules r1 to be labeled as correct.
our rules are certainly not exhaustive neither for defining semantic similarity nor for defining patch correctness.
we call on a community effort to augment these rules to enable reproducible research.
due to space constraints we only detail here a single rule.
consider rule r5 in the illustration example the developer patch ensures that boundaries are checked by calling a function that implements it.
in contrast a patch generated by acs directly inserts the necessary code to check the boundary.
both patches which are not syntactically identical are semantically similar.
in the end plausible and correct patches have the following relationship let pandcbe sets of plausible and correct patches respectively.
it always holds c p. we compute c p as the correctness ratio cr of generated plausible patches that are correct.
.
.
halting threshold.
in the apr community it is commonly accepted that patch generation processes are halted if a system runs out of the time budget before being able to find a valid patch.
as discussed in section time can be a biased metric.
therefore in this study we propose to halt the repair systems by setting a threshold of repair attempts for a given bug.
we set the threshold of attempts as .
this number is selected based on the reported average number .
of patch candidates generated by prapr for its fixed bugs.
given that prapr works at the mutation level and does not require re compilation the number of attempts could be higher than that of other tools and it is high enough for the apr tools employed in this study.
.
terminology given that correct patches are first and foremost plausible patches we propose in this work to use the term valid patches when referring to all plausible patches including correct ones .
unless otherwise specified we will also refer to as plausible all valid patches that have not yet been manually assessed as correct.
we consciously avoid the term incorrect since the definition of correctness in section .
.
is sound to some extent3 but is not complete i.e there are some cases of semantic similarity that are missed .
.
efficiency metric npc as motivated in section we employ as efficiency metric in this study the number of patch candidates npc generated by apr tools until the first plausible patch is found.
this metric was initially proposed by qi et al.
as a proxy to measure the performance of fault localization techniques based on program repair tools.
jaid and prapr recently used them to highlight the performance of their approaches.
nevertheless efficiency has not been systematically assessed before.
in this study we further differentiate generated patches that turn out to be invalid into two groups nonsensical patch such a patch cannot even make the patched buggy program successfully compile .
in plausible patch such a patch lets the patched buggy program successfully compile but fails to pass some test cases in the available test suite.
3the developer patch provided in the benchmark which we use as ground truth may be erroneous as well.our efficiency metric is then computed by summing the number of patches in each category npc npc nonsensical npc in plausible npc valid in practice npc valid 1since the generation of patches is halted as soon as the first valid patch is found.
in this study since we aim to investigate the repair efficiency we focus on bugs for which the repair attempts were successfully concluded.
thus our experimental data do not mention the cases where many patch candidates are generated but none of them was valid.
we leave this investigation as a future study.
study results we now provide experimental data as well as the key insights that are relevant to our research questions.
.
rq1 repairability across time table 4provides execution outcomes of repair tools on the defects4j benchmark.
we count the number of bugs that are plausibly fixed by each tool implementation and further provide the number of plausible patches that can be considered as correct following the rules of patch validation cf.
section .
.
.
table numbers of defects4j bugs that are correctly plausibly fixed by the different apr tools.
apr tool c cl l m mc t total cr jgenpr og genprog a .
jmutrepair .
kpar .
rsrepair a .
jkali kali a .
dynamoth n a .
nopol n a .
acs .
cardumen .
arja .
simfix .
fixminer .
avatar .
tbar .
thenumb ers outside the parentheses indicate the bugs fixed with correct patches while the numbers inside parentheses indicate the number of plausible patches.
the missing numbers are marked with n a as we failed to change the fault localization input for closure program bugs for dynamoth and nopol of which fault localization is tightly tied with gzoltar .
.
.
c cl l m mc and t represent chart closure lang math mockito and time respectively.
the same as table .
template based repair tools are the most effective.
we observe that kpar fixminer avatar and tbar which are template based repair tools present better repair performance than other tools in terms of the number of fixed bugs.
the state of the art simfix also performs among the top.
note that although it is classified as heuristics based and does not use templates explicitly it performs transformations based on similar changes and thus has been presented in previous studies as template based.
patch ordering strategies are necessary to increase the likelihood of hitting correct patches.
among the repair tools acs exhibits the highest ratio of plausible patches that are found to be correct.
this experimental finding confirms the strategy used by the authors to increase precision 4in patch generation these are dependencybased ordering document analysis and predicate mining.
4precision is the terminology employed by its authors to refer to the ratio of correct patches to plausible patches.
620table number of overlapped fixed bugs per repair tool.
jgenpr og genprog a jmutrepair kpar rsrepair a jkali kali a dynamoth nopol acs cardumen arja simfix fixminer avatar tbar jgenpr og .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
genpr og a .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jmutrepair .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kpar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rsrepair a .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jkali .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kali a .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dynamoth .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nopol .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
acs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cardumen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
arja .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
simfix .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fixminer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avatar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tbar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the intersection of tool x row and tool y column contains the percentage of bugs fixed by x which are also fixed by y. for instance of the bugs fixed by jgenprog row are also fixed by genprog a column .
on the contrary .
of the bugs fixed by genprog a row are also fixed by jgenprog column .
while the diagonal cells present the number of bugs exclusively fixed by each repair tool.
2019all fixed bugs including newly and previously fixed bugs previously fixed bugs 2019all correctly fixed bugs including newly and previously fixed bugs previously correctly fixed bugs year ofproposing repair approaches figure evolution of the number of fixed bugs across time.
196exclusive non exclusive ofcorrectly fixed bugsjgenprog genprog a jmutrepair kpar rsrepair a jkali kali a dynamoth nopol acs cardumen arja simfix fixminer avatar tbar10 figure repairing exclusivity of each apr tool correct patches .
through time repair tools tend to subsume their predecessors in terms of which bugs are fixed.
table 5provides statistics on the percentage of fixed bugs that are overlapping between two repair tools.
in this table the tools in column headers and row headers are ordered chronologically concerning the date of approach publication.
note that jgenprog ranked based on the genprog publication year although the tool itself was implemented years later.
we note that the upper right side of the table is relatively darker than the rest the percentages of overlapping are higher for these cells.
these results suggest that overall the bugs that are fixed by earlier tools are also generally covered by more recent tools.
besides evolution trends presented in figure 4show that although the number of bugs that are fixed by the different tools over the years is increasing the number of new bugs is increasing with small increments.
this result suggests that the strategies implemented in new approaches tend to have similar outcomes as merging past techniques to cover previous bug sets that were fixed each via different approaches.
recent apr tools tend to correctly fix more bugs than their predecessors.
in the right part of figure a visible breakthrough is the sharp increase of the light grey area indicating that recent tools increasingly correctly fix bugs which have not been fixed by previous tools.
we further summarize in figure 5the numberof bugs that each tool can correctly fix exclusively or not.
simfix acs avatar and tbar are leading repair tools that generate correct fixes for more bugs.
in contrast jgenprog genprog a jmutrepair rsrepair a jkali kali a dynamoth and cardumen do not correctly fix any defects4j bug that is not also correctly fixed by another tool.
finally we observe that java targeted implementations of genprog i.e jgenprog and genprog a and kali i.e.
jkali and kali a by different research groups yield diverging repair performance on the same benchmark.
overall the systematic study of repairability of apr tools across time reveals that recent tools tend to fix more bugs than their predecessors each newly proposed repair tool however plausibly fix few bugs that were not fixed by other tools more bugs can be correctly fixed by lately proposed apr tools and template based repair tools are the most effective to eventually produce plausible patches.
it thus remains unclear whether the strategies proposed by record setting tools are improving the state of the art of patch generation.
we propose to focus on efficiency as a complementary metric to assess performance gains.
.
rq2 patch generation efficiency following our motivation argument in section we use the npc scores i.e.
number of generated patch candidates that are checked until a valid patch is found to measure repair efficiency of apr tools.
for each tool the results focus on defects4j bugs that are fixed i.e.
a valid patch was eventually found .
indeed through efficiency we attempt to measure the ability of the apr tool to avoid wasting computing resource time and energy in patch validation towards generating a valid patch.
figure 6overviews the general distributions of npc scores of the repair tools on the defects4j benchmark.
for all tools the median npc is lower than patch candidates.
however the distribution spread among bugs is not only significant for several out of tools but also varies across tools.
efficiency is not yet a widely valued performance target.
simfix tbar and kpar exhibit the highest npc scores which can go beyond patch candidates for some bugs.
correlating this data with repairability findings section .
we note that tools with highest repairability scores also have the highest npc scores hence lower efficiency .
in particular we note that apr approaches which rely on change patterns i.e.
standard template based tools or heuristically search for donor code based on code similarity e.g.
simfix tbaravatarfixminersimfixarjacardumenacsnopoldynamothkali ajkalirsrepair akparjmutrepairgenprog ajgenprog of patch candidatesfigure the distribution of npc scores for apr tools.
produce the largest number of patch candidates.
they are effective since they end up finding a valid patch but they are not efficient as they generate too many patches comparing against other approaches for repair attempts.
on the other hand constraint based apr tools e.g.
acs have the lowest npc scores.
there is therefore an insight that constraint solving and synthesis strategies although they might require more computing effort to traverse the search space eventually yield patches which waste less resource during test based validation.
the state of the art can avoid generating nonsensical patches.
figure 7illustrates the contribution of nonsensical and in plausible patches to the npc scores.
the distributions of nonsensical patches are interesting with respect to different claims in the literature.
indeed to motivate their seminal work on template based program repair kim et al.
authors of the par tool stated that pioneer genetic programming based repair tools had the limitation that it could generate nonsensical patches.
our empirical assessment results back up this claim.
however our results also reveal that template based repair tools e.g.
kpar and tbar have not fulfilled the claimed promise since they produce the largest numbers of nonsensical patches.
this finding calls for a triaging strategy targeting nonsensical patches within the search space.
in this regard our experimental results highlight three tools i.e.
dynamoth nopol and simfix which do not generate any nonsensical patches.
nopol uses an smt solver to address the condition patch synthesis problem.
dynamoth leverages the runtime context collects variable and method calls to synthesize conditional expression patches.
simfix heuristically searches similar code from the intersection of two search spaces one is for donor code and the other one is for code change actions to generate patches.
a noteworthy result is that while nopol and dynamoth overall generate few candidates simfix generates the largest number of patch candidates none of which is ever found nonsensical.
this finding suggests that code similarity has a large influence and can be useful for effectively triaging the repair search space.
besides nopol dynamoth and simfix five repair tools i.e jmutrepair jkali kali a cardumen and arja generate significantly more in plausible patches than nonsensical ones.
jmutrepair jkali and kali a are implemented with simple mutation operators that are unlikely to prevent the programs from compiling.
however these mutation operations can lead to test failures.
arja s efficiency tbaravatarfixminersimfixarjacardumenacsnopoldynamothkali ajkalirsrepair akparjmutrepairgenprog ajgenprog of patch candidates nonsensical patches in plausible patchesfigure distributions of n pc nonsensical and n pc in plausible scores for each apr tools.
w.r.t.
nonsensical patch generation is likely due to the combination of different search strategies that drive its genetic programming.
the more templates an apr system considers the more nonsensical and in plausible patches it will generate.
tbar contains more fix templates than kpar fixminer and avatar since it merges all literature templates.
therefore each suspicious buggy location has a higher probability in tbar to be matched with more templates leading to more patch candidates than other tools.
this finding highlights the importance of strategies for fix template matching and donor code searching to improve the repair efficiency of template based repair tools.
specialized templates increase the efficiency of apr tools.
among the template based repair tools kpar has the smallest number of templates.
indeed it includes templates manually prepared by kim et al.
while avatar includes tbar integrates and fixminer considers .
nevertheless experimental results for npc scores cf.
figure and the dissection in non sensical and in plausible categories cf.
figure reveal that kpar is the least efficient.
according to the authors source code of the tools these tools use the same search space traversal strategy and implementation.
therefore the only difference being about the included templates we can safely conclude that the nature of these templates is driving the efficiency performance.
avatar indeed focuses on templates obtained by curated datasets of fixes all mined code changes are for static analysis violations which are systematically validated as actual fixes.
fixminer on the other hand augments its templates with relevant contextual information to ensure that they are applied on code locations that are syntactically similar to the locations where the templates where mined.
long et al.
presented an initial study which revealed that correct patches can be considered as sparse in the search space and that overfitting patches i.e.
only plausible but not correct are vastly 622more abundant.
we extend their study to consider the cases of inplausible patches that are produced before any plausible patch i.e.
including if it is correct vs. before a correct patch i.e.
only if the plausible is correct .
figure 8illustrates the distributions of npc in plausible scores for all fixed bugs and only correctly fixed ones.
we observe that for tools such as tbar avatar fixminer and kpar the median of npc in plausible scores for correctlyfixed bugs is lower than the median for all fixed bugs.
this means that when a correct patch can be found the number of in plausible patches that are generated before is fewer than when only a plausible patch can be found.
the situation is the converse for simfix and arja.
therefore we note that for most tools a correct patch is more efficiently found when the search space is less noised i.e.
fewer in plausible patches .
tbaravatarfixminersimfixarjacardumenacsnopoldynamothkali ajkalirsrepair akparjmutrepairgenprog ajgenprog of in plausible patch candidatesconsidering all fixed bugs only correctly fixed bugs figure number of in plausible patch candidates generated before the first plausible patch.
table 6provides more detailed statistics to drive an in depth correlation study around efficiency and correctness.
based on the mean values except for acs arja and avatar apr tools tend to generate more patch candidates when considering all bugs than when considering only the correctly fixed ones.
this tendency is much more apparent for search based apr techniques such as jgenprog genprog a simfix and rsrepair a .
although tbar is a template based approach it has characteristics of search based tools since its search space has been enlarged by incorporating any fix templates in the literature.
the previous experimental data overall suggest that simply giving more time to the apr tool to repair a buggy program does not guarantee to find correct patches.
on the contrary it seems that when allowing less attempts the correctness ratio is improved.
we propose to simulate a simple strategy of threshold setting to investigate the impact on the correctness ratio i.e.
ratio of correctly fixed bugs to plausibly fixed bugs .
we consider a scenario where the apr tool is halted when a certain number of in plausible patches is checked.table upper whisker median and mean values of n pc n pc in plausible scores in figures 6and8.
apr toolsupperwhisker median mean bugs all correct all correct all correct jgenpr og genpr og a jmutrepair kpar rsrepair a jkali kali a dynamoth nopol acs cardumen arja simfix fixminer avatar tbar the upper whisker value is determined by .
iqr interquartile ranges where iqr 3rd quartile 1st quartile as defined in .
all denotes all fixed bugs and correct denotes correctly fixed bugs.
the numbers outside the parentheses indicate the related n pc score values and the numbers inside the parentheses indicate the related n pc in plausible score values.
implies that the n pc and n pc in plausible values of correct are higher than those of all .
bugs denotes the number of bugs correctly fixed by each repair tool.
table cr after setting a n pc in plausible threshold.
tool th fixed bugscr tool th fixed bugscr jgenprog .
nopol genprog a .
acs jmutrepair .
cardumen .
kpar .
arja .
rsrepair a .
simfix .
jkali .
fixminer .
kali a .
avatar .
dynamoth .
tbar .
the threshold th for each repair tool is set with its upper bound n pc in plausible score shown in figure .
table 7presents the results on how correctness ratio is influenced when we set a threshold on the number of in plausible patches basically we propose to stop the repair attempts by a given tool if a certain number of generated patches turned out to be in plausible i.e.
do not pass the test cases .
we observe that the ratio of generated plausible patches to be correct is increased at varying degrees for out of repair tools.
nopol and acs do not show any improvement initially they produce few in plausible patches.
it should be noted that this result should be put in perspective as when discussing precision and recall threshold setting while useful to increase correctness ratio may also lead to an overall reduction of the number of bugs that are correctly fixed.
overall our systematic study of patch generation efficiency reveals that efficiency is not yet a widely valued performance target state of the art can avoid generating nonsensical patches the more templates an apr system considers the more nonsensical and in plausible patches it will generate specialized templates increase apr tool efficiency and correct patches are sparse in the search space.
.
rq3 impact of fault localization noise a recent study by liu et al.
has reported empirical results suggesting that fault localization results can adversely affect the performance of the repair.
the authors experimented on a single tool kpar and focused on repairability i.e.
how many bugs are not fixed due to localization errors .
our study already takes steps to avoid the bias of presenting various experimental results with apr tools which use different fault localization inputs.
thus we 623have put an effort to harmonize all fault localization configurations for the apr tools under study cf.
section .
.
.
to evaluate the impact of fault localization noise for different tools we propose to compare results obtained so far with our standard spectrum based fault localization gzoltar ochiai against experimental results where the apr systems are directly given the ground truth fix locations.
we compare the results both in terms of repairability and repair efficiency.
.
.
impact of fault localization noise on repairability.
first we measure the impact on repairability where we estimate for each repair tool how many bugs can be fixed by each apr system if it is precisely pointed to the ground truth fix locations?
table 8illustrates the details on the impact of repairability.
except for cardumen we observe that in general the correctness ratio improves by up to percentage points if the fix locations are provided.
it suggests that false positive bug locations hence fault localization noise has an impact on the likelihood to generate correct patches.
there are however anecdotical cases that are noteworthy although our configuration of fault localization did not yield the developer provided fix position for bug lang acs patch generation eventually produced a correct patch for this bug.
this patch which targets a different code location was found semantically similar to the developer provided patch following rule r2 cf.
section .
.
.
this finding reminds us that the benchmark that is used is not a complete ground truth neither for repair oriented fault localization nor for patch generation.
table impact on repairability when ground truth fix locations are directly given to the apr system.
apr tool c cl l m mc t total cr jgenpr og .
genprog a .
jmutrepair .
kpar .
rsrepair a .
jkali kali a .
dynamoth n a .
nopol n a acs .
cardumen .
arja .
simfix .
fixminer .
avatar .
tbar .
this table shows variations of repairability w.r.t.
results of our generic configuration of fault localization provided in table .
x y means that if given exact fix locations the tool can correctly fix x more bugs but plausibly fixes y less bugs we observe that jkali now fails to correctly fix respectively when it is given the developer provided fix locations.
this finding suggests that the repair tool is rather misled in the cases of specific bugs when it is given the right bug positions.
instead some sibling positions are better inputs to drive correct fixing.
however data in table 8show fault localization has different impacts on performance for plausible fixing than for correct fixing.
furthermore based on results of overlapping in repairability in terms of plausible patches performance as depicted in figure we note that many bugs are only fixed plausibly when the fault localization does not precisely point to the fix locations.
this is a surprising but interesting finding to be investigated by aprtargeted fault localization research.
another immediate observation that we make from the experimental results in table 8is that repairability with given fix positions repairability with gzoltar v .
.
jgenprog genprog a jmutrepair kpar rsrepair a jkali kali a dynamoth nopol acs cardumen arja simfix fixminer avatar tbar figure overlap and difference between normal fault localization and given fix positions for repair tools.
bugs from the mockito project are not easy to fix.
according to reported results in table only three tools i.e.
fixminer avatar and tbar are able to fix mockito bugs even if ground truth fix locations are provided.
we carefully proceed to investigate the possible reasons for this situation mockito bugs i.e.
bug ids and are associated to program code that cannot be compiled under jdk which is the jdk that is mentioned in the requirements of defects4j .
our results further confirm a recent study by wang et al.
who reported that the state of the art simfix and capgen are not able to fix any mockito bugs even when provided with ground truth fix locations.
our study enlarges the scope of their studies.
in the end our systematic assessment results for all bugs better sheds light on a common phenomenon in the literature where mockito project bugs are not considered when reporting repair performance.
these results call for modular configuration of execution environment as well as for better integration of advances in fault localization to support apr systems.
besides mockito bugs many bugs in other projects cannot be fixed since they are not precisely localized.
overall consider again figure .
for all tools except jmutrepair we observe that some bugs are fixed only when the actual fix locations are directly given to the system.
.
.
impact of fault localization noise on repair efficiency.
we investigate the npc scores i.e.
the number of patch candidates that are generated by the different apr systems when they are pointed to the developer provided fix locations.
figure 10shows the corresponding distribution of npc scores for each repair tool.
template based program repair tools are highly sensitive to fault localization noise.
we observe from figure 10that except for dynamoth nopol and acs the remaining repair tools have significantly smaller distribution ranges of npc scores than the distribution ranges when the apr system was run under our generic fault localization configuration cf.
figure .
a straightforward explanation is that under a typical fault localization configuration a repair tool will attempt to generate patch candidates for each suspicious statement that is ranked by the fault localization.
when the fault localization is noisy i.e.
top suspicious statement s are false positives more in plausible and even non sensical patches might be generated.
in particular for repair tools that are based on pattern matching and code similarity i.e.
simfix and the template based repair tools the gap of repair efficiency has reduced substantially by an order of magnitude when correct fix locations are given to the tool.
for example the median npc score of simfix is around when using our generic configuration of fault localization but is around when using directly correct fix locations.
such tools are thus more sensitive to fault localization noise than other tools.
in conclusion we confirm the finding of the study of liu et al.
.
however we delimitate its validity to template based repair tools.
other tools e.g.
constraint based repair tools such as acs or 624nopol which use specific techniques to triage the search space do not present any increase in repair efficiency when pointed to the fix locations.
this finding suggests that they have limited sensitivity to fault localization noise.
tbaravatarfixminersimfixarjacardumenacsnopoldynamothkali ajkalirsrepair akparjmutrepairgenprog ajgenprog of patch candidates figure npc score distribution of each tool given fix positions.
fault localization is an important step in a repair pipeline.
its false positives however have a significant impact on both repairability and repair efficiency.
in particular we found that accurately localizing the bug can reduce the number of generated patches by an order of magnitude thus drastically enhancing efficiency.
from the perspective of repairability better fault localization will increase the probability to generate correct patches i.e.
the correctness ratio .
threats to validity external validity.
our study considers only the defects4j benchmark and only java repair tools.
all findings might thus be valid only for this configuration.
nevertheless this threat is mitigated by the fact that we use a large set of repair tools and a renowned defect benchmark to study a performance criterion that was largely ignored in the literature.
internal validity.
our implementation of fault localization as well as the manual assessment of patch correctness may threaten the validity of some of our conclusions.
we mitigate this threat by reusing common fault localization components from the repair literature as well as by enumerating and sharing the rules for defining patch correctness.
two authors were in charge of assessing the correctness and they cross reviewed each other s decisions.
in case of conflict other authors were called to create a consensus.
construct validity.
by construct to limit resource exhaustion we added a threshold on the number of patches to validate.
however this threshold may penalize some tools.
we mitigate this threat by carefully selecting a threshold based on empirical results on prapr a recent related work which mutates directly bytecode allowing it to generate many more patches since no compilation is needed .
related work performance evaluation.
initially evaluation of test based program repair was focused on counting the number of bugs fixed by a repair tool out of all bugs in a benchmark .
however valid patches are sometimes incorrect as they overfit on incomplete test suites and might cause issues during maintainance .
thus plausibility and correctness became widelyaccepted to define metrics for assessing repairability of repair tools .
in this study we also follow the metric to revisit the repairability of repair tools.
nevertheless we differ from studies in the literature by ensuring that apr tools use the same controlled configuration for fault localization.
repair efficiency.
along with the performance evaluation serval studies simply reported the repair efficiency in terms of cpu time consumption of fixing bugs .
however it could be biased to assess the efficiency with time cost for various reasons cf.
section .
instead we leverage the number of patch candidates generated by repair tools to measure the repair efficiency which should be intrinsic to the repair approaches.
ghanbari et al.
provided information on the number of patch candidates generated by prapr.
this information however could not be put into perspective against other tools.
our study fills this gap.
empirical study.
to boost the development of program repair various empirical studies have been conducted in this direction.
le goues et al.
re assessed genprog on real bugs while several studies on overfitting followed .
yang et al.
explored better test cases for better program repair.
yi et al.
empirically investigated the effectiveness of test suite metrics in controlling the repairing reliability of genprog.
motwani et al.
investigated to what extent important bugs can be fixed by apr tools.
liu et al.
investigated the fl bias in benchmarking apr tools with only one apr tool.
durieux et al.
conducted a large scale empirical study for java apr tools to investigate their repairability on different benchmarks.
empirical studies for apr tools have been studied from different scenarios in the literature but these studies mainly focus on the traditional apr tools and the latest state of the art tools e.g.
acs simfix and tbar have not been studied systematically.
our study fills this gap by looking back at years of test based program repair research and focusing on the under valued performance criterion that is efficiency.
conclusion this paper reports on a large scale study on the efficiency of test suite based program repair.
efficiency is defined based on the number of patch candidates that are generated before a repair system can hit a valid patch.
our study comprehensively runs repair systems from the literature under identical configuration of fault localization.
our experiments explore repairability i.e.
repair effectiveness repair efficiency as well as the impact of fault localization on both performance criteria.
beyond the statistical data we call on the community to invest in strategies for making repair efficient in order to facilitate adoption in a software industry where computing resources are managed sometimes with parsimony.
artefacts all data and tool support for replication are available at