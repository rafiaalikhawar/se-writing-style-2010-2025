ranking relevant tests for order dependent flaky tests shanto rahman1 bala naren chanumolu2 suzzana rafi2 august shi1 wing lam2 1the university of texas at austin usa shanto.rahman august utexas.edu 2george mason university usa bchanumo srafi winglam gmu.edu abstract one major challenge of regression testing are flaky tests i.e.
tests that may pass in one run but fail in another run for the same version of code.
one prominent category of flaky tests is order dependent od flaky tests which can pass or fail depending on the order in which the tests are run.
to help developers debug and fix od tests prior work attempts to automatically find od relevant tests which are tests that determine whether an od test passes or fails depending on whether the od relevant tests run before or after the od test.
prior work found od relevant tests by running different tests before the od test without considering each test s likelihood of being od relevant tests.
we propose rankf to rank tests in order of likelihood of being od relevant tests finding the first od relevant test for a given od test more quickly.
we propose two ranking approaches each requiring different information.
our first approach rankf l relies on training a large language model to analyze test code.
our second approach rankf o relies on analyzing prior testorder execution information.
we evaluate our approaches on od tests across open source projects.
we compare rankf against baselines from prior work where we find that rankf finds the first od relevant test for an od test faster than the best baseline depending on the type of od relevant test rankf takes .
to .
seconds on median compared to the baseline s .
to .
seconds on median.
i. i ntroduction software developers rely on regression testing the process of rerunning tests after every code change to check that code changes do not introduce faults .
however regression testing suffers from the presence of flaky tests which are tests that can pass and fail for the same version of code .
a flaky test failure after a change can mislead developers into thinking they introduced a fault in their changes when instead the test could have failed even without those changes.
several studies found various reasons for flaky tests with one prominent reason being test order dependency.
orderdependent flaky tests od tests are tests that can pass or fail due to the order in which the tests are run.
od test failures occur when tests depend on some global state such as static variables the file system databases network etc.
that other tests modify.
developers cannot always enforce a specific test order in which the od tests pass especially if they rely on regression testing techniques such as test prioritization selection and parallelization which purposefully reorder or run subsets of tests to speed up regression testing .to repair od tests recent prior work relies on odrelevant tests i.e.
the tests that modify the global state that the od test depends on.
finding od relevant tests has three main purposes enable the use of automatic repair techniques enable the use of mitigation techniques and help developers debug and reproduce od test failures.
shi et al.
proposed the use of delta debugging to find od relevant tests.
follow up work proposed finding od relevant tests by running pairs of tests one by one obo where each test is paired before the od test.
both approaches can be costly as they rerun tests many times.
we propose rankf a means to rank tests based on their likelihood to be od relevant tests for a given od test.
the goal is to rank true od relevant tests higher than non odrelevant tests so fewer tests need to be run before finding an od relevant test.
we propose two different approaches.
our first approach rankf l analyzes only the test method body i.e.
the code directly under a test method to compute likelihood.
rankf lfine tunes a pre trained large language model llm bigbird to take as input the test method bodies of a potential od relevant test and the od test to then output a score indicating the likelihood of the potential test being an actual od relevant test for the od test.
we train the model on a large dataset of known od tests and their corresponding od relevant tests .
while prior work used llms to predict which tests are flaky none has explored using llms to predict od relevant tests which requires analyzing the relationship between two or more tests.
our second approach rankf o analyzes the test results of previous test order execution information to compute the likelihood of each test being od relevant tests.
developers may already have several different test orders that they ran before either as part of regression testing where tests were run in different test orders in the past or as part of an od test detection process that runs tests in different test orders to search for existing od tests .
rankf ouses several heuristics to compute likelihood scores for each test based on how often each one appears before the od test in the passing test orders and failing test orders as well as how far away each test is from the od test in these test orders.
both rankf land rankf ohave the same goal of ranking tests based on likelihood of being od relevant tests.
however the approaches represent different use scenarios.
rankf oisuseful if developers already have some results from different test orders.
if developers do not have such information they can use rankf l which operates just on test code.
we evaluate rankf land rankf oon a dataset of known od tests from prior work .
this dataset contains od tests along with the corresponding od relevant tests for each od test.
in total there are od tests from modules across open source projects in this dataset and we use it to measure how highly each approach ranks a true od relevant test.
to evaluate efficiency in an actual use scenario we measure how long the approaches take to both rank tests and then run them in a ranked order to confirm finding a true od relevant test.
we compare against prior baselines for finding od relevant tests obo and delta debugging .
for the majority of our subjects either rankf lor rankf ois faster than all of the baselines at finding od relevant tests achieving a range of median times depending on type of od relevant test from .
to .
seconds compared to the .
to .
seconds achieved by the baselines.
we make the following contributions in this paper we propose two new approaches for ranking tests based on their likelihood of being od relevant tests for an od test rankf luses an llm to rank tests based on test code and rankf oanalyzes test results from different test orders to rank tests based on their positions relative to a given od test in those test orders.
developers can find od relevant tests faster using these rankings.
our evaluation shows our approaches are faster at finding od relevant tests than prior baselines.
our implementations scripts and data are publicly available .
ii.
b ackground anorder dependent flaky test od test is a flaky test whose outcome depends on the test order in which it is run .
we refer to a test order in which the od test passes as a passing test order and a test order in which it fails as a failing test order .
the reason that an od test fails in a failing testorder is because it shares some global state with some other test where that other test either runs before the od test in the failing test order and pollutes the shared state leading to the od test to fail when run afterwards or runs after the od test in the failing test order and is not setting up a shared state required by the od test to pass.
shi et al.
provided names for these different types of od tests as well as their corresponding od relevant tests .
an od test that passes when run on its own but fails when run after some other test is avictim the test that pollutes the shared state in the failing test order of a victim is a polluter .
an od test that fails when run on its own is a brittle .
a brittle fails when run on its own because it relies on some other test to set up its initial state this other test is a state setter and a brittle will pass when run after it.
finally shi et al.
found that for the case of victims and polluters there are other tests in the test suite that can actually reset the shared state when these other tests are run after the polluter but before the victim resulting in the victim passing.
these other tests are referred to as cleanersfor a polluter victim pair.
shi et al.
relied on these cleaners to automatically repair victims .
we collectively refer to polluters state setters and cleaners as od relevant tests .
shi et al.
previously proposed ifixflakies which can find od relevant tests.
given a victim brittle and a failing passing test order for the test respectively ifixflakies identifies a polluter state setter for the od test respectively via delta debugging the tests that come before the od test in the test order.
an od test may need more than one other test to run before the od test for it to fail pass but shi et al.
found this scenario to be very rare .
lam et al.
later proposed detecting od relevant tests in a simpler way by running each test one by one with the od test to check whether the od test s outcome changes .
this one by one search is also effective for finding cleaners for a given pollutervictim pair by running each test in between the polluter and victim ifixflakies also uses this one by one strategy to find cleaners .
both delta debugging and the one by one search depend on the given test orders potentially running for a long time depending on where the od relevant tests are within the test orders.
our evaluation shows these approaches often run many irrelevant tests before finding any od relevant test.
a. example figures and illustrate simplified versions of an example brittle od test and its state setter od relevant test respectively.
the brittle testdubboprotocolwithmina and state setter testrpcfilter are from the apache dubbo project which has over stars on github and is an easy to use web and rpc framework that provides multiple language implementations for different services e.g.
communication security .
the brittle fails because it explicitly specifies the server is mina in its url at line which tells the code to use the mina server for communication.
both the brittle and statesetter have a protocol sets up a demoservice to be accessed over the network and proxy handles calls to the network variable which are separate class instances.
using these variables line exports the service and initiates the server based on the url and configuration .
we find that this method call performs various server initialization logic including for the mina server when no server information is specified in the url.
however as line contains the server information in addparameter constants.server key mina the code skips the mina server initialization and directly tries to use the server.
if the mina server is not already initiated skipping initialization results in an exception.
when state setter testrpcfilter runs before this brittle we find that protocol.export is called without specifying any server at line which initializes various servers including mina .
once this state setter starts the mina server the brittle passes when run afterwards.
finding this state setter among all tests can be timeconsuming as there are total tests in the test suite.
for example we can use the one by one strategy by running each test before the brittle to see which one makes1public void testdubboprotocolwithmina 2demoservice service new demoserviceimpl 3protocol.export proxy.getinvoker service ... url.valueof dubbo .
.
.
.
addparameter constants.server key mina ... fig.
example brittle from the dubboprotocoltest class in the module dubbo rpc dubbo rpc dubbo of theapache dubbo project .
10public void testrpcfilter demoservice service new demoserviceimpl protocol.export proxy.getinvoker service ... url.valueof dubbo .
.
.
... fig.
example of a state setter for the brittle in figure .
testrpcfilter is from the rpcfiltertest class and the same module and project as the brittle.
the brittle pass.
in our experiments we can find the statesetter after running .
tests averaged from using the tests positioned before the brittle in different passing test orders .
on the other hand if we first rank the tests based on likelihood of being od relevant tests such as by leveraging a llm section iii a we can find the state setter after running only .
tests on average.
if we instead rank tests based on analyzing test outcomes from a large number of test orders section iii b we can find the state setter after running only .
test on average.
developers can spend less time to find od relevant tests by first ranking tests and then running them in the ranked order taking on average .
and .
seconds using the llm and test order analysis approach respectively.
meanwhile the one by one baseline takes .
seconds on average while the delta debugging baseline proposed by shi et al.
takes .
seconds on average.
iii.
r anking od r elevant tests given a known od test the goal is to rank the set of other tests in terms of their likelihood of being an od relevant test for the given od test.
first we determine the type of od test which according to shi et al.
we can determine whether an od test is a victim or brittle by running it by itself section ii .
in the case of a victim we search for a polluter.
for a brittle we search for a state setter.
for a victim and a found polluter for that victim we search for a cleaner corresponding to the given victim polluter pair.
we propose two different approaches rankf land rankf o for computing the likelihood of tests as od relevant tests each one relying on different information.
for a given od test each approach ranks all other tests as candidate od relevant tests giving each one a score representing its likelihood.
then we iterate through each candidate od relevant test according to the ranking to confirm whether the test is a true od relevant test for the given od test.
to confirm we run just the candidate test along with the od test.
if the od test is a victim we fig.
overview architectural diagram of rankf ltraining.
run the candidate before the victim to see whether the victim fails.
if the od test is a brittle we run the candidate before the brittle to see whether the brittle passes.
in the case of searching for cleaners we require both a victim and polluter and we run the candidate in between the polluter and victim to see whether the victim passes which normally fails when run right after the polluter.
the search stops once the process confirms a candidate test to be a true od relevant test.
prior work found that finding just one od relevant test is often enough to debug or repair an od test .
this search becomes more efficient when the ranking is better i.e.
the real od relevant tests are ranked earlier as fewer tests need to be run before the confirmation of a true od relevant test.
note that an odrelevant test can also be an od test but this situation is rare only one such reported case in prior work .
if there is a test that is both a victim and polluter then we will find the polluters for this test as a victim then separately find it later as a polluter for another victim.
we carry out a similar process for brittles and state setters.
a. rankf l rankf lleverages a large language model llm to compute likelihood scores for od relevant tests.
given an od test a user can use rankf lto compute a likelihood score for each of the other tests to be an od relevant test.
we build separate models for scoring polluters for victims state setters for brittles and cleaners for victim polluter pairs.
to train a model we use a training dataset of labeled tuples of tests.
each tuple consists of either a victim brittle or victim polluter pair depending on the type of od test the model is specialized for along with some other tests in the test suite.
the tuple has a positive label if the other test is a true od relevant test for the victim brittle or victim polluter pair and negative otherwise.
figure shows the training process for creating a rankf lmodel.
bigbird we use bigbird roberta base as the pretrained llm that we fine tune and build the model from.
bigbird was constructed based on the transformer neural architecture with a focus on understanding long sequences and can process a large number of tokens especially in comparison to other llms .
processing many tokens is especially important for our problem as we need to analyze the code from multiple tests as opposed to just a single test that is used for other similar llm based tasks .
extracting features from test code for a given od test or polluter victim pair we obtain all of its candidate tests and use each one to form a tuple with the given od test.
for each candidate and od test we use srcml to extractthe signature of the test method and the test method body.
we also eliminate any code comments from the method body.
we provide each tuple consisting of the signature and body of a candidate test and od test polluter victim pair with a positive or negative label to the model for training.
we utilize bigbird s built in tokenizer that uses sentencepiece with byte pair encoding bpe to create a sequence of tokens representing this aggregated code.
we also utilize batch encode plus from the tokenizer to encode the token sequences into numerical representations token id and attention mask in a batch outputting a vector of numbers where each number represents a token.
while bigbird can process at most tokens setting it to always process this many tokens can introduce unnecessary token padding and memory usage that negatively impacts the model s performance.
in our evaluation dataset over of the tuples have fewer than tokens.
hence we set the maximum token length to .
similar to prior work if the number of tokens is over this limit we truncate .
fine tuning models we fine tune the bigbird model to distinguish between positive and negative labeled tuples from the dataset.
as bigbird is a large model fine tuning all of its layers is computationally expensive.
therefore we freeze the first five layers lower level meaning their weights remain unchanged during training and fine tune the remaining seven layers of the model .
the output of each bigbird model is a vector of length .
we feed this new vector as an input to a dense layer that outputs a vector of length which we found to be the most effective based on some preliminary experiments.
we send this length vector to a prediction head which leads to an output layer of two neurons.
we use relu as an activation function for each neuron.
we add dropout layers of .
to eliminate some neurons randomly from the network during the training phase to avoid overfitting .
we use the softmax function to convert the output layer s logits into the probability of a positive or negative label based on the results of the output layer.
finally we use the adamw optimizer for parameter optimization and nllloss negative log likelihood loss to compute the loss from predicted log probabilities.
the result of this training is a model that given token representations of a tuple of tests outputs two scores positive class score and negative class score reflecting the likelihood of a positive and negative label respectively.
section iii c further details how these scores are used.
we fine tune the pre trained model for epochs to enhance the model s performance.
given all the tuples from a training set with a balanced set of both positive and negative labeled tuples we randomly set aside of them for validation.
for each epoch we evaluate the model s validation loss.
after each epoch if the model achieves the lowest validation loss we proceed to update the weights of the model for the next epoch using this current optimal model s weights.
additionally we utilize early stopping which is configured to halt training if there is no improvement after consecutive epochs to mitigate overfitting.
we ultimately produce the best modelwith the minimum validation loss among the epochs .
to preserve determinism in our results we set the seed for the random number generator during testing and inference .
we find that our model provides deterministic outputs when we run inference with it on the same input times.
b. rankf o rankf oleverages execution information of tests in different test orders to compute scores for likely od relevant tests.
given an od test and the execution result of the test suite in different test orders rankf ooutputs a likelihood score for each test being an od relevant test for the given od test.
if the od test is a victim then a polluter must be one of the tests that run before the victim in a failing testorder.
conversely if the od test is a brittle then a statesetter must be one of the tests that run before the brittle in a passing test order.
intuitively the more often we see the same tests that come before the victim brittle in a failing testorder passing test order respectively the more likely that test is the polluter state setter for the victim brittle respectively.
similar to rankf l rankf oalso computes two scores for each test the positive class score for the likelihood to be an od relevant test and negative class score for the likelihood to not be an od relevant test.
rankf ocomputes these scores based on occurrence of each test and the position of each test when the od test is passing and failing.
for example for a victim depending on whether it is failing or passing in a given test order we increment the appropriate score i.e.
positive class scores for failing test order and negative class scores for passing test order.
the more test orders processed the more confident rankf ois that the scores reflect the likelihood of tests being true od relevant tests for the od test.
we propose five different heuristics for scoring tests plus one this heuristic gives a score of 1for each test before an od test in a given test order.
the intuition is that a test that frequently appears before a victim brittle when the od test is failing passing respectively is more likely to be an od relevant test than a test that never appeared before the od test e.g.
if a particular test is always running before a victim when it fails then this test is likely a polluter .
methods m this heuristic uses the number of tests before an od test ot in a test order to give a score of1 indexof ot to each test before an od test where indexof returns the index of a specific test in a given testorder.
the intuition is that when a victim brittle fails passes respectively the test orders where the number of tests before the od test is small should have more weight on the likelihood of the tests being od relevant tests than test orders in which there were more tests before the od test.
distance d this heuristic takes the distance between a given test gt and the od test ot in a test order and gives a score of1 indexof ot indexof gt to the given test.
the intuition is that tests closer to the od test is more likely to be an od relevant test.
combined d this heuristic uses plus one to give scores to the tests and uses distance to break ties.fig.
example of rankf ocombined d heuristic.
combined m d this heuristic uses methods to give scores to the tests and uses distance to break ties.
after the positive negative class scores are calculated rankf land rankf ouse the scores to rank tests.
section iii c describes in detail how these scores are used.
regardless of how they are used it is possible for the scores to tie i.e.
be the same for multiple tests .
the intuition for the two combined heuristics is to combat the problem where the use of scores from the methods and plus one heuristics results in tests with tied scores especially when the number of test orders given to rankf ois small.
we only consider these two combined heuristics because combined heuristics where distance is first will rarely result in the same scores unlike methods and plus one which updates the score of every test the same amount in each test order.
we also do not consider combining the two methods and plus one heuristics with each other because intuitively they are not as effective as distance at breaking ties.
for the distance methods and plus one heuristics if there are ties in the ranks of the tests after the scores are used the tied tests are alphabetically ordered in the ranked list.
for the two combined heuristics if there are ties in the ranks of tests the tied tests are ordered in descending order of their distance to the od test in the last test order.
example for rankf o figure showcases an example of the combined d heuristic of rankf o. in this example t4 is a victim and its true polluter is t2 along with a cleaner t5.
order is a failing test order the true polluter t2 runs before t4 with no cleaners in between therefore the positive class scores of tests before t4 t1 t2 t3 are increased by .
order is a passing test order t5 is a cleaner and runs inbetween t2 and t4 so the negative class scores of tests before t4 t1 t2 t5 are increased by .
after processing the final testorder order tests t1 and t2 have the same highest positive class scores and lowest negative class scores.
the combined d strategy then relies on distance to break the tie by considering the distance between t1 and t2 to the victim in order ultimately finding that t2 is closer to t4 and ranking it correctly higher than t1 as an od relevant test for t4.
c. ranking tests after obtaining positive class scores and negative class scores for tests from rankf lor rankf o we use those scores to rank the tests in order of likelihood to be od relevant tests.
we use three strategies to rank tests positive class strategy negative class strategy and combined class strategy.positive class strategy organizes all potential od relevant tests by their positive class scores in descending order placing those with the highest likelihood at the forefront.
negative class strategy arranges the candidate tests by their negative class scores in ascending order aiming to position tests with the highest likelihood of being notan od relevant test at the bottom.
combined class strategy involves subtracting the test s negative class score from its positive class score subsequently organizing the tests by this score difference in descending order.
the idea is that tests with higher difference scores are those where the positive class score substantially outweighs the negative class score suggesting that such tests are more likely to be true od relevant tests.
in section v c we evaluate how our approaches perform using either and both scores.
iv.
e xperimental setup we study the following research questions rq1 what is the efficiency and effectiveness of rankf l and rankf oat ranking confirming od relevant tests?
rq2 what effect do different test orders and heuristics have on rankf o s effectiveness?
rq3 what effect do different ranking strategies have on the effectiveness of rankf land rankf o?
we evaluate rq1 to see whether rankf can find the true od relevant tests faster than baselines from prior work that also find od relevant tests .
we evaluate rq2 to assess how rankf operforms using different test orders and heuristics.
finally we evaluate rq3 to see which ranking strategies enable rankf land rankf oto be the most effective.
a. dataset rankf is meant to help find od relevant tests after od tests have been detected.
as such rankf s evaluation requires a dataset of known od tests and od relevant tests.
there are various datasets that contain od tests however to the best of our knowledge the only dataset that contains both od tests and od relevant tests is from wei et al.
.
this dataset consists of od tests detected from modules1across open source projects.
this dataset also provides for each od test the corresponding od relevant tests e.g.
the polluters for a victim and the cleaners for a pollutervictim pair .
we further confirm the correctness of this data by explicitly running all the od tests with their corresponding od relevant tests i.e.
we run each polluter state setter before their respective victim brittle to ensure the test fails passes as expected and for each polluter victim pair we run the cleaner in between to check that the victim passes.
we find that some od tests in the dataset could not be reproduced from running tests with maven surefire and after sharing our findings with wei et al.
to confirm we exclude them from our dataset.
in the end we are able to reproduce od tests and their respective od relevant tests from modules .
we also collect for each module the list of all tests in the test suite.
we first run the test suite in each module to obtain 1a maven project may contain many modules each with their own test suite.table i details of our evaluation subjects.
b. represents brittle v .
represents victim p. represents polluter.
the module corresponding to each id is in our artifact .
num.
of suite id project tests od b. v .
p. v .
runtime m1 activiti activiti .
m2 apache struts .
m3 alibaba fastjson .
m4 apache dubbo .
m5 apache dubbo .
m6 apache dubbo .
m7 apache dubbo .
m8 apache dubbo .
m9 apache dubbo .
m10 apache hadoop .
m11 c2mon c2mon .
m12 ctco cukes .
m13 doanduyhai achilles .
m14 dropwizard dropwizard .
m15 elasticjob elastic job lite .
m16 fhoeben hsac... .
m17 hexagonframework spring... .
m18 jhipster jhipster registry .
m19 kevinsawicki http request .
m20 ktuukkan marine api .
m21 openpojo openpojo .
m22 spring projects spring boot .
m23 spring projects spring boot .
m24 spring projects spring boot .
m25 spring projects spring boot .
m26 spring projects spring... .
m27 spring projects spring ws .
m28 tbsalling aismessages .
m29 vmware admiral .
m30 vmware admiral .
m31 wikidata wikidata toolkit .
m32 wikidata wikidata toolkit .
m33 wildfly wildfly .
m34 zalando riptide .
total avg.
.
tests excluding the tests skipped by maven .
we then extract the test method body of each test using srcml .
we exclude tests that we cannot parse their test method bodies.
upon inspection of a few such tests we find them to inherit code from parent test classes that are not even in the same module as the tests.
in total we have tests in our evaluation modules.
table i lists the subjects used in our evaluation.
for each module the number of tests od tests and polluter victim pairs that have cleaners are shown along with the time to run the test suite of each module.
b. obo baseline our first baseline is to perform the one by one obo approach section ii a straightforward way to find odrelevant tests.
we simulate obo by looking for the first polluter state setter in a failing passing test order respectively going through each test as a candidate od relevant test within the given test order.
as obo s effectiveness is dependent on the given test order we randomly generate different failing passing test orders for each victim brittle respectively ensuring they are valid test orders i.e.
no interleaving of tests across test classes .
by definition an od test must have both a failing and a passing test order for it to be considered an od test.
we can determine whether an od test passes or fails in a generated test order based on the position of the odrelevant tests relative to the od test in a given test order.
the possible failing passing test orders depends on the number oftests and the number of od relevant tests for each od test e.g.
four tests containing one victim and one polluter has failing test orders while five tests containing one victim one polluter and one cleaner has failing test orders.
all of our subjects have more than failing passing test orders.
we also verify that each of our generated test order is unique .
we refer to the average results of test orders for obo as obo avg.
we also simulate a theoretical worst case scenario for obo to find od relevant tests by creating the test order in which the first od relevant test is in the worst possible position.
to create the test order we move all od relevant tests to the very end of the test order the other tests can be permuted in any ordering.
if there are multiple od relevant tests we sort them by runtime keeping the od relevant test with the highest runtime first among the od relevant tests.
so ensures that obo will find this highest runtime od relevant test before the others.
there can be more than one worst testorder if there are multiple od relevant tests with the highest runtime but choosing any such order will result in the same worst case scenario.
we refer to this baseline as obo max.
c. delta debugging baseline shi et al.
previously used delta debugging in ifixflakies to find od relevant tests section ii which we use as a baseline.
given a failing passing test order for a victim brittle respectively delta debugging divides the sequence of tests before the od test in that test order in half checking which half makes the victim brittle fail pass respectively and recursively searches down that half.
the search process stops once it reaches a single test that makes the victim brittle fail pass respectively that test is an od relevant test.
while delta debugging can be directly used to find the polluter state setter it may not always find a cleaner for a given polluter victim pair.
the starting condition for deltadebugging is a passing test order where the polluter runs before the victim so the tests in between contain a cleaner but such a test order is not guaranteed i.e.
the passing testorders for a victim has the victim run before the polluter.
if the passing test order does not have the polluter before the victim then ifixflakies resorts to using obo on the entire passing test order to find cleaners.
our use of delta debugging is the same as ifixflakies we rely on delta debugging if the polluter is before the victim and rely on obo otherwise.
d. rankf lsetup for rankf l we prepare a labeled dataset for training using the od tests we use in our evaluation.
the training dataset needs both positive and negative labeled tuples of tests where a positive label means the tuple represents an od test and its true od relevant test or a polluter victim pair and its true cleaner while a negative label means the other test in the tuple is not a true od relevant test for the od test.
we ensure that the tuples always include at least one positive label tuple.
we train the models in a per module manner similar to prior work on predicting flaky tests .
when we evaluate on tests from one project we train using tuples from all other projects.
thegoal is to simulate a scenario where a developer takes a model previously trained on other projects and uses it for their own project.
for such a simulation our model is trained without observing any tests from the project that is being evaluated.
as we are guaranteed both failing and passing test orders for every od test we can use those test orders to help rankf l run fewer tests.
specifically rankf lneeds to score only the tests before the od test in failing passing test orders for victim brittles respectively the tests after the od test need not be scored and ranked.
for cleaners rankf lscores and ranks all tests in the test suite as the first cleaner may be before or after a victim in any given test order.
rankf luses the same failing passing test orders used by obo avg section iv b .
e. rankf osetup for rankf o we construct valid random test orders of the tests.
we use two types of test orders random test orders per module test orders at which one can be at least sure of a given od test to pass fail at least once and then evaluate how rankf operforms to evaluate its effectiveness as it analyzes more test orders.
developers may use rerunbased od test detection techniques such as idflakies to obtain different test orders for rankf o. we choose orders to match the number suggested by idflakies.
f .
runtime environment for our evaluation we run on ubuntu machines with i7 eight core processors and gb of ram to run rankf land rankf oand to measure test runtimes.
for rankf l we finetune bigbird using a different linux machine equipped with a single nvidia rtx a5000 gpu and gb of ram.
this setup is based on cuda version .
.
v. e valuation a. rq1 rankf efficiency and effectiveness efficiency methodology we evaluate efficiency in terms of time per approach to find the first od relevant test.
for rankf l this time includes the time to run the model on the tests in a given test order.
for rankf o this time includes the time to analyze the provided set of test orders we assume the sets of test orders were run previously as part of some detection process .
after obtaining rankings we measure the confirmation time as the time needed to run each test in the ranked order together with the od test until we confirm the first correct od relevant test.
each time we run a group of tests we include the overhead from invoking maven surefire.
we obtain the overhead time to invoke surefire from running the entire test suite of a module and subtracting the total time taken by all tests as specified in surefire reports from the total time taken by surefire we run this process times to obtain an average overhead time.
we report the total time for using an approach as the sum of the analysis time and the time to run the tests in the ranked order.
as both rankf land rankf oproduce ranked lists of od relevant tests we report the average time for the lists per od test for each approach.
for obo we compute the average time inthe same way as rankf l except we count only the time to run the tests with the same test orders used by rankf l. for delta debugging we compute the cumulative time to run all tests together plus the surefire overhead time from each iteration of the search where each iteration may run multiple tests along with the od test.
the overall time for an od test and a given test order is the sum of the time for all iterations until an od relevant test is found.
efficiency results table ii table iii and table iv show the time for our two proposed approaches and the baselines under the column time to rank confirm in seconds .
we further highlight the cell corresponding to the best lowest time across all approaches and baselines for each module.
we see that rankf always outperforms obo avg and it outperforms delta debugging for all modules except for two modules for polluters and one module for state setters.
for example table iii shows rankf land rankf oachieve a median time of .
and .
seconds respectively to find the first state setter compared to .
and .
seconds for obo avgand delta debugging respectively.
when looking at medians rankf oalways finds the first od relevant test faster than the baselines and rankf l. although rankf o s average performance is affected by extreme outliers e.g.
m22 in table ii it is still the fastest for a majority of modules fastest for modules in tables ii to iv .
if we compare just rankf lwith the baselines rankf lis always faster than obo avg and is faster than delta debugging on median except for finding polluters in which they have the same median time.
from table ii we see that delta debugging is the faster at finding a polluter for most modules .
however rankf loutperforms delta debugging for finding the first state setter modules or cleaner modules .
unlike the baselines rankf lprovides rankings for all tests a developer looking to find more than one od relevant test can use those rankings to speed up their search.
we also show under time to rank in seconds the time it takes for rankf land rankf oto just rank the tests along with the percentage of that ranking time relative to the overall time to both rank and confirm tests.
rankf ltakes longer to rank tests than rankf o which has a negligible cost of ms .
rankf lneeds to process tuples of test code larger sized tuples take longer processing time and uses a complex llm which substantially increases its computational time.
effectiveness methodology to measure the effectiveness of rankf at ranking od relevant tests we report the number of od tests for which the approach finds a correct od relevant test as the top ranked test essentially rank the more the better and the rank of the first od relevant test found by the approach in the ranked list the lower the better .
number of od test with od relevant test at rank .
for rankf l we consider it to have found the od relevant test as top ranked if such a test is rank for at least of the test orders we use per od test section iv d .
for rankf o we randomly sample sets of test orders each with testorders for each module section iv e .
we consider rankf o to have found the od relevant test as top ranked if such a testtable ii results of finding polluter p. for a given victim.
victim w p. rank is the number of victims where p. is found at rank one.
1st p. ranking is the rank of the first p. time to rank confirm in seconds is the sum of the time to rank and confirm the first p. highlighted cells represent the best approach in ranking confirming.
victim w p. rank 1st p. ranking time to rank confirm in seconds time to rank in seconds min id rankf l rankf o rankf l rankf o obo avg obo max rankf l rankf o obo avg obo max dd rankf l rankf o orders m1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m8 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m9 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m10 .
.
.
.
.
.
.
.
.
.
.
.
.
m11 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m12 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m14 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m15 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m18 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m19 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m21 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m22 .
.
.
.
.
.
.
.
.
.
.
.
.
m23 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m26 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m27 .
.
.
.
.
.
.
.
.
.
.
.
.
m28 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m30 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m32 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m33 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m34 .
.
.
.
.
.
.
.
.
.
.
.
.
.
total avg.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
median .
.
.
.
.
.
.
.
.
.
.
.
.
.
table iii results of finding state setter ss.
for a given brittle.
brittle w ss.
rank is the number of brittles for which ss.
is found at top one.
1st ss.
ranking is the rank of first ss.
time to rank confirm in seconds is the sum of the time to rank and confirm the first ss.
highlighted cells represent the best approach in ranking confirming ss.
brittle w ss.
rank 1st ss.
ranking time to rank confirm in seconds time to rank in seconds min id rankf l rankf o rankf l rankf o obo avg obo max rankf l rankf o obo avg obo max dd rankf l rankf o orders m1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m3 .
.
.
.
.
.
.
.
.
.
.
.
.
m8 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m13 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m17 .
.
.
.
.
.
.
.
.
.
.
.
.
m24 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m25 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m29 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m31 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m33 .
.
.
.
.
.
.
.
.
.
.
.
.
.
total avg.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
median .
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv results of finding cleaner c. for a given victim polluter.
victim w c. rank is the number of victims for which c. is found at top one.
1st c. ranking is the rank of first c. time to rank confirm in seconds is the sum of the time to rank and confirm the first c. highlighted cells represent the best approach in ranking confirming c. victim w c. rank 1st c. ranking time to rank confirm in seconds time to rank in seconds min id rankf l rankf o rankf l rankf o obo avg obo max rankf l rankf o obo avg obo max dd rankf l rankf o orders m1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m8 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m9 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m10 .
.
.
.
.
.
.
.
.
.
.
.
.
m11 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m12 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m14 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m15 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m18 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m19 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m21 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m22 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m26 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m27 .
.
.
.
.
.
.
.
.
.
.
.
.
.
m32 .
.
.
.
.
.
.
.
.
.
.
.
.
.
total avg.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
median .
.
.
.
.
.
.
.
.
.
.
.
.
.0is rank for at least of the sets of test orders.
we do not report rank for obo avg obo max and delta debugging because they have no ranking scheme so any od relevant test ranked first is by pure chance or such tests will always be ranked after non od relevant tests for obo max.
rank of the first od relevant test.
for rankf l we report the average rank for the first true od relevant test found per od test across the test orders.
similarly we average the rank across sets of test orders to compute the rank of the first od relevant test for rankf o. for the baseline obo avg we use the same test orders used by rankf lfor each od test.
we find the rank of the first od relevant test for each od test in each test order and report that average rank.
for obo max we compute the rank of the first od relevant test found in the generated worst test order section iv b .
we do not report for delta debugging as it does not rank tests.
effectiveness results from tables ii iii and iv we find that rankf land rankf ohave a non trivial number of od tests with od relevant tests at rank .
for example we find that a substantial number of brittles up to can already have a state setter at rank for rankf o. that being said these results suggest that there can still be meaningful future work that improves the number of od tests with odrelevant tests at rank .
we also see that the time each approach takes is highly correlated to the rankings where the better the rankings of real od relevant tests are the faster the tests are found.
this relationship between the rank and time directly influences how rankf ocompares with all baselines in our results i.e.
if an approach has a better rank than another approach then the first approach must also have a better runtime than the second approach .
any technique where the time to rank is high can result in the relationship being not true which we find is not true for rankf l section v a2 .
for instance in table iv module m27 there are two od tests and rankf lidentify the corresponding cleaner at rank for both od tests while rankf oidentify only one od test s cleaner at rank .
despite having more cleaners at rank rankf l s total time is still worse than rankf o because rankf ltakes more time to rank.
b. rq2 effect of test orders and heuristics on rankf o rq2 methodology we evaluate the effect of using a different number of test orders on rankf o s performance.
to do so we use sets of test orders where each set is called sto compared against sto which represents a dynamic number of test orders that is calculated so that there is a chance the od test will pass and fail at least once when all test orders are run.
both sto 20and sto always draw from the same superset of test orders e.g.
if sto is then sto and sto 20will be the exact same set of test orders .
the way we calculate the number of orders is based on the probability of od test failure obtained from prior work .
e.g.
for brittles we use numorders log .
log x where xrepresents the number of state setters.
rq2 results for sto on average we used .
and .
test orders for victims and brittles respectively slightlytable v comparison of rankf o20ranking functions with average rank of polluter p. state setter ss.
cleaner c. .
plus one methods distance id p. ss.
c. p. ss.
c. p. ss.
c. m1 .
.
.
.
.
.
.
.
.
m2 .
.
.
.
.
.
m3 .
.
.
m4 .
.
.
.
.
.
m5 .
.
.
m6 .
.
.
m7 .
.
.
.
.
.
m8 .
.
.
.
.
.
.
.
.
m9 .
.
.
.
.
.
m10 .
.
.
.
.
.
m11 .
.
.
.
.
.
m12 .
.
.
.
.
.
m13 .
.
.
m14 .
.
.
.
.
.
m15 .
.
.
.
.
.
m16 .
.
.
m17 .
.
.
m18 .
.
.
.
.
.
m19 .
.
.
.
.
.
m20 .
.
.
.
.
.
m21 .
.
.
.
.
.
m22 .
.
.
.
.
.
m23 .
.
.
m24 .
.
.
m25 .
.
.
m26 .
.
.
.
.
.
m27 .
.
.
.
.
.
m28 .
.
.
m29 .
.
.
m30 .
.
.
m31 .
.
.
m32 .
.
.
.
.
.
m33 .
.
.
.
.
.
m34 .
.
.
avg.
.
.
.
.
.
.
.
.
.
more than the default test orders used by idflakies and sto .
to use sto one needs to know the number of od relevant tests which a developer may not know a priori.
we refer to rankf othat uses sto 20as rankf o20 and rankf othat uses sto as rankf o95 .
we randomly sample sets of test orders with the corresponding number of test orders in each set depending on the variant.
we average the ranking of the first od relevant test found between the sets of test orders.
the average ranking of the first od relevant test for rankf o20is actually lower than rankf o95 and the differences in the average ranking of the first polluter statesetter and cleaner found are .
.
and .
respectively higher for rankf o95 than rankf o20.
our results suggest that a small change in test orders do not appear to increase the effectiveness of rankf o and may even appear detrimental although this change has no impact on the relative ranking of rankf ocompared to rankf land the baselines i.e.
rankf o still appears best .
we also evaluate the effects of the five different scoring heuristics section iii b for rankf o. table v shows the average ranking of the first od relevant test found by rankf ofig.
number of modules where a strategy is the best at ranking od relevant tests.
using three scoring heuristics.
the cells with indicate that there is no corresponding od test of a particular category in that module.
this table shows only the results using rankf o20 given that rankf o20and rankf o95 have similar results.
we notice that rankf o20never needed to use any tie breaking.
hence combined d is the same as plus one and combined m d is the same as methods so we do not show their results.
on average rankf o20needs .
.
.
test orders for distance methods plus one heuristics respectively to not have any ties in the test scores.
we find that the plus one heuristic generally performs the best finding the first od relevant test at the lowest rankings between all scoring heuristics on average except for finding the first statesetter per brittle.
as such we present results using plus one in all other tables involving rankf o. we also compute the minimum number of test orders that rankf oneeds for it to always rank a correct od relevant test at rank and for its ranking to not change even if more test orders are provided.
to determine the minimum number of test orders we generate sets of test orders which the first in each set was used for sto and find the test order where a true od relevant test is ranked first and remains first even when subsequent test orders are processed by rankf o. we repeat this process for each of the sets to get an average minimum number of test orders.
tables ii iii and iv show this minimum number of test orders to be .
.
and .
on average needed for polluters state setters and cleaners respectively.
these numbers are substantially more than the test orders used by sto indicating that a developer may need to run many more test orders if they want to obtain stable rank results from rankf o. however based on rq1 we see that just test orders can still substantially reduce the time needed to rank and confirm od relevant tests.
c. rq3 effect of ranking strategies we compare the effectiveness of the three ranking strategies section iii c against each other by counting the number of modules in which a particular strategy obtains the lowest odrelevant test ranks among all strategies.
that is we obtain the average rank of od relevant tests for each approach and strategy then count a module for a strategy when it produces the lowest average rank among all strategies.
figure showsthe effectiveness of the different ranking strategies used for rankf land rankf o with the left subfigure for rankf land the right for rankf o. for each strategy we show three bars each corresponding to finding a different od relevant test polluter blue bars with diagonal lines state setter orange bars with star patterns and cleaner green bars with a crosshatched pattern .
rankf lbenefits the most from using the negative class strategy.
for polluters the negative class strategy achieves the best ranking for out of modules only one less than the positive class strategy and combined class strategy .
for state setters and cleaners the negative class strategy is better than the other strategies outperforming them by modules for state setters and modules for cleaners .
rankf obenefits the most from using the combined class strategy.
for polluters the combined class strategy achieves the best ranking for out of modules.
for state setters the negative class strategy and combined class strategy achieve the best ranking for out of the modules.
for cleaners the combined class strategy achieves the best ranking for out of modules.
the results shown in all other tables use the best ranking strategy for each approach namely negative class strategy for rankf land combined class strategy for rankf o. vi.
d iscussion reason for rankf leffectiveness .
rankf lis effective at predicting od relevant tests despite analyzing only test code.
to better understand its effectiveness we use shap to analyze how the model attributes importance scores to tokens.
we find that the model assigns higher importance scores to tokens that appear in common between tests that are truly related but these tokens receive lower scores for unrelated tests.
as it seems the model relies on similarities between tests we further investigate using tf idf to compute similarity scores between the code of the test pairs with the assumption that tests with higher similarity are more likely related.
we find that using tf idf this way results in a worse ranking of tests suggesting that the rankf lmodel analyzes more complex interactions beyond the presence of similar tokens.
practicality of rankf o. rankf oneeds to analyze many testorders to effectively rank od relevant tests though it performs well even with just test orders section v b .
a developer using tools e.g.
idflakies to detect od tests would already have many test orders which rankf ocan use to find od relevant tests more efficiently.
developers may also be running tests in random test orders as part of their development process during regression testing which rankf omay also use.
if a developer has very few or no test orders to consult we recommend the use of rankf l. learning to rank evaluation .
our evaluation metrics focus on the practical application of running tests in a ranked order to more quickly find the first od relevant test.
there are other learning to rank metrics that could be used to evaluate rankf s effectiveness.
if we evaluate using the standard map score with relevance score as we find that rankf l rankf o and obo avgachieve average map scores of .
.
and .
respectively for ranking polluters higher is better .
for state setters the scores are .
.
and .
and for cleaners the scores are .
.
and .
for rankf l rankf o and obo avg respectively.
we see that rankf land rankf oachieve higher map scores than obo avgfor all types of od relevant tests.
we do not have delta debugging map scores as delta debugging does not rank tests.
more details on these results are in our artifact .
use of gpt .
we evaluate whether we can prompt a pretrained llm namely gpt without fine tuning it to find odrelevant tests.
we design a prompt for gpt .
turbo that uses both code from an od test paired with code from another candidate test asking whether the other test is an odrelevant test for the od test2.
we encounter two main problems with using gpt gpt is a third party service with request limitations and there is a monetary cost.
to reduce the cost of our experiments we conducted a preliminary experiment with modules and found that gpt performs worse than the bigbird model used by rankf l. we excluded two modules m3 and m22 because they have many tests over .
vii.
t hreats to validity our work does not handle cases where one od test is dependent on two or more tests e.g.
for victim v to fail it must run after two tests in order p1 p2 v while v passes in both p1 v and p2 v .
however prior work found dependence on multiple tests at once to be very rare .
we use srcml to extract the test method body.
while srcml is a well developed tool we still encountered challenges such as needing to parse special characters within the code.
in our manual analysis of a few dozen examples we noticed only one inconsistency in extracting the method body.
for rankf l we use bigbird which can be problematic when dealing with smaller test methods leading to excessive padding.
to mitigate this problem we used a smaller limit than bigbird s absolute maximum .
our results may not generalize to projects not in our evaluation.
to mitigate this threat we used the data from wei et al.
.
this data consists of od and od relevant tests in popular java projects that use maven.
viii.
r elated work luo et al.
conducted the first empirical study on flaky tests in open source projects finding that test order dependence was among the top three reasons for test flakiness.
prior work proposed to detect od tests by generating random testorders using code changes systematically generating test orders and analyzing the code .
these techniques are only able to detect od tests and categorize od tests victim or brittle but they cannot find the corresponding od relevant tests.
a developer can use rankf to efficiently find od relevant tests to help debug and fix od tests.
rankf odoes leverage random test orders such as 2a sample of our prompt can be found in our artifact .those taken from idflakies future work can evaluate rankf o s effectiveness when using systematically generated test orders .
rankf oand spectrum based fault localization share a common foundation of analyzing contrasting execution data to localize relevant entities.
rankf o ranks tests based on their likelihood of being od relevant tests while spectrum based fault localization ranks program statements by their suspiciousness scores.
researchers also proposed techniques to automatically repair od tests .
these approaches all rely on odrelevant tests to generate a patch.
the prominent baselines for finding od relevant tests are either running tests one byone obo with the od test or to apply delta debugging to search for them within a test order .
in our work we propose the idea of first ranking tests based on likelihood of being od relevant tests and running them in that ranked order to confirm.
we compare against obo and delta debugging as baselines finding that we can find od relevant tests faster.
there has been recent work in leveraging llms for flakytest related tasks.
fatima et al.
trained a llm to predict which tests are flaky without running them.
others have recently proposed training a llm to further predict the category of a flaky test.
fatima et al.
proposed using a llm to predict the type of fix needed for flaky tests.
chen and jabbarvand leveraged gpt to repair flaky tests including od tests .
unlike prior work we leverage llms to compute the likelihood of od relevant tests for a given od test.
our approach requires the model to process features from multiple tests as od relevant tests only matter w.r.t.
an od test.
we evaluate effectiveness and efficiency in this problem domain i.e.
in how quickly one finds a correct od relevant test instead of machine learning metrics like f1 score.
ix.
c onclusions we propose rankf land rankf oto rank tests by their likelihood of being od relevant tests for an od test.
prior approaches for finding od relevant tests are often timeconsuming so we propose first ranking tests based on their likelihood of being od relevant tests before running them to confirm thereby reducing the overall search time.
rankf l employs an llm that considers only the test method body the code explicitly within a test method to rank tests whereas rankf oanalyzes historical test order execution data for the same purpose.
our results show that both rankf land rankf o are more efficient than baselines at finding od relevant tests.
in the future we plan to improve rankf by giving it other sources of information such as dynamic execution traces of tests and explore the combination of different techniques e.g.
using the results of rankf lor rankf oto guide the other .