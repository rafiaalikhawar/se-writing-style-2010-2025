explaining mispredictions of machine learning models using rule induction j rgen cito tu wien and facebook austriaisil dillig ut austin u.s.a.seohyun kim facebook u.s.a. vijayaraghavan murali facebook u.s.a.satish chandra facebook u.s.a. abstract while machine learning ml models play an increasingly prevalent role in many software engineering tasks their prediction accuracy is often problematic.
when these models do mispredict it can be very difficult to isolate the cause.
in this paper we propose a technique that aims to facilitate the debugging process of trained statistical models.
given an ml model and a labeled data set our method produces an interpretable characterization of the data on which the model performs particularly poorly.
the output of our technique can be useful for understanding limitations of the training data or the model itself it can also be useful for ensembling if there are multiple models with different strengths.
we evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within facebook.
we also compare our algorithm against related rule induction techniques to illustrate its advantages in the context of explaining mispredictions of machine learning models.
ccs concepts software and its engineering computing methodologies rule learning keywords explainability rule induction machine learning acm reference format j rgen cito isil dillig seohyun kim vijayaraghavan murali and satish chandra.
.
explaining mispredictions of machine learning models using rule induction.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
work done at facebook as visiting scientist.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction over the last decade machine learning models have started playing an increasingly prevalent role in automating many types of software engineering tasks.
for instance machine learning has become a popular tool for automating code completion tasks predicting which tests to run finding relevant code fragments from large corpora isolating crash inducing event sequences from telemetry performing type inference and synthesizing or repairing programs .
while software engineering tools powered by machine learning achieve remarkably good results overall it can be frustrating to understand when they do not produce the intended result.
for instance consider a predictive model that given a crash report predicts which of the recent code commits was the likely culprit.
if such a tool does not produce the expected result how do we understand why it behaves as it does?
is it because of some limitation of the model or is it because of a lack of training data or is it something else?
thus in order to understand and improve modern software engineering tools or more generally any program that uses machine learning we need a way to debug trained statistical models.
unfortunately manually debugging modern machine learning models can be an extremely daunting task due to the opaque nature of these models as well as the high dimensionality of the input data.
motivated by this observation there has been significant recent work on improving the interpretability of ml models.
for example local interpretability techniques like lime shap and integrated gradients aim to provide accompanying evidence for predictions on a specific input.
on the other hand global interpretability techniques such as try to shed light on the global behavior of a model either by highlighting which features are the most important or by constructing a simpler surrogate model that emulates a more complex model.
however with the possible exception of there is almost no work that can explain the shortcomings of machine learning models.
that is given a machine learning model how can we identify the salient characteristics of the input data on which the model s predictions are particularly off?
we believe that techniques for understanding the shortcomings of ml models are particularly important in the context of software engineering due to the growing popularity of complex ml models being used to support routine software engineering tasks like debugging program repair and more.
based on this observation the goal of this paper is to take a step towards semi automatically debugging machine learning modelsesec fse august athens greece j rgen cito isil dillig seohyun kim vijayaraghavan murali and satish chandra and shedding light on when a model does not work well.
given a labeled data set dand a trained model m our technique automatically identifies properties of the data on which mperforms particularly poorly.
for instance our technique might uncover that the model is much more likely to mispredict when the crash report pertains to a certain module or when it is excessively long or indeed a combination of such properties.
as we demonstrate in this paper this kind of information can be invaluable for improving the model or identifying problems in the training data.
this information constitutes an explanation in the same sense as in delta debugging delta debugging isolates failure inducing part of the input and analogously our method finds properties of inputs on which a model mispredicts.
one of the appealing aspects of our proposed technique is that it ismodel agnostic .
in other words our method can be applied with equal ease to a complex deep learning model as it can be applied to a simple model like an svm or a decision tree.
furthermore our method produces interpretable results that are easy for a human to understand and act upon.
from a technical perspective the key idea underlying our technique is to learn simple rules i.e.
properties of the input data that are highly correlated with mispredictions of the machine learning model.
in particular our method learns a set of rules that a collectively cover a large portion of the model s mispredictions and b each of which correlate strongly with model mispredictions.
the learnt rules are conjunctions of predicates over the input features and are therefore easily interpretable.
since these rules are intended to be read by a human an attempt is made to keep rules simple.
our specific technical method is an instance of rule induction .
the learnt rules explain the mispredictions of a model globally over a large number of inputs as opposed to explaining misprediction locally for a specific input.
given the importance of this problem there are other works that pursue similar model debugging strategies e.g.
but as explained in more detail in sections .
and our primary objective is precision as opposed to accuracy which has been shown to work well in our case studies.
we have implemented our method in a tool called mdand evaluate it in the context of software engineering tasks such as code completion and predicting code crashes.
in particular we consider models used within facebook and provide case studies illustrating how mdhas been useful for improving these models.
we also empirically evaluate the rules synthesized by our method against those inferred by other rule learners and show that our technique learns rules that correlate more strongly with mispredictions while still keeping rules simple and interpretable.
key contributions.
we introduce the model misprediction diagnosis problem for improving the accuracy of machine learning models adding to the emerging literature on model debugging.
we present a model agnostic technique for finding interpretable rules highly correlated with mispredictions.
we apply our method to ml powered software engineering tools and provide case studies to illustrate how our method has led to useful insights or improvements in these tools.
we compare our method against two existing rule induction techniques and show that it yields rules that are better suited to the task of explaining mispredictions.
figure manual ml model debugging process table samples from a dataset used to train a machine learning model that predicts whether a commit is likely to lead to a crash.
the of diffs that crashed are .
whereas the of mispredictions crashed xor pred are .
loc experience modules ... crashed pred medium false true medium false false low false false low true true high false true overview imagine you have trained a machine learning model for predicting whether a code commit will lead to a crash.
the model might base its predictions on lines of code changed loc experience level of the developer experience number of modules touched in the process modules and potentially dozens of additional columns.
table provides a small sample of the dataset including columns that show the model s prediction pred and the ground truth outcome crashed .
the accuracy of this model on the test set1is .
.
how would you go about improving the accuracy of this model?
in most cases a first step in this direction is to understand the types of input on which the model does not perform well.
in other words what are some salient characteristics of .
of the test data for which the model mispredicts?
to answer this question one would first need to formulate a hypothesis such as the model performs poorly when the lines of code changed loc is rather small .
next to test this hypothesis one needs to write a query to extract this subset of the data evaluate the model on this subset and compare accuracy on this subset against that of the entire dataset.
if is not substantially lower than the hypothesis is incorrect and should be discarded.
in this case the user would need to formulate a different hypothesis and restart the process.
on the other hand if accuracy is indeed much lower on this subset of the data one would likely want to investigate what percentage of the mispredictions the hypothesis explains.
for example if this hypothesis explains of all mispredictions it would be very useful to investigate this hypothesis in more detail.
1we use the term test data generically to mean the data set used for validation of the model.explaining mispredictions of machine learning models using rule induction esec fse august athens greece table different ways a user can act on misprediction explanations produced by md scenario possible action case study training data not rich enough improve training data bug2commit sec .
.
enough training data for ebut model still mispredicts improve model algorithm autocomplete sec .
.
cannot improve training data or model suppress model s output if input belongs in e oncall prediction sec .
multiple models with different misprediction explanationsselect from models based on input diff review sec .
.
on the other hand if it only explains of the mispredictions one may want to find a better hypothesis.
as illustrated by this discussion and depicted schematically in figure this process is extremely inefficient and requires significant human insight and manual labor.
this is particularly true when the model is trained on datasets with a large number of features.
in the particular setting of table it turns out that a hypothesis that explains mispredictions particularly well is experience!
low loc modules which has a precision of .
and a recall of .
.
this means that for this combination of properties in the data the model mispredicts in .
of cases and it explains .
of all mispredictions that occur in the dataset.
the techniques proposed in this paper are intended to simplify this kind of manual effort involved in debugging machine learning models.
in particular our method largely automates steps depicted in figure and produces tested hypotheses that explain when the model performs poorly.
our method dubbed misprediction diagnoser ormdfor short takes as input a data set d labeled with ground truth predictions machine learning model m and a minimum target coverage parameter and automatically produces a misprediction explanation esuch that m s accuracy is much lower for inputs that conform to ecompared to the whole data set and if we discard inputs that conform to e thenm s accuracy improves a significant amount i.e.
eexplains at least a minimum threshold of all mispredictions .
hence the explanations produced by our method pass the checks shown in steps and of figure and obviate the need for manually constructing and testing these hypotheses.
we envision a number of ways in which the output of mdmay be helpful to its users.
in particular as summarized in table the output of mdmay be useful for improving the training data or the model itself and it may also pinpoint opportunities where suppressing the model s output or ensembling different models may be beneficial.
for instance suppose that mdproduceseas a misprediction explanation but the training data does not contain sufficiently many samples that conform to e. this would suggest a distribution shift between the training and test data which in turn calls for improving the training data or for performing data augmentation.
if this is not the case it is likely due to a shortcoming of the model itself which can be improved by understanding why the model mispredicts on the subset of the data conforming to e. if there is no room for improving either the model or the data the output of mdcan still be beneficial.
for example if we know where a code completion engine is likely to mispredict we can choose not to make any predictions rather than confusing developers with incorrect completions.
similarly if we have access to two models m1andm2with different misprediction explanations e1 e2 we canuse this information to obtain a useful ensemble.
we have selected concrete case studies from software engineering to illustrate each of these scenarios section .
.
problem formulation in this paper we focus on machine learning models used for classification.
a misprediction explanation efor such a model mis a boolean function such that inputs xsatisfyingeare likely to be misclassified by mwith high probability.
in the remainder of this section we formalize the desired properties of ein terms of an optimization problem section .
and then describe our hypothesis space section .
.
.
optimization objective letd x y be a labeled dataset and let m x y be a trained statistical model.
for x x we define the following indicator function i x i x ifd x m x otherwise in other words i x is1iff the machine learning model produces the wrong label for x. our goal is to find a boolean function e x that maximizes the following optimization objective e argmax ep i x e x x x in other words the misprediction explanation e should maximize the probability that a given input is misclassified by the model.
in addition because explanations that only explain a tiny fraction of the mispredictions are not very useful we additionally require thate should explain some minimum fraction of the mispredictions p e x i x x x we refer to the value p e x i x x x as the coverage of explanation e. thus putting these together given a model mand parameter our problem in this paper is to find a boolean function ethat solves the following constrained optimization problem for inputs xdrawn from distributionx maximizep i x e x subject to p e x i x .
hypothesis space in principle anyboolean functionx including a deep neural network can serve as a misprediction explanation.
however esec fse august athens greece j rgen cito isil dillig seohyun kim vijayaraghavan murali and satish chandra because our goal is to help humans diagnose problems in their classifiers it is very important that such explanations be easily interpretable.
thus in this work we restrict our hypothesis space todecision lists also called ordered rule sets defined by the following grammar e if then elsee xc c xc c xn c xn c in other words misprediction explanations we consider in this work are simple if else if programs where each conditional is a conjunction of atomic predicates of the form xopcwhere xis a feature and cis a constant.
we use the notation xc xnto indicate categorical and numeric ordinal features respectively.
for categorical features we only allow the equality and disequality operators whereas for numerical ordinal features we allow the and operators.
in the remainder of this paper we use the term ruleto refer to a conjunction of atomic predicates.
we believe that decision lists are well suited to our problem setting because they essentially correspond to an ordered sequence of non overlapping rules that explain different problem cases for the model.
thus an end user can inspect each rule in order perform one of the possible actions listed in table to address the corresponding problem and move on to the next one.
discussion.
the reader may wonder if our hypothesis space is too restrictive in that atomic predicates refer to individual attributes features of the input data.
however it is worth noting that these attributes need notnecessarily correspond to features of the input data that the model was trained on.
for instance consider a classifier that that is trained on a data set that has features like mass and volume and the user wants to know whether there is some correlation between the model s mispredictions and density.
in that case the user can augment the data set to include such a density feature.
taking this one step further it might even be the case that the data set that the model is trained on is entirely different from the data set used for computing misprediction explanations.
for instance consider an image classification model where attributes in the original data set are individual pixels.
since such features may be too low level to be useful for debugging the user may choose to replace the original data set dwith another data set d where features ind correspond to outputs of simpler ml models e.g.
for detecting basic shapes like triangles .
generating misprediction explanations the problem formulation that we presented in section can be addressed by a rule induction technique.
in this section we present a rule induction technique that can be viewed as an instance of subgroup discovery .
in section .
we compare against a couple of other approaches to rule induction and give a broader overview section .
at a high level our approach produces decision lists that solve the constrained optimization problem from section .
.
however since exactly solving this optimization problem is computationally intractable for large datasets our approach approximates this objective using a greedy approach.
note that our approach differs from1 procedure explain d m input labeled data set d x y ground truth input ml modelm x y input target coverage output misprediction explanation for m l labeldata d m a genatoms l rs a list of rules representing decision list cvg current coverage for rs cur l whilecvg do learnrule cur a rs rs cur filter cur cvg computecoverage l rs returnrs algorithm top level algorithm other techniques for learning decision lists in that it optimizes a different objective function.
.
top level algorithm algorithm presents our top level procedure called explain for generating misprediction explanations.
this procedure takes as input a labeled data set dcontaining ground truth labels an ml model m and a target coverage .
we now explain how this procedure generates a misprediction explanation.
construct new data set.
theexplain algorithm starts by calling a procedure called labeldata that constructs a new dataset l x such that l x d x m x in other words the new data set lmaps each input in dto a boolean value indicating whether on not it is mispredicted by the given model m. generate atomic predicates.
next our algorithm calls a procedure named genatoms to generate a universe of candidate atomic predicates of the form xiopcwherexiis a feature and cis a constant.
ifxiis a categorical variable we generate all predicates of the formxi cjandxi cjwherecj domain xi .
for numerical and ordinal features we use the operators and generate the constantscjusing equal frequency binning .
in particular let xi be a numerical feature that takes on values c c1 ... cn in the data set.
to generate atoms of the form xiopcj we first partition the sorted set cintokbins2where each bin has roughly equal size and then use the highest value in each bin as one of the constants in our predicates.
main learning loop.
after the initialization phase lines the algorithm enters a loop that iteratively adds new rules to the decision list lines until the learned decision list achieves the desired coverage parameter .
the algorithm represents the 2the value ofkis a hyper parameter and is set to by default.explaining mispredictions of machine learning models using rule induction esec fse august athens greece learned decision list rsas a list of predicates so for example the list corresponds to the following misprediction explanation if 1 then 1else if 2 then 1else at a high level the learning loop synthesizes the target decision list using a standard sequential covering approach .
in particular it first learns a rule 1for the whole data set then filters out elements satisfying 1 then learns another rule 2for the remaining elements and so on until the target coverage is reached.
thus the predicate ilearned during the i th iteration corresponds to the i th branch in the final misprediction explanation.
intuitively the predicate in the i th branch is the best predictor for mispredictions in the subset of the data not covered by the earlier predicates.
in more detail each iteration of the loop considers a subset of the data called cur initialized tolin the beginning and learns a new rule that serves as a conjunctive misprediction explanation forcur.
this is done at line via the call to learnrule discussed in detail in the next subsection and added as a new branch of the decision list line .
next it removes from curall inputsx x satisfying predicate and computes coverage cvg for the misprediction explanation rsas follows line x x rs x l x x x l x which is exactly the probability from eqn .
since the algorithm terminates only when cvgexceeds the output of the explain procedure is guaranteed to satisfy the coverage constraint from eqn .
.
rule learning we now describe the learnrule procedure for learning a conjunction of predicates over afor a given data set l.3as stated in eq.
our goal is to learn a rule such that px x l x x is maximized i.e.
we want to learn rules that are highly correlated with mispredictions.
observe that this optimization problem is equivalent to maximizing the following alternative optimization objective p x x x l x x x x which corresponds to precision orpositive predictive value .
however if our rule learning algorithm aims to maximize solely p the top level procedure from algorithm may in practice take many iterations to converge.
in particular recall that algorithm aims to find a set of rules that collectively satisfy the coverage threshold.
but if each individual rule i.e.
branch has very low coverage we may need to learn many rules in order to reach the coverage threshold.
beyond making the algorithm slow to converge this would also result in misprediction explanations that are very large thereby compromising interpretability.
thus rather than optimizing only precision our rule learning algorithm also takes into account recall as well as rule size.
in particular our optimization 3recall thatlindicates whether a data point is mispredicted or not.
procedure learnrule l a input labeled data setlwhere a label l indicates misprediction or not input a set of atomic predicates output rule conjunct over atomic predicates b beam initialization done false while done do done true for all i pi b a do i pi eval l getworst b if then b b done false return getbest b algorithm rule learning algorithm based on beam search objectiveois a linear combination of precision recall and rule size o 1 p 2 r 3 size whereris the recall orcoverage defined as in eqn.
and 1 2 3 are tunable hyper parameters.
since our primary goal is to find rules that are highly correlated with mispredictions precision is the primary factor and is therefore given a higher weight by default the other hyperparameters are mainly used for regularization and accelerating convergence.
our rule learning algorithm maximizes this optimization objective using standard beam search as shown in algorithm .
in particular given a beam size of n algorithm initializes all nrules in the beam to true line .
then it enters a while loop that terminates when we fail to improve the objective value of any of the rules in the beam.
in particular in each iteration of the while loop we construct new rules by adding a single atomic predicate pito one of the rules iin the beam.
if the resulting rule yields a better value of the objective function from eqn.
compared to the worst rule in the beam line then we replace with the new rule .
at the end of the loop the algorithm returns the best rule i.e.
one with the highest objective value among all the rules in the beam.
running example.
we illustrate the algorithm on the example model on relating diffs to crashes we introduced in sec .
table shows the universe of atomic predicates generated based on the input feature space illustrated in table .
in every step of our beam search in algorithm we build up new rules that consist of conjunctions of predicates from our atomic predicate universe e.g.
modules loc experience medium .
we evaluate new rule candidates and replace them with the worst performing rule present in our current beam.
in our main learning loop in algorithm we iteratively add the best rule in our beam to the decision list until it achieves our the 4we have also experimented with other optimization objectives such as f1 score but we found that it does not achieve the right balance between different goals in our context.esec fse august athens greece j rgen cito isil dillig seohyun kim vijayaraghavan murali and satish chandra table generated universe of atomic predicates based on the dataset in table atomic predicates experience low experience!
low experience medium experience!
medium experience high experience!
high modules modules modules modules modules modules 9loc loc loc loc loc loc desired coverage parameter e.g.
if modules loc experience medium elseif exp!
low loc .
table shows the final results produced by md.
.
implementation we have implemented mdas a python library that takes as input a pandas dataframe a target variable target coverage and a set of optional parameters and returns a set of decision lists paired with precision recall and coverage metrics.
the main parameters to be provided are the number of bins default is and the beam width default is .
despite taking a greedy approach to solving our optimization objective the proposed method can still be quite slow on large data sets especially where the number of features is very large.
thus our method uses several optimizations to make this computation tractable.
most of the heavy computation in our implementation involves producing subsets of the data by applying filters and comparing metrics of the subset with metrics of the overall data and those of other subsets .
we were able to achieve these significant efficiency improvements by introducing proper indexing and eventually using pandas built in indexing utility .
further we have a set of hyper parameters that dictate when we discard predicates and rules that do not meet certain precision and relevance thresholds.
an implementation is available as an open source project on github evaluation in this section we describe a series of evaluations that are designed to answer the following research questions rq1 how does the rule learning algorithm of md compare with other techniques with respect to finding conditions for mispredictions?
rq2 canmdidentify problems in ml models used in software engineering tasks?
rq3 ismduseful for improving these models?
to answer rq2 and rq3 we deployed md within facebook and report on case studies with selected teams using proprietary data and models.
for experiments comparing mdto existing rule learners rq1 we additionally use public datasets and models from kaggle.
.
comparison with other rule learners generally speaking rule induction techniques differ along three dimensions shape of the learnt rules the quality metric usedfor evaluating the rules i.e.
optimization objective and the search technique.
these characteristics have implications of how well suited the results of the different rule induction techniques are for any particular task.
in this section we compare our proposed algorithm with other rule learning techniques.
in particular we compare against contrast set mining csm and a decision list learner implementing ripper.
csm has been previously used to diagnose problems in software engineering tasks .
ripper is a well known ruleset learning algorithm that has been used in a variety of classification tasks .
we briefly explain where the stucco a csm algorithm and ripper algorithms lie along the axes of rule shape quality metric and search algorithm.
stucco algorithm.
the stucco algorithm aims to discover rules that maximally contrast the target categories.
in the remainder of this discussion we only consider the scenario where there are two target categories.
shape of the learnt rules.
in contrast to our technique that learns decision lists stucco only learns conjunctions of predicates i.e.
rules instead of rule sets .
quality metric.
the stucco algorithm evaluates the quality of a rule based on two metrics largeness andsignificance .
largeness means that the difference of support for each of the two target groups must be larger than a certain threshold where support is the percent of rows in a group that match a rule.
the other consideration is significance which measures whether the difference in support is statistically significant and not just random fluctuation based on a chi squared test.
unlike a direct emphasis on precision and recall within the misprediction group this algorithm optimizes as the name says for contrast between two groups.
search algorithm.
similar to our learnrule procedure csm s stucco algorithm is also based on beam search.
ripper algorithm.
ripper is rule learning algorithm tailored primarily for classification we focus on binary classification .
shape of the learnt rules.
similar to our approach ripper learns decision lists rather than conjunctions of predicates.
quality metric.
the quality metric used by the ripper algorithm is information theoretic gain .
in particular it favors rules that provide the maximum information gain.
search algorithm.
similar to our top level algorithm ripper learns decision lists using a sequential covering approach.
however its procedure for learning rules is notbased on beam search and involves three steps namely grow prune and optimize.
the grow step adds conjuncts to a rule based on the information gain criterion.
the second step prunes rules that do not reduce entropy.
these grow and prune steps are repeated until a stopping criterion is reached.
finally the optimize step attempts to improve the learnt rules using a variety of heuristics.
.
.
evaluation subjects.
as evaluation subjects we use two models from our case studies bug2commit sec .
.
and diff review sec .
.
.
we also consider two publicly available ml models fromexplaining mispredictions of machine learning models using rule induction esec fse august athens greece table overview of final decision lists generated for dataset introduced in section learned rulesets precision recall ifmodules loc experience medium .
.
ifexperience!
low loc modules .
.
if modules loc experience medium elseif experience!
low loc .
.
if experience!
low modules elseif experience!
low loc .
.
figure precision recall curves comparing different rule learners on four different tasks.
for diff review we see few data points for mdbecause it finds rules with high recall even if we specify the coverage parameter to be small.
kaggle with the goal of both providing a replicable study as it is not possible for us to share company internal data as well as to evaluate whether our quantitative results extend to external models.
for kaggle we want to select models that still have room for improvement so we use models that have an accuracy of less than .
based on these criteria we evaluate mdon the the following pairs of data sets and models heart failure dataset with an xgboost model accuracy and cervical cancer risk classification dataset with a naive bayes model .
accuracy .
.
.
results.
our main results are presented in figure which shows precision recall curves for the different misprediction explanation tasks.
in particular remember that mdtakes as input a parameter that denotes target recall i.e.
percentage of mispredictions covered by the explanation .
to generate these precision recall curves we run mdwith different coverage thresholds.
however since the other techniques do not have such a parameter we simply show all rules produced by stucco and ripper after filtering out redundant rules .
also as mentioned earlier stucco only generates conjunctive rules.
as we can see from figure the explanations produced by md outperform stucco and ripper in most cases.
that is for similar recall rates the explanations from stucco and ripper often have lower precision compared to md.
in some cases this is not true for ripper.
however in these cases the explanations produced by ripper are significantly more complex than those of md.
table provides an illustrative overview of the rule complexity produced by each of the techniques that we have observed throughout our study ripper finds rules with high precision and recall by sacrificing interpretability producing rules with very high complexity i.e.
number of conditions .
while both stucco and mdproduce quite succinct rules we find that mdfinds a better balance of precision and recall on the pareto frontier.
while there is no good way of illustrating rule complexity in the plots we separately report mean and standard deviation of totaltable representative rule from each of the techniques rule prec.
rec.
mdifexp!
low mod loc elseif exp!
low loc .
.
csm ifexp m loc .
loc .
.
.
ripperifexp high loc loc elseif loc loc exp m elseif exp high loc loc elseif loc loc exp m elseif loc loc exp m mod elseif loc loc mod .
.
table overview of total rule size mean and standard deviation across techniques and datasets method dataset mean std ripper cervical cancer .
.
md .
.
csm .
.
ripper heart failure .
.
md .
.
csm .
.
ripper bug2commit .
.
md .
.
csm .
.
ripper diff review .
.
md .
.
csm .
.
rule size for each dataset and technique combination in table .
we see that mdgenerally produces rules with lower complexity especially compared to ripper.esec fse august athens greece j rgen cito isil dillig seohyun kim vijayaraghavan murali and satish chandra .
case studies in this section we present case reports applying mdto four ml models used within facebook.
we discuss the insights mdrevealed and how developers were able to improve their tools by acting on these insights.
one of the common themes in applying mdto software engineering models was the need to augment the original data set with additional purely diagnostic features.
that is in addition to the input data we worked with model developers to come up with additional features that could provide useful diagnostic values.
we illustrate this aspect of our case study in the subsections that follow.
.
.
bug2commit.
bug2commit is a lightweight ml model developed at facebook.
given a crash report in the form of text including stack frames when available and a large set of code commits in the form of meta data from the commit but possibly some content as well bug2commit aims to answer the following question which commit is most likely the cause of the crash?
we expect bug2commit to find the true blame commit among the top in a ranked list of suspect commits.
the model is based on information retrieval and computes the cosine distance between vector representations of both the crash report and each of the commits.
bug2commit is known to miss the true blame commit in more than of the cases.
we set out to find out whether there is a characterization of crashes that are particularly prone to misprediction.
we identified several attributes that could be of interest length of the crash report the repository to which it pertains ios android etc.
the number of files modified in the true blame commit which are known for this dataset and so on.
mdfound the following rule among the top with high precision repository ios length trace .
that is if the commit is for ios and the length of the crash trace is greater than lines it leads to misprediction much more often than in the overall dataset.
this rule s precision is .
and recall is .
.
our algorithm discovered this rule without requiring any human insight.
to gain better intuition about how to improve the model we used our domain knowledge to augment the dataset with an additional diagnostic feature namely the overlap of words between the crash report and the blame commit as a percentage of top words in the crash report.
in particular since the model is based on vector distance between the top weighted words bm25 we conjectured that overlap could be a useful diagnostic feature.
with this additional feature md found repository ios length trace overlap with precision of .
and recall of .
.
this now gives an actionable idea can we improve word overlap?
based on the insights we obtained using md we augmented the content for each commit with the names of the modified functions in the changed files.
with this augmentation and retraining the pipeline we got significant reduction in ios misses without deteriorating anything else.
on a dataset with about instances we had mispredictions from ios .
with training over augmented data we had .
mispredictions from ios.
the data indicates that there is more scope to improve overlap between words which we will explore going forward.
.
.
diff review.
the diff review project builds ml models to assess the quality of review that a diff code commit has gone through.
a diff contains various features associated with it such as i a title and summary of the commit ii files that are modified and iii the actual code changes.
different models were built based on each of these three features in addition to a fourth model that combines these features using a deep neural network.
the deep model performs the best overall in terms of precision but it is also the slowest in terms of inference time.
in addition there are some inputs that are predicted correctly by one of the three simpler models but mispredicted by the deep neural network model.
in this case study we use mdto better understand these blind spots for the deep model and construct an ensemble with both better precision and faster inference.
to use md we again came up with additional diagnostic features that could be useful.
in particular we used as attributes the number of modified files of each type in each diff.
for instance if a diff modified java files and python files it would have attributes modified java and modified python .
we then ran md three times once for each of the simpler models with a target where the simpler model predicted correctly and the deep model did not.
md revealed several predicates that indicated that the deep model mispredicted when a significantly large number of files of particular types are modified e.g.
modified javascript .
we confirmed with the designer of the deep model that when a large number of files are modified in a diff it ignores the code changes due to memory constraints and switches to only using the other features.
we proceeded to see if this is something that can be fixed online without re training models.
using the predicates surfaced by md we implemented a quick model selection routine that switches to using the simpler models when the predicates surfaced by md were satisfied.
this eliminated mispredictions compared to the original model.
as an additional benefit we were able to improve inference time by over .
.
.
autocomplete.
we used mdto generate misprediction explanations for a machine learning model used for code auto completion.
this is a classical sequence prediction setting where a standard rnn learns a language model over a token stream considering a context window of size up to .
the model outputs a probability distribution via softmax over the top ntokens and is known to predict the correct next token among the top results in approximately of the cases.
to apply mdfor this purpose we first came up with diagnostic features that could be potentially useful for improving the model.
these features involve the vocabulary of the corpus as well as properties of the token predicted by the model.
some of our diagnostic features include frequency rank of the target token in the vocabulary whether the top prediction is out of vocabulary oov indicated by special unk token whether the receiver of attribute access the token that comes before the .
is out of vocabulary the probability delta between the top prediction and the next one whether the target occurs in the recent context window and number of tokens in the context.
mduncovered that the feature that is most highly correlated with misprediction to be whether the top prediction is out of vocabulary precision of .
and recall rate of .
.
this means that whenexplaining mispredictions of machine learning models using rule induction esec fse august athens greece the top prediction is unk the correct token is not in the top5 results in .
of cases.
some of the other highly correlated features included whether the probability delta between the top prediction and next one was .
precision .
and if the receiver is oov precision .
this information was useful in two ways.
first we were able to use the misprediction explanations of mdto decide when to suppress the output.
that is if the model s predictions exhibit the characteristics uncovered by md we chose not to make predictions rather than frustrating users with incorrect predictions e.g.
suppressing predictions on incidence of the above delta condition would reduce misprediction rate from to .
in addition the output of mdwas also useful for inspiring the developers of the auto completion tool to improve their model.
in particular given the findings of mdwith respect to out of vocabulary tokens they are currently exploring the use of pointer networks and copy mechanisms to improve precision .
.
oncall recommendation in this last case study we consider a machine learning model for assigning points of contact for various tasks in the software engineering life cycle e.g.
debugging code review oncall rotation .
we used mdwith the goal of improving the rank accuracy of a particular model that ranks a set of oncall candidates on files in the source code repository.
the test set for the model is composed of ground truth data on previous oncall assignments.
md uncovered the following rule which has high precision .
and recall .
legacy system score .
inline comment ratio .
decision members ratio .
this rule includes a feature that relies on a score retrieved by a legacy subsystem that was previously responsible for suggesting oncall assignments.
we investigated retraining the model without using the legacy feature.
while we were able to see improvements in model accuracy they were relatively small improvement .
however the rules uncovered by md paired with domain knowledge helped the team uncover sources of noise introduced by one of the features on the relationship of rank accuracy and ground truth.
specifically there can be multiple contenders for a rank1 prediction if more than one person has the same probability score assigned by the model .
when investigating the impact of the legacy system score feature they uncovered that this feature added just enough noise to rank someone as rank even though they should have also been at rank .
upon closer inspection the team learned that this noisiness was only relevant for some parts of the involved repositories.
a possible solution that was discussed was suppressing predictions for this particular part of the input space.
related work the work presented in the paper relates to several topics.
rule learning techniques.
the misprediction explanation finding technique presented in this paper can be viewed as an instance of rule based methods used in machine learning and data mining.
in particular since our method produces decision lists it is particularly related to techniques for learning decision lists and figure illustration of different objectives in our case vs classification.
since our primary objective is precision the green box serves as a better misprediction explanation than the yellow box for our purposes.
on the other hand the yellow box has higher accuracy5compared to the green box vs. and is therefore better for classification.
however it has lower precision vs. .
more recently decision sets .
however the goal of these prior techniques is to perform accurate classification rather than characterizing mispredictions of ml models.
due to these differing goals our method learns rules that optimize a different objective that prioritizes precision rather than accuracy due to this difference our approach learns decision lists that are highly correlated with mispredictions rather than learning rules that strike a good balance between explaining both correct and incorrect predictions.
the difference between our primary objective and the one in classification is shown schematically in figure .
here the green box serves as a better explanation than the yellow box in our case since all data points inside it are mispredicted indicated by so it has higher precision.
on the other hand the yellow box has higher accuracy and would therefore be preferable for classification purposes.
more generally rule learning techniques can be classified as either descriptive rule discovery which aims to find patterns in data orpredictive rule discovery which is used for making predictions for new data id3 cn2 and ripper are in the second class.
since our goal is to discover which types of inputs are misclassified by a model our method is much more closely related to descriptive rule discovery which can be further categorized into three main classes contrast set mining csm emerging pattern mining epm and subgroup discovery sd .
while these techniques are closely related they differ in the following ways csm aims to find statistically meaningful differences between multiple groups epm aims to find new patterns that emerge in new versions of the same dataset and sd aims to find statistically interesting subgroups with respect to some property of interest .
since our goal is to find subgroups in the data for which misprediction ratio is particularly high our approach can be cast as an instance of subgroup discovery which has traditionally been useful in identifying sub populations that are at risk for certain medical conditions .
while many subgroup discovery algorithms have been proposed depending on the application domain they vary with respect to their optimization objectives shape of the 5accuracy is defined as true positives true negatives all.esec fse august athens greece j rgen cito isil dillig seohyun kim vijayaraghavan murali and satish chandra learnt subgroups and search strategies.
in contrast to existing subgroup discovery algorithms our method learns decision lists using a sequential covering approach because we want to identify a set of subgroups that collectively cover a high percentage of the mispredictions.
among different rule learning techniques ours is perhaps most similar to that of lakkaraju et al.
in that their objective function is also a linear combination of different desiderata that aim to strike a good balance between the primary optimization objective and other factors such as conciseness and interpretability.
however as mentioned earlier and illustrated in figure their primary objective is accuracy whereas ours is precision.
furthermore we choose decision lists rather than decision sets due to the additive nature of the discovered rules in terms of recall.
rule learning in software engineering.
rule learning techniques have also been applied in the context of software engineering.
contrast set mining has been used to help debug crashes by identifying properties unique to sets of crash reports .
castelluccio et al.
use stucco to find statistically significant correlations in crash groups at mozilla .
qian et al.
use csm to learn what distinguishes groups of crashes at facebook.
they extend the algorithm to be directly applicable to continuous variables instead of using discretization that can lead to scalability issues .
different rule learning techniques have also been used in defect prediction .
rodriguez et al.
study the use of subgroup discovery algorithms to obtain rules identifying defect prone modules .
song et al.
uses descriptive rules obtained from association rule learning to predict defect associations and correction efforts .
ml model interpretability.
while our primary goal is to help users debug their machine learning models our approach is nonetheless related to the fast growing field of model interpretability .
efforts in this space can be classified as focusing on either local orglobal interpretability.
techniques for local interpretability such as lime anchors and integrated gradients aim to provide evidence to justify a specific prediction made by the model.
in contrast techniques for global interpretability aim to shed light on the overall behavior of a model.
for example gale aims to compute globally important features whereas other methods such as construct a simpler and more interpretable surrogate model for a much more complex model.
more relevant to the software engineering community automatically extracts rules that emulate deep learning models and applies this technique to software engineering tasks such as binary analysis and malware detection.
debugging ml models.
more similarly to this paper there has also been recent interest in developing techniques to debug machine learning models.
for example wu et al.
propose a methodology for debugging ml models used in natural language processing nlp .
specifically they propose a domain specific language for formulating and testing hypotheses about nlp models however it is up to the users to manually formulate these hypotheses in the proposed dsl and decide whether they constitute good misprediction explanation.
in contrast the technique proposed here is intended to automate the task of finding misprediction explanations to a large extent.another related approach is data slicing where the goal is to find a slice i.e.
subset of the data where the difference in error loss is statistically significant and the so called effect size is above a certain threshold .
key applications of data slicing include evaluating model fairness and fraud detection.
while the goal of this work is similar to ours our method differs from theirs in that a we only assume access to the classification result as opposed to error loss b we optimize a different objective function consisting of precision and recall that are easier to evaluate c the rules we produce are additive in terms of coverage as opposed to the topk rules and d we use different techniques for optimizing our objective function.
recently there has also been been a proposal for automatically repairing neural models .
in particular they propose a technique mode for identifying so called faulty neurons and then use this information to select inputs that have a high presence of features that are important for misclassification.
in contrast to mode where the goal is to automatically generate additional training data our goal is to come up with an interpretable explanation of when a model mispredicts.
as discussed earlier this information can be used for data augmentation but it may also be useful for other purposes such as ensembling output suppression or improving model architecture.
in addition our method is model agnostic and does not focus solely on neural models.
limitations while we have shown that md can help explaining mispredictions of models it does have certain limitations in its ability to do so.
firstly mdsearches the space of attributes for likely explanations but the attributes themselves have to be defined by the model designer.
typically one can start with metadata that already exists for the problem from the input space e.g.
length of trace for bug2commit and if the generated explanations are not actionable add further diagnostic features e.g.
overlap percentage .
in some cases this process might require human intervention to converge to the right set of actionable attributes.
secondly mddoes not guarantee that its explanations will lead to significant improvements for the underlying model.
its main purpose is to help the model designer understand dark corners of the model and take appropriate action as in table .
md should not be relied on as a means to produce an improved model which is only a possible side effect of the action.
third we designed md to be model agnostic so that it can be applied to different types of ml models.
while this design choice allows broader applicability it also prevents our approach from taking advantage of white box knowledge that could potentially allow better scalability.
conclusion we have proposed misprediction explanations as a useful concept for debugging and improving machine learning models.
we also presented a model agnostic technique for generating useful and interpretable misprediction explanations.
we demonstrated through case studies that misprediction explanations are useful for improving ml models used in machine learning and we also demonstrated the advantages of our technique compared to other rule learning techniques such as ripper and stucco.explaining mispredictions of machine learning models using rule induction esec fse august athens greece