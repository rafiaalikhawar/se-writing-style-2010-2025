llms meet library evolution evaluating deprecated api usage in llm based code completion chong wang kaifeng huang jian zhang yebo feng lyuye zhang yang liu and xin peng school of computer science and engineering nanyang technological university singapore chong.wang jian zhang yebo.feng ntu.edu.sg zh0004ye e.ntu.edu.sg yangliu ntu.edu.sg school of computer science and technology tongji university china kaifengh tongji.edu.cn school of computer science and shanghai key laboratory of data science fudan university china pengxin fudan.edu.cn abstract large language models llms pre trained or fine tuned on large code corpora have shown effectiveness in generating code completions.
however in llm based code completion llms may struggle to use correct and up to date application programming interfaces apis due to the rapid and continuous evolution of libraries.
while existing studies have highlighted issues with predicting incorrect apis the specific problem of deprecated api usage in llm based code completion has not been thoroughly investigated.
to address this gap we conducted the first evaluation study on deprecated api usage in llm based code completion.
this study involved seven advanced llms api mappings from eight popular python libraries and completion prompts.
the study results reveal the status quo i.e.
api usage plausibility and deprecated usage rate of deprecated api and replacing api usage in llm based code completion from the perspectives of model prompt and library and indicate the root causes behind.
based on these findings we propose two lightweight fixing approaches replace api and insert prompt which can serve as baseline approaches for future research on mitigating deprecated api usage in llm based completion.
additionally we provide implications for future research on integrating library evolution with llmdriven software development.
i. i ntroduction large language models llms have significantly advanced various aspects of software engineering including code completion code understanding defect detection and program repair .
these models pre trained or fine tuned with extensive knowledge of code on large corpora are effective for tailoring to different downstream tasks.
in the realm of code completion the state of the art has evolved from statistics based methods to llm based techniques .
code completion is a sophisticated task that suggests variables functions classes methods and even entire code blocks which depends on developers practical needs.
motivation.
to accelerate development developers heavily rely on third party libraries interacting with them through kaifeng huang is the corresponding authorapplication programming interfaces apis .
however this reliance presents a challenge for code completion tools.
thirdparty libraries constantly evolve to undergo refactorings fix bugs apply security patches or introduce new features.
this rapid evolution leads to frequent api changes with older apis being deprecated and replaced by newer ones.
deprecated apis are discouraged to use because of their incompatiblity with newer features or data which will eventually disappear in future library updates .
taking pytorch a popular deep learning library for instance the api torch.gels was deprecated in version .
august in favor of torch.lstsq .
then torch.lstsq was deprecated in version .
june in favor of torch.linalg.lstsq .
consequently newly developed code should avoid using the deprecated torch.gels andtorch.lstsq .
therefore it s crucial for code completion tools to suggest correct and up to dated apis to developers.
literature.
however to the best of our knowledge the capabilities of llm based code completion regarding api deprecation is understudied .
although there emerges a substantial number of evaluation on code completion a body of research focused on assessing the overall accuracy across various benchmarks .
interestingly ding et al.
identified undefined names and unused variables as the most common syntactic errors produced by llms in python code completions.
izadi et al.
found that incorrect function name predictions were prevalent accounting for of all token level errors.
furthermore recent studies highlighted the issue of hallucinations in llmgenerated code.
their findings indicate the prevalence and potential risks of using unexpected apis.
nevertheless while researchers have noted the prevalence of incorrect function name predictions they have not investigated this issue in depth.
library apis which constitute an important part in predicting external function names are worth attached importance to.
study.
to address this gap we conducted a study to examine the deprecated api usage in llm based code completion.
the study aims to answer the primary research question arxiv .09834v3 feb 2025what are the status quo i.e.
api usage plausibility aup and deprecated usage rate dur and potential root causes of deprecated and replacing api usage in llmbased code completion?
this question is explored through three detailed aspects model perspective rq1 investigates the status quo and potential causes based on the performance of various llms prompt perspective rq2 examines the impact of different prompts on the status quo and potential causes library perspective rq3 analyzes the status quo and potential causes across different libraries.
to address these research questions we conducted a series of experiments involving various libraries and llms.
we collected api mappings between deprecated apis and their replacements from eight popular python libraries.
based on these mappings we retrieved outdated functions and up to dated functions using the deprecated apis and replacing apis respectively.
then we identify the locating lines of the deprecated or replacing apis mask them and subsequent code and use the remaining parts as the code completion prompts.
the original deprecated or replacing api is referred as the reference api.
these prompts were then inputted into seven advanced code llms including codellama and gpt .
to generate completions and analyze the predicted api usages.
if the predicted api usage corresponds to either the deprecated or replacement version of the reference api it is annotated as plausible otherwise it is annotated as others .
the study results reveal the following findings finding in rq1 all evaluated llms encounter challenges in predicting plausible api usages and face issues with deprecated api usages due to the presence of deprecated api usages during model training and the absence of api deprecation knowledge during model inference.
finding in rq2 for the two categories of prompts derived from outdated and up to dated functions the llms performance in predicting plausible and deprecated api usages differs significantly influenced by the distinct code context characteristics of these prompts.
finding to rq3 across the eight libraries the llms exhibit significant differences in their use of deprecated apis influenced by the characteristics of api deprecations during library evolution.
lightweight mitigation.
based on the study results and findings we explored the feasibility of using two lightweight fixing approaches to mitigate deprecated api usage in llmbased code completion.
given a completion containing a deprecated api usage the first approach named replace api directly replaces the deprecated api usage with the replacement and regenerates the remaining parts e.g.
argument list during the decoding process.
the second approach named insert prompt inserts an additional replacing prompt after the original prompt to guide the llms to use the replacing api and then regenerate the completions.
we then evaluate the effectiveness of the proposed approaches in terms of fixing deprecated api usages and the accuracy in predicting line level completions rq4 .
the evaluation results demonstrate thatreplace api effectively addresses deprecated api usages for all evaluated open source llms achieving fix rates exceeding with acceptable accuracy measured by edit similarity and exact match compared to ground truth completions.
while insert prompt does not currently achieve sufficient effectiveness and accuracy in fixing completions containing deprecated api usage it shows potential for future exploration.
contribution.
this paper makes the following contributions we conducted the first study that reveals the status quo and causes of deprecated api usages in llm based code completion from prespectives of model prompt and library.
we proposed two lightweight approaches named replace api andinsert prompt to serve as baselines for mitigating deprecated api usage in llm based completion.
we provide implications for future research on the synergy of library evolution and llm driven software development.
ii.
r elated work a. library evolution library evolution involves refactorings bug fixes and new feature introductions.
typically refactorings can deprecate old apis and introduce new replacements.
several studies have examined the reasons that developers deprecate apis and how the clients react to such deprecations .
the reasons include improving readability reducing redundancy avoiding bad code practices and fixing functional bugs.
deprecated apis can affect hundreds of clients particularly when clients struggle to keep pace with rapidly evolving software .
mcdonnell et al.
found that only of outdated api usages are eventually upgraded to use replacing apis.
similarly hora et al.
found that client developers consumes considerate time to discover and apply replacing apis with the majority of systems not reacting at all.
when clients do not upgrade their apis they silently accumulate technical debt in the form of future api changes when they finally upgrade .
to locate the replacing api existing works leverage change rules written by developers developer recordings similarity matching mining api usage in libraries and in client projects .
henkel and diwan developed an ide plugin that allows library developers to record api refactoring actions and client developers to replay them.
godfrey and zou proposed a semi automated origin analysis using similarities in name declaration complexity metrics and call dependencies.
wu et al.
introduced a hybrid approach combining call dependency and text similarity analysis to identify api change rules.
recently proposed repfinder to find replacing apis for deprecated apis in library updates from multiple sources.
in this work we aim to comprehend the statuses and causes of deprecated api usage in llm based code completion and provide implications for mitigating the deprecated api usages.
b. llm based code completion code completion is an important functionality in modern ides and editors.
historically researchers have explored statistical models .
with the advent in natural languagestudy setup sec.
study results sec.
apimapping collectioncode completion prompt construction rq1 llm perspectiverq2 prompt perspectivellm based code completion code completionslibrary docs rq3 library perspectivestatus quooapi usage plausibilityodeprecated usage rate mitigation approaches sec.
approach replaceapiapproach insertprompt code repos llms completion result annotationrq4 effectivenessofixed rateoedit similarityoexact matchroot causesoroot cause analysis fig.
overview of our study processing researchers have embraced deep learning for code completion because they are similar in token based prediction.
to explore the capability of code completion tools driven by llms numerous evaluations of llms have been proposed.
ciniselli et al.
conducted a large scale study exploring the accuracies of state of the art transformer based models in supporting code completion at various granularity levels from single tokens to entire code segments.
zeng et al.
found that pre trained models significantly outperform non pre trained state of the art techniques in program understanding tasks.
they also reveal that no single pre trained model dominates across all tasks.
xu et al.
evaluated the performance of llms on the humaneval dataset.
ding et al.
identified undefined names and unused variables as the most common errors produced by language models in python code completions.
izadi et al.
evaluated the llms using real auto completion usage data across languages.
they found that incorrect function name predictions were prevalent accounting for of all tokenlevel errors.
besides liu et al.
proposed evaluplus which benchmarks the functional correctness of llm synthesized code using test cases.
in addition to accuracy concerns llmbased approaches face issues such as security vulnerabilities and hallucinations.
sallou et al.
explored threats posed by llms including unpredictability in model evolution data leakage and reproducibility.
liu et al.
categorized the hallucinations brought by llm generated code.
the findings on incorrect function predictions partially motivate our study.
however we focus on the severity of predicting deprecated api usages in llm based code completion.
iii.
s tudy setup we chose python a popular programming language which ranks first among the most popular ones based in the recent year .
we targeted eight popular python libraries.
five of these libraries were used in a previous study on python api deprecation including numpy pandas scikit learn scipy and seaborn.
additionally we added three popular deep learning libraries i.e.
tensorflow pytorch and transformers.
the setup of our study is presented in figure .
it includes four steps.
the api mapping collection gathers mappings betweentable i statistics of our collected api mappings library version range mappings functions outdated up to dated numpy .
.
.
.
pandas .
.
.
.
scikit learn .
.
.
.
scipy .
.
.
.
seaborn .
.
.
.
tensorflow .
.
.
.
pytorch .
.
.
.
transformers .
.
.
.
total deprecated apis and their replacements from various libraries.
the completion prompt construction step involves creating completion prompts by identifying instances of deprecated and replacing api usage in open source python repositories.
the llm based code completion step uses various llms to generate code completions for these prompts.
finally the completion result annotation step automatically annotates the generated completions and calculates relevant metrics.
a. api mapping collection we identified api mappings i.e.
deprecated apis and the mapping replacements from the documentation and change logs from each library version following the previous study .
for each library we selected their versions that were released after january .
we chose january as the starting point for collecting api mappings providing a five year window that balances sufficient version numbers with manageable human effort.
the version ranges are presented in table i. specifically we reviewed the documentation and change logs of each library and manually look for deprecated api occurrences which indicate the corresponding the mapping replacements.
for instance in the api documentation of pytorch version .
.
a deprecation message indicates that torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future pytorch release.
where the mapping of the deprecated api to the replacing api is torch.lstsq torch.linalg.lstsq .
for one to many mappings i.e.
one deprecated api mapped to multiple replacing apis we split them into many one to one mappings.
two authors independently collected the data with a third author resolving inconsistencies.
the process took about three working days and the jaccard coefficient between the first two collectors mappings was .
.
in total we collected a preliminary number of api mappings.
b. completion prompt construction we constructed code completion prompts by searching realworld code snippets that contain usages of either deprecated apis or replacing apis from collected api mappings in open source python repositories to simulate realistic code completion scenarios.
outdated and up to dated function location.
we utilized sourcegraph a widely used code search service to search code snippets.
it supports integration with github where we can retrieve python source files from millions of open source code repositories.
for each deprecated or replacingapi we constructed search queries using both its full qualified name fqn e.g.
torch.lstsq and a logical disjunction of its constituent parts e.g.
torch and linalg and lstsq to ensure comprehensive retrieval.
for each retrieved python source file we parsed it into an abstract syntax tree ast and extracted the containing functions that invoked the deprecated or replacing apis.
specifically we located function definition nodes in the ast and traversed its descendants.
for each descendant we checked if it is a function call node and matched the function call to the deprecated or replacing apis.
to correctly match the function call via api fqns we performed lightweight object type resolution and alias resolution similar to .
object type resolution in the object oriented programming oop languages the apis can be encapsulated into a class as a method.
therefore determining the fqn of the api invocation need to resolve the corresponding type of the invoking object.
for example the pandas library defines a core class dataframe with a member method loc and the client creates an object of class dataframe assigns to a variable dt and invokes the method using dt.loc .
typically it requires resolving the type of dt.
to that end we analyzed theassign statements to track object definitions enabling us to determine the class names for objects in function calls and infer the called apis.
for instance if the object dt in the call dt.loc was created in a preceding assign statement dt pandas.dataframe ... we could infer that the corresponding api was pandas.dataframe.loc .
alias resolution developers can alias packages classes and functions in python using the import as feature .
this mechanism requires resolving api aliases by analyzing import statements.
for example the pandas package is often imported with the alias pd via the statement import pandas as pd .
in this case pd.dataframe.loc was resolved topandas.dataframe.loc .
additionally python provides afrom import mechanism allowing developers to use apis with short names instead of their fqns.
for example through from torch.linalg import lstsq the api in torch.linalg can be directly called via lstsq .
these short names were resolved by analyzing the import statements.
after the lightweight object type resolution and alias resolution we obtained the corresponding fqn for each function call.
we checked whether the corresponding fqns matched the apis in the collected api mappings identifying the first matched api as the reference api.
we denote the containing function as an outdated function if a deprecated api was matched.
meanwhile we denote it as anup to dated function if a replacing api was matched.
we collected python source files by querying sourcegraph.
we filtered out api mappings that returned either no instances ofoutdated orup to dated functions.
as a result we collected outdated and up to dated functions from api mappings.
the statistics are presented in table i. incomplete code extraction.
in the task of code completion developers usually have started with a few lines of code and pause in the middle waiting for llms to return the suggested content based on the upward context.
therefore api mapping torch.lstsq torch.linalg.lstsqcompletion promptreference apifig.
illustration of completion prompt construction for an up to dated function.
to evaluate the performance of llms in the scenario we constructed the line level code completion prompts .
for each outdated or up to dated function we located the invocation line of the deprecated or replacing apis respectively.
we collected the preceding lines before the invocation line into our linelevel code completion prompts for an outdated or up to dated function which is usually incomplete.
figure represents one of our collected up to dated functions.
the function invokes a api of pytorch i.e.
torch.linalg.lstsq in the fourth line.
the line level code completion prompt for this function is highlighted in the wine red dotted rectangle.
after processing all outdated and up to dated functions we obtained two corresponding datasets denoted as oandu respectively.
each sample in oanduwas formatted as pmpt dep rep where pmpt is a code completion prompt pmpt anddep repdenotes an api mapping from the deprecated api to the corresponding replacing api.
c. llm based code completion we leverage multiple llms including open source and closed soure with varying parameter sizes and observe their performance on the code completion task.
the complete llm list is presented in table ii.
codegen 350m 2b 6b codegen is a family of llms developed by salesforce specifically for code generation.
deepseek .3b deepseek coder is designed on top of transformer and tailored for code related applications.
starcoder2 3b starcoder2 is an llm optimized for coding tasks.
it leverages large scale pre training on code datasets to understand programming languages deeply.
codellama 7b codellama is a specialized variant of meta s llama adapted for programming tasks.
gpt .
gpt .
is a general purpose language model developed by openai.
while it is not exclusively designed for coding it possesses powerful code generation capabilities due to its extensive training on diverse text including programming languages.
the first six code llms were downloaded from hugging face .
for llms with python specific versions available we utilized those versions to improve completion results for python functions.
consequently the versions used for codegen and codellama were codegen 350m 2b 6b mono and codellama 7b python hf which were fine tuned on additional python corpora.
for deepseek and starcoder2 the versions employed were deepseek coder .3b instruct starcoder2 3b respectively.
for the general purpose llm i.e.
gpt .
wetable ii evaluated large language models llms model params python fine tuned open source codegen 350m m codegen 2b b codegen 6b b deepseek .3b .
b starcoder2 3b b codellama 7b b gpt .
b unknown queried the model via its official online apis using the gpt .
turbo version released in january .
for the six llms specifically tailored for code related tasks i.e.
three versions of codegen deepseek starcoder2 and codellama the constructed prompts can be directly fed into the models to generate completions.
meanwhile for the generalpurpose gpt .
we provided an instruction to specify the task preventing the model from performing beyond code completion.
the instruction was complete and output the next line for the following python function pmpt .
for all these llms we utilized greedy decoding i.e.
choosing the token with highest possibility at each decoding step to generate one completion for each prompt in ooru.
the maximal output token limit was set to .
the greedy decoding for gpt .
was implemented through setting the temperature parameter to .
the procedure of llm based completion is defined as comp llm pmpt d. completion result annotation for each sample pmpt dep rep we examined the completions generated by llms and determine whether the studied api was predicted and whether the deprecated api or replacing api was predicted.
specifically we extract the fqn of the api invocation in the predicted line using the same object type resolution and alias resolution in sec.
iii b .
the annotation procedure is formally described as follows depc repc others anno comp specifically depc denotes that the llm gives a deprecated completion suggestion and repc denotes that the llm gives a replacing completion suggestion.
we identify depc and repc by matching the fqn of an invocating api to either dep orrep.others denotes that the llm suggests neither of the mapping apis.
moreover if a completion was annotated as either depc orrepc we treat it as plausible .
this indicates that the llm successfully understood the code context and selected a plausible api functionality.
e. metrics we investigate the performance of the llms using the following metrics api usage plausibility aup this metric measures the portion of plausible completions which were annotated as depc orrepc .
aup is defined as aup p x p pi anno llm p depc repc deprecated usage rate dur this metric calculates the rate of plausible completions that were annotated as depc .
dur is defined as dur p p pi anno llm p depc p p pi anno llm p depc repc pis the prompt set i.e.
ooru and i is a binary function that returns if the passed argument is true and otherwise.
the api usage plausibility aup measures how effectively llms predict accurate apis without considering their deprecation status.
this metric is crucial because merely counting deprecated api usages dur does not fully capture how llms manage api deprecations.
for instance a low number of deprecated api usages might be due to the prevalence of others completions suggesting influences beyond deprecation.
to provide fair comparisons across models prompts and libraries we calculate the deprecated usage rate dur based on aup.
a balanced relationship between aup and dur characterized by a relatively high aup and low dur indicates that llms effectively predict both up to date and accurate apis.
in contrast an imbalance suggests either a lower rate of accurate predictions or a prevalence of outdated apis.
all the llms exhibited low aups due to the generation of many others predictions likely because many apis e.g.
pytorch apis share similar usage contexts and offer flexible combinations.
in addition although these others may provide alternative solutions for similar functionalities they fall outside our focus on llms handling of api deprecations.
to clarify we formally defined both others and aup to avoid confusion with terms like wrong or incorrect .
nevertheless we recognize the value of exploring this issue and leave a broader question for future work how to effectively evaluate the functional correctness of generated completions given the numerous alternative implementations for the same goal.
iv.
s tudy results we present the experimental results and key findings on the status quo and root causes of deprecated and replacement api usage in llm based code completion.
as illustrated in figure the results are categorized into three detailed aspects model perspective rq1 prompt perspective rq2 and library perspective rq3 .
for each rq the results and findings are presented by first showing the status quo through api usage plausibility anddeprecated usage rate followed by providing an in depth root cause analysis .
a. rq1 model perspective for status quo and root causes status quo analysis figure shows the distribution ofrepc anddepc completions by the different llms and table iii presents the aup and dur metrics.
api usage plausibility.
the completion distribution and the low aup highlight that all the llms faced challenges in predicting plausible api usages for the given prompts.
the aup of the llms for overall dataset i.e.
all o u ranges from to indicating that a majority of predictions were others .
among the llms codellama 7b achieved thefig.
distribution of repc depc and others completions by llms for prompts from oandu table iii aup and dur for o u ando u modelaup dur o u all o u all codegen 350m .
.
.
.
.
.
codegen 2b .
.
.
.
.
.
codegen 6b .
.
.
.
.
.
deepseek .3b .
.
.
.
.
.
starcoder 3b .
.
.
.
.
.
codellama 7b .
.
.
.
.
.
gpt .
.
.
.
.
.
.
highest aup .
while starcoder2 3b had the lowest overall aup .
.
this may be attributed to the fact that starcoder2 was not specifically fine tuned on additional python corpora unlike other llms such as codellama and codegen.
comparing the three versions of codegen suggests that the capacity of llms to predict plausible api usages increased with model size i.e.
.
.
and .
for codegen350m 2b and 6b respectively given the model architecture and training data remain consistent.
however it s noteworthy that the largest llm gpt .
did not achieve a high aup .
possibly due to the instruction used not being finely tuned with advanced prompt engineering techniques such as chain of thought cot and in context learning icl .
summary all the evaluated llms faced challenges in predicting plausible api usages with aup ranging from to .
effectiveness of llm based code completion generally improved with model size and language specific fine tuning.
deprecated usage rate.
the distribution shown in figure along with the dur metric presented in table iii indicates that all llms faced issues with using deprecated api usages.
the dur of the llms for the overall dataset i.e.
all o u ranges from to with larger models e.g.
codegen 6b codellama 7b and gpt .
generallypredicting more usages of deprecated apis.
considering the differences among the llms codellama 7b and codegen6b demonstrated the best balance between aup and dur.
they achieved significant improvements in aup compared to other llms with a comparable dur of .
.
conversely starcoder2 3b and gpt .
exhibited higher dur i.e.
.
and .
despite having much lower aup.
these results indicate that the preference of llms for using deprecated or replacing apis is not closely related to their capacity for predicting plausible completions.
summary all the evaluated llms faced issues with deprecated api usages with dur ranging from to and larger models exhibiting higher dur.
among the llms codellama 7b and codegen 6b demonstrated the best balance between aup and dur.
potential cause discussion from the model perspective the causes of deprecated api usages consist of two main points model training as libraries evolve both deprecated apis and their replacements are commonly found in open source code repositories.
since the training data for llms are primarily sourced from these repositories without filtering for deprecated apis they often include instances of deprecated api usage.
while the training datasets for specific llms like codellama and gpt .
are not publicly available preventing direct inspection the distribution of deprecated api usages in opensource code repositories can serve as a proxy for understanding the nature of their training data.
in this study we identified instances of deprecated api usage and instances of their replacements in open source code repositories.
these coarse statistics suggest that a significant portion of the training data likely includes deprecated apis.
moreover prior studies have demonstrated a relationship between training data distribution and model output behavior .
when trained on data containing deprecated apis llms may memorize these apis and associated usage patterns as part of their learned knowledge .
the different training datasets also led to different aup and dur of the llms.
model inference llms generated completions by predicting token probabilities based on their learned prior knowledge e.g.
memorized api usage contexts and applying token selection strategies e.g.
greedy search or beam search .
given certain contexts llms were likely to predict deprecated api usages due to high token probabilities without considering any posterior api deprecation knowledge.
to support this hypothesis we calculated the generative likelihoods of both depandrepfor each prompt pmpt across open source llms by analyzing predicted token probabilities during decoding.
to begin we input pmpt into an llm followed by sequentially feeding each token in dep orrep into the model.
throughout this process we recorded the predicted token probability distributions prior to each token in dep orrep being input.
next we extracted the predicted probabilities for tokens in dep orrep from these distributions using their vocabulary indices.
the generative likelihoods of depandrepwere then computedby summing the log probabilities of their tokens a common method in language modeling .
we performed a paired t test on these likelihoods.
the t statistic values indicate that for all six open source llms the likelihoods of deprecated apis are significantly higher than those of replacing apis for prompts from o whereas the opposite is true for prompts from u. the pvalues are approximately zero.
summary there are two primary reasons why llms predict deprecated apis the presence of deprecated api usages in corpora during model training and the absence of posterior knowledge about api deprecations during model inference.
b. rq2 prompt perspective for status quo and root causes status quo analysis table iii presents the aup and dur for prompts from the two datasets i.e.
oandu.
api usage plausibility.
between the prompts from the two different datasets oandu there are some differences in aup for all llms with relative differences i.e.
aupu aupo aupo ranging from .
to .
.
this disparity may be attributed to the imbalance in the number of outdated and up to dated functions in the llms training corpora .
indirect evidence for this is that in this study the up to dated functions collected from open source code repositories were about twice as many as the outdated functions i.e.
vs. even though there was no collection preference.
given that llms were often trained on opensource code repositories they likely learned more up to dated functions than outdated functions leading to better aup for theudataset.
additionally larger llms showcased smaller aup differences e.g.
for codellama 7b possibly due to the better generalizability.
summary the llms showcased difference in aup between the two datasets oandu.
this difference is possibly attributed to the different distribution of outdated and up to dated functions in the training corpora of llms.
deprecated usage rate.
when considering oandu separately all llms consistently demonstrated extremely high deprecated usage rates for o dur and relatively low rates for u dur .
this significant difference is also evident in the distribution of repc anddepc completions shown in figure .
this observed discrepancy can be attributed to the tendency of llms to predict reference apis i.e.
the apis used in the original functions for most prompts in both the oandudatasets reflecting the unique contextual characteristics of each prompt.
to quantify this we define an auxiliary metric the reference usage rate rur as follows rur p p pi llm p reference p p pi anno llm p depc repc which measures the proportion of reference apis in plausible completions.
rur is equal to dur for oand dur foru.
thus no significant difference exists between oandu in terms of rur which is consistently within the range for both.
nonetheless the dur for uindicates that llms still predicted the usage of deprecated apis even for the prompts from up to dated functions.
summary the llms consistently exhibited a significant difference in dur between the two datasets with extremely high deprecated api usage rates for o dur and relatively low rates for u dur .
potential cause discussion the contextual characteristics of the input completion prompts can significantly influence llms use of deprecated apis.
since the completions were generated based on the input prompts specific contexts can lead llms to use deprecated apis.
as presented above the llms showed significantly different dur for prompts from oandu.
upon comparing the prompts from the two datasets we found that contextual characteristics of the prompts such as specific variables and function calls lead to the discrepancy of llms predictions i.e.
whether to use deprecated apis or their replacements.
to quantitatively assess differences in contextual characteristics we conducted lightweight contextual feature extraction for prompts from oanduusing static analysis.
specifically we grouped prompts according to their reference api denoted as api with each group represented as p api .
for each prompt in p api we identified the function used to construct it cf.section iii b and extracted the variable and function call names in this function.
after analyzing all prompts within p api the extracted variable and function call names were compiled into a contextual feature set represented by feat p api for the prompt group.
through this approach for each api mapping dep rep we obtained two paired prompt groups p dep o andp rep u along with their feature sets feat p dep andfeat p rep .
we then assessed the contextual similarity between p dep andp rep using feat p dep feat p rep feat p dep feat p rep .
the overall contextual similarity between oanduwas determined by averaging the cxt sim scores of paired prompt groups across all api mappings.
the final similarity is .
.
this low similarity highlights a significant difference in the contextual characteristics between the prompts from oandu influencing the model s reference decoding direction cf.section iv a2 .
summary the contextual characteristics of the input prompts such as the defined variables and function calls contribute a significant influence to the deprecated api usage.
c. rq3 library perspective for status quo and root causes we conducted a detailed analysis to examine the llms completions across different libraries.
status quo analysis we observe how the aup and dur metrics vary with different libraries.
api usage plausibility.
the results of api usage plausibility are illustrated in the scatter plots depicted in figure where each data point signifies the aup of an llm for a specific library.
across the libraries most llms exhibited relativelyfig.
aup by different llms across eight libraries low api usage plausibility for both oandu with aup below .
notable exceptions were pandas and tensorflow where codellama 7b codegen 6b and codegen 2b represented by symbols and respectively achieved better aup around or greater than .
this aligns with the results presented in model perspective where codellama 7b codegen 6b and codegen 2b achieved the best results for the overall dataset.
on the other hand among the libraries completion prompts from scipy and seaborn posed the most difficulty for llms in predicting plausible api usages with aup consistently below .
summary most llms exhibited relatively low api usage plausibility across the libraries with aup below .
codellama 7b codegen 6b and codegen 2b achieved better aup for pandas about and tensorflow around .
deprecated usage rate.
the results are presented in the scatter plots shown in figure where each data point represents the dur of a particular llm for a specific library.
the results reveal significant differences in the usage of deprecated apis across the libraries with dur ranging from approximately to .
specifically llms generally showed low dur around or below for numpy scikit learn and tensorflow.
in contrast llms exhibited consistently high dur for scipy approximately and pytorch approximately and unstable dur for pandas approximately seaborn approximately and transformers approximately .
summary llms showed significant differences in the usage of deprecated apis across various libraries with dur ranging from approximately to .
llms exhibited consistently high dur for scipy and pytorch and unstable dur for pandas and transformers.
potential cause discussion after analyzing the completion prompts and llms predictions we found that the aup differences between these libraries were primarily due to the characteristics of the api usage context.
more speciffig.
dur by different llms across eight libraries ically the usage contexts of certain apis followed common patterns and were surrounded by related apis allowing advanced llms like codellama 7b to infer the desired api functionality based on the completion prompt.
for example many utility apis in tensorflow such as the deprecated tensorflow.compat.v1.initialize all variables and its replacement tensorflow.compat.v1.global variables initializer were often used in recognizable patterns that were easier for llms to predict.
conversely some apis were used more flexibly in diverse contexts and lacked obvious combinations with other apis leading to low aup for llms in predicting such apis.
for instance many apis in scipy like the deprecated scipy.misc.comb and its replacement scipy.special.comb can be used in diverse contexts to produce combinations for a data sequence.
since the data sequence can originate from numerous sources and be structured in various ways e.g.
numpy array and pandas series it is challenging for llms to predict these apis based on the completion prompt without additional hints.
the characteristics of api deprecations during library evolution also significantly impacted the use of deprecated apis in llm based code completions.
some apis were deprecated due to simple package refactoring leading to similar usage patterns for the deprecated apis and their replacements.
for example in the version .
.
release of pytorch many apis for tensor linear algebra were moved from the torch package to thetorch.linalg package without changes to the api parameters or usage patterns e.g.
torch.lstsq torch.linalg.lstsq .
in such cases llms found it more difficult to distinguish between deprecated apis and their replacements resulting in more frequent use of deprecated apis in the predicted completions.
to further validate this attribution we gathered prompts associated with each library from oandu denoted as ol andul.
we then performed feature extraction and calculated the contextual similarity between olandulfollowing the same approach as in section iv b .
a higher contextual similarity indicates that olandul before and after api deprecation share more common contextual characteristics and usage patterns.
higher ol ulcontextual similarity suggests that it is more challenging for llms to discern whether totable iv relationship between ol ulcontextual similarity and dur range across eight libraries library similarity dur range numpy .
pandas .
scikit learn .
scipy .
seaborn .
tensorflow .
pytorch .
transformers .
use deprecated or replacing apis based on the given prompts.
table iv shows the relationship between ol ulcontextual similarity and dur ranges across the eight libraries indicating that the dur range generally increases with higher ol ul contextual similarity.
summary the characteristics of api deprecations during library evolution significantly impacted the use of deprecated apis in llm based code completions.
minor changes between deprecated apis and their replacements such as simple package refactoring often led to more pronounced issues with deprecated api usages.
v. l ightweight mitigations based on the findings regarding the causes of deprecated api usage we proposed two lightweight mitigation approaches which can serve as baselines for future investigation.
a. motivation as analyzed in the rqs the causes of deprecated api usage in llm based completions can be attributed to model training model inference prompt and library aspects.
in this section we further explore the feasibility of mitigating deprecated api usage issues from these four perspectives.
during model training a direct mitigation is to clean up the code containing deprecated apis from the training corpora.
however this is impractical due to the constant library evolution.
new api deprecations would cause repeatedly training corpora rebuilding and model retraining.
from the library perspective we cannot control the evolution of the library and api deprecation.
therefore mitigation should focus on decoding strategies in model inference and prompt engineering.
b. two lightweight approaches our basic idea is illustrated in algorithm .
given an llm its generation process can generally be formulated as follows to llm ti where tiandtoare the input token sequence and output token sequence respectively.
in the context of llm based code completion ticorresponds to the input prompt pmpt andtocorresponds to the predicted completion comp line .
when comp contains a deprecated api dep line we need to perform fixing approaches i.e.
thefixprocedure to replace depwith the corresponding replacement rep line .
note thatalgorithm deprecation aware code completion input prompt pmpt api mappings m output completion comp 1comp llm pmpt 2for dep rep m do if c ontains comp dep then comp fix pmpt comp dep rep break algorithm approach r eplace api 1procedure f ix pmpt comp dep rep prefix replace dep comp dep rep suffix llm pmpt prefix comp prefix suffix return comp during the contains procedure in line alias resolution is conducted similarly to the process in section iii b1.
the fixprocedure includes reconstructing input through either i replacing the deprecated api tokens or ii inserting additional replacing prompts and then regenerating output approach replace api replacing deprecated api tokens then regenerating.
as shown in algorithm replace depremoves the tokens corresponding to dep and any subsequent tokens from comp and then appends the tokens of rep line .
this results in a prefix prefix that includes rep. the prefix is concatenated with the pmpt and the llm generates the suffix line i.e.
arguments forrepand the remaining tokens to complete the code line.
this concatenation forms the fixed completion comp .
approach insert prompt inserting additional replacing prompt then regenerating.
as shown in algorithm create deppmpt constructs an additional replacing prompt pmpt line formatted as inline comments to guide the llm to use repinstead of dep.
by expressing the replacing instruction into inline comments the re written prompt i.e.
pmpt pmpt can be naturally processed by the llm to continue writing the code.
the replacing prompt is structured as follows where comp dep and rep are placeholders for the original completion deprecated api and replacing api respectively and ... represents the indentation to ensure syntax correctness.
... comp ... dep is deprecated use rep instead and revise the return value and arguments.
the created pmpt is then concatenated with pmpt and fed into the llm to generate a new completion comp line .
algorithm approach i nsert prompt 1procedure f ix pmpt comp dep rep pmpt create reppmpt comp dep rep comp llm pmpt pmpt return comp table v evaluation results of proposed approaches modelreplace api i nsert prompt fr es em fr es em codegen 350m .
.
.
.
.
.
codegen 2b .
.
.
.
.
.
codegen 6b .
.
.
.
.
.
deepseek .3b .
.
.
.
.
.
starcoder 3b .
.
.
.
.
.
codellama 7b .
.
.
.
.
.
gpt .
.
.
.
c. evaluation we conducted experiments to address the following research question rq4 how effectively can the proposed approaches fix deprecated api usage in completions?
evaluation procedure for each llm we selected upto dated samples from uwhere the llm predicted depc completions using deprecated apis i.e.
t pmpt dep rep u anno llm pmpt depc to constitute the evaluation data.
these samples were chosen because they have corresponding ground truth completions e.g.
the line following the prompt in figure which are essential for assessing the effectiveness of the proposed fixing approaches.
for each sample we employed the deprecationaware code completion illustrated in algorithm with the two fixing approaches replace api and insert prompt to prompt the llm to generate a completion.
we used the following three metrics to assess their effectiveness fixed rate fr this metric indicates the proportion of repc completions predicted by the fixing approaches.
edit similarity es this metric measures the similarity between the predicted completions and the ground truth completions by analyzing the edit operations required to transform one into the other.
exact match em this metric calculates the rate of predicted completions that exactly match the ground truth completions after normalizing the return values of function calls i.e.
replacing each element in return value with .
results and analysis the evaluation results of the proposed fixing approaches are presented in table v. replace api.using the replace api fixing approach algorithm all the llms achieve high fixed rates fr with values exceeding .
failures in fixing are mainly due to syntax errors or incorrect function calls caused by erroneous tokens following the replaced apis.
for example consider a depc completion meta graph def tf.saved model.loader.load ... predicted by the original completion procedure algorithm line .
replace api replaces the deprecated api tf.saved model.loader.load with its replacement tf.saved model.load producing a prefix of meta graph def tf.saved model.load line of algorithm .
however codegen 2b then predicts a suffix of meta graph def ... line of algorithm resulting in an erroneous function call tf.saved model.load meta graph def .this issue arises because the replacement operation in replace api can disrupt the naturalness of the code context and the llms decoding process.
an additional interesting finding is that for the three versions of codegen the fr decreases as the model size increases suggesting that larger models might be more sensitive to interventions during decoding.
the completions generated by llms using the replace api approach exhibit high es of over and em rates between and with these rates increasing alongside model size.
the inaccuracies in the completions often involve incorrect return values and arguments for the replacing apis.
incorrect return values arise because the replacing api might include different elements compared to the deprecated api and the replace api approach cannot resolve such inconsistencies in theprefix line of algorithm .
the incorrect arguments are primarily due to the llms limitations in correctly utilizing replacing apis especially those with complex argument lists.
insert prompt .when applying the i nsert prompt fixing approach algorithm the llms exhibited significantly varied fixed rates ranging from .
to .
.
this variation suggests that larger models generally possess a stronger capacity to interpret and utilize inserted prompts formatted as inline comments.
an exception to this trend is deepseek .3b which achieved a notably high fr of .
.
this success can be attributed to using the deepseek coder .3b instruct version which has robust zero shot instruction following capabilities.
moreover the fr differences among various llms highlight their sensitivity to prompt construction which indicates that different llms may require specialized additional prompts in insert prompt for optimal performance.
considering edit similarity and exact match the completions generated by gpt .
and deepseek .3b using the insert prompt approach do not perform as well as their fixed rates suggest.
despite being fine tuned with an instruct tuning corpus they are not specifically fine tuned on python code.
as a result they can follow the instructions in the additional prompt to use replacing apis but struggle to use those apis correctly.
comparison.
the comparison between replace api and insert prompt suggests that direct interventions in the decoding process are more effective than zero shot prompt engineering.
however the results also reveal the potential of insert prompt .
first replace api cannot be applied to black box llms like gpt .
as their decoding processes cannot be controlled by users.
second the additional prompt employed by insert prompt was not carefully tuned for each llm and was performed in a zero shot manner.
in the future fine tuning the llms with instructions specifically designed for fixing deprecated usage could enhance effectiveness.
summary replace api effectively addresses deprecated api usage for all open source llms achieving fix rates exceeding .
the fixed completions demonstrate acceptable accuracy.
while insert prompt does not achieve sufficient effectiveness and accuracy in fixing completions containing deprecated api usage it provides future potential of exploration.vi.
d iscussion a. implications validating llm generated code completions and issuing alerts for deprecated api usages.
our evaluation study reveals that llms frequently use deprecated apis during code completion.
such deprecated api usages can be easily overlooked potentially introducing bugs or security vulnerabilities into software projects.
therefore implementing a validation mechanism for deprecated api usage in llmgenerated code completions is crucial to ensure the reliability of the code.
such a validation mechanism can be further integrated into the post processing of llm based code completion such as issuing alerts to developers.
fixing and updating outdated api knowledge in llms by model level repair.
the current fixing approaches mitigate the issues by intervening in decoding process and rewriting prompts without addressing the outdated knowledge about deprecated api usages stored in the llms.
given the constant evolution of libraries lightweight model repair techniques are potential solutions for fixing outdated knowledge.
model editing is one such direction which can be categorized into the following main categories memory based approaches locating then editing approaches and meta learning approaches .
compared to the proposed fixing approaches model editing can directly update the outdated knowledge about deprecated api usages even incorporating the information of entirely new replacing apis i.e.
the replacing apis introduced after model training and unseen in the training corpora into the llms.
leveraging retrieval augmented generation for up todated code completion.
as discussed in our study findings a key cause of the deprecated api usage in llm based completions is the lack of posterior knowledge about api deprecations.
retrieval augmented generation rag is a suitable technique that can perfectly align with the need for posterior knowledge .
we can explore the possibility of adopting rag to mitigate deprecated api usage in llm based code completion by retrieving related knowledge pieces such as documentation and usage examples.
designing agent multi agent systems for incorporating library evolution into llm driven software development.
in modern software development driven by llms the issues brought by library evolution are encountered not only in code completion.
with advancements in ai agents we can potentially develop autonomous agents or multi agent systems capable of automatically discovering deprecated api usages identifying correct replacements upgrading dependent libraries and fixing the code .
to ensure comprehensive recognition of deprecated api usage and up to date fixes we should design an effective multi agent collaboration pipeline.
one agent should scan the generated code to identify all pieces related to api usage.
another agent should continuously fetch information online checking the latest official api documentation to aid in discovery andcorrection.
finally a dedicated agent should be responsible for implementing the necessary library upgrades and code fixes.
b. threats to validity internal threats.
the primary threat to our study is the soundness of the static analysis used for function location and result annotation.
given that python is a dynamic programming language the lightweight object type resolution and alis resolution we employed may have missed some function calls of deprecated and replacing apis during the matching process.
nevertheless they did not contribute bias to deprecated apis or replacing apis in our extracted function calls.
in the future we aim to address this issue by implementing advanced type inference techniques.
additionally our study currently focuses on function level api deprecation overlooking parameter level deprecations.
future research would investigate a broader type of deprecated apis to provide more comprehensive analysis.
external threats.
a primary threat lies in the choice of python libraries and llms.
to mitigate this threat we reused libraries examined in previous studies and introduced three popular deep learning libraries to ensure diversity and timeliness.
for the llms we selected models covering various architectures model sizes training corpora and training strategies to ensure the generalizability of our findings.
another external threat is that the study was conducted solely on the python language which may limit the applicability of our findings to other languages such as java and c .
in the future we plan to explore the impact of library evolution on llm based code completion across a broader range of programming languages.
vii.
c onclusion in this work we conducted an evaluation study to investigate the statuses and causes of deprecated api usages in llmbased code completion.
the study results all evaluated llms encounter challenges in predicting plausible api usages and face issues with deprecated api usages influenced by the distinct code context characteristics of the prompts and the characteristics of api deprecations during the evolution of these libraries.
we propose two lightweight fixing approaches to mitigate the deprecated api usages and can serve as baselines for future research.
we also provide implications for the research directions for the combination of library evolution and llm driven code completion and software development.
we released the code and data of our study at the website .
acknowledgement this research project is supported by the national key r d program of china 2023yfb4503805 the national research foundation singapore and the cyber security agency under its national cybersecurity r d programme ncrp25 p04taicen dso national laboratories under the ai singapore programme aisg2 gc national natural science foundation of china under grant no.
shanghai sailing program no.
24yf2749500 and the ministry of education singapore under its academic research fund tier rg96 .