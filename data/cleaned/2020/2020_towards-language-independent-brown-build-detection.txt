towards language independent brown build detectiondoriane olewickipolytechnique montr almontr al canadadoriane.olewicki polymtl.camathieu nayrollesubisoft montr almontr al canadamathieu.nayrolles ubisoft.combram adamsqueen s universitykingston canadabram.adams queensu.caabstractin principle continuous integration ci practices allow modernsoftware organizations to build and test their products after eachcode change to detect quality issues as soon as possible.
in reality issues with the build scripts e.g.
missing dependencies and orthe presence of f laky tests lead to build failures that essentiallyare false positives not indicative of actual quality problems of thesource code.
for our industrial partner which is active in the videogame industry such brown builds not only require multidisci plinary teams to spend more e ort interpreting or even re runningthe build leading to substantial redundant build activity but alsoslows down the integration pipeline.
hence this paper aims toprototype and evaluate approaches for early detection of brownbuild results based on textual similarity to build logs of prior brownbuilds.
the approach is tested on projects closed source fromour industrial collaborators and open source graphviz .
we ndthat our model manages to detect brown builds with a mean f1 score of on the studied projects which is three times more thanthe best baseline considered and at least as good as human experts but with less e ort .
furthermore we found that cross project pre diction can be used for a project s onboarding phase that a trainingset of weeks works best and that our retraining heuristics keepthe f1 score higher than the baseline while retraining only every4 weeks.keywordsbrown build build automation continuous integration classi ca tion concept drift.acm reference format doriane olewicki mathieu nayrolles and bram adams.
.
towardslanguage independent brown build detection.
in44th international confer ence on software engineering icse may pittsburgh pa usa.acm new york ny usa pages.
introductionproducing high budget video games aaa games takes a lot ofe ort and organization.
modern aaa games are composed of tensof millions of lines of code scattered across hundreds of thousandsof les and tens of thousands of code changes created by hundreds ofdevelopers.
furthermore modern aaa games developers need topermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro t or commercial advantage and that copies bear this notice and the full citationon the rst page.
copyrights for components of this work owned by others than acmmust be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci c permission and or afee.
request permissions from permissions acm.org.icse may pittsburgh pa usa association for computing machinery.acm isbn .
.
.
.
additional complexities due to the multidisciplinary teams i.e.
artists devs physics experts data scientists very di erentfrom code only projects and to be compatible and scale across amultitude of platforms various consoles pc and mobile devices .the combination of online imperatives and the multiplicity ofplatforms has made reliable continuous integration ci pipelinesparamount to producing aaa games with a limited number of bugs if not bug free.
whenever a code change is submitted for review it is forwarded to the ci pipeline which automates compilation testing and other required activities e.g.
static analysis .
if allthe steps are successful the ci status isgreenand the change isintegrated into the project.
otherwise it turnsredand the developerneeds to debug x the code change based on the ci s build log.1unfortunately the ci build results are not always a reliableindication of a code change s quality since builds can fail becauseof factors related to the build process e.g.
missingdependencies or network disk access on the build machines orbecause of f laky tests i.e.
tests with non deterministicoutcomes .we de ne brown builds as a build failure that changes to a successon at least one build rerun without changing the build setup orsource code.
for instance a build could fail if the communicationbetween the ci pipeline and physical game consoles is interrupted.a test could be failing on an under provisioned overused ci jobworker due to which operations are executed in a di erent orderthan the test expects but pass otherwise.
simply rerunning sucha brown build on the same code change could make the buildfailure disappear.in our experience at one of the world leading aaa game produc ers we saw that brown builds hinder the con dence of developersin their ci and impact the productivity of developers and testersalike.
instead of immediately investigating the source code upon abuild failure the potential presence of brown builds tends to pushdevelopers to manually trigger reruns of builds just to be sure.
onsix large industrial projects analyzed in this paper .5k .4k of commits had at least one manually rerun build job and 27k 179k of build jobs were rerun at least once.
this is not only awaste of hardware resources for ci but also of developers produc tivity since they have to wait for the reruns to nish while the cipipeline is blocked.
while the rerun could still cost less time thanmanually checking the source code in vain it adds to an alreadycongested ci pipeline and delays even further the manual testingprocess of games since testers wait for a green build.hence there is a strong need for pragmatic approaches that candistinguish real build failures from brown builds.
at a minimum such approaches provide a second opinion that could con rm de velopers suspicions about a build failure restoring their trust in ci1in recent years the term ci has started to refer to these pipelines instead of to theoriginal agile practice.
the rest of this paper will do the same.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pi t tsburgh pa usaolewicki et al.results.
one would also expect the approaches to be integrated intothe ci pipeline for example to automatically re run a subset of thebrown builds or to perform other automated resolution techniques making better use of ci resources.
while approaches have been pro posed for f laky test detection based on code dependencies dynamic code analysis or test smell f laky tests are onlypart of the problem since brown builds can be due to the buildprocess itself as mentioned before.
furthermore industrial soft ware projects feature a variety of programming languages and tools making adoption in practice of existing f laky test models hard.
fi nally brown builds are an issue for both young and old projects yet only the latter have su cient historical build information tobuild models the so called cold start problem .to address these shortcomings this paper presents a language independent approach to identify brown builds that leverages thebuild logs produced within the ci.
we extract and lter vocabularyfrom the build logs transform the resulting words into a vector based representation using tf idf then train boosted tree based classi ers to predict if a job is brown or not.
we empiricallyevaluate the classi ers on six industrial projects of a leading aaa games developer and one open source project.this paper addresses the following research questions rq1.can we accurately detect brown builds in a language project agnostic way ?our best models got an f1 score of precision of recall of with f1 score on par with experts prediction to suggesting that our models are pragmatic.rq2.can a brown build prediction model be used on an other project ?a model trained on a project can be used for anew project s prediction during its onboarding phase but a project speci c model should be used as soon as onboarding is over.rq3.how long can a model stay relevant without beingretrained ?we found that we can schedule the retraining of themodel every weeks depending on its performance evolution andage.
data older than weeks does not signi cantly improve thef1 score .
and even harms the model for some projects.our major contributions are a language independent brownbuild detection approach with f1 score two to three times higherthan baseline models for large projects and on par with humanexperts as well as the empirical evaluation of heuristics to counterthe impact of concept drift over time.
a replication package isavailable online .
background related worka continuous integration ci server automatically rebuildsand retests the source code of a project whenever a developer pushesa code change to their version control system in order to detectfaults and merge con f licts as soon as possible.
a typical ci pipelinelike jenkins or travisci consists of a sequence of build stages e.g.
compile followed by test each of which are composed of one ormore parallel build jobs e.g.
compile jobs on linux and windows .the behavior of such a build job is speci ed via build scripts in adomain speci c language like gnu make maven or gradle.
suchscripts typically transform source code into an executable programby invoking con guration tools preprocessors and compilers theyautomate the execution of test harnesses and or can even deploythe produced build artifacts .while conceptually a ci server is thought of as performing onebuild for each new code change in practice its role is much morecomplex.
first the build dashboards of large open source organiza tions like mozilla s treeherder or openstack s zuul show amulti dimensional matrix that tries to summarize results for dozensof ci pipelines and build jobs ranging from classic compilationand unit test execution to deployment or even static analysis.
fur thermore each such pipeline is run multiple times for a given codechange since in each build stage multiple jobs should be run tocover the major feature and environment con gurations the codebase is expected to run on.
if a project has features and shouldsupport operating systems ideally x build jobs should bescheduled in each build stage.
since a typical project has a muchlarger number of features and its environment comprises of notonly di erent operating systems versions but also di erent de vices processor models library dependencies supporting databasesand web servers each code change potentially yields a combinato rial explosion of build jobs to run.
of course if a build is deemed tofail all builds would need to be repeated for the proposed code x.while this build in f lation phenomenon increases con dence in build results it brings a number of major disadvantagesas well.
first it increases the build infrastructure and energy costfor organizations.
google for instance performs 800k builds perday which schedule 150m test runs .
google s breakneck codevelocity of one commit per second coupled with its linearly in creasing test corpus implies a quadratically growing need in buildresources .
similar to many other organizations including open stack they have moved to ci scheduling algorithms that groupmultiple code changes e.g.
all changes arriving within minutes before starting a new build on the entire group instead of execut ing separate builds for each code change.
while successful buildsat the group level leave out many builds at the individual level failures at the group level do require additional follow up builds todetermine the individual code changes responsible for the failures.furthermore while interpretation of build failures is the biggestchallenge of ci users the large number of builds generated bybuild in f lation makes build failures harder to interpret.
for exam ple gallaba et al.
s analysis of .
million github build jobs found that of passing ci builds contain failing or skipped buildjobs that the ci system was asked to ignore by developers with 2out of breakages occuring more than ones.
furthermore ofthe studied build failures were environment dependent i.e.
onlyoccurred for some environments.even worse than the presence of noise and build in f lation due todi erent environments is the ambiguity of build results caused byso called brown build jobs.
these are build jobs that fail inconsis tently due to issues with asynchronous calls multithreading or testorder dependencies .
only by repeating such build jobsa su ciently large number of times we could determine for surewhether a build job really failed or succeeded.
in the meantime the code contribution pipeline conservatively would be locked ba sically preventing other teams to merge in their contributions.
ifone could predict that the build is truly brown i.e.
not a real buildfailure the pipeline could remain open and one could also avoidpropation of brown build results.
this is because in a typical orga nization di erent software components are reused across di erentlibraries and products and the apparent success of a new build2178authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards language independent brown build detectionicse may pi t tsburgh pa usais considered as a go signal for dependent products to adopt thenew release of the component potentially inheriting brown ness.while certain build tools like maven have support for rudimen tary f laky test detection through build repetition this is not e cient nor accurate.
for this reason existing work in this area has focused on empirically understanding the causes of f laky tests as well as ways to detect such tests for speci c pro gramming languages.
other reasons of brown builds than f lakytests are not considered nor approaches that are independent ofprogramming language.
ironically there are often resources avail able to optimize and x the build environment or other resourcescode responsible for f laky builds yet those need to be briefed withconcrete starting points which currently are unknown.
a recentlanguage independent approach proposed by lampel et al.
will be discussed in section .
.3language independent brown builddetection approachthis section will present each step of our methodology for language independent brown build detection as well as the research processaccording to which the approach was designed.
.
vocabulary extraction from log f ileswe rst extract the vocabulary from the log le produced by eachbuild job.
to reduce the dimensionality of this vocabulary we ap plied the following series of rules rule 1all url and le paths identi ed by a regular expres sion are replaced by a known string.rule 2commit ids series of characters containing at leastone letter and one number are replaced by a known string.rule 3any non letter characters are removed.rule 4camelcase notations are split.rule 5english stop words are removed.rule 6a stemming algorithm extracts the root of words.on average these rules reduced the size of the log les by on the dataset studied.
we then split the text into words and per form n gram extraction since using n grams sequences ofywords convey more meaning .
in this study we applied n grams with u1d441 and u1d441 see section .
.
for the hyper parameter tweak ing .
we discarded our experiments with u1d441 and u1d441 becausethey were computationally too expensive and did not signi cantlyimprove the results.
as output of this step each build job is repre sented as a dictionary of features words or series of words to thenumber of their occurrences in the analyzed log le.
the resultingdictionaries vary in size depending on the size and variety of thevocabulary in each le.
we also keep track of metadata surroundingeach ci build job such as the date on which it was submitted thejob id the commit id and the number of retries.
.
vectorizationin this step we create a uniform representation of the data whereeach build job is represented by a vector of relevant features.
thefeatures consist of the tf idf computation of textual build job data words and series of words and other build related metrics.filtvalidfilttrainsub1 unionfeaturesbigtrainmatrixlast ftrainmatrixtestmatrixsub setssplit and filtered by f set alldatafilttestsub2subnf 1f 2f nvalidmatrixsmall train settf idf sub matricesfeature selectionunion of selected featurestf idf on selected featuresfeature selectionfinal tf idf matricesm 1m 2m nfigure iterative vectorization approaches.
.
.
tf idf computation.we used tf idf to represent each tex tual feature in order to reduce the impact of large log les on thevocabulary word counts .
.
.
kbest feature selection.despite the ltering applied on thelog the number of unique series of words used in the vector rep resentation of our data is still signi cantly larger than the numberof observations ci build log les .
consequently we used featureselection to further reduce the number of features used.
for this weused the selectkbest feature selection from the sklearn package in python with anova f value score function.other algorithms considered for feature selection i.e.
infogain correlation computations had similar results as kbest in termsof selected features but were computationally taxing.
.
.
iterative computation.while kbest s time and memory com plexities are acceptable for most cases our dataset s dimensionalityand size made it impractical to apply to the entire data set at once.consequently we perform selectkbest on subsets of our dataset then the union of the best features of each subset is analyzed againas summarized in the middle of figure .in particular we split the training set into sub matrices of 000build jobs then perform tf idf and kbest on each sub matrixindividually.
afterwards the extracted sets of selected features u1d439 u1d456are united into one large matrix.
the kbest feature selectionalgorithm is nally applied one last time on the resulting union set u1d439 u1d462 u1d45b u1d456 yielding the nal matrix with selected features u1d439 u1d453 u1d456 u1d45b u1d44e u1d459.for the iterative approach to be acceptable we need to validatethat the size of the union ed matrix u1d439 u1d462 u1d45b u1d456is similar to the size ofthe individual u1d439 u1d456sets u1d439 u1d462 u1d45b u1d456 similarequal u1d439 u1d456 .
we con rmed that in ourcase study a sub matrix size of led to the size of the u1d439 u1d462 u1d45b u1d456setexceeding k by less than which we considered to be acceptable.
.
.
other metrics.apart from the build log vocabulary we alsoconsidered a number of other features related to the life cycle ofci jobs and to the position of the build job in that cycle.
first we computed the number of prior reruns fails and successes foreach build job.
also we compute the number of commits sincethe last brown job commit since brown to control for temporalinformation about when brownness was found previously.
.
classi f icationas shown in figure the dataset is split into training test validationsets.
vectorization and feature selection is done on the training2179authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pi t tsburgh pa usaolewicki et al.othermetricsshapmatrixfinaltraining validationtf idfmatricesmodel1xgboostxgboostmodel2trainextractfigure two step model training.model1model2a build jobtf idf vectorother metricspred1pred2shapvaluesfinal predvotein outextractfigure prediction with two models.
a vote is computedbetween both model s prediction.set then applied to the two other sets such that the model is notcontaminated by either validation or test set.
we also avoid set contamination by gathering for each commit all its jobs in the sameset.
the test set will be used for the model s performance estimation while the validation set is used to optimize the classi cation model.our models classify failed jobs since successful builds do notblock the ci pipeline and hence cannotbe brown.
however suc cessful jobs might still bring information about brownness andthus helps the model identify brown features to be used on failedjobs predictions.
we de ned a lter to be applied on the dataset to identify from which set we lter out all the successful jobs.
the lter application is shown in figure with the notation u1d439 u1d460 u1d452 u1d461 .
none no lter applied.
train lter applied to training set all lter applied to training and validation set.we use the xgboost algorithm extreme gradient boosting to predict the brownness of each build job using a two step trainingprocess figure .
xgboost is a directed classi cation algorithmbased on random forest.
each prediction is a real value between0 and where brown and safe .
the model also generatesso called shap values which provide the impact of each featureon the training and validation sets predictions.we opted for a two step training process since the resulting com posite model allows to rst focus only on data from the job s buildlog then to add the ci lifecycle related information section .
.
.such a two step model has been used before .
the rst step ofour training process only considers the vocabulary related features while the second one considers the other metrics and combinesthose with the shap values of the rst model.
then we pass bothpredictions through a vote in order to have a nal classi cation ofthe build before making the nal classi cation based on a threshold u1d6fd.
the vote is the weighted sum of both predictions as follows predf u1d6fc100pred1 u1d6fc100pred2 classi cation braceleftbigg u1d435 u1d45f u1d45c u1d464y if predf u1d6fd u1d44e.
u1d452 otherwise both hyper parameters u1d6fcand u1d6fdwere chosen experimentallyduring the validation phase and will be discussed in the section .
.
.
research processthe brown build detection approach presented in this section wasdeveloped using a design science process .
in particular weperformed the following activities inferring objectivesconsultations with several teams to es tablish kpis for model accuracy and concept drift design and developmentexploring build features then iter ating over di erent build log vectorization approaches on apilot project project a of section .
eventually adding asecond model to our approach ci metrics demonstrationof pilot to teams evaluationon projects b f and oss section .
to validategeneralizability.
case study setup4.
projects studied and data extractionwe gathered data from six large projects of our industrial part ner and of one open source project.
these projects were di erentin language purpose i.e.
code analysis cross platform computa tion animation and path nding and size in order to reduce thethreats to external validity of our approach.
table shows the char acteristics of the seven projects with the closed source projects names elided for con dentiality reasons and project os being theopen source project graphviz .
we chose to work with graphvizbecause it is a sizeable multi language open source project withavailable brown build data.we extracted data about build jobs from the ci cd platformsused by the projects gitlab3and teamcity4 see table .
usingthe rest apis provided by both ci cd platforms we are able toextract the logs produced by each build job which forms the inputof our approach see section .
.in order to obtain labeled data for our study we leverage de velopment guidelines adopted by the analyzed software projectsregarding brown build jobs.
in the absence of prediction models the developers and other contributors of the six industry projectshave to rerun a failed build job if it is suspected to be brown.
thisproportion is shown in the second column of table .
out of thefailed build jobs that were rerun those that changed build outcome without any change to the build setup or source code are consid ered to be brown builds by our industrial partner and hence bythis paper third column of table .
other build jobs are labeled astrue result safe.the brown failure ratio u1d435 u1d439 u1d445is the percentage of brown jobfailures over the total number of job failures including the reruns.table shows that the projects vary in their bfr among the failedjobs from e and a b down to os f and c d .
while graphviz does not have an explicit policy to rerunsuspicious builds we notice that its brown failure ratio bfr of13 is close to the median brownness of the other analyzed projects which suggests its oracle is representative.2graphviz gitlab link licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards language independent brown build detectionicse may pi t tsburgh pa usatable information on the projects studied.projfull project historyscraped data contri butors commitmain lan guagesage devopsplat form months jobs failingjobsbrownfailureratio u1d435 u1d439 u1d445 mean job du ration a153kgo js3.5gitlab1823k300830 04b292kc 42963k810037 14c647kc c31722k59865 30d473kc c 1teamcity388k87116 19e454kc 5253k131058 07f473kpy js c 5229k170510 46os2014kc c 17gitlab4347k123713 17table information on the projects studied regarding the brownness labeling.proj failed rerun failed brown failed rerun reruns only for brown cases max reruns0123 a34 14b35 29c8 38d6 9e65 24f18 32os13 .
validation approach4.
.
model building.hyper parameter tweaking.in the methodology section weidenti ed a list of hyper parameters to tweak.
those are gatheredin the following list with a summary of their purpose and the rangeof values that were chosen.
u1d439 u1d460 u1d452 u1d461 lters to apply on the sets.
range none no lter train only fails in the training set andall only fails in all sets u1d441 number ngram to consider.
range u1d43e number of features to be chosen by the feature selector.
range to by u1d6fc weight of the rst model s prediction in the nal predic tion of eq.
.
range to by u1d6fd threshold for the classi cation see eq.
.
range to by for all studied projects we trained models with all hyper parametercombinations.
we used cross validation to validate the results onthe data set obtained in the previous subsection.cross validation settings.to do the cross validation all buildjobs were randomly given a group number from to in order toobtain folds.
the group separation respects the constraint thatall builds related to a given commit id are in the same data setgroup.
furthermore the cross validation is strati ed conservingthe same proportion of brown jobs in each fold.then for each iteration u1d456 the following sets are de ned train set all folds but fold u1d456 of dataset table baselines bfris brown failure ratio per project .baselineproba.brownsafenamebrownpredf1prerecprerecrandom5050 bfr1 2bfrbfr121 bfr12randombbfr bfr2bfrbfr1 bfr1 bfralwaysbrown100 bfr1 bfrbfr1na0 valid set half of fold u1d456 of dataset test set the other half of fold u1d456 of dataset .cross validation is performed once for each hyper parametercombination with the train set used to vectorize section .
thevalidation set used to optimize xgboost s internal parameters onthe trained models and the test set to predict the model on anunseen data set.
the validation evaluation is done twice so thatboth halves of the subgroups are used once as a validation set thentest set resulting in models being trained per cross validation.performance metrics.we use the commonly known precision u1d447 u1d443 u1d447 u1d443 u1d439 u1d443 recall u1d447 u1d443 u1d447 u1d443 u1d439 u1d443 f1 score u1d45d u1d45f u1d452 u1d45f u1d452 u1d450 measures to evaluate our models.
we calculate precision and recall separatelyfor the brown and safe labels.
these metrics are used to compareour models to the baselines in table as well as to determine the op timal con guration of hyper parameters to use for our models.
wecomputed local and global optimizations of the hyper parameters.the local optimization u1d43f u1d45c u1d450 u1d442 u1d45dis the hyper parameter combination u1d450that optimizes the f1 score of the prediction for each given project u1d45d.
as such the optimal local hyper parameter combination may bedi erent for each project u1d43f u1d45c u1d450 u1d442 u1d45d u1d45d arg max u1d450 u1d4391 u1d450 .
in contrast the global optimization is the hyper parameter combination u1d450that2181authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pi t tsburgh pa usaolewicki et al.minimizes the sum of the squared di erences between the f1 scoreof a hyper parameter combination and the local optima across allprojects u1d43a u1d459 u1d45c u1d44f u1d442 u1d45d arg min u1d450 summationtext.
u1d45d u1d45d u1d45f u1d45c u1d457 u1d43f u1d45c u1d450 u1d442 u1d45d u1d45d u1d4391 u1d450 .baselines.the baselines we use are based on random predictionmodels whose theoretical performance is calculated based on agiven percentage of predictions being brown or real failures asshown in table .
since the projects we studied are multi languagein nature it was di cult to apply existing language dependentapproaches as baselines.
for example deflaker covers f lakytest detection in java.
furthermore pinto et al.
predict f laky testcases based on the tests tokens yet brown builds do not necessarilyrelate to test cases or even code .another approach for brown build detection language independentas well was proposed by lampel et al.
who leverage addi tional resource metrics related to execution time and cpu usage.the choice for these metrics is based on observations at mozillathat brown builds would typically take longer to nish than realbuild failures.
while promising most of the metrics required bythis approach were unavailable from our industry partner s ci.
thisis simply due to the ci system being deployed in a cloud whichmakes accurate readings of execution time and cpu usage non trivial because of multi tenancy and unknown changes tothe cloud s underlying hardware.
as such a project might have alow bfr running on slow hardware then start to exhibit a largebfr on better hardware.to validate this hypothesis we scraped our projects buildduration data the only resource metric tracked by our industrypartner for build failures from .
we only included build jobcon gurations with at least one brown build then performed amann whitney test between the build duration distributions of non brown failures u1d6fc .
.
results are available online in ourreplication package .we found that at least on the studied projects multi tenancy andevolving infrastructure impact the applicability of resource metrics.in particular only projects a and f showed a signi cant di erence small cli s delta e ect size con rming lampel et al.
s hypothesis.for those projects we then split the data chronologically into 5groups comparing the build duration of brown and real failureswithin each split.
for project a out of groups show a longer buildduration for brown builds 2x large e ect 2x small .
for projectf build duration di erences alternate over time between non signi cance 2x small .to conclude in the context of language independent brown buildprediction on the studied projects we were only able to use randomprediction models as baselines.
.
.
manual validation.we also compare our model to experts prediction and decision time.
to do so we asked experts of twoprojects to answer a survey related to their project.
these expertscomprise developers of the projects who are knowledgeable of whatthe code changes are and have been exposed to ci feedback.per project the experts were split into two groups project a hasone expert per group project b two experts per group .each survey contained build jobs selected randomly amongthe dataset using the following constraints at most one buildjob per commitid was selected and tp tn fp fn jobs from ourmodel s prediction were equally represented 14of the set of jobs inthe survey for each category .
while each group of experts receivedthe build jobs to evaluate half of the jobs were provided withour model s prediction either correct or incorrect and the otherhalf without.
the jobs coming with predictions for group did notcome with predictions for group and vice versa.each group was asked to evaluate for each given build job if theywould label it as brown based on the associated commit id name di build log other metrics like rerun and commit since brown see section .
.
and for half of the jobs our model s prediction.
.
.
cross project validation.cross project predictions meanstraining the model using a training and validation set of a givenproject then using it to predict the results on another project.
ifthe results are satisfying during the early stages of a new project onboarding phase models built on other projects could be usedinstead of having to wait until enough builds would have been runfor the new project.
in our evaluations we apply each project smodels on the other projects to evaluate cross project performance.
.
.
concept dri f t.our concept drift validation aims to evaluate how long the training set data and the trained model should stayup when the model should be retrained and with which partof the data.
this is important to keep the performance competitive since new cases of brown builds might be missed while old types ofbrown builds might never reappear once corrected making modelsobsolete at some point.intuitively we might think that the more data we gather thebetter results we will get.
however training on a large amount ofdata can be time and resource consuming.
furthermore old datacould be outdated with given data patterns never reappearing inthe newest build jobs traces e.g.
when xing a f laky test or thebuild machine .furthermore we evaluate the question of when to retrain amodel.
on the one hand predicting after each new build job iscomputationally expensive and the bene t of adding a single newbuild job to the training set is relatively small.
on the other hand using the same model forever disregards any new data.
as such new types of brown builds might never be identi ed by the model.first we want to evaluate how our approach is impacted byconcept drift.
second we propose a number of switching heuristics to decide when to retrain the model.
for this validation we usedthe hyper parameters that get the best global optimization.regarding the impact of concept drift on our approach weneeded to split the data sets into sub data sets per period of time which will be referred to as groups we chose to split the data intoweekly groups sunday to monday .
we then need to choose a train ing window size which will be the number of consecutive groupsthat are used as the training data set.
the window is then shiftedacross the whole data set to simulate a model being retrained eachweek each retraining is referred to as a drifting step see figure .the three sets needed for our model computation and validation training validation and test sets are computed respecting timeisolation iso h by selecting consecutive ordered groups iso htime isolation the data set groups are ordered suchthat if groupicomes before groupj all the jobs in groupiprecede all the jobs in groupj.the sets are then de ned for each drifting step u1d456 2182authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards language independent brown build detectionicse may pi t tsburgh pa usa train valid m0train valid m1train valid m2train valid m3 ijbestbestupbestlocthreshcumprodmodel numbertime 0figure switching heuristics.
train set build group u1d457 u1d457 u1d456 u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45b valid set build group u1d457 u1d457 u1d456 u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45b test set build group u1d457 u1d457 u1d456 u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45b by varying the size of the training window we can evaluate howfar in the past data must be retrieved to achieve relevant predictionperformance.to evaluate how the model ages across time we evaluate theperformance on the test sets for each drifting step we can thenobserve the concept drift impact on the brown detector approachby observing the performance of each drifting step by group week and by the number of weeks since the model was trained.we also observe the drift of features to analyze whether there isan evolution in the set of selected features in the vectorization step.for this we measure the weekly feature surviving ratio wheremodeli s feature surviving ratio at weekjis the percentage offeatures selected at weekithat still are selected in the model ofweekj.
if the surviving ratio decreases consistently over time for allmodels then the feature selection and the model are impacted bythe concept drift and a retraining would be relevant.
we show themedian feature surviving ratio of models after u1d465weeks of existence.
.
.
model switching heuristics.based on the evolution of theperformance of each model over time we evaluate when to changefrom one model ot another in order to obtain the best performance.for this we de ne di erent switching heuristics that will be usedto decide when to retrain and switch models.
those algorithmswill either usea prioriora posterioriheuristics.a prioriheuristicschoose to switch to the model of week u1d456based on information aboutall weeks u1d457before u1d456 u1d457 u1d456 .a posterioriheuristics choose to switch tothe model of week u1d456based on information about all weeks u1d457before u1d456and week u1d456included u1d457 u1d456 .
the latter are unrealistic modelssince they need information about the current week before it evenhappened but can be used as baseline for the a priori heuristics.for the de nition of the model switching heuristics let us as sume that the drift parameters are u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45b u1d464 u1d456y u1d463 u1d44e u1d459 u1d456 u1d451 u1d44a with u1d464 u1d456y u1d463 u1d44e u1d459 u1d456 u1d451 the window size of the validation set and that we have u1d44a u1d441weeks of data and that for each week u1d464 u1d456with u1d456 thereis a model u1d45a u1d456created with the data from weeks with u1d456 .
we also de ne the model u1d45a u1d460 u1d456as the model selectedby a switching heuristic at u1d464 u1d456.
the performance of model u1d45a u1d457onweek u1d464 u1d456is u1d45d u1d452 u1d45f .
u1d456 u1d457 .
figure shows an example with u1d44a .for the following switch heuristics description we suppose thatwe are currently starting week u1d464 u1d456and that the previous chosenmodel u1d45a u1d460 u1d456 1is u1d45a u1d457.best a posteriori algorithm where the model at u1d464 u1d456is chosento be the model u1d45a u1d458with u1d458 u1d456with the highest performance u1d45d u1d452 u1d45f .
u1d456 u1d458 as shown in orange on figure .bestup same as best but with u1d460 u1d456 u1d458 u1d456 in red on figure .bestloc same as best but with u1d458 u1d460 u1d457or u1d458 u1d456 in blue onfigure .diagonal a priori algorithm where the model at u1d464 u1d456is u1d45a u1d456 with out looking at other models performance switch every week tothe newest model .fix a priori algorithm where the model is switched every xednumber of weeks to the most recent model.
if week u1d464 u1d456needs toswitch because the model has been used for the xed numberof weeks the chosen model is u1d45a u1d456.
otherwise keep u1d45a u1d460 u1d457 withoutlooking at the performances.thresh a priori algorithm where the chosen model at u1d464 u1d456is u1d45a u1d456if the performance u1d45d u1d452 u1d45f .
u1d456 u1d460 u1d456 of u1d45a u1d460 u1d457during week u1d464 u1d456 1waslower than a threshold u1d447 as shown in green on figure .cumprod a priori algorithm where the model chosen at u1d464 u1d456is u1d45a u1d456if the cumulative product producttext.
u1d456 u1d457 u1d450 u1d462 u1d45f u1d45f u1d45d u1d452 u1d45f .
u1d457 u1d460 u1d457 of the perfor mance of the models of week u1d464 u1d457with u1d460 u1d457 u1d457 u1d456 was lowerthan a threshold u1d447.
this is shown in purple on figure .
we use acumulative product such that the model ages over time since themultiplication will reduce the value over time.algorithmsbest bestupandbestlocare weekly upper boundsfor our analysis representing three ways to choose the best modelsa posteriori.
whilebestwill always outperform the other twoa posteriori algorithms we included the latter two to improveunderstanding of the ndings.
on the a priori side diagonalandfixare heuristics based only on a model s age whereasthreshandcumprodare based on the evaluated performance and age ofa model.to compare model switching heuristics we aggregate the per formance over the whole period weeks u1d4640to u1d464 u1d441 for each week sselected model by summing up the weekly confusion matrices intoone overall confusion matrix i.e.
counting up the true positives false positives etc.
averaging the weekly performance would nothave worked due to weeks with very few or no brown jobs at all.we will as well compare those switching heuristics withalways brown which is a random prediction based on the brownnessratio introduced in section .
and de ned in table .
finally eachswitch heuristic has a life expectation lifeexp which is the mediannumber of weeks a model is used before a new model is trained.
rq1 can we accurately detectbrown builds in a language project agnostic way ?
.
hyper parameter optimization.motivation.the purpose of this rst validation is to identifythe best hyper parameters for our model and to evaluate if theclassi ers performance is relevant in practice.approach.for this rq we use all projects thus including theopen source project using cross validation to select the best set2183authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pi t tsburgh pa usaolewicki et al.table model performance.projectbrown detectorbaselinelocal opti global opti random50 randomb alwaysbrown brownsafebrownsafebrownsafebrownsafebrownsafef1 pre recpre recf1 pre recpre recf1 pre recpre recf1 pre recpre recf1 pre recpre reca67 na0b73 na0c38 na0d35 na0e88 na0f52 na0os63 na0median63 na0mean59 na0of hyper parameters per project local optimization and for theseven projects at once global optimization see section .
.
thebest hyper parameter values for both local and global optimizationsare gathered in table .results.our models are two to three times better than thebest random baselinealwaysbrownin terms of f1 score foreach studied project.table shows the performance per project of the local and globaloptimization of hyper parameters in comparison with the baselines.we observe that performance varies substantially from one projectto another.
this is due to the unbalance of brownness in build jobsacross the studied projects the higher the ratio of brownness inthe failed build job the higher the f1 score will be.
for instance project e has the best f1 score and the highest brown failureratio among the studied projects u1d435 u1d439 u1d445 while project c hasthe second worst f1 score and the lowest brown failure ratio u1d435 u1d439 u1d445 .when switching from local to global optimization the f1 scoredecreases between for project c and for projects d and oswhen choosing a globally optimal set of hyper parameters.the baseline that gets the best f1 score isalwaysbrown.
yet the median f1 score of this baseline is more than three times compared to worse than the global optimization of our models.furthermore the os project shows results close to project f whichis the project with the closest u1d435 u1d439 u1d445 providing initial evidence thatthe results could be generalized to open source projects.in addition to f1 score table also shows the other performancemetrics.
the median precision recall goes from for localoptimization to for global optimization.
having precisionclose to can seem problematic however let us not forget thatthe dataset is highly unbalanced and that the values are higher thanthe baselines.
furthermore of the true failures non brown are correctly identi ed by our model and of failure predictionsare actually true failures which meets our objective of making theci results more reliable.regarding feature selection we analyzed the features with sig ni cant tf idf values of the os project to better understand whatdrives the models.
we observed that occurrences of read databas in jobs tend to be related to brownness and of return error tendto be related to true failures which are intuitive to understand.table optimal hyper parameter values per project in termsof f1 score.optimizationhyper parameterstype project u1d439 u1d441 u1d43e u1d6fc u1d6fdlocalatrain ball ctrain dall eall fall ostrain global allall however less intuitive observations were seen with features suchas build object or occur dure that can be related to both labels depending on the other terms in the logs.
.
manual validationmotivation.while the previous subsection compared our modelsto intuitive baselines this section compares our models perfor mance to human experts from our industrial partner to validatethe extent to which our prototype can perform as well if not betterthan experts in terms of predicting brownness quicker.approach.the manual validation was done on projects a and bonly.
each expert is given failed build jobs and must identify ifthey are true failures or brown.
the nal expert s performance iscomputed respecting the original confusion matrix of our modelprediction.
for project a this contains tp tn fp fn and for project b tp tn fp fn .table shows the mean prediction results for the experts in eachproject a and b by category.
we use the mean because there wereonly respectively and experts in the projects and only jobsper category making outliers unlikely.results.project a s experts manually predict brown buildswith mean f1 score better than our models while projectb s experts manually identify jobs as brown with mean f1 score lower than our models.from table i.e.
the results for project a we observe thatexperts of project a have predicted tp jobs correctly in of the2184authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards language independent brown build detectionicse may pi t tsburgh pa usatable manual validation s mean prediction of theexperts by category.expected to beexpected to bemean evaluationbrown true failure time by build jobprojecttp fntn fp a83 44b68 55table manual validation s performance results.projectcomparisonbrownsafef1 pre recpre recapred vs oracle62 75expert vs oracle66 73bpred vs oracle67 64expert vs oracle50 37cases and tn in whereas our model by de nition identi es both groups correctly in of the cases.
however the experts correctly identify of fn and of fp whereas our model misclassi es all the jobs.
project b shows lessvariation from one category of results to another showing that thedevelopers on that project globally identify jobs as brown in of the cases without signi cant di erences between categories.
forboth projects no signi cant di erence was found in the survey seesection .
.
for predicting jobs when we provided our predictionor not.in table we evaluated the same metrics as for the hyper parameter validation this time comparing the global optimizationof the hyper parameters for projects a and b with their respectiveexperts performance.
the metrics are computed by weighing thedi erent categories tp tn fp fn with the real ratio of each cat egory from the globally optimal model.
we observe that expertsfrom project a have better results than our model with an f1 scoreof higher than our model .
for project b we see that ourmodel has better results with an f1 score of higher thanthe experts .even though our models precision and recall are not perfect weobserve that they are similar to or better than experts performance.our model can thus be used to improve the ci system by detectingbrown builds before the developer has to rerun them manually saving the experts precious time.
in fact our model once trained only takes seconds to predict the status brown or safe of a newbuild jobs reducing the e ort needed by experts since the experts ofprojects a and b took on average 2min44 and 2min55 respectively to interpret the brown ness of the analyzed build jobs.
rq2 can a brown build predictionmodel be used on another project ?motivation.one of the hypotheses we de ned from the begin ning was that a classi er have to be trained for each project.
this isa limitation as each project needs a cold start period to gather databefore being able to provide predictions while brown build detec tion would be required from the start.
to challenge this hypothesis here we build and evaluate cross project prediction models.table cross project f1 score.
the project pred is pre dicted by the a model trained on the project train .
diago nal is the global optimization and uni2605 isalwaysbrown.
predtrainf1 scorea b c d e f os uni2605a6232 523b156727 427c2 85d8 45e53 2337f10 na 1846299os11 45212approach.for this validation we used each project s model topredict brown builds on other projects which we call to ap proach .
the models use the hyper parameter values that yieldedglobally optimal performance across the projects for within projectprediction .
we also considered applying a leave one out approach training with all datasets but one then testing on the latter butthis approach would be computationally much more expensive thanthe to approach due to the very large training set .in this section we will refer to cross project prediction whena project is predicted by a model trained on another project andwill refer to by project prediction otherwise see section .
.results.f1 score of cross project prediction is between 4and median of lower than the prediction by projectfor all studied projects.
provided the right training project s can be identi f ied cross project prediction could be a viablealternative at least until a project speci f ic model is available.in table we gathered the performance of cross project vali dation for the seven studied projects for proj u1d465and proj u1d466.altin stud ied projects where u1d465 uni2260 u1d466.alt proj u1d465is predicted by a model trainedon proj u1d466.alt .
it can be seen that the cross project prediction for allprojects is to lower than the global hyper parameter optimiza tion.
some cross project combinations still give results close to theby project prediction.
for instance project e predicted by projectb gives an f1 score of lower than by project prediction .other combinations do not perform prediction correctly at all forinstance project f s prediction by project c always identi es failureas true failure yielding an na result.one of the authors who is an expert on project a manuallyanalyzed a sample of tp true brown predictions for project a. weextracted the corresponding tp predictions of each cross projectcombination.
from the sampled predictions corresponded tocases the author was aware of while identi ed new brown buildcases due to issues with the le system and network.furthermore the f1 score of cross project results are not con sistently higher than thealwaysbrownbaseline varying from animprovement of to a loss of on the f1 score with a medianof .
this validates that a project speci c model is essential assoon as historical data is available but that given the right trainingproject s cross project prediction could be a feasible temporarysolution.2185authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pi t tsburgh pa usaolewicki et al.table switcher comparison with a u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsize of .
the table vertically is split into a posteriori a priori heuristics andbaselines.
bold indicates the line with the best f1 score life expectancy of a project per category.switcherproject aproject bproject cproject osbrownsafelifebrownsafelifebrownsafelifebrownsafelifef1 pre recpre recexpf1 pre recpre recexpf1 pre recpre recexpf1 pre recpre recexpbest post 984bestup post 984bestloc post rst51 9716diagonale56 x weeks x weeks x weeks x weeks 988cumprod u1d447 .
978cumprod u1d447 .
978cumprod u1d447 .
975thresh u1d447 .
972thresh u1d447 .
972alwaysbrown23 na027 na055100 na012 na0best hyper param62 figure feature surviving ratio per project with a u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsize of .
rq3 how long can a model stayrelevant without being retrained ?motivation.this rq evaluates how well models can deal withand mitigate concept drift.
in particular we aim to evaluate thesize of the data needed for a model to achieve optimal predictionresults and to evaluate when to change models and thus retrain .approach.for this validation we used hyper parameter valuesof the globally optimal models see table .
since we are evaluatingthe prototype over time we needed datasets with a long historyand a representative number of builds by month.
this is why weselected projects with at least one year of data and at least 1k buildjobs per month leaving us with projects a b c and os.
in the caseof os only the last weeks of data were considered to respect thecondition of 1k build jobs per month.results.a window size of weeks of data is sufficient tohave signi f icant performance since adding more weeks doesnot considerably increase the performance .
and inthe case of project a using more than weeks even harmsthe performance.we plotted the median performance f1 score precision recall of the models over time depending on the project and u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45btraining window size.
for space concerns we added these plots tothe replication package.
these plots showed how the curves of thethree metrics decrease between and per week for each project.this decrease in performance over time indicates that retraininga new model at some point would be bene cial.
we also nd thatthe smaller the u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsize is the lower the f1 score is.
however as we increase the u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsize the median improvement on thef1 score becomes .
.
for project a a closer analysis showedthat u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsizes and obtain an f1 score worse than .
thisshows that using too old data harms the model since the data isnot coherent with more recent data.
closer analysis showed that a u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsize gets the best results for all studied projects.
projectos was ignored for this experiment since the number of weeks was48 and we needed up to weeks of data in the training set.the feature surviving ratio drops down to after oneweek then decreases consistently over time for all project.figure shows the training feature surviving ratio for a u1d464 u1d456y u1d461 u1d45f u1d44e u1d456 u1d45bsize of weeks i.e.
the optimal value .
projects a and b showsimilar values dropping quickly to a median feature drifting rateof and respectively then reducing linearly to at week38.
project c and os both have a median feature surviving ratiodrops to and then reduce to at week and atweek respectively.
the rst drop for the three projects seems tobe due to features local to the week of the training only relevantto the prediction of the rst few to weeks after training.
theconsistent decrease after that shows that other features get rejectedover time even if they stay relevant longer.a priori switching heuristics achieve an f1 score about lower than a posteriori heuristics while keeping results2 to times higher than thealwaysbrownbaseline.2186authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards language independent brown build detectionicse may pi t tsburgh pa usatable shows all performance metrics that we used as well asthe life expectationlifeexp in number of weeks when using aparticular switcher.
we observe that the a priori switching heuristicshave an f1 score under the a posteriori heuristicbest exceptfor the project os.
for the latter project thebestheuristic choosesthe best model each week performance diagonal yet has a loweroverall f1 score because the number of true false positive and falsenegative f luctuates across the studied weeks known as simpson sparadox .
compared todiagonal cumprodgets the same f1 score for project b and c and a higher score for project a whileneeding less training computation the medianlifeexpfor projecta is and for project b and c is .
project os s f1 score is lowerthan the diagonal with thefix weeks heuristic.when comparing our switch heuristics with thealwaysbrownbaseline we observed that our f1 score is two to four times better.when compared with the best hyper parameter combination s per formance the f1 score performance loses depending on theproject when including the time constraint and using a priori switchheuristics instead of posteriori heuristics.
our prototype once ex tended with a switch heuristic has thus results signi cantly higherthan the baseline.
the model can adapt through time retrainingwhen it is needed to have the best performance possible.8t h r e a t s t o v a l i d i t yexternal validity.the approach targets projects involving multiple programmingtechnologies of which gaming projects are an extreme case sincebuilds involve code ai physics 3d models sound objects etc.on dozens of platforms each with their own sdk versions.
ouralgorithm was evaluated on seven large projects with various brownfailure ratios and build activity.
we observed that the recall andprecision performance decrease as the brown failure ratio decreases.hence the approach is expected to generalize to projects withbuild logs with default verbosity and a reasonable bfr i.e.
balanced unlike projects c d .
a project having a brown failure ratio lowerthan might not justify the need for our algorithm since theperformance might not be su cient.we considered only one open source project graphviz amongthe studied projects.
further analysis on other projects would berelevant to analyze the generalization of our results.our approach focuses on the identi cation of brown builds butdoes not propose any solution on how to x the identi ed brownbuilds rerunning builds does not target the root cause of brownbuilds .
similar to the related eld of f laky tests subset of brownbuilds xing f lakiness is an ongoing research area espe cially since brown builds are even more challenging to deal with due to the occurrence of di erent languages and build technology.construct validity.the oracle used for evaluating the brownbuild identi cation models relies on a heuristic i.e.
if a job is re run at least once and changes results for the same commit id itis identi ed as brown.
however most of the build jobs are runonly once for a given commit id and hence are considered to betrue build failures or successes.
if a developer forgot to re run abrown build our oracle would have missed it.
we believe the riskof losing such builds in our oracle is limited because brown buildsare relatively rare and the developers of the commercial projectshave as practice to rerun the known cases of brown build to checkif the build s status switches to success.in our evaluation we optimize the models for f1 score whichmaximizes both precision and recall.
however depending on theuse case organizations adopting our models might prefer to tweakthe model for better precision less false alarms at the expense ofrecall missing brown builds or vice versa.internal validity.internal validity refers to alternative explana tions of our research results.
the ground truth has been labeled decision to re run a build failure due to suspicion of being brown by build experts of the corresponding project right after the build nished.
finding a better expert or time to do the labeling wouldnot be possible.regarding the validity of the user study the participating expertswere experts in the project they were asked to evaluate builds for but they were shown month old builds they may not have beeninvolved with.
this design was used to counter potential learninge ects.
conclusiondevelopers regularly experience brown builds i.e.
build failuresnot due to code changes test cases or build logic but due to factorsoutside their control.
our empirical study on build results of multi language projects developed by one of the leading aaa gameproducers and one open source project graphviz observed thatbetween and of failed build jobs were brown dependingon the project highlighting the need to address this brown buildproblem and propose a detection algorithm.our brown build detection algorithm is language and project agnostic and obtained a median f1 score of more than twotimes higher than thealwaysbrownbaseline for all projects andsimilar to experts f1 score to while reducing the e ortneeded by those experts.
we showed that cross project predictioncan be a workaround for on boarding new projects but their perfor mance is not consistently higher than the baselines.
we recommendswitching to a project speci c model as soon as possible.our study of the impact of concept drift on the models shows thatour approach in its current form is sustainable over time and formsa solid base for future research on brown builds.
while modelsand data age over time and impact the performance we found asweetspot weeks in terms of the size of the training set andmodel change frequency weeks and we proposed promisingheuristics for deciding about switching to a new version of a model.in terms of implications for practitioners we have shown howour language independent models perform at least as well as humanexperts rq1 and also function in a predictive setting rq3 withthe right training project cross project prediction can bootstrapa new project rq2 .
in terms of research implications we haveshown how build logs are su ciently rich to predict brown buildsin a real setting independent from programming technologies.these implications open new research directions.
apart fromvalidating the approach and future incarnations on other systems we believe that future work could focus on specialized models foridentifying di erent subsets of brown builds e.g.
due to time outvs.
hardware failure as well as on strategies to x brown builds once identi ed.2187authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pi t tsburgh pa usaolewicki et al.