learning probabilistic models for static analysis alarms hyunsu kim kaist korea hyunsu.kim00 kaist.ac.krmukund raghothaman university of southern california usa raghotha usc.edukihong heo kaist korea kihong.heo kaist.ac.kr abstract we present bayesmith a general framework for automatically learning probabilistic models of static analysis alarms.
several probabilistic reasoning techniques have recently been proposed which incorporate external feedback on semantic facts and thereby reduce the user s alarm inspection burden.
however these approaches are fundamentally limited to models with pre defined structure and are therefore unable to learn or transfer knowledge regarding an analysis from one program to another.
furthermore these probabilistic models often aggressively generalize from external feedback and falsely suppress real bugs.
to address these problems we propose bayesmith that learns the structure and weights of the probabilistic model.
starting from an initial model and a set of training programs with bug labels bayesmith refines the model to effectively prioritize real bugs based on feedback.
we evaluate the approach with two static analyses on a suite of c programs.
we demonstrate that the learned models significantly improve the performance of three state of the art probabilistic reasoning systems.
introduction to deal with the challenges of accuracy and alarm relevance various probabilistic program reasoning mechanisms have been proposed for static program analyzers.
such systems initially report a set of alarms in the target program using the underlying analysis and compute the probability of each alarm based on a probabilistic model.
then they prioritize static analysis alarms by incorporating external feedback on semantic facts from various sources such as the users the old version of the program or dynamic analysis results .
upon receiving a response they generalize from the feedback and prioritize the remaining alarms depending on their relevance to those inspected by the user.
by rapidly focusing attention on the real bugs in the target program these systems achieve a dramatic improvement in the usability of the static analyzer.
despite their experimental success much of this previous research has focused on the problem of inference rather than on learning.
existing approaches based on probabilistic reasoning such as alarm ranking only learn limited forms of transferable knowledge such as the assignment of weights to the underlying probabilistic model using standard methods such as the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn .
.
.
.
algorithm .
in our observation however the capability of learning is fundamentally limited to the underlying structure of the probabilistic model.
in this paper we propose a general framework for learning probabilistic models for static analysis alarms that is applicable to various probabilistic reasoning systems .
the underlying systems initially construct bayesian networks from the derivation structure of the alarms and prioritize alarms using the induced confidence values.
this ranking is repeatedly updated as the user inspects alarms and reports their findings.
ideally these responses should always improve the confidence scores of true bugs and decrease those of false alarms.
in practice however because of approximations caused by the underlying abstraction and during model recovery they often incorrectly prioritize false alarms over true bugs.
our goal is to improve the accuracy of probabilistic models and mitigate the impact of these false generalization events.
given a set of training programs with bug labels we learn logical rules that produce accurate bayesian network models to reduce the number of user interactions until discovering all true alarms.
notice that our problem i.e.
learning is fundamentally different from that i.e.
inference addressed in previous papers .
learning and inference are complementary problems in ai ml research especially for bayesian networks.
the existing ones solely focused on the inference of bayesian probabilistic models generated by a fixed hand written set of rules.
instead we shed light on the capability of learning the bayesian models that can significantly improve the performance in multiple instances.
our approach is based on two key ideas feedback directed and syntax guided refinement.
we first construct the bayesian networks with an initial set of rules and evaluate the quality of interactive alarm prioritization using a given labeled data set.
by observing the results we capture the moments when a response to one alarm falsely generalizes to other alarms and degrades the overall quality of rankings.
for such cases our learning algorithm refines the rules that generate the inaccurate part of the bayesian network.
the refinement is guided by program syntax and encodes more detailed context of the labeled alarm to the rules by adding syntactic features which are directly derived from the grammar of the target language.
we have instantiated this approach in a tool named bayesmith and evaluate its effectiveness on a suite of widely used c programs each comprising kloc with two analyses for c an interval analysis for buffer overrun errors and a taint analysis for formatstring and integer overflow errors.
we measure the effectiveness ofbayesmith with three state of the art probabilistic program reasoning systems each of which incorporates feedback from the users the previous version of the program and dynamic analysis results .
the learned rules by bayesmith significantly ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hyunsu kim mukund raghothaman and kihong heo improve the performance of the existing systems with manually designed models by .
.
and .
respectively.
contributions.
this paper makes the following contributions we present a framework for learning probabilistic models for static analysis alarms from data.
we propose a feedback directed algorithm that learns probabilistic models by mitigating false generalization events.
we evaluate our approach with two state of the art static analyzers and demonstrate significant improvements in three probabilistic program reasoning systems on a suite of c programs.
overview we illustrate our approach using the c program excerpted from wget .
in figure .
given the name of file file the function ftp parse vms ls reads a line from the file line and tokenizes the line line .
then the function iterates over a token backward line and removes all digits after a special character by adding line .
however a buffer underflow bug can occur when all the elements in a token are digits.
pointer variable pmay point to the starting position of such a token so that the memory access top 1at line is potentially unsafe.
the buffer over underflow analysis in sparrow detects the bug but also reports false alarms at the other buffer access expressions in the function.
sparrow s sound interval analysis can estimate that the memory access at line can underflow the buffer.
on the other hand it cannot precisely capture the fact that the pointerpalways points to a valid memory region even after the loop because the contents of the string tokare determined at runtime so that the loop terminating condition becomes complicated.
once such a set of alarms is generated sparrow ranks the alarms using a bayesian alarm ranking system bingo .bingo computes a confidence score for each alarm and provides a ranking to the user.
then the user labels the top ranked alarm so that bingo performs bayesian inference to update the scores of the remaining alarms.
this probabilistic inference enables the user to interactively reason about the correctness of the program and effectively filters out false alarms while highlighting true alarms.
although the alarm ranking system significantly reduces the alarm inspection burden of users it often misinterprets user s response.
for wget .
sparrow reports alarms in total.
after user interactions with bingo the false alarm at line is ranked at the top and the true alarm is also highly ranked 9th in figure a .
once the user labels the top ranked alarm as false bingo generalizes the feedback thereby lowering the ranking of other correlated false alarm such as the alarm at line .
however this feedback also undesirably drops the rank of true alarm at line from rank to .
because the true alarm is also closely related to the labeled false alarm bingo falsely generalizes the user s label so that degrades the ranking.
a similar issue again happens after user interactions.
the ranking of the true alarm drops from to after the user s labeling on the false alarm at line .
our goal is to learn an effective probabilistic model based on bayesian network that mitigates such false generalizations.
we observed that false generalizations can happen because the structure of the bayesian network is too coarse to capture the complex behavior of programs.
in the original bingo this network is derived from1void ftp parse vms ls char file 2file fp fopen file r 3char line read line fp 4char tok strtok line 5char p tok strlen tok 6while p tok if !c isdigit p break false alarm p 10if p !
true alarm buffer underflow p false alarm figure an example code excerpted from wget .
.
a set of simple hand written rules that approximates the abstract semantics of the underlying analyzer using def use relations between program points .
the rules are general enough to capture the behavior of a large class of static analyses but insufficient to differentiate various contexts.
in the rest of this section we illustrate why the false generalization occurs and how we learn more sophisticated rules from data.
.
approximating static analysis via datalog the buffer overflow analysis in sparrow is based on the abstract interpretation framework and uses a flow field and contextsensitive interval analysis .
while its actual abstract semantics involves sophisticated reasoning about numeric pointer and array values it can be approximated by simple inference rules based on general def use relations .
the inference rules written in datalog are depicted in figure from which bingo derives the bayesian network structure.
notice that the approximated rules are used only for modeling portions of the whole reasoning process which are important for alarm ranking.
they do not affect the behavior of the underlying analyzer written in arbitrary languages.
the datalog program takes tuples in the input relations and derives tuples in the output relations using the set of rules.
for example input tuple duedge represents that there exists a direct data flow from the definition point of variable pat line to its use point at line in figure .1input tuple overflow approximates the detailed reasoning by the interval analysis and represents the fact that a buffer overflow may happen at line .
the inference rules derive new output tuples from input and other output tuples.
for example rule r2derives an indirect data flow dupath c1 c3 from program point c1toc3using another data flows dupath c1 c2 andduedge c2 c3 .
the ultimate goal of the analysis is to derive alarm tuples alarm c that represent the fact that a potentially erroneous trace reaches program point c. we repeatedly apply the rules to all tuples until no more new output tuples are derived i.e.
fixed point .
then we construct a derivation graph that shows all the reasoning steps to derive all alarms.
an example derivation graph is shown in figure b that explains how alarm andalarm are derived.
for example alarm is derived by applying r3to tuple overflow anddupath which is derived by r2with duedge anddupath .
notice that the nodes with rule names mean grounded rules.
for example r2 describes rule 1we assume the ssa style def use relation.
thus we consider the phi node for variables p i.e.
line at the loop head as a definition point.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning probabilistic models for static analysis alarms icse may pittsburgh pa usa rank alarm prob.
alarm .
alarm .
alarm .
rank alarm prob.
alarm .
alarm .
rank alarm prob.
alarm .
alarm .
rank alarm prob.
alarm .
a ranking changes after and interactions dupath dupath duedge alarm overflow duedge dupath alarm overflow ... r2 r3 r3 r2 b derivation graph interactions0500100015002000250030003500rank bayesmith bingowget c performance comparison figure false generalization.
each data point in c represents the sum of the rankings of true alarms.
input relations duedge c1 c2 immediate data flow from c1toc2 overflow c potential buffer overrun at c output relations dupath c1 c2 transitive data flow from c1toc2 alarm c potentially erroneous trace reaching c analysis rules r1 dupath c1 c2 duedge c1 c2 .
r2 dupath c1 c3 dupath c1 c2 duedge c2 c3 .
r3 alarm c2 dupath c1 c2 overflow c2 .
figure approximated interval analysis with simple inference rules.
all variables indicate program points.
r2is triggered with tuples dupath andduedge then derives conclusion dupath .
.
bayesian alarm ranking system next we convert the derivation graph into a bayesian network following the scheme in the previous work .
the central idea is to quantify the incompleteness of each rule as a probability.
static analysis rules are typically designed to be sound but not always complete.
this incompleteness is one of the main sources of false alarms.
suppose both dupath andduedge are true in figure b .
however it does not necessarily mean the conclusion dupath is always true in the concrete semantics because complicated runtime behavior is approximated such as different loop iterations or nontrivial termination conditions.
bingo encodes this incompleteness using conditional probability.
each rule ris associated with a probability pr.
for example the rules pr r2 dupath duedge pr2 pr r2 dupath duedge represent the rule r2is successfully triggered with probability pr2if both dupath andduedge are true otherwise it is never triggered.
meanwhile bingo considers a conclusion of a rule is always true if the rule successfully fires.
for example pr dupath r2 .we use the default probability .
of the bingo system for rules and input tuples that do not have deriving rules.
based on the bayesian network constructed from the derivation graph bingo computes the probability of each alarm pr alarm c e conditioned on the set of labels e. the set eis initially empty and gradually includes more labels from the user.
every time theuser provides a label for the top ranked alarm bingo reranks the remaining alarms according to pr alarm c e .
.
the false generalization problem ideally the user s labeling on a false alarm should only degrade the probability of other correlated false alarms in the network.
consider the alarm rankings before and after the user gives a negative label on alarm at the top of figure a .
the labeling successfully generalizes to another false alarm alarm and degrades its probability from .92to0.
.
however the negative feedback also falsely generalizes to the true alarm alarm and undesirably degrades its ranking from to .
the same effect also happens with another negative label on alarm .
in practice such false generalizations hinder the user s effort to discover true alarms as early as possible.
we illustrate why this false generalization occurs with the derivation graph in figure b .
given a negative label on alarm bingo computes the conditional probability pr alarm alarm that indicates the probability of the true alarm after the negative labeling.
according to the bayesian network structure alarm andalarm are conditionally independent given dupath .
therefore the posterior probability is computed as follows pr alarm alarm pr alarm dupath pr dupath alarm .
the first term pr alarm dupath evaluates to .
.
for simplicity we assume that the prior probability pr dupath .
.
then by bayes rule the second term is evaluated as follows pr alarm dupath duedge .
.
the probability pr alarm dupath duedge is the sum of following disjoint cases r2 misfired pr alarm r2 pr r2 dupath duedge pr2 .
r2 fired overflow was true but r3 misfired pr alarm r2 pr r2 dupath duedge pr3 .
.
2all the details of the calculations in this subsection are described in appendix ?
?.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hyunsu kim mukund raghothaman and kihong heo r2 fired but overflow was false pr alarm r2 pr r2 dupath duedge .
.
.
overall pr dupath alarm is evaluated to .
.
.
.
.
finally the true alarm now has a significantly low probability by multiplying the two terms pr alarm alarm .
.
.
.notice that the probability of the first term is high enough.
the false generalization is mainly caused by the second term especially because of the high probabilities of pr2andpr3in and respectively.
intuitively this means that the negative label excessively blames the common root cause dupath of the two alarms.
it decreases the posterior probability of dupath that also leads to decrease the probabilities of its descendants including the true alarm.
concretely such excessive blames are incurred from unreasonably high rule probabilities that eventually lead to the false alarm.
for example the probability of a rule that derives dupath should have been lower than those of other rules that derive data flow paths in straight line code.
the data flow from line to is involved in a loop.
since static analyses typically merge the effects of different loop iterations into a single abstract memory using join or widening operators such data flows across loop iterations may have a higher chance of deriving false conclusions.
however the current monolithic derivation rules do not separate different circumstances.
.
learning bayesian networks our main goal in this paper is to automatically derive an effective bayesian network that minimizes the effect of false generalization.
the idea is to learn more detailed features for bayesian networks from labeled data.
figure shows an overview of our learning system bayesmith.
first the system constructs an initial bayesian network from the analysis results using a set of initial rules as in figure .
given the bayesian network and the bug labels bayesmith simulates user interactions with the alarm ranking system and observes false generalizations.
then bayesmith learns new rules that are likely to reduce the effect of the most serious false generalizations.
if the quality of ranking improves by the new rules then we repeat the learning process using the rules.
otherwise bayesmith tries another candidate rule for false generalization.
for each false generalization observed in the simulations we find grounded rules that are the main sources of the false generalization such asr2 in the previous example.
then bayesmith refines the corresponding rule based on the syntax of the program part.
for example the rule r2can be refined to two disjoint rules r21 dupath c1 c3 dupath c1 c2 loop c2 duedge c2 c3 r22 dupath c1 c3 dupath c1 c2 !loop c2 duedge c2 c3 where loop c indicates program point ccorresponds to a loop head.
the rule r21will fire when the data flow is involved in a loop andr21will fire otherwise.
bayesmith associatesr21with a lower probability and r22with a higher one.
if the new set of rules leads to improve the quality of the alarm ranking system bayesmith continues the learning process with the learned rules.
otherwise it seeks to find other candidates for the false generalization or moves the focus to another false generalization in the data.figure c shows how the ranking of the true alarm in wget .
changes whenever the user provides a label until discovering the true alarm.
with the learned rules by bayesmith bingo requires the user to inspect .
fewer alarms by significantly reducing the magnitude and frequency of false generalizations compared to the original bingo.
the derivation graph generated by the learned rules is depicted in figure .
notice that it is not achievable by only learning weights without refining rules or uniformly refining every component in the rules.
in section we will show that bayesmith significantly outperforms these approaches.
in the rest of this section we will describe the details of the learning process.
finding candidates.
bayesmith collects a set of candidate rules to be refined from observed false generalizations.
given a false alarm and a true alarm whose ranking is degraded by the false generalization we first find a closest common ancestor of the alarms in the bayesian network.
for example in figure b the closest common ancestor of alarm andalarm isdupath .
then we collect all grounded rules between the false alarm and the common ancestor.
in the example r2 andr3 are collected.
bayesmith tries to refine r2andr3to more precisely represent the specific circumstances at line and .
such refinement is likely to increase the posterior probability of the true alarm.
in equation the conditional probability pr alarm alarm rises if we increase the probability pr dupath alarm .
the refinement by bayesmith would increase this probability by decreasing the probabilities for r2andr3 see the pr2and1 pr3factors in equations and .
syntax guided rule refinement.
next for each candidate grounded rule bayesmith performs refinements guided by the syntax of the program.
we assume that a program is represented as a control flow graph and each node cis associated with a command.
bayesmith is parameterized by the target language grammar for the underlying analyzer.
here we assume the following general c like grammar command c lv e assume e call e e loop l value lv x e expression e n lv e e we represent the grammar in a datalog style representation cmd c assign c e cmd c assume c e cmd c call c e cmd c loop c .lval l var l .
lval l deref l e exp e .
exp e const e .
exp e lvalexp e l lval l .
exp e binop e e1 e2 exp e1 exp e2 .
for all arguments of a given candidate grounded rule bayesmith finds the corresponding grammar rule that fires with the particular syntactic element.
for example given r2 bayesmith can capture line corresponds to a loop head so that the new rule that fires with loop heads and its complement are obtained as in the rules and .
this refinement maintains the same derivability for all the output tuples.
every time a rule is refined bayesmith updates the rule probability to prwhere 1is a hyperparameter andpris the original rule probability of r. the probability for the complement rule remains unchanged.
then bayesmith evaluates the quality of the refined rule by running bingo on the labeled training data.
if the new rules reduce the number of user interactions in the simulation we accept the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning probabilistic models for static analysis alarms icse may pittsburgh pa usa code corpus with bug labels a1 a2 an x b1 b2 bn y b1 b2 bn z ojujbm 3vmft bzftjbo mbsn 3bolfs fbsofe bzftjbo fuxpslanalyzer fbsofe 3vmft 3bolfe mbsnt1.
... .
... .
... .
..... .
..... .
.....bayesian alarm ranker bcfmfe mbsnt analyzerprogramlearning algorithm a1 a2 an x b1 b2 y ojujbm bzftjbo fuxpsl fbsojoh ogfsfodf figure system overview dupath duedge dupath alarm overflow ... r21 r3 loop ... figure a portion of the learned derivation graph.
rules and repeat the same process with another false generalization if exists.
when a previously refined rule is selected as a candidate again we refine the rule further following the grammar structure.
for example the following refined rule dupath c1 c3 dupath c1 c2 assign c2 duedge c2 c3 can be further refined with the following body dupath c1 c2 assign c2 e var e duedge c2 c3 .
the rule probability then becomes 2 pr.
in this way bayesmith learns rules and weights that derive bayesian networks from a given set of training programs with bug label data.
we observed that the learned models are applicable to various probabilistic program reasoning systems and significantly reduce the user s alarm inspection burden in real world programs compared to the previous ones.
we will discuss the detailed experimental results in section .
preliminaries .
program analysis and syntactic features a datalog program d i o r is a triple of a set of input relations i a set of output relations o and a set of rules r .
each relation is a set of tuples.
the output relations are derived from the input relations using the set of rules each of which is of the formrh vh r1 v1 ... rk vk whererhis a derived output relation and r1 ... rkare premises.
we assume that the program analysis is represented as a datalog program da ia oa ra .
for program analyses that are not written in a declarative language we approximate the behavior with a datalog program .
the program analysis result for program pis a tuplea p i c a gc whereiis the set of input facts cis the set of output facts ais the set of alarms and gcis the set of grounded clauses.
the analysis initially sets c iandgc .
then we accumulate the conclusions rh ch and the grounded clauses r1 c1 ... rk ck rrh ch wheneverr1 c1 ... rk ck c. the analysis iterates the step until reaching fixpoint.we define a syntactic feature extractor as a datalog program dg ig og rg that extracts relational representations of programs in grammar g. the input relations igconsist of atomic relations such as constants and variables.
the output relations og are derived from the input relations using the grammar rules rg.
.
bayesian alarm prioritization we present the bayesian alarm prioritization that is a basis of various probabilistic reasoning systems .
it first extracts the derivation graph from the static analysis converts the graph into a bayesian network and then computes a confidence score for each alarm.
upon user feedback it performs bayesian inference that updates the scores.
the results of program analysis i c a gc form a derivation graph over the vertices c gc.
there exists an edge from a tuple t cto a clauseg gciftis an antecedent of g and an edge from gtotiftis the conclusion of g. next we convert the derivation graph into a bayesian network.
a bayesian network is a tuple g p wheregis a directed acyclic graph andpis an assignment that associates each node with a conditional probability distribution.
while a bayesian network is an acyclic graph by definition a naive derivation graph from program analysis may have cycles.
we assume that the cycle elimination algorithm in the previous work is used to reduce the derivation graph to be acyclic while still preserving the derivability of all tuples.
in the rest of the paper we write gfor the reduced derivation graph.
then we assign a conditional probability for each node in the reduced graph using a given assignment pand rule probabilities pr.
for each input tuple t we assign the predefined probability pt p t pt.
consider a grounded clauses g gcof the form t1 ... tk rth.
then the conditional probability associated withgis as follows p g t1 ... tk prandp g t1 ... tk .
consider a tuple tthat is a conclusion of the grounded clausesg1 ... gn.
the conditional probability associated with tis as follows p t g1 ... gk 1andp t g1 ... gk .
given a set of alarms from the program analysis the bayesian alarm ranking system derives a list of alarms sorted in descending order of confidence.
let eibe the set of user feedback after iiterations and denote the top ranked alarm tuple at the next iteration argmaxa aupr a ei whereauis the set of all unlabeled alarms.
given the user feedback on the top ranked alarm we update the set ei 1toei if the user labels as true otherwise ei .
then the ranking system derives the next alarm ranking by updating the confidence scores of all remaining alarms.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hyunsu kim mukund raghothaman and kihong heo learning framework .
goal the goal of our learning process is to find an optimal alarm ranking system that minimizes the number of user interactions until discovering all true alarms in the target program.
we assume that a set of training programs and their bug labels are given t p1 l1 ... pn ln .
bayesmith aspires to achieve the goal by learning a finite set of datalog rules r that derives accurate bayesian network models.
the rules are refined by enriching the original analysis rules with syntactic features.
given a program analysis da ia oa ra and a syntactic feature extractor dg ig og rg we formalize the goal as the following optimization problem r argmin r r pi li tinteraction pi li r where r n 0ra nrgandinteraction pi li r is the number of user interactions until discovering all the true alarms in pi with the ranking system derived by program analysis augmented with the syntactic features ia ig og oa r .
the set ra nrg is a collection of sets of analysis rules enriched with syntactic features byntimes ra nrg r rg r ra n 1rg n ra n the operator refines the original analysis rules raby adding more syntactic features from the grammar rules rg.
we elaborate the details of in the following section.
.
rule refinement consider an analysis rule rand a grammar rule rg.
we first define r rgas the extensions of the analysis rule rwith structure from rg.
as an example consider the rules r dupath c1 c3 dupath c1 c2 duedge c2 c3 .
rg cmd c loop c .
in this case r rgis defined as r1 r2 where r1 dupath c1 c3 dupath c1 c2 loop c2 duedge c2 c3 .
r2 dupath c1 c3 dupath c1 c2 !loop c2 duedge c2 c3 .
the ruler1fires when all the conditions from the original analysis rule and the grammar rule are satisfied.
the rule r2covers the complement of r1so that all the output tuples derived by the original rule rare also derived by r1andr2 and vice versa.
the rule probabilities of the refined rules are defined as pr1 pr andpr2 prwhere is a hyperparameter between 0and1.
we can lift the definition of r rgto sets of analysis rules and grammar rules in a natural way r rg.
we postpone the formal definition of these ideas to the appendix.
the following theorem states that the refinement preserves the derivability of all output tuples from the original analysis.
theorem .
preservation .
consider two datalog programs d1 i1 o1 r1 andd2 i2 o2 r2 .
for any set iof input facts n .tupletis derivable fromd1 tis derivable fromd whered i1 i2 o2 o1 r andr r1 nr2 .algorithm bayesmith t da dg wheretis a set of training programs da ia oa ra is a program analysis anddg ig og rg is a feature extractor.
1leti ia ig og 2initialize r raand cost fg run t i oa ra 3repeat for g af at fgdo for r rg candidate g af at do rnew r r r rg cost fg run t i oa rnew ifimproved cost cost then r rnew cost fg cost fg goto 12until fg or timeout 13return r .
learning algorithm this section describes an algorithm that approximately solves the optimization problem by refining rules from a set of training programs with bug labels.
ideally the problem can be solved by maximizing true generalization and minimizing false generalization.
however it is intractable to find a global optimum because of the huge search space and non trivial computational cost of probabilistic inference.
instead our algorithm tries to solve the latter only using a greedy method.
the main idea is to observe false generalization as feedback and refine rules to reduce the effect.
algorithm shows the refinement process of bayesmith.
initially the algorithm starts with the set of rules of the program analysis line .
given a set of training programst bayesmith first runs the bayesian alarm ranking system for all training programs and simulates user interactions.
then we evaluate the quality by computing the number of user interactions until discovering all the true alarms cost .
the details of the algorithm will be described in the rest of this section.
bayesmith observes all false generalizations fg during the simulation.
for each observed false generalization bayesmith repeatedly refines the current rules to eliminate the false generalizations within the time budget or until there is no more false generalization line .
a false generalization is a tuple g af at that represents that the negative label on the false alarm afdegrades the ranking number of the true alarm atin the next ranking.
we define the height of a false generalization as the difference between the rankings of true alarms before and after the negative label.
in our implementation we iterate through the candidates in descending order of height.
once a false generalization is observed we collect candidate nodes to be refined in the bayesian network line .
given false generalization g af at we first find the lowest common ancestor tcofatandaf.
then all grounded clauses on the path from tctoaf ingare the refinement candidate nodes in the bayesian network.
the set of refinement candidates candidate g af at is formally defined as follows r rg r rg gcis on the path from tctoafingand k n.rk ck is derived bydgusing rulerg authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning probabilistic models for static analysis alarms icse may pittsburgh pa usa table benchmark characteristics.
program kloc bugs bug type reference gzip .
.4a buffer overrun fribidi .
.
buffer overrun bc .
buffer overrun cflow .
buffer overrun patch .
.
buffer overrun wget .
buffer overrun readelf .
buffer overrun grep .
buffer overrun sed .
buffer overrun sort .
buffer overrun tar .
buffer overrun jhead .
.
integer overflow shntool .
.
integer overflow autotrace .
.
integer overflow sam2p .
.
integer overflow sdop .
format string latex2rtf .
.
integer overflow urjtag .
format string optipng .
.
integer overflow a2ps .
format string wheregc n i 0ri ci rrh ch andrk og.
for each candidate bayesmith refines the selected rule using the refinement operator defined in section .
line .
the quality of the rules are examined by simulating user interaction with the newly learned alarm ranking system line .
for evaluation we check line whether the total number of user interactions for all training programs decreases and whether the number of improved cases is larger than that of hindered cases.
while the first condition generally specifies the ultimate goal the second one is checked to avoid overfitting by a few dominating programs that have a large number of alarms.
if the conditions are satisfied bayesmith repeats the process with the newly learned rules.
otherwise other candidates of rules or false generalization are selected.
example .
.
consider the bayesian network in figure b .
given the false alarm alarm and the true alarm alarm the lowest common ancestor is dupath .bayesmith collects all grounded clauses on the path from dupath toalarm r2 r3 .
then the algorithm searches for a grammar rule that derives a tuple whose argument is or .
if there exists a rule rg cmd c loop c that derives cmd the pair r2 rg is selected as a candidate.
in this case the algorithm refines the rule and produces two new rules and in section .
.
tables ?
?
?
?in appendix ?
?
provide additional examples of refined rules learned by bayesmith.
experimental evaluation we designed experiments to answer the following questions how effective are the learned probabilistic models?
does bayesmith reduce the frequency and magnitude of false generalizations?
how robust is bayesmith with different training data?
how scalable are the learned ranking systems?
.
setting we conducted all experiments on linux machines with intel xeon .2ghz.
we performed bayesian inference using libdai and set a timeout of hours for learning.
instance analyses.
we have implemented bayesmith on top of sparrow a static analysis framework for c programs .
we used two instance analyses in sparrow an interval analysis for buffer overrun errors and a taint analysis for format string and integer overflow errors.
the taint analysis detects whether malicious format strings and overflowed integers are used as arguments ofprintf like and malloc like functions respectively.
we used the same mechanism as in the previous work to extract def use relations from analysis results of sparrow following the sparse analysis framework .
for the grammar rules we used the c like grammar as in section that describes program syntax and alarm expressions in sparrow.
baselines.
we apply probabilistic models learned by bayesmith for three state of the art probabilistic program reasoning systems each of which prioritizes alarms based on the feedback from user the old version of the program and dynamic analysis respectively.
all three baselines are based on probabilistic models derived from the same set of hand written rules.
we compare the performance of the learned models for each system.
benchmarks.
we evaluated bayesmith on a suite of widely used c programs in table .
the benchmarks are collected from previous work applying sparrow and recent cve reports.
we excluded too small programs whose sizes are less than 5kloc and alarm inspection requires less than user interactions with bingo.
learning configuration.
by default we evaluated the performance ofbayesmith using the leave one out cross validation for each instance analysis and set the hyper parameter in section .
to .
.
our leave one out cross validation measures the number of user interactions for each individual program using a bayesian network learned from the other programs.
for example we used programs for training and the remaining one program for the test in the case of the interval analysis.
because each analysis is based on the different abstract domains and semantics we separated the learning processes per analysis.
.
effectiveness of probabilistic models .
.
user guided alarm prioritization.
we evaluate the effectiveness of the learned models for user guided alarm prioritization compared to three baselines bingom bingoem and bingou.bingom derives bayesian networks using the initial set of rules and the rule probabilities are assigned with a heuristically chosen value .
as in the previous work .bingoemconstructs the same bayesian networks as bingombut the rule weights are learned by an algorithm presented in the previous work .
the algorithm learns the best rule probabilities that explain the bug labels based on the standard em algorithm.
we ran bingoemfive times for each benchmark and report the average.
bingouuses a refined set of rules that are derived by uniformly unrolling all the components of the original set of rules by once.
we measure the number of user interactions to discover all true bugs in the data.
the experimental results are shown in table .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hyunsu kim mukund raghothaman and kihong heo table effectiveness of bayesmith in user guided alarm prioritization for the interval top and taint bottom analysis.
alarms reports the number of alarms.
each column reports the number of iterations until discovering all bugs.
program alarms bingombingoembingoubayesmith gzip .
.4a fribidi .
.
bc .
cflow .
patch .
.
wget .
readelf .
grep .
sed .
sort .
tar .
total jhead .
.
shntool .
.
autotrace .
.
sam2p .
.
sdop .
latex2rtf .
.
urjtag .
optipng .
.
a2ps .
total the weight learning performed by bingoemis not effective in most cases and only improves the ranking quality for out of programs compared to bingom.
for some cases such as grep2.19andgzip .
.4a the number of interactions even significantly increases.
this result is mainly because of two reasons.
first the labeling is extremely sparse in our setting.
for example we only have tens of bug labels for the interval analysis while the networks have 2k 12k nodes.
this hinders the converge of the algorithm within hours that leads to poor performance.
the other reason is that the networks have simplistic structures derived by the small and monolithic set of rules that hinders to learn general knowledge for detailed contexts.
the results also show that the uniform rule refinement does not improve the quality of the ranking system.
in the interval analysis bingououtperforms bingomfor benchmarks but increases the number of interactions by .
for all benchmarks on average.
similarly bingouimproves only .
in the taint analysis compared to bingom.
overall the uniformly unrolled rule does not effectively solve the false generalization problem according to our results.
this is mainly because the unguided refinement can reduce not only false generalizations but also true generalizations thereby degrading the overall performance.
furthermore one step refinements are sometimes not enough to remove false generalizations.
on the other hand the learned ranking systems substantially reduce the amount of alarm inspection burden.
bayesmith outperforms bingomfor out of programs and reduces the total number of user interactions by .
and .
for the interval and taint analysis respectively.
for the remaining cases bayesmith wgetreadelfgrepsedsort tar optipngshntoollatex2rtfurjtagavg.
interactionsdrake bayesmith a drake .
bccflowgrepgzippatchreadelfsedsorttar optipnglatex2rtfshntoolavg.
interactionsdynaboost bayesmith b dynaboost .
figure improvement in ranking performance enabled by bayesmith shows the same results compared to bingom.
this is mainly because the underlying analyzer already reports a low false positive ratio e.g.
latex2rtf .
.
or bingomalready significantly reduces alarm inspection burden e.g.
grep .
.
.
.
application to other systems.
we evaluate the effectiveness of learned models with two other probabilistic reasoning systems using bayesian networks drake anddynaboost.
drake bootstraps the probabilistic alarm ranking system using the old version of the program and prioritizes alarms for the new version by the relevance to the difference .dynaboost incorporates observed dataflow facts from dynamic analysis into the alarm ranking system .
as in the prior work both of the baselines use the same set of rules as bingomin the previous section.
likewise we measure the number of interactions after each bootstrapping until discovering all the bugs.
since they require old versions and test cases we selected benchmark programs only used in the previous work.
overall the learned models significantly improve the performance of the probabilistic reasoning systems.
figure a shows the number of user interactions on top of the alarm ranking by the relevance.
in comparison to the vanilla versions of drake the alarm inspection burden with the learned models is reduced by .
on average.
one exceptional case is grep .
where the baseline already significantly reduces the number of interactions to only to discover the bug.
while the learned rules show a small regression the absolute number of user interactions is still small enough for the user inspection.
figure b also demonstrates the impact on the ranking system incorporating dynamic analysis.
the learned models reduce the number of interactions for out of programs for dynaboost and the user inspection burden is reduced by .
on average.
.
.
learned insights.
the learned rules capture important aspects of each analysis.
regardless of the instance analysis bayesmith commonly captures the insight that loop head is an important feature since loops are often the main source of inaccuracy because of join or widening.
another insight is that library calls also increase uncertainty of static analysis results because their semantics is complicated and the source code is unavailable.
bayesmith not only discovers these general insights in static analysis but also fine tunes the rules with multiple conjunctions and disjunctions which can require non trivial manual efforts3 dupath v0 v1 ... loop v2 assign v2 .
.
dupath v0 v1 ... libcall v2 v3 lval v3 .
.
3for brevity we omit negated tuples and boilerplate parts in the rule bodies.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning probabilistic models for static analysis alarms icse may pittsburgh pa usa interactions0100200300400500rank bayesmith bingosort interactions0100200300400500rank bayesmith bingocflow interactions0255075100125150175rank bayesmith bingoreadelf interactions20040060080010001200rank bayesmith bingoautotrace figure comparison of the ranking performance.
each data point represents the sum of the rankings of all bugs.
bayesmith also captures analysis specific features.
for the interval analysis it is usually harder to estimate buffer overflow errors when arithmetic operators are involved in branch conditions or buffer accesses.
also the learned rules capture the general knowledge that analyzing pointer dereferences or complicated libraries such asstrncpy is more tricky compared to array index expressions.
these insights are captured by the following rules and weights dupath v0 v1 ... assume v2 v3 binop v3 .
.
alarm v1 ... derefexp v1 v2 binop v2 .
.
alarm v1 ... strncpy v1 v2 v3 cast v2 cast v3 .
.
for the taint analysis arithmetic operators such as multiplication and casting operators are captured by bayesmith which are the main sources of imprecision for integer overflow detection alarm v1 ... mallocsize v1 v2 castexp v2 v3 lval v3 .
.
alarm v1 ... mallocsize v1 v2 binop v2 v3 mult v3 .
.
.
reduction of false generalization bayesmith achieves its effectiveness by reducing false generalizations.
we justify this argument by measuring the frequency and magnitude of false generalizations.
table shows the number of false generalizations and their average magnitudes by bingomand bayesmith.
we measured the magnitude of false generalization in each round as the sum of decreased amount in ranking per true alarm.
the numbers are the average throughout the whole interaction rounds.
the overall changes in the ranks of true alarms are shown in figure .
each graph describes the rankings of true alarms for each iteration.
we categorize the results into three notable cases.
first in most of the cases bayesmith reduces the number of false generalizations and magnitudes and leads to the performance improvement of their rankings such as sort .
andcflow .
.
second the learned bayesian networks improve the quality of ranking systems in terms of not only generalization but also initial ranking.
for example the initial ranking of the bug in readelf .
is improved from to and hence the reduced number of total interactions.
this means that the learned probabilistic models estimate the behaviortable reduction of false generalizations.
freq and mag report the number of false generalizations and their average magnitude for the interval top and taint bottom analysis.
bingom bayesmith program freq mag freq mag freq mag freq mag gzip .
.4a .
.
.
.
fribidi .
.
.
.
.
.
bc .
.
.
.
.
cflow .
.
.
.
.
patch .
.
.
.
.
.
wget .
.
.
.
.
readelf .
.
.
.
.
grep .
.
.
.
.
sed .
.
.
.
.
sort .
.
.
.
.
tar .
.
.
.
.
average .
.
.
.
jhead .
.
.
.
.
.
shntool .
.
.
.
.
.
autotrace .
.
.
.
.
.
sam2p .
.
.
.
.
.
sdop .
.
.
.
.
latex2rtf .
.
.
.
.
.
urjtag .
.
.
.
.
optipng .
.
.
.
.
.
a2ps .
.
.
.
.
average .
.
.
.
of programs and static analyses with more accurate prior probabilities.
third the learned ranking systems sometimes report more false generalizations such as autotrace .
.
.
however notice that bingomexhaustively inspects all the alarms to discover the true alarms.
the rankings of the true alarms are consistently low and all the bugs are discovered in the last iterations hence less number of false generalizations.
on the other hand the first true alarm is discovered only after iterations with bayesmith that improves the rankings of the other true alarms.
this effect makes a high chance of introducing false generalizations but significantly improves the overall rankings.
in summary the learned rules by bayesmith reduce the negative effect by false generalizations.
for the interval analysis bayesmith reduces the average frequency and magnitude of false generalizations by .
and .
respectively.
for the taint analysis the average magnitude is reduced by .
.
the average number of false generalizations increases because of the exceptional cases but even for such cases the quality of overall ranking improves.
in terms of overall impact freq mag bayesmith shows .
and .
of improvement respectively.
.
robustness of the learning algorithm in order to measure how data splitting affects the quality of learned models we evaluate the performance of bayesmith with different quantities of training and testing data.
for each analysis we randomly chose of the benchmark programs as a training set and the remaining as a test set.
we repeated this process ten times and report the averages.
we measure the total number of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa hyunsu kim mukund raghothaman and kihong heo table statistics of user interactions using the alarm ranking systems learned with different training set.
bayesmith bayesmith program avg.
stddev.
avg.
stddev.
interval .
.
.
.
taint .
.
.
.
iterations for each test set bayesmith compared to that of the leave one out setting that is the same as section .
bayesmith .
table shows the performance of bayesmith with different combinations of training and test sets.
on average bayesmith reduces the number of user interactions by .
for the interval analysis and .
for the taint analysis.
compared to bayesmith bayesmith 80shows .
and .
less improvement for each of the analysis task.
in total the results are comparable to those with bayesmith 90in section .
that shows the learning algorithm is robust with different training data.
.
scalability the computational overhead of bayesian inference typically increases as the size of bayesian network grows because of the refinements by bayesmith.
we measure the size of bayesian networks in terms of the number of tuples and grounded clauses in derivation trees and the average running time for each bayesian inference.
the bayesian networks are optimized using optimization algorithms described in the previous work .
table shows the average size and response time.
bingo uses a monolithic set of rules while bayesmith derives and rules on average for each analysis.
on average the synthesized networks have .4x more nodes for the interval analysis and .1x for nodes for the taint analysis compared to the baseline.
as the networks become larger the average running time also increases by .1x and .6x respectively.
in most cases it only requires less than seconds.
even for some exceptional cases such as readelf .
the average response time is about a minute which is reasonable for real world deployment.
limitations and opportunities this section identifies the limitations of our approach and challenges for future work.
first bayesmith requires background knowledge in the form of existing analysis rules.
we demonstrated a set of rules for def use relations based on the sparse analysis framework is a reasonable starting point.
while def use relations are used in a large class of analyses there can be other analyses that require the analysis designers to derive initial rules based on different background knowledge.
to address this challenge we plan to devise a purely data driven synthesis algorithm without requiring background knowledge.
second bayesmith only considers a syntax guided refinement that might not accurately reflect deeper semantic aspects.
for future work we plan to develop a refinement algorithm considering the characteristics of abstract domains and semantics.
finally bayesmith assumes an ideal user interaction model where the user always gives correct feedback on the top ranked alarm.
however the user might make a mistake give partial feedback or diagnose arbitrary alarms.
thus developing a more advanced interaction model is also an important future direction for our work.
related work our approach aims to overcome the fundamental limitation of the existing user guided approaches.
there have been several techniques to improve static analysis accuracy by leveraging user feedback.
bayesian alarm ranking systems compute a confidence score for each alarm and update rankings based on the user feedback .
alarm clustering computes logical correlations between alarms so that a set of alarms are logically grouped together such that each alarm group is resolved by inspecting a single representative alarm.
alarm classification techniques ask questions about labels or root causes of alarms to classify analysis reports by solving optimization problems .
none of the existing approaches learn detailed knowledge from an analysis for one program to that for other programs.
instead we provide a learning framework for probabilistic models and outperform the existing approaches.
recently many techniques for synthesizing datalog programs have been proposed but their goal is different from bayesmith.
their goal is to efficiently synthesize datalog rules from input output specifications.
zaatar proposes a constraintbased synthesis by encoding the output of candidate datalog programs as smt constraints.
alps leverages a search space pruning technique for efficient enumerative synthesis.
difflog and prosynth employ provenance information combined with continuous optimization and sat solving .
one notable difference is that datalog programs are typically interpreted over a boolean semiring where each tuple is either derived or not while our focus in alarm prioritization is to rank alarms in order of confidence so each tuple is additionally associated with a real valued score.
existing structure learning techniques that have been developed by the machine learning community are not applicable to our problem.
the goal of structure learning algorithms for bayesian networks and markov logic networks has been to find a model with a high likelihood on the observed data.
in contrast bayesmith finds a refinement of an existing model which is tightly coupled with the underlying static analysis while preserving the derivability of the original set of alarms theorem .
.
furthermore conventional structure learning algorithms have limited scalability since they typically learn rules from scratch.
the standard approaches to structure learning typically require complete data i.e.
labels for all random variables .
however this assumption is unrealistic for our problem since it is hard to obtain labels for all intermediate results of static analysis.
although several techniques can learn structures from incomplete data they are only applicable to small datasets with up to a few dozen random variables while our benchmarks on average have more than 1k variables .
there is a large body of research for automatically learning heuristics for static analysis .
the existing approaches use various learning techniques to derive heuristics for controlling context sensitivity variable relationship resource management and unsoundness .
while all these approaches rely on handcrafted features our approach learns syntactic features that are directly derived from the grammar of the target language.
jeon et al .
automatically learns authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning probabilistic models for static analysis alarms icse may pittsburgh pa usa table scalability of our learning approach.
t and c are the average number of tuples and clauses of the networks.
r indicates the number of rules to construct a network.
time is the average duration of bayesian inference.
bingo bayesmith program t c time s r t c time s interval .
.
taint .
.
graph based heuristics without manual features but their approach is specialized for pointer analysis.
chae et al .
automatically generates features from program syntax.
they represent features as small programs that concisely capture the behavior of static analysis with flow sensitivity or relational domain.
however these approaches are not directly applicable to learning bayesian networks for alarm ranking systems.
conclusion we presented bayesmith a general learning framework for bayesian alarm ranking systems.
bayesmith fundamentally improves the quality of user guided program reasoning systems by learning the structure of the underlying probabilistic model.
bayesmith learns accurate bayesian networks by minimizing the effect of false generalizations observed from simulated user interactions with labeled data.
in the experiments with two instance analyses we demonstrated the effectiveness of learned rules and significant improvement in terms of the user s alarm inspection burden.