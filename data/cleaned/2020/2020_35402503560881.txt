discrepancies among pre trained deep neural networks a new threattomodel zooreliability diegomontes purdueuniversity usa montes10 purdue.edupongpatapeepeerapatanapokin purdueuniversity usa ppeerapa purdue.edujeff schultz purdueuniversity usa schul203 purdue.edu chengjunguo purdueuniversity usa guo456 purdue.eduwenxinjiang purdueuniversity usa jiang784 purdue.edujamesc.davis purdueuniversity usa davisjam purdue.edu abstract trainingdeepneuralnetworks dnns takessignificanttimeand resources.apracticeforexpediteddeploymentistousepre trained deepneuralnetworks ptnns oftenfrommodelzoos collections ofptnns yet thereliabilityofmodelzoosremainsunexamined.in theabsenceofanindustrystandardfortheimplementationandperformanceofptnns engineerscannotconfidentlyincorporatethem intoproductionsystems.asafirststep discoveringpotentialdiscrepanciesbetweenptnnsacrossmodelzooswouldrevealathreat tomodelzooreliability.
prior works indicatedexistingvariances in deeplearningsystemsintermsofaccuracy.however broadermeasuresofreliabilityforptnnsfrommodelzoosareunexplored.this work measures notable discrepancies between accuracy latency and architecture of ptnns across four model zoos.
among the top10discrepancies wefinddifferencesof1.
2. inaccuracy and9 131 inlatency.wealsofindmismatchesinarchitecture for well known dnn architectures e.g.
resnet and alexnet .
our findingscallforfutureworksonempiricalvalidation automated tools formeasurement andbest practices forimplementation.
ccsconcepts softwareanditsengineering reusability computing methodologies neural networks .
keywords neuralnetworks modelzoos software reuse empiricalsoftware engineering acm referenceformat diego montes pongpatapee peerapatanapokin jeff schultz chengjun guo wenxinjiang andjamesc.davis.
.discrepanciesamongpre trained deep neural networks a new threat to model zoo reliability.
in proceedingsofthe30thacmjointeuropeansoftwareengineeringconference andsymposiumonthefoundationsofsoftwareengineering esec fse november 14 18 singapore singapore.
acm new york ny usa 5pages.
introduction with the growing energy consumption of training dnns takingadvantageofthere usabilityofptnnscansignificantlyreduce esec fse november 14 18 singapore singapore copyright held bytheowner author s .
acm isbn978 .
costs of training .
in particular transfer learning can result in shorter training times and higher asymptotic accuracies compared to other weight initialization methods .
this kind of technique accelerates model reuse and development.
the history of ptnns and their impact on the development of artificial intelligence has been extensively documented .
as such collectionsofptnnshavebeencreated referredtoas modelzoos .
notably maintainersofpopularmachinelearningframeworks such as tensorflow maintain corresponding model zoos developed with theirframework suchasthe tensorflowmodel garden .
there are many model zoos and an expanding use ofptnnsinproductionsystems .pastworkhasemphasizedthe difficultiesin adopting software engineering practicesin machine learning andspecifically thechallengeswithreproducingmachine learningresearchpapers .thesereproducibilityissuesmay affect ptnns leading to variations across model zoos .
disparitiesintheaccuracy latency orarchitectureofaptnncould negativelyaffectadeeplearningsystem threateningptnns reuse potential.
consider a model zoo that has an incorrect implementation of a well known dnn architecture which has increased its latency significantly.
if an engineer were to use the ptnn from this zoo they would unknowingly be receiving a lower quality ptnn than they might otherwise have from a different model zoo.
theengineer s efforttoenable aquickturnaroundtime with a ptnn would have become harmful.
discovering discrepancies wouldshine alight on the reliability ofmodelzoos.
toexplorethereliabilityofmodelzoos weperformedameasurement study to identify discrepancies among image classification ptnn architectures across four model zoos tensorflow model garden tfmg onnx model zoo onnx torchvision models torchvision andkeras applications keras .
the ptnns weremeasuredalongthreedimensions accuracy latency andarchitecture.
we find the differences in accuracies on ilsvrc cls dataset imagenet can be as large as .
.1similarly over of the ptnns measured had latency differences flops of or more when comparing ptnns of the same name across the model zoos.
lastly we discover architectural differences in several ptnns includingimplementationsof alexnetandresnetv2 .we concludewithanagendaforfutureresearchonfurtherempirical validation automatedtoolsformeasurement andbestpracticesfor implementingmodelzooptnns.
1theilsvrc cls image dataset has validation images.
a accuracy differenceis equivalent to500 images.
thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse november14 18 singapore singapore diegomontes pongpatapee peerapatanapokin jeffschultz chengjunguo wenxin jiang andjames c.davis backgroundand related work ptnns are applied in a wide variety of domains .
with the demand for engineers far exceeding supply companies are lookingforbestpracticesthatcanboosttheproductivityoftheirengineers.
major companies e.g.
google and microsoft have shared best practices on machine learning development and informed futuredirectionsonmodelreuse .acasestudyfromsapindicatespossiblecompatibility portability andscalabilitychallenges in machine learning model deployment which may affect their performance .
there have been many efforts to improve the quality of model zoos.
for example ibm has developed a tool to extract model metadata to support better model management.
bannaet al.promote best practices for reproducing and publishing high quality ptnns .
however the reliability of model zoos has not been validatedbyprior works.
the ability to replicate the accuracy of a dnn in identical training environments is hindered by non deterministic factors.
accuracy differences of up to .
stemming purely from nondeterminism have been reported with populardnn architectures .closelyrelated researchhasinvestigatedandbenchmarked theperformancevariancestiedtodeeplearningframeworks .thisvariabilitythreatensthereliabilityofnewdeeplearning techniques.
as such automated variance testing has been proposedtoassurethevalidityofthesecomparisons.however ptnns in model zoos may also suffer from varying architectural implementations affecting more than just accuracy.
our work measures the disparities in ptnns across different model zoos as opposed toattemptingtoimprovethestandardinjustone .ourresults enlighten future works validating the quality and promoting the standardizationofmodelzoos.
methodology we perform a measurement study to assess our problem statement whether discrepancies exist between the accuracy latency and architecture ofptnns acrossdifferentmodelzoos.
.
subjects amodel zoo is a collection of ptnns for various tasks.
we carry out a selection process for four model zoos.
our selection criteria included the model zoobeing maintainedalongside amachine learning framework this increases the likelihood of the model zoo beingactivelymaintained.furthermore toensurethepopularityof the model zoo the zoo must have a public github repository with at least three thousandstars .
using github search2to identify potentialmodelzoocandidates 11modelzooswereselectedthat metthecriteria.3theptnnswithinthe11modelzooswerecategorized into deep learning tasks including image classification object detection and naturallanguage processing.we focusedon image classification models because it is the most common type in 8ofthe modelzoos.
aptnn availabilityanalysiswasdone onthe candidatemodel zoostoassesshowmanymodelzoosofferedthesameimageclassificationptnnarchitectures.basedonthelargestsharedavailability we chose tensorflow model garden onnx modelzoo torchvision 3the identified potential model zoos are as follows tensorflow model garden onnxmodel zoo torchvision models kerasapplications tensorflow modelhub pytorchmodelzoo mxnetmodelzoo gluonmodelzoo deeplearning4jmodelzoo caffe modelzoo and openvinomodelzoo.
figure1 overviewofthemeasurementprocess.wegather ptnnsfromthemodelzooswiththesamename perform measurementsoneachptnn andcomparefordiscrepancies.
models andkeras applications .
within these model zoos we selected all the image classification ptnn architectures that were present in at least two of the four model zoos yielding ptnn architectures.theselectedptnnsareeitherdirectlydownloadable from the model zoos github repositories or can be pulled using the modelzoos apis.
.
evaluationmetrics accuracy.
image classification dnns effectiveness is measured in accuracy which is a critical component of a ptnn.
we are measuringdiscrepanciesbetweentheclaimsofmodelzoos asopposed to verifying them.
top accuracy is the conventional accuracy where model prediction must exactly match the expected label while top accuracy measures the fraction of images where any of the topfivepredictedlabelsmatchesthetargetlabel .35image classificationptnnarchitecturesreportedtop 1imagenetclassification accuracies meanwhile only reported top imagenet classification accuracies.
latency.
the latency of a dnn is a key factor that engineers consider .forexample mobilenet isadnnimageclassification architecture thatprioritizeslow latencyonmobile and embedded systems .
we used open source tools to measure thelatencybycountingthefloatingpointoperations flops .
flopsareframeworkandhardware agnostic allowingforunbiased comparisons.
architecture.
ptnnsaretrainedweightsbasedonresearchpapers that propose dnn architectures.
as a result model zoos advertise ptnns by their architecture name.
the observed accuracy differences and past work on dnn vulnerabilities motivated us to examinearchitecture .qualitativeobservationsofdiscrepanciesinthedescriptions sourcecode andvisualizationsofptnn architectureswereemployed.specifically netron anopen source neural network visualizer was used to inspect the architecture of the ptnns .
however not all neural network weight formats are supported so all ptnns were converted to the onnx format for architectural analysis using an appropriate tool for each framework .
the source code for the implementations of the ptnns are present in the model zoos github repositories and wasusedas an additionalform of ptnn inspection.
1606discrepanciesamong pre traineddeep neural networks a newthreat to model zoo reliability esec fse november14 18 singapore singapore table frequency at which each model zoo had the most or least accurate modelordered by highesttop accuracy.
highest top lowest top highest top lowest top torchvisionmodels tf model garden kerasapplications onnxmodel zoo results and analysis .
accuracy we compared the top accuracy of ptnn architectures and the top accuracy of ptnn architectures by using imagenet.
notably 12ofthe35profiledptnnarchitectureshadtop 1accuracydifferencesgreaterthan0.
.fortop 5accuracies 6ofthe32 ptnnarchitectureshaddifferencesgreaterthan0.
.thelarge differencespresentinfigure 2havesignificantconsequences.for example resnet v1 from keras is noticeably less accurate than theptnnbythesamenamefromtorchvision with top 1accuraciesof76.
and78.
respectively.thisdifferenceispronounced enough that resnet v1 from torchvsion with top accuracy of .
ismore accuratethan resnetv1 from keras.
figure top largest top accuracy differences.
for a ptnnarchitecture theaccuracyoftheptnnwiththelowest reported top accuracy is subtracted from that of the ptnn with thelargest top accuracy.
table1shows the aggregation of accuracy differences across model zoos highlighting how often a model zoo had the highest or lowest top or top accuracy for a given ptnn architecture.
as seen of the ptnns that were available on torchvision had the highest top accuracy among the model zoos.
on the other hand kerashadthelowesttop 1accuracy44 ofthetimeforits selection ofptnns.
.
latency ptnn architectures were measured for their flops.
figure showsthatthereare8ptnnarchitectureswheretheptnnwiththe highestamountofflopshadgreaterthan10 moreflopsthanthe 4resnet v1 was originally reported to be .
less accurate than resnet v1 .
figure3 top10largestflopsdifferences.foraptnnarchitecture the flop count of the ptnn with the most flops is divided by theflopsofthe ptnnwith the fewest.
ptnnwiththelowestflopcount.attheextreme torchvision s squeezenet1.
sittingat819.08millionflops had2.
theflops of onnx s squeezenet .
.
likewise the three ptnn architectures from the resnet v2 family all had greater than more flops thanthelowestflopsptnn.
allthehighflop count resnetv2 come from tfmg.
we discussthe possibleexplanations forthe flopsdifferences seeninfigure .thehighflopsdifferencemeasuredin squeezenet .0can be explained by looking at its successor squeezenet .
.
squeezenet1.
isadvertisedbyonnxtocontain2.
lesscomputation than the former.
however squeezenet .
from onnx has the samenumberofmeasuredflopsasthe .0ptnnoffered.onnx hasbeenadvertising squeezenet1.
asits1.0counterpart.similarly looking at the resnet v2 from tfmg a primary contributor to the large amount of flops is the input shape.
resnet v2 architectures accordingtotheoriginpaper accept224 224inputs however tfmgstatesthatthe resnetv2 ptnnsitprovidesuseinception pre processing and an input image size of .
a trade off betweenaccuracyandthroughput flops waspotentiallymade here bythe modelzoomaintainers.
across all flop counted ptnns torchvision had the highest flopsptnnsfor78 oftheptnnsitoffered.closebehind tfmg had .pointedly keras neverhad the highest flopsptnn and hadthe lowestflops implementation81 of the time.
.
architecture we frame our results for architecture in terms of the discrepancies wediscoveredinouranalysis.specifically wediscussdifferences amongptnns for alexnet resnetv1 resnetv2 andresnet v2 101andagainst the ptnns originpapers.
thealexnetfromtorchvisioncitesadifferentoriginpaperthan othermodelzoos .bothpaperscontainthesamefirstauthor however only the latter contains an explicit description of a dnn architecture.assuch ouranalysispegstheptnnagainstthelatter paper .wenoticetwomaindiscrepancies theptnnismissing the response normalization layers and the kernel size and number of kernels for the convolution layers are incorrect.
for instance torchvision sptnn has64 kernels inthe first convolution layer as opposedto the that are describedinthe originpaper.
1607esec fse november14 18 singapore singapore diegomontes pongpatapee peerapatanapokin jeffschultz chengjunguo wenxin jiang andjames c.davis figure4 resnetv250 architecturedifferencesbetween keras applications left and onnxmodelzoo right .thetop right convolutiononthelefthasastridesizeof2 whilethetoprightconvolution on therighthasastride size of1.
theresnetv1101 fromonnxandkerascontainconvolution shortcuts which were only introduced in the resnet v2 paper but notinthe resnetv1 originpaper .torchvision sandtfmg s resnetv1101 donotincludethisshortcut.alsointhe resnetfamily both theresnet v2 andresnet v2 have a shareddiscrepancy.
asseeninfigure keras resnetv250 implementationcontains maxpoolskipconnections whicharenotpresentinthepaper and uses convolutions withlarger strides intheseresidual blocks .
the observed discrepancies in architecture may affect the accuracyandlatency.forexample thelargerconvolutionstridesand max pool skip connection in the resnet v2 from keras allows the network to use less compute flops compared to the ptnn from onnx.
this can be seen in the flop measurements of the resnet v2 from keras and onnx.
onnx s resnet v2 has .
billion flops while keras ptnn only has .
billion flops an .
difference.
moreover the keras ptnn did not sacrifice accuracy through this implementation reporting a top accuracy which is greater than onnx s resnet v2 top accuracy of75.
.whilethekerasmaintainersdidnotimplement resnet v2 50faithfully to the origin paper they produced a more accurate ptnn withlower latency.
discussion and futurework empirical validation.
the top accuracy differences depicted in figure2suggest that the choice of model zoo matters.
specifically oftheptnnarchitectureshavingtop 1accuracydifferences greater than .
is not easily overlooked.
an engineer may receive a ptnn that incorrectly classifies greater than validation imagesonimagenetmorethanaptnnfromadifferentmodelzoo.
modelzoochoiceshouldnotresultinanoticeableimpactonthe accuracyofptnnsthatengineersreceive.althoughmodelzoos currentlyreporttheaccuracyoftheptnnstheyoffer ourworkhas shownthatthisdoesnotguaranteethatthereisnotanothermodel zoowiththesameptnnatahigheraccuracy.publiclyavailable andactivelymaintainedcomparisonsofmodelzooptnnswould allowengineers tobe moreinformed whenchoosinga modelzoo.
furthermore weonlystudiedtheaccuraciesofimageclassificationmodels at face value.
we recommend future works focus on empiricalvalidationontheclaimsofptnnsinmodelzoostocheckfor the existenceoffalse advertising.
new metrics and automated tools.
the measured flop disparities seen in figure 3have consequences especially in edge devices with limited compute.
for example onnx incorrectly listingsqueezenet1.
assqueezenet1.
mayleadtoconfusionwhenan engineer switches to squeezenet .
fromsqueezenet .
expecting adropinlatency.similarconfusionmayarisefrominstanceslike theoneseenintfmg sselectionof resnetv2 .whiletheincreased input size is stated the impact on latency is not made clear.
to effectivelyinformengineersofthelatencyofptnns modelzoos shouldreportflopcountsalongsideaccuracy.alsoofinterestis the energyusage oftheseptnns anotherimportantpropertyfor edgedevices.thelackofreportingofthesepropertiesmaymake choosingptnnsmoredifficult.werecommendfutureworkscreate newmetrics to measure the reliabilityandquality of ptnns from model zoos and develop tools for automatically measuring these properties.publishingupdatedresultsfrequentlycansupporteasier decision making ofmodels for deployment.
naming conventions.
the differences in the architectures of ptnns may indicate an underlying improper documentation standardandaneedforimprovednamingconventionsinmodelzoos.
as indicated in .
torchvision s alexnetdid not adhere to the originpaperwhilestillclaimingtobe alexnet.seemingly model zoos are advertisingptnns labeled as well known dnnarchitectures like resnetandalexnet butwhentheydothis theyreally meanthattheptnnsarebasedonthednnarchitectureandare not strict implementations.
this inadequate naming convention leads to a false sense of equality and thus confusion.
we recommendthecommunitycomprehensivelydocumentptnnnaming conventions to increase cohesion among model zoos.
likewise we suggestfutureworksinvestigatetheexpectationsofengineerswith regards to the ptnns from model zoos to see whether they prefer exact reproductions or more accurate and lower latency ptnns.
the result of such a study would inform model zoo maintainers on howto bestimplement andtrainptnns.
conclusion we present an investigation of the discrepancies between image classification ptnn architectures from four popular model zoos throughaccuracy latency andarchitectureanalyses.wefindseveralsignificantdiscrepanciesamongthesethreeaxesthatchallenge the well established use of ptnns from model zoos suggesting that an engineer will receive a ptnn with different characteristics based on the model zoo.
the ptnn s goal of shortening model deployment time is diminished because of the time investment neededtoverifythepropertiesoftheptnn.wediscusstheimportance of future works to validate the claims of model zoos develop automated tools for measurement and explore best practices for implementingmodelzooptnns.