log based anomaly detection without log parsing van hoang le and hongyu zhang the university of newcastle nsw australia vanhoang.le uon.edu.au hongyu.zhang newcastle.edu.au abstract software systems often record important runtime information in system logs for troubleshooting purposes.
there have been many studies that use log data to construct machinelearning models for detecting system anomalies.
through ourempirical study we find that existing log based anomaly detec tion approaches are significantly affected by log parsing errorsthat are introduced by oov out of vocabulary words and2 semantic misunderstandings.
the log parsing errors couldcause the loss of important information for anomaly detection.to address the limitations of existing methods we proposeneurallog a novel log based anomaly detection approach thatdoes not require log parsing.
neurallog extracts the semanticmeaning of raw log messages and represents them as semanticvectors.
these representation vectors are then used to detectanomalies through a transformer based classification model which can capture the contextual information from log sequences.our experimental results show that the proposed approach caneffectively understand the semantic meaning of log messages andachieve accurate anomaly detection results.
overall neurallogachieves f1 scores greater than .
on four public datasets outperforming the existing approaches.
index t erms anomaly detection log analysis log parsing deep learning i. i ntroduction high availability and reliability are essential for largescale software intensive systems .
with the increasing complexity and scale of systems anomalies have becomeinevitable.
a small problem in the system could lead to perfor mance degradation data corruption and even a significant lossof customers and revenue.
anomaly detection is therefore necessary for the quality assurance of complex software intensive systems.
software intensive systems often generate console logs to record system states and critical events at runtime.
engineerscan utilize log data to understand the system status detectthe anomalies and identify the root causes.
as the amountof logs could be huge anomaly detection based on manualanalysis of logs is time consuming and error prone.
overthe years many data driven methods have been proposed toautomatically detect anomalies by analyzing log data .
machine learning based methods suchas logistic regression support vector machine invariant mining extract log events and adopt supervisedor unsupervised learning to detect the occurrences of systemanomalies.
recently some deep learning based approaches have been proposed.
for example logrobust and loganomaly adopt word2vec model to obtain embedding vectors oflog events then applied an lstm model to detect anomalies.
hongyu zhang is the corresponding author.however the existing approaches rely on log parsing to preprocess semi structured log data.
log parsers remove thevariable part from log messages and retain the constant partto obtain log events.
to investigate the inaccuracy of logparsing we have performed an empirical study on real worldlog data.
we find that existing log parsers produce a noticeablenumber of parsing errors which directly downgrade anomalydetection performance.
the log parsing errors are mainly dueto the following two reasons the logging statements couldfrequently change during software evolution resulting in newlog events that were not seen in training data valuableinformation could be lost while parsing log messages into logevents which may lead to misunderstanding of the semanticmeaning of log messages.
our empirical study also findsthat the log parsing errors can affect the follow up anomalydetection task and decrease the detection accuracy.
to overcome the above mentioned limitations of existing approaches we propose neurallog a novel anomaly de tection approach which can achieve effective and efficientanomaly detection on real world datasets.
unlike the existingapproaches neurallog does not rely on any log parsing thuspreventing the loss of information due to log parsing errors.each log message is directly transformed into a semantic vec tor which is capable of capturing both semantic informationembedded in log messages and the relationship between logmessages.
then taking a sequence of semantic vectors asinput a transformer based classification model is applied todetect anomalies.
the transformer based model with multi head self attention mechanism can learn the contextualinformation from the log sequences in the form of vectorrepresentations.
as a result neurallog is effective for log based anomaly detection.
we have evaluated the proposed approach using four public datasets.
the experimental results show that neurallog canunderstand the semantic meaning of log data and adequatelyhandle oov words.
it achieves high f1 scores all greaterthan .
for anomaly detection and outperforms the existinglog based anomaly detection approaches.
the main contributions of this paper are as follows we perform an empirical study of log parsing errors.
we find that existing log based anomaly detection approaches are adversely affected by the log parsing errorsintroduced by the oov words and semantic misunder standing.
we propose neurallog a novel deep learning based approach that can detect system anomalies without logparsing.
neurallog utilizes bert a widely used pre trained language representation to encode the semantic 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee meaning of log messages.
we have evaluated neurallog using public datasets.
the results confirm the effectiveness of neurallog for representing log messages and detecting anomalies.
ii.
b ackground a. log data large and complex software intensive systems often produce a large amount of log data for troubleshooting purposesduring system operation.
log data records the system s eventsand internal states during runtime.
by analyzing logs oper ators can better understand systems status and diagnose thesystem when a failure occurs.
fig.
.
an example of hdfs logs and parsed results figure shows a snippet of raw logs generated by hdfs hadoop distributed file system .
the raw log messagesare semi structured texts which contain header and content.
the header is determined by the logging framework andincludes information such as timestamp verbosity level e.g.
warn info and component .
the log content consistsof a constant part keywords that reveal the event template and a variable part parameters that carry dynamic runtimeinformation .
log parsing automatically converts each logmessage into a specific event template by removing parametersand keeping the keywords.
the log events can be grouped intolog sequences i.e.
series of log events that record specific execution flows according to sessions or fixed sliding timewindows .
b. log parsing methods log parsing automatically converts each log message into a specific event template by removing parameters and keeping the keywords.
for example the log template served block to can be extracted from the first log message in figure .
here denotes the position of a parameter.
there are many log parsing techniques including frequent pattern mining clustering language modeling heuristics etc.
theheuristics based approaches make use of the characteristicsof logs and have been found to perform better than othertechniques in terms of accuracy and time efficiency .in this study we evaluate four top ranked parsers includedrain ael iplom and spell .
theyutilize the characteristics of tokens e.g.
occurrences po sition relation etc.
and special structures e.g.
a tree torepresent log messages and extract common templates.
drainapplies a fixed depth tree structure to represent log messagesand extracts common templates effectively.
spell utilizes thelongest common subsequence algorithm to parse logs in astream manner.
ael separates log messages into multiplegroups by comparing the occurrences between constants andvariables.
iplom employs an iterative partitioning strategy which partitions log messages into groups by message length token position and mapping relation.
these log parsers arewidely used in existing studies and have proven their efficiencyon real world datasets .
c. log based anomaly detection over the years many log based anomaly detection approaches have been proposed.
some of them are based on unsupervised learning methods which require only unlabeleddata to train.
for example xu et al.
employed princi pal component analysis pca to generate two subspaces normal space and anomaly space of log count vectors.
ifa log sequence has its log count vector far from the normalspace it is considered an anomaly.
im and adr discover the linear relationships among log events from logcount vectors.
those log sequences that violate the relationshipare considered anomalies.
there are also many supervisedanomaly detection approaches.
for example represent log sequences as log count vectors then appliedsupport vector machine svm logistic regression lr and decision tree algorithm to detect anomalies respectively.these approaches have many common characteristics.
they allrequire a log parser to preprocess and extract log templatesfrom log messages.
then the occurrences of log templatesare counted resulting in log count vectors.
finally a machinelearning model is constructed to detect anomalies.
figure 2shows an example of log sequence and log count vectors fromlog templatesproduced within drain .
log sequence served block to served block to got exception while serving to ... log sequence served block to served block to got exception while serving to ... log sequence count vector count vector count vector fig.
.
an example of log sequence and log count vector in recent years many deep learning based models have been proposed to analyze log data and detect anomalies .
for example deeplog first applies the spell parser to extract log templates.
then it leverages theindexes of log templates and inputs them to an lstm modelto predict the next log templates.
finally deeplog detectsanomalies by determining whether or not the incoming logtemplates are unexpected.
loganomaly uses log countvector to detect the anomalies reflected by anomalous logevent numbers.
it proposes a synonyms and antonyms based 493method to represent the words in log templates.
logrobust incorporates a pre trained word2vec model namely fasttext and combines with tf idf weight for learning therepresentation vectors of log templates which are generatedby drain .
then these vectors input an attention basedbi lstm model to detect anomalies.
due to the imperfectionof log parsing the above methods tend to the lose semanticmeaning of log messages thus leading to inaccurate detectionresults.
iii.
a nempirical study of logparsing errors in this section we describe an empirical study on the problem of existing log parsers and their impact on log based anomaly detection.
we use two public datasets in ourstudy namely blue gene l bgl and thunderbird .
the datasets are collected between and 2006from real world supercomputing systems and consist of14 log messages in total among which logmessages are manually labeled as anomalies.
a. log parsing errors introduced by oov words during development and maintenance developers can add new log statements to source code and modify the content of existing log statements.
besides runtime information can beadded to log messages as parameters to record system status.as a consequence new words i.e.
out of vocabulary oov words frequently appear in log data.
for example for a logmessage memory manager address parity error in bgl if the word parity did not appear in historical logs it isan oov word.
to determine oov words we first sort logmessages by the timestamps of logs and leverage the frontp according to the timestamps of logs as the training data and the rest as the testing data.
then we split each training logmessage into a set of tokens by the whitespace character andbuild a vocabulary from these tokens.
oov words are those words in testing data that do not exist in the vocabulary.
inthis section we increase the percentages of training data from20 to .
then we calculate the proportion of oov wordsin all the splits.
to facilitate understanding we use the bgl data at the splitting the first of the bgl dataset is used fortraining and the rest is for testing to explain in detail theanalysis of oov words.
the training set contains 786unique words and the testing set contains uniquewords.
among the unique words in the testing set 123words .
are unseen in the training set.
these oovwords only concentrate in a small subset of logs i.e.
.
of the testing set which is out of logmessages .
these oov words result in unseen logevents i.e.
log templates with oov words in the testingset accounting for .
of the total number of log eventsin the testing set.
figure shows the percentages of oov words in testing data when the percentages of training data increase from20 to on bgl and thunderbird datasets.
on the bgldataset there are always more than of words in the testingset that are unseen in the training set.
on the thunderbirddataset with the growth of training data the proportion ofoov words in the testing set is gradually decreasing.
however when we use thunderbird logs to train the model we stillfind .
of oov words in the testing set.
.
.
.
.
percentage of training setpercentage of oov words in testing set bgl thunderbird fig.
.
analysis of oov words in log messages in public datasets next we evaluate the number of log messages log lines and log templates that contain oov words.
the percentage of log messages containing oov words is shown in figure a .it can be seen that the percentage of log messages containingoov words in the testing set of both bgl and thunderbirddatasets is small.
when we use logs to train the model we find only .
and .
log messages containing oovwords in the testing set of the bgl and thunderbird dataset respectively.
.
.
.
percentage of training setpercentage of log messages with oov words in testing setbgl thunderbird a log messages with oov words20 .
.
.
.
percentage of training setpercentage of log templates with oov words in testing setbgl thunderbird b log templates with oov words fig.
.
analysis of log with oov words figure b shows the percentages of log templates produced by drain that contains oov words on bgl and thunderbird datasets as the percentage of training dataincreases from to .
we observe that all testing setshave log templates with oov words even when trained with80 of the data.
the proportion of log templates containingoov words on the bgl dataset is always more than nomatter how much data is used for training.
the percentage oftemplates containing oov words on the thunderbird datasetdecreases with the growth of training data but still more than60 when of data is trained.
the results show that a small number of log messages containing oov words can produce many unseen log eventsin the testing set.
there are three main reasons for this finding many log events only appear during a specific period .
for example there are events that only appear in thelast of logs in the splitting.
the distribution of log events is imbalanced.
for exam ple the event generating appears in log messages .
of the dataset while others such as memory manager buffer only appear less than times.
oov words can cause log parsing errors and lead to manyextra log events.
these extra log events usually appear afew times but still make up a majority of log templates.for example log events only appear once in thebgl dataset.
our finding indicates that anomaly detection methods based only on log events could lead to many inaccurate detectionresults.
for example svm and lr which transformlog sequences into log count vectors cannot take new logevents as input because the dimension of log count vectorsis fixed i.e.
the number of original log events .
moreover deeplog using the indexes of log templates to predictthe next log event considers all new log events as anomaliesbecause they cannot be predicted by the model.
b. log parsing errors introduced by semantic misunderstanding we identify two main cases of parsing errors that are introduced by semantic misunderstanding case misidentifying parameters as keywords.
case misidentifying keywords as parameters.
case case parsing results l3 ecc status register l3 ecc status register l3 global control register 0x001249f0 l3 control register ground truth log template l3 register parsing results machine check enable machine check machine check interrupt machine check ground truth log templates machine check enable machine check interrupt fig.
.
examples of log parsing error drain for case the parameters in log messages are misidentified as keywords and included in the log templates produced by the log parsers thus leading to many extra log events.
we comparethe parsed template of each log message with the ground truthof the bgl dataset.
if a template contains more keywords thanthe ground truth it is considered wrongly parsed and an extralog event.
the two log messages in case in figure onlyrefer to one log template but are parsed into two different logtemplates.
figure shows the percentages of extra log eventsproduced by the four log parsers on two datasets.
for example there are about extra log events on the bgl dataset and72 extra log events on the thunderbird dataset using drain.bgl thunderbird00.
.
.
.
.
.
.
.
.
.
.
.53percentagedrain spell ael iplom fig.
.
percentages of extra log events produced by four log parsers for case some essential keywords in log messages could be removed after log parsing resulting in different log messages being parsed into one log event.
figure shows anexample of this case.
two different log messages are parsedinto the same log event machine check .
however one indicates a normal behavior i.e.
machine check enable while the other indicates a system anomaly i.e.
machine check interrupt .
the errors of this type make the detectionmodel difficult to distinguish between normal or abnormallogs based only on log events.
figure shows examples ofcase parsing errors introduced by the four log parsers.
eachexample shows one normal log and one abnormal log whichare parsed into the same log event.
valuable information suchas the reason for login failure i.e.
figure a is missingfrom log events and leads to many wrong detection results.
anomaly ciod login chdir p gb1 stella raptor failed input output error normal ciod login chdir home bertsch2 src bgl hello failed permission denied parsed event ciod login failed a errors introduced by drain anomaly mptscsih ioc0 attempting task abort!
sc 00000101bfc7a480 normal mptscsih ioc0 task abort success sc 00000101bfc7a480 parsed event mptscsih ioc0 sc b errors introduced by spell anomaly floating point unavailable interrupt normal floating point instr.
enabled..... parsed event floating point c errors introduced by ael anomaly ciod error creating node map from file map.dat no child processes normal ciod error creating node map from file map.dat bad file descriptor parsed event ciod error creating node map from file d errors introduced by iplom fig.
.
examples of valuable information removed by log parser table i shows examples of wrongly parsed log templates by the drain parser .
for instance on the bgl dataset there are log messages that have the template floating point .
however only log messages in the form of floating point unavailable interrupt are labeled as anomalies while others such as floating point instr .
enabled or floating point alignment exceptions indicate normal system behavior.similarly drain also produces log messages that have thelog templates indicating both normal and abnormal states onthe thunderbird dataset.
we also observe similar results for other log parsers.
on bgl the numbers of misidentified log messages producedby spell ael and iplom are and 495respectively.
on thunderbird the numbers of misidentified log messages produced by spell ael and iplom are and respectively.
table i examples of log parsing errors introduced by drain anom.
log template occu.
bgl 460floating point machine check ciod error creating node map from file ciod login failed thunderbird 934mptscsih ioc0 attempting ext3 fs error device aborted out of memory killed process note anom.
denotes the number of anomalies.
occu.
denotes the number of occurrences of the log template.
c. the impact of log parsing errors on anomaly detection the existing approaches share a common process they all utilize a log parser to parse the log messages into log events i.e.
the templates of log messages construct log sequences and then build unsupervised or supervised machine learningmodels to detect anomalies.
the existing approaches can beadversely affected by the log parsing errors introduced by theoov words and semantic misunderstanding.
in this section we evaluate the impact of log parsing errors on two representative anomaly detection methods svm basedmethod and logrobust .
svm represents those ml based approaches that use the log count vectors as input.logrobust represents recent dl based approaches that utilizethe semantic vectors of log templates as input.
they bothuse a log parser to generate a set of log events.
svm based method represents log sequences as log countvectors and then constructs a hyperplane to separate normaland abnormal samples in a high dimension space.
logrobust incorporates a pre trained word2vec model to learnthe semantic vectors of log templates instead of countinglog events occurrences.
using the word2vec model allowslogrobust to discover the semantic relationship between logevents and handle the instability of log data.
figure shows the results of the svm based method and logrobust with four different log parsers i.e.
drain spell ael and iplom .
we observe thatthe performance of current anomaly detection methods isaffected by the accuracy of log parsing and different logparsers could lead to different results.
svm achieves betterresults when using drain and ael since theseparsers produce a smaller amount of inaccurate log events as discussed in section iii b. figure a and figure c show that svm with drain achieves an f1 score of .
and0.
on bgl and thunderbird datasets respectively.
whilesvm with spell only produces f1 scores of .
on thebgl dataset and .
on the thunderbird dataset.
logrobustcan achieve better results than svm since logrobust canidentify unstable log events with similar semantic meaningthrough semantic vectorization.
still logrobust suffers fromthe log parsing errors caused by semantic misunderstanding see section iii b and achieves f1 scores of less than .
onboth datasets.precision recall f measure00.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.46accuracydrain spell ael iplom a svm on bglprecision recall f measure00.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.67accuracydrain spell ael iplom b logrobust on bgl precision recall f measure00.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.19accuracydrain spell ael iplom c svm on thunderbirdprecision recall f measure00.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.73accuracydrain spell ael iplom d logrobust on thunderbird fig.
.
results of anomaly detection with different log parsers on bgl and thunderbird datasets next we manually fix the errors produced by log parsing methods on the bgl dataset then apply svm and logrobust to confirm whether or not the accuracy of anomaly detectionmethods is improved if log parsing performs more accurately.we leverage the ground truth log templates for the bgl datasetfrom .
for each wrongly parsed log message we match itwith the most similar log template in the ground truth.
afterthe fixing process the outputs produced by the log parser areactually the ground truth.
also the outputs of different logparsers are the same after fixing as they are all the sameas the ground truth .
figure shows the accuracy measuredin terms of f measure of each individual parser before andafter fixing the log parsing errors.
we can see that both svmand logrobust perform better when the log parsing errors arefixed on average improvement for logrobust and improvement for svm .
drain spell ael iplom00.
.
.
.
.
.
.
.
.
.
.
.
54f measurebefore after a accuracy of svmdrain spell ael iplom00.
.
.
.
.
.
.
.
.
.
.
.91f measurebefore after b accuracy of logrobust fig.
.
the accuracy of anomaly detection before and after fixing the log parsing errors on the bgl dataset overall the results show that log parsing accuracy affects the performance of anomaly detection.
existing log parsing methods cannot handle well the oov words in new logs 496thus losing semantic information while detecting anomalies.
furthermore current log parsing methods could produce er rors due to semantic misunderstanding.
therefore existinganomaly detection methods that leverage log events are unableto achieve satisfying results due to the imperfections of logparsing methods.
iv .
n eural log log based anomaly detection without logparsing to overcome the limitation of existing approaches we propose neurallog a new log based anomaly detection ap proach that directly uses raw log messages to detect anomalies.the overview of the proposed approach is shown in figure10.
overall neurallog consists of three steps preprocessing section iv a neural representation section iv b andtransformer based classification section iv c .
the first stepis log preprocessing.
after that each log message is encodedinto a semantic vector by using bert.
in this way ourapproach can prevent the loss of valuable information from logmessages.
finally we leverage the transformer model todetect the anomalies.
history logs new logs .
preprocessing raw log messages ... .
info dfs.datablockscanner verification succeeded for blk 4980916519894289 ... contents ... .info dfs datablockscanner verification succeeded for ... remove number punctuation ... .
neural representation contents ... .info dfs datablockscanner verification succeeded for ... semantic vectors ... .
.
.
.
.
... ... wordpiece tokenization bert encoder .
transformer based classification ... transformer encoder anomaly?
pooling dropout linear softmax ... positional embeddings train predict fig.
.
an overview of neurallog a. preprocessing preprocessing log data is the first step for building our model.
in this step we first tokenize a log message into a set of word tokens.
we use common delimiters in thelogging system i.e.
white space colon comma etc.
to splita log message.
then every capital letter is converted to alower letter and we remove all non character tokens from theword set.
these non characters contain operators punctuationmarks and number digits.
this type of tokens is removedsince it usually represents variables in the log message andis not informative.
as an example the raw log message info dfs.datablockscanner v erificationsucceeded for blk is first split into a set of words based on common delimiters.
then non charactertokens are excluded from the set.
finally a set of words info dfs datablockscanner verification succeeded is obtained.
b. neural representation each log message records a system event with its header and message content.
the message header contains fieldsdetermined by the logging framework such as component andverbosity level.
the message content written by developersreflects a specific state of the system.
existing methods usuallyanalyze only message content and remove other information.in this paper neurallog uses all textual information suchas verbosity component and content to extract the semanticmeaning of log messages.
in order to reserve semantic in formation and capture relationships among existing and newlog messages the representation phase tries to represent logmessages in the vector format.
subword tokenization tokenization can be considered as the first step to handle oov words.
in our work we adoptthe wordpiece tokenization which is widely usedin many recent language modeling studies .
wordpiece includes all the characters and symbols into its base vocabulary first.
instead of relying on the frequencyof the pairs wordpiece chooses the one that maximizes thetraining data s likelihood.
it trains a language model startingfrom the base vocabulary and picks the pair with the highestlikelihood.
this pair is added to the vocabulary and thelanguage model is again trained on the new vocabulary.
thesesteps are repeated until the desired vocabulary is reached.
forexample the rare word datablockscanner is split into more frequent subwords data block scan ner .
in this way the number of oov words is reduced and their meaningsare captured.
the reason we choose wordpiece is that it can effectively handle the oov words and reduce the vocabulary size.
com pared with other tokenization chunking approaches word piece is more effective.
for example space stemming camelcase based tokenization strategies can lead to many oovwords and a big vocabulary .
log message representation after preprocessing and tokenization neurallog transforms each log message into aset of words and subwords.
conventionally words of log con tent are further transformed into vectors by using word2vec then the representation vector of each sentence wouldbe calculated based on the word vectors.
however word2vecproduces the same embedding for the same word.
in manycases a word can have different meanings based on itsposition and context.
bert is a recent deep learningrepresentation model that has been pre trained on a hugenatural language corpus.
in our work we employ the featureextraction function of pre trained bert to obtain the semanticmeaning of log messages.
more specifically after tokenizing the set of words and subwords is passed to the bert model and encoded intoa vector representation with a fixed dimension.
neurallogutilizes the bert base model that contains layers oftransformer encoder and hidden units of each transformer.
497each layer generates embeddings for each subword in a log message.
we use the word embeddings generated by the lastencoder layer of bert in our work.
then the embedding of alog message is calculated as the average of its correspondingword embeddings.
as any word that does not occur in thevocabulary i.e.
oov words is broken down into subwords bert can learn the representation vector of those oovwords based on the meaning of subword collections.
besides the positional embedding layer allows bert to capture therepresentation of a word based on its context in a log mes sage.
bert also contains self attention mechanisms that caneffectively measure the importance of each word in a sentence.
c. transformer based classification to better understand the semantics of logs we adopt the transformer model which has been introduced to overcome the limitations of rnn based models.
takingthe semantic vectors of log messages as input i.e.
x x x2 ... x n we use a transformer encoder based model for anomaly detection.
in this section we briefly describethe proposed transformer based classification model whichcontains positional encoding and transformer encoder.
a positional encoding the order of a log sequence conveys important information for the anomaly detectiontask.
bert encoder represents a log message into a fixed dimensional vector where log messages with similar meaningsare closer to each other.
however those vectors do notcontain the relative position information of log messages ina log sequence.
therefore a sinusoidal encoder is applied togenerate an embedding p iusing sinandcosfunctions for each position iin the log sequence x .
then piis added to the semantic vector xiat position i and xi piwill be used to feed the transformer based model see figure step .
inthis way the model can learn the relative position informationof each log message in the sequence and can distinguish logmessages at different positions.
b transformer encoder this model is based on the transformer architecture which contains self attentionlayers followed by position wise feed forward layers.
givenan input x x x2 ... x n the positional embeddings are added before it enters into the transformer.
in the transformermodule multi head attention layers calculate the attentionscore matrices for each log message with different attentionpatterns.
the attention score is calculated by training the queryand key matrices of the attention layers.
different attentionpatterns are obtained with multi head self attention layers which enable the model to consider which attention scoreis significant.
the inter layer features are connected into afeed forward network which contains two fully connectedlayers in order to reach the combination of different attentionscores.
then the output of the transformer model is fed intothe pooling dropout and a fully connected layer.
the classprobabilities which identify normal abnormal log sequences are calculated using the softmax classifier.
the architecture ofthe classification model is shown in figure step .d.
anomaly detection following the above steps we can train a transformer based model for log based anomaly detection.
when a set of new logmessages arrives neurallog firstly conducts preprocessing.then it transforms the new log messages into semantic vectors.the log sequence represented as a list of semantic vectors is fed into the trained model.
finally the transformer basedmodel can predict whether this log sequence is anomalous ornot.
v. e v aluation a. experimental design research questions in this section we evaluate our approach by answering the following research questions rqs rq1 how effective is neurallog in log based anomaly detection?
rq2 how effective is neurallog in understanding the semantic meaning of log data?
rq3 how effective is neurallog under different settings?
datasets in this paper we evaluate neurallog on four public datasets namely hdfs blue gene l thunderbird and spirit.
hdfs dataset contains 629log messages collected from a hadoop distributed file systemon the amazon ec2 platform.
each session identified by blockid in the hdfs dataset is labeled as normal or abnormal.
bgldataset contains log messages collectedfrom the blue gene l supercomputer at lawrence livermorenational labs.
thunderbird and spirit datasets were col lected from two real world supercomputers at sandia nationallabs.
each log message in these datasets was manually labeledas anomalous or not.
in this experiment we leverage 10million continuous log messages from the thunderbird dataset and 1gb log messages from the spirit dataset which were alsoused in prior work .
the details of the datasets are shownin table ii.
table ii the details of log datasets category size messages anomalies hdfs distributed system .
g blue gene l supercomputer m thunderbird supercomputer .
g spirit supercomputer .
g implementation and environment in our experiments neurallog has one layer of the transformer encoder.
thenumber of attention heads is and the size of the feed forward network that takes the output of the multi head self attention mechanism is .
the transformer based modelof neurallog is trained using adamw optimizer withthe initial learning rate of 3e .
we set the mini batch size and the dropout rate to and .
respectively.
we use thecross entropy as the loss function.
we train the transformer based model for a maximum of epochs and perform earlystopping for five consecutive iterations.
498we implement neurallog with python .
and keras toolbox and conduct experiments on a server with windows server r2 intel xeon e5 cpu 128gb ram and annvidia tesla k40c.
evaluation metrics to measure the effectiveness of neurallog in anomaly detection we use the precision recall and f1 score metrics.
we calculate these metrics as follows precision the percentage of correctly detected abnormal log sequences amongst all detected abnormal logsequences by the model.
p recision tp tp fp.
recall the percentage of log sequences that are correctly identified as anomalies over all real anomalies.recall tp tp fn.
f1 score the harmonic mean of precision and recall.
f1 score p recision recall p recision recall tp true positive is the number of abnormal log sequencesthe are correctly detected by the model.
fp false positive isthe number of normal log sequences that are wrongly identifiedas anomalies.
fn false negative is the number of abnormallog sequences that are not detected by the model.
b. rq1 how effective is neurallog?
this rq evaluates whether or not neurallog can work effectively on public log datasets.
for the hdfs dataset we construct log sequences by correlating log messages with thesame block id as the data is labeled by blocks.
then werandomly select of log sequences for training and therest of the dataset is used for testing.
for bgl thunderbird and spirit datasets we first sort the log messages by time.then we leverage the first according to the timestampsof logs log messages as the training set and the rest as the testing set.
this design ensures that the testing datacontains new log messages previously unseen in the trainingset.
following the previous work we apply a slidingwindow with a length of messages and a step size of 1message to construct log sequences.
table iii results of different methods on public datasets dataset lr svm im logrobust log2vec neurallog p .
.
.
.
.
.
hdfs r .
.
.
.
.
.
f1 .
.
.
.
.
.
p .
.
.
.
.
.
bgl r .
.
.
.
.
.
f1 .
.
.
.
.
.
thunder birdp .
.
.
.
.
r .
.
.
.
.00f1 .
.
.
.
.
p .
.
.
.
.
spirit r .
.
.
.
.
f1 .
.
.
.
.
denotes timeout hours p denotes precision r denotes recall and f1 is the f1 score.
we compare the results of neurallog and five existing approaches including support vector machine based approach svm logistic regression based approach lr invariant mining im logrobust and log2vec .
traditional approaches such as svm lr and im transform the log sequences into log count vectors then build unsupervised or supervised machine learning models to detectanomalies.
in our work we utilize drain to generate thelog events for svm lr and im.
logrobust incorporates apre trained word2vec model to learn the representationsvector of log templates instead of counting the occurrencesof log events.
logrobust then leverages an attention basedbi lstm to learn and detect anomalies.
log2vec accu rately extracts the semantic and syntax information from logmessages and leverages the deeplog model to improvethe accuracy of anomaly detection.
we do not compare withdeeplog because previous studies already showed thatlog2vec outperforms deeplog .
note that there are someother recent state of the art methods such as loganomaly .
however loganomaly has no publicly availableimplementation and requires operators domain knowledge to manually add domain specific synonyms and antonyms .therefore it is not experimentally compared in this paper.
the comparison results are shown in table iii.
overall neurallog achieves the best results on bgl thunderbird andspirit datasets and comparable results on the hdfs dataset.it is worth noting that the recall value achieved by neurallogon the hdfs dataset is .
which means that neurallogcan identify all anomalies captured by the dataset with highprecision.
neurallog achieves the best f1 score of .
onthe bgl dataset .
on the thunderbird dataset and .97on the spirit data.
as discussed in section ii c existing approaches including svm decision tree and lr are heavily affected by theaccuracy of log parsing.
besides these approaches cannotcapture the semantic information of log messages.
therefore these approaches perform poorly on bgl and thunderbirddatasets when the log parsing is inaccurate.
they can achievea high f1 score on the spirit dataset since the parsing errorrate is only .
for this dataset.
logrobust which encodes log templates into semantic vectors using the fasttext pre trained model cannotwork well on out of datasets.
logrobust shows alower f1 score of .
and .
on bgl and thunderbirddatasets respectively.
the main reason is that logrobustutilizes the drain log parser to obtain log templates.as aforementioned in section iii c the drain parser couldinaccurately parse a noticeable number of log messages onbgl and thunderbird datasets.
log2vec transforms rawlog messages into semantic vectors thus can avoid errors fromlog parsers.
besides log2vec also adopts mimick anapproach of inducing word embedding from character levelfeatures to handle oov words to improve anomaly detectionperformance.
however it is hard to extract contextual infor mation from characters to form meaningful words .
therefore log2vec could not effectively handle somedomain specific words such as technical terms or entity names .
consequently compared to neurallog thatuses subword level feature to handle oov words log2vecachieves lower f1 scores on bgl and thunderbird datasets .
and .
respectively .
we also evaluate the time efficiency of neurallog on the four datasets.
on average it takes neurallog .
minutes per dataset to encode all log messages and .
minutes to train adetection model epochs .
the average time of the anomalydetection phase is .
milliseconds per log sequence.
baselinemethods that require log parsing take minutes on averagefor preprocessing.
log2vec spends an average of minutesin the preprocessing phase.
svm and lr models can finishtraining within .
.
minutes.
logrobust and log2vec can train a detection model in an average of .
and .
minutes respectively.
in the detection phase it takes logrobust .2milliseconds per log sequence and it is .
milliseconds forlog2vec.
neurallog can scale to large datasets.
for example neurallog is able to handle the hdfs dataset which contains11 log messages.
it took neuralog .
minutes forpreprocessing .
minutes for training and .
minutes fortesting to perform the experiment for this rq on the hdfsdataset.
in summary the experimental results confirm that neurallog can work effectively and efficiently for log based anomalydetection.
c. rq2 how effective is neurallog in understanding the semantic meaning of log data?
in this section we evaluate the ability of neurallog to capture the semantic meaning of log messages.
to this end we examine the effectiveness of the encoding component thatrepresents log messages as semantic vectors and the subwordtokenization component that handles oov words.
in neurallog we preprocess the raw log messages and directly encode the preprocessed log messages into semanticvectors.
we compare neurallog with two variants neurallog index the indexes of log templates obtainedby drain are simply encoded into numeric vectorsand passed to the transformer model for anomaly detec tion.
the rest of neurallog is kept the same.
neurallog template we utilize bert to encode thelog templates produced by the drain into semanticvectors.
we then feed these semantic vectors to thetransformer model for anomaly detection.
the rest ofneurallog is kept the same.
table iv shows the results of two variants of neurallog.
we can see that on hdfs and spirit dataset these two variantscan achieve high f1 scores.
the reason is that log parsersperform well on these datasets.
we find that the parsing errorrate on the spirit dataset is only .
.
besides the hdfssystem records relatively simple operations with only eventtypes making log parsers easy to analyze.
in contrast theresults of the variants on bgl and thunderbird datasets aregreatly affected by log parsing methods because they cannotprecisely represent the meaning of log messages especiallywhen using the indexes of log templates.
for example themodel using log templates indexes and template embeddingsonly achieve f1 scores of .
and .
on the bgl dataset which are much lower than the .
f1 score achieved byneurallog which uses raw log messages .
table iv results of different representation methods dataset metricneurallogindexneurallogtemplateneurallog precision .
.
.
hdfs recall .
.
.
f1 score .
.
.
precision .
.
.
bgl recall .
.
.
f1 score .
.
.
precision .
.
.
thunderbird recall .
.
.
f1 score .
.
.
precision .
.
.
spirit recall .
.
.
f1 score .
.
.
we next evaluate whether or not neurallog can effectively handle oov words.
neurallog utilizes wordpiece to split an oov word into a set of subwords and then extractsthe embedding of the oov words based on its subwords.
wecompare neurallog with two variants neurallog word2vec we use a pre trained word2vecmodel to generate the embeddings of log messages.those words that do not exist in the vocabulary areremoved from log messages.
then embedding vectorsare passed to the transformer model to detect anomalies.
neurallog nowordpiece we exclude wordpiece tok enizer from the model see figure .
log messages after preprocessing are directly input to the bert modelto obtain the semantic vectors.
these vectors are theninput to the transformer model for anomaly detection.
inthis way oov words that do not exist in the vocabularywill be removed instead of broken down into subwords.
the experimental results are shown in table v. neurallog achieves the best performance since it utilizes wordpiece to tokenize an oov word into a set of subwords.therefore the meaning of an unseen word is kept by its sub words.
both variants achieve lower f1 scores than neurallogsince they rely on a fixed size vocabulary and cannot handlethe oov words.
for example on the thunderbird dataset f1 scores achieved by neurallog word2vec and neurallog nowordpiece are .
and .
respectively which are muchlower than the f1 score of .
achieved by neurallog.
in summary our results show that neurallog can effectively represent the semantic meaning of log messages.
sinceneurallog uses raw log messages after preprocessing foranomaly detection the problem of inaccurate log parsingcan be avoided.
the results also show that neurallog caneffectively learn the meaning of oov words.
d. rq3 effectiveness of neurallog under different settings neurallog utilizes bert as a pre trained language representation to understand the semantic meaning of log messages.
in this rq we would like to evaluate the performance 500table v results of handling oov words dataset metricneurallogword2vecneurallognowordpieceneurallog precision .
.
.
hdfs recall .
.
.
f1 score .
.
.
precision .
.
.
bgl recall .
.
.
f1 score .
.
.
precision .
.
.
thunderbird recall .
.
.
f1 score .
.
.
precision .
.
.
spirit recall .
.
.
f1 score .
.
.
of neurallog with different pre trained language models.
we replace the bert model in neurallog with gpt2 androberta and then perform experiments to evaluate theperformance of log based anomaly detection.
for gpt2 androberta encoders we use their base model with layers attention heads and hidden units.
table vi shows theresults.
we observe that these three pre trained models can allunderstand the semantic meaning of log messages and achievepromising results.
overall the performance of bert is higherthan that of gpt2 and roberta.
table vi results of different pre trained models dataset metric bert gpt2 roberta precision .
.
.
hdfs recall .
.
.
f1 score .
.
.
precision .
.
.
bgl recall .
.
.
f1 score .
.
.
precision .
.
.
thunderbird recall .
.
.
f1 score .
.
.
precision .
.
.
spirit recall .
.
.
f1 score .
.
.
the number of attention heads and the feed forward network size are two major hyperparameters of the transformermodel used in neurallog.
to evaluate the impact of theseparameters on detection accuracy we vary their values andperform experiments on the four datasets.
the resulting f1 scores are shown in figure .
we observe that reducingthe number of attention heads and feed forward network sizecan slightly hurt the performance of neurallog.
for example neurallog achieves f1 scores ranging from .
to .
whenusing attention heads.
these results are higher than thoseobtained by using only one attention head .
.
.similarly f1 scores achieved by a larger feed forward networkare usually better.
overall the transformer model achievespromising results with different hyperparameter values mostf1 scores are above .
.
we observe that the performanceis best when the number of attention heads is between and12 and the feed forward network size is from to .
.
.
.
.
number of attention headsf1 score hdfs bgl thunderbird spirit a results of neurallog with different number of attention heads2048 .
.
.
.
size of feed forward networkf1 score hdfs bgl thunderbird spirit b results of neurallog with different size of feed forward network fig.
.
results of different hyperparameter settings vi.
d iscussion a. why does neurallog work?
there are three main reasons that make neurallog perform better than the related approaches.
first neurallog directly uses raw log messages instead of using a log parser inpreprocessing.
since there is no loss of information fromlog messages neurallog can precisely learn the semanticrepresentation of log messages compared to other approachesthat depend on log parsing.
second neurallog leveragesbert and wordpiece to capture the meaning ofoov words at the subword level.
moreover the transformer based classification model can also improve the performanceof anomaly detection.
the transformer utilized by neurallogcan learn different sequence patterns in log messages anddetermine which patterns are more relevant to anomalies.
our study has demonstrated the effectiveness of neurallog for anomaly detection.
however neurallog still haslimitations.
our approach is based on the learning of thesemantic meanings of log messages.
given a log message we first remove those words that contain numbers and specialcharacters.
however in some cases the removed words maycarry important information such as node id task id ipaddress or exit code.
these information could be useful foranomaly detection in certain scenarios.
in our future work wewill encode more log related information and investigate theirimpact on log based anomaly detection.
b. threats to v alidity we have identified the following major threats to validity.
subject datasets.
in this work we use datasets collected from the distributed system i.e.
hdfs and supercomputer including bgl thunderbird and spirit .
although thesedatasets all come from real world systems and contain millionsof logs the number of subject systems is still limited and donot cover all the domains.
in the future we will evaluate theproposed approach on more datasets collected from a widevariety of systems.
tool comparison.
in our evaluation we compared our results with those of related approaches i.e.
svm lr im logrobust and log2vec .
we adopt the implementation ofsvm lr and im based methods provided by loglizer .we adopt the implementation of logrobust and log2vec 501provided by their authors.
we apply the default parameters and settings e.g.
sliding window size step size etc.
used inthe previous work .
still the correctness of theseimplementations could be a threat.
to reduce this threat wemake sure that the implementation of related work can producesimilar results as those reported in the original papers.
noises in labeling.
our experiments are based on four public datasets that are widely used by related work .
these datasets are manually inspectedand labeled by engineers.
therefore data noise false posi tive negatives may be introduced during the manual labelingprocess.
although we believe the amount of noise is small if it exists we will investigate the data quality issue in ourfuture work.
vii.
r elated work a. log parsing errors the log parsing accuracy highly influences the performance of log mining .
log parsers could produce inconsistentresults depend on the preprocessing step and the set ofparameters .
the preprocessing step can furtherimprove log parsing accuracy and despite the simplicity it still requires some additional manual work .
zhu et al.
benchmarked automated log parsers on a total of 16datasets.
they found that drain is the most accurate logparser which attains high accuracy on out of datasets.
theother top ranked log parsers include iplom ael and spell .
they also found that some model parametersneed to be tuned manually and some models did not scalewell with the volume of logs.
he et al evaluated fourwidely used log parsers including slct iplom lke and logsig .
in practice new types of logs always appear as oov words can be added to log templates and lead to many extralog events which will confuse the downstream tasks.
zhanget al.
indicated that log data is unstable meaning thatnew log events often appear due to software evolution at itslifetime.
their empirical study on a microsoft online servicesystem shows that up to .
logs are changed in thelatest version.
in our work we perform an empirical studyof the log parsing errors caused by the oov problem andsemantic misunderstanding and investigate their impact on theperformance of anomaly detection.
b. log representations as described in section ii most of the existing log based anomaly detection approaches use log parsers to obtain log events and represent log messages as log events.
therefore the existing approaches suffer from the oov problem and theinaccurate log parsing.
recently deep learning based modelshave been adopted into log based anomaly detection.
deeplog applies spell to extract log events then each logevent is assigned with an index.
since deeplog represents logmessages as the indexes of log templates it cannot preventsemantic information loss and could produce many wrongdetection results .
logrobust leverages drain to obtain log templates then encodes these templatesusing the fasttext framework combined with tf idf weight.
loganomaly applies ft tree to parselog messages to templates then proposes template2vec toencode these templates based on word2vec .
swisslog obtains the semantic information of log messages af ter parsing log messages using a dictionary based approach.due to imperfect log parsing these methods could fail tocapture the semantic meaning of log messages and produceincorrect results.
log2vec transforms raw log messagesinto semantic vectors.
as it utilizes character level features it could not effectively handle some domain specific words .
besides log2vec adopts word2vec basedmodel that ignores the contextual information in sentences thus it cannot fully understand the semantic meaning oflog messages.
nedelkoski et al.
proposed logsy whichis a classification based method to learn log representationsin a way to distinguish between normal data from the targetsystem and anomaly samples from auxiliary log datasets.
itdoes not provide mechanism for handling oov words in logmessages either.
to overcome the limitations of existing methods we propose neurallog a deep learning based anomaly detectionapproach using raw log data.
neurallog utilizes wordpiecetokenization to effectively handle oov words that constantlyappear in log messages.
it also leverages bert a widelyused pre trained language representation to understand thesemantic meaning and capture the contextual informationof raw log messages.
combined with a transformer basedclassification model neurallog achieves high accuracy onanomaly detection.
furthermore we only use log data fromthe target systems and do not require any auxiliary data.
viii.
c onclusion log based anomaly detection is important for improving the availability and reliability of large scale software systems.our empirical study shows that existing approaches sufferfrom inaccurate log parsing and cannot handle oov wordswell.
to overcome the limitations introduced by log parsing in this paper we propose neurallog a log based anomalydetection approach that does not require log parsing.
ourapproach employs bert encoder to capture the semanticmeaning of raw log messages.
to better capture contextualinformation from log sequences we construct a transformer based classification model.
we have evaluated the proposedapproach using four public datasets.
the experimental resultsshow that neurallog is effective and efficient for log basedanomaly detection.
our source code and experimental data are publicly available at a cknowledgment this research was supported by the australian government through the australian research council s discovery projectsfunding scheme project dp200102940 .
502references e. bauer and r. adams reliability and availability of cloud computing.
john wiley sons .
r. s. kazemzadeh and h. a. jacobsen reliable and highly available distributed publish subscribe service in 28th ieee international symposium on reliable distributed systems.
ieee pp.
.
j. breier and j. brani sov a anomaly detection from log files using data mining techniques in information science and applications.
springer pp.
.
b. zhang h. zhang p. moscato and a. zhang anomaly detection via mining numerical workflow relations from logs in international symposium on reliable distributed systems srds .
ieee pp.
.
m. du f. li g. zheng and v .
srikumar deeplog anomaly detection and diagnosis from system logs through deep learning in proceedings of the acm sigsac conference on computer and communica tions security pp.
.
m. chen a. x. zheng j. lloyd m. i. jordan and e. brewer failure diagnosis using decision trees in international conference on autonomic computing .
proceedings.
ieee pp.
.
w. xu l. huang a. fox d. patterson and m. i. jordan detecting large scale system problems by mining console logs in proceedings of the acm sigops 22nd symposium on operating systems principles pp.
.
x. zhang y .
xu q. lin b. qiao h. zhang y .
dang c. xie x. yang q. cheng z. li et al.
robust log based anomaly detection on unstable log data in proceedings of the 27th acm joint meeting on european software engineering conference and symposium on thefoundations of software engineering pp.
.
h. guo s. yuan and x. wu logbert log anomaly detection via bert arxiv preprint arxiv .
.
p. bodik m. goldszmidt a. fox d. b. woodard and h. andersen fingerprinting the datacenter automated classification of performancecrises in proceedings of the 5th european conference on computer systems pp.
.
j. g. lou q. fu s. yang y .
xu and j. li mining invariants from console logs for system problem detection.
in usenix annual technical conference pp.
.
w. meng y .
liu y .
zhu s. zhang d. pei y .
liu y .
chen r. zhang s. tao p. sun et al.
loganomaly unsupervised detection of sequential and quantitative anomalies in unstructured logs.
in ijcai vol.
pp.
.
t. mikolov k. chen g. corrado and j. dean efficient estimation of word representations in vector space arxiv preprint arxiv .
.
a. vaswani n. shazeer n. parmar j. uszkoreit l. jones a. n. gomez l. kaiser and i. polosukhin attention is all you need arxiv preprint arxiv .
.
j. zhu s. he j. liu p. he q. xie z. zheng and m. r. lyu tools and benchmarks for automated log parsing in ieee acm 41st international conference on software engineering software engineeringin practice icse seip .
ieee pp.
.
p. he j. zhu s. he j. li and m. r. lyu an evaluation study on log parsing and its use in log mining in 46th annual ieee ifip international conference on dependable systems and networks dsn .ieee pp.
.
m. nagappan and m. a. v ouk abstracting log lines to log event types for mining software system logs in 7th ieee working conference on mining software repositories msr .
ieee pp.
.
r. vaarandi and m. pihelgas logcluster a data clustering and pattern mining algorithm for event logs in 11th international conference on network and service management cnsm .
ieee pp.
.
h. dai h. li c. s. chen w. shang and t. h. chen logram efficient log parsing using n gram dictionaries ieee transactions on software engineering .
l. tang t. li and c. s. perng logsig generating system events from raw textual logs in proceedings of the 20th acm international conference on information and knowledge management pp.
.
h. hamooni b. debnath j. xu h. zhang g. jiang and a. mueen logmine fast pattern recognition for log analytics in proceedingsof the 25th acm international on conference on information andknowledge management pp.
.
k. shima length matters clustering system log messages using length of words arxiv preprint arxiv .
.
s. thaler v .
menkonvski and m. petkovic towards a neural language model for signature extraction from forensic logs in 5th international symposium on digital forensic and security isdfs .
ieee pp.
.
p. he j. zhu z. zheng and m. r. lyu drain an online log parsing approach with fixed depth tree in ieee international conference on web services icws .
ieee pp.
.
z. m. jiang a. e. hassan p. flora and g. hamann abstracting execution logs to execution events for enterprise applications short paper in2008 the eighth international conference on quality software.
ieee pp.
.
a. a. makanju a. n. zincir heywood and e. e. milios clustering event logs using iterative partitioning in proceedings of the 15th acm sigkdd international conference on knowledge discovery and datamining pp.
.
m. du and f. li spell streaming parsing of system event logs in ieee 16th international conference on data mining icdm .
ieee pp.
.
s. he j. zhu p. he and m. r. lyu experience report system log analysis for anomaly detection in ieee 27th international symposium on software reliability engineering issre .
ieee pp.
.
d. el masri f. petrillo y .
g. gu eh eneuc a. hamou lhadj and a. bouziane a systematic literature review on automated log ab straction techniques information and software technology vol.
p. .
y .
liang y .
zhang h. xiong and r. sahoo failure prediction in ibm bluegene l event logs in seventh ieee international conference on data mining icdm .
ieee pp.
.
a. joulin e. grave p. bojanowski m. douze h. j egou and t. mikolov fasttext.
zip compressing text classification models arxiv preprint arxiv .
.
g. salton and c. buckley term weighting approaches in automatic text retrieval information processing management vol.
no.
pp.
.
a. oliner and j. stearley what supercomputers say a study of five system logs in 37th annual ieee ifip international conference on dependable systems and networks dsn .
ieee pp.
.
s. he j. zhu p. he and m. r. lyu loghub a large collection of system log datasets towards automated log analytics arxiv preprint arxiv .
.
logpai.
.
available m. schuster and k. nakajima japanese and korean voice search in ieee international conference on acoustics speech and signal processing icassp .
ieee pp.
.
y .
wu m. schuster z. chen q. v .
le m. norouzi w. macherey m. krikun y .
cao q. gao k. macherey etal.
google s neural machine translation system bridging the gap between human andmachine translation arxiv preprint arxiv .
.
j. devlin m. w. chang k. lee and k. toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint arxiv .
.
v .
sanh l. debut j. chaumond and t. wolf distilbert a distilled version of bert smaller faster cheaper and lighter arxiv preprint arxiv .
.
k. clark m. t. luong q. v .
le and c. d. manning electra pretraining text encoders as discriminators rather than generators arxiv preprint arxiv .
.
r. m. karampatsis h. babii r. robbes c. sutton and a. janes big code!
big vocabulary open vocabulary models for source code in2020 ieee acm 42nd international conference on software engineer ing icse .
ieee pp.
.
q. le and t. mikolov distributed representations of sentences and documents in international conference on machine learning.
pmlr pp.
.
bert pretrained models.
.
available google research bert loghub.
.
available k. yao h. li w. shang and a. e. hassan a study of the performance of general compressors on log files empirical software engineering vol.
no.
pp.
.
i. loshchilov and f. hutter decoupled weight decay regularization arxiv preprint arxiv .
.
x. li p. chen l. jing z. he and g. yu swisslog robust and unified deep learning based log anomaly detection for diverse faults in ieee 31st international symposium on software reliability engineering issre .
ieee pp.
.
w. meng y .
liu y .
huang s. zhang f. zaiter b. chen and d. pei a semantic aware representation framework for online log analysis in2020 29th international conference on computer communications andnetworks icccn .
ieee pp.
.
y .
pinter r. guthrie and j. eisenstein mimicking word embeddings using subword rnns arxiv preprint arxiv .
.
s. sasaki j. suzuki and k. inui subword based compact reconstruction of word embeddings in proceedings of the conference of the north american chapter of the association for computationallinguistics human language technologies v olume long and shortpapers pp.
.
j. zhao s. mudgal and y .
liang generalizing word embeddings using bag of subwords arxiv preprint arxiv .
.
z. hu t. chen k. w. chang and y .
sun few shot representation learning for out of vocabulary words arxiv preprint arxiv .
.
a. radford j. wu r. child d. luan d. amodei and i. sutskever language models are unsupervised multitask learners openai blog vol.
no.
p. .
a. conneau k. khandelwal n. goyal v .
chaudhary g. wenzek f. guzm an e. grave m. ott l. zettlemoyer and v .
stoyanov unsupervised cross lingual representation learning at scale arxiv preprint arxiv .
.
loglizer.
.
available s. nedelkoski j. bogatinovski a. acker j. cardoso and o. kao selfattentive classification based anomaly detection in unstructured logs arxiv preprint arxiv .
.
r. vaarandi a data clustering algorithm for mining patterns from event logs in proceedings of the 3rd ieee workshop on ip operations management ipom ieee cat.
no.
03ex764 .
ieee pp.
.
q. fu j. g. lou y .
wang and j. li execution anomaly detection in distributed systems through unstructured log analysis in ninth ieee international conference on data mining.
ieee pp.
.
s. zhang w. meng j. bu s. yang y .
liu d. pei j. xu y .
chen h. dong x. qu et al.
syslog processing for switch failure diagnosis and prediction in datacenter networks in ieee acm 25th international symposium on quality of service iwqos .
ieee pp.
.