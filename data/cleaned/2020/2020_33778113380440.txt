posit simultaneously tagging natural and programming languages profir petru p rt achi profir petru.partachi.
ucl.ac.uk university college london london united kingdomsantanu kumar dash s.dash surrey.ac.uk university of surrey guildford surrey united kingdom christoph treude christoph.treude adelaide.edu.au university of adelaide adelaide south australia australiaearl t. barr e.barr ucl.ac.uk university college london london united kingdom abstract softwaredevelopersuseamixofsourcecodeandnaturallanguage texttocommunicatewitheachother stackoverflowanddevelopermailinglistsaboundwiththismixedtext.taggingthismixedtextisessentialformakingprogressontwoseminalsoftwareengineering problems traceability and reuse via precise extraction of code snippets from mixed text.
in this paper we borrow code switching techniquesfromnaturallanguageprocessingandadaptthemto apply to mixed text to solve two problems language identification and token tagging.
our technique posit simultaneously providesabstractsyntaxtreetagsforsource codetokens part of speechtags fornaturallanguagewords andpredictsthesourcelanguageofa tokeninmixedtext.torealizeposit wetrainedabilstmnetwork withaconditionalrandomfieldoutputlayerusingabstractsyntax treetagsfromtheclangcompiler andpart of speechtags from thestandardstanfordpart of speechtagger.
positimprovesthe state of the artonlanguageidentificationby10 .
andpos ast tagging by .
in accuracy.
ccs concepts generalandreference generalconferenceproceedings software and its engineering documentation formal language definitions.
keywords part of speec htagging mixed code code switching language identification acm reference format profir petru p r t achi santanu kumar dash christoph treude and earl t.barr.
.posit simultaneouslytaggingnaturalandprogramming languages .
in 42nd international conference on software engineering icse may23 seoul republicofkorea.
acm newyork ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction programmersoftenmixnaturallanguageandcodewhentalking about the source code.
such mixed text is commonly found in mailinglists documentation bugdiscussions andonlineforasuchas stackoverflow.searchingandminingmixedtextisinevitablewhen tacklingseminalsoftwareengineeringproblems liketraceability andcodereuse.mostdevelopmenttoolsaremonolingualorworkat a levelof abstractionthat does notexploit language specificinformation.fewtoolsdirectlyhandlemixedtextbecausethedifferences between natural languages and formal languages call for different techniques and tools.
disentangling the languages in mixed text whilesimultaneouslyaccountingforcrosslanguageinteractions iskeytoexploitingmixedtext itwilllaythefoundationfornew tools that directly handle mixed text and enable the use of existing monolingual tools on pure text snippets extracted from mixed text.mixed text awaretoolingwillhelpbindspecificationstotheir implementation or help link bug reports to code.
themixedtexttagging problemisthetaskoftaggingeachtoken inatextthatmixesatleastonenaturallanguagewithseveralformal languages.
it has two subproblems identifying a token s origin language language tagging and identifying the token s part of speech pos oritsabstractsyntaxtree ast tag pos asttagging .
a token may have multiple pos ast tags.
in the sentence i foo ed the string bar .
foo is a verb in english and a method name of an object of type string .therefore pos ast tagging involves building amap that pairs alanguage to thetoken s pos ast node in that language for each language operative over that token.
we present posit to solve the mixed text tagging problem positdistinguishesanaturallanguage english fromprogramminglanguagesnippetsandtagseachtextorcodesnippetunderitslanguage sgrammar.tothisend positjointlysolvesboththelan guagesegmentationandtaggingsubproblems.positemploystech niquesfromnaturallanguageprocessing nlp forcode switched text.code switchingoccurswhenmultilingualindividualssimultaneouslyusetwo ormore languages.thishappenswhenthey want to use the semantics of the embedded language in the host language.
within the nlp space such mixed text data tends tobe bi and rarely tri lingual.
unique to our setting is as our datataught us the mixing of more than three languages one natural 1thefactthatthenlpliteratureusestheword code intheirnamefortheproblem ofhandlingtextthatmixesmultiplenaturallanguagesisunfortunateinourcontext.
they mean code in the sense of coding theory.
ieee acm 42nd international conference on software engineering icse and often many formal ones in our corpus many posts combine a programming language file paths diffs json and urls.
tovalidateposit wecompareittoponzanelli et al.
spioneering work stormed the first context free approach for mixed text.
they use an island grammar to parse java json and xml snippetsembeddedwithinenglish.asenglishisrelegatedtowater stormed neglects natural language builds asts for its islands then augments them with natural language snippets to create heterogenous asts.
posit tags both natural languages and formal languages butdoesnotbuildtrees.bothtechniquesidentifylanguageshiftsandbothtoolslabelcodesnippetswiththeirastlabels.
positisdesignedfromthegrounduptohandle untagged mixed textaftertraining.stormedlooksfortagsandresortstoheuristics in their absence.
on the language identification task stormed achieves anaccuracy of onthe same dataset posit achieves .
.tocomparestormedandpositonthepos asttagging task we extracted ast tags from the stormed output.
despite not being designed for this task stormed establishes the existing stateoftheartandachieves61 .
againstposit s85 .
.posit outperformsstormedhere inpart becauseitfindsmanymore small code snippets in mixed text.
in short posit advances the state of the art on mixed text tagging.
posit is not restricted to java.
on the entire stack overflow corpus javaandnon javaposts positachievesanaccuracyof .
for language identification and .
for pos or ast tagging.
a manual examination of posit s output on stack overflow posts containing tokens showed performance consistent withposit sresults ontheevaluationset .
accuracy onlanguagetaggingand93 .
onpos asttagging.toassesswhether positgeneralisesbeyonditstwotrainingcorpora wemanually validated it on e mails from the linux kernel mailing list.
here posit achieved .
accuracy on language tagging and .
on pos ast tagging.
positisdirectlyapplicabletodownstreamapplications.first itslanguageidentificationachieves95 balancedaccuracywhen predictingmissedcodelabelsandcouldbethebasisofatoolthatau tomaticallyvalidatespostsbeforesubmission.second tasknav is a tool that extracts mixed text for development tasks.
posit s languageidentificationandpos asttaggingenablestasknavto extractmorethantwonew reasonabletasksperdocument onacorpus of lkml e mails it extracts new tasks of which are reasonable.
our main contributions follow wehavebuiltthefirstcorpusformixedtextthatistagged at token granularity for english and c c .
wepresent posit annlp basedcode switching approach for the mixed text tagging problem positcandirectlyimprovedownstreamapplications itcan improve the code tagging of stack overflow posts and it improves tasknav a task extractor.
we make our implementation and the code comment corpus used for evaluation available at motivating example the mix of source code and natural language in the various documentsproducedandconsumedbysoftwaredeveloperspresentson fri aug xxx xxx xxx.xxx wrote looking at the change that broke this we have diff removed for brevity where real was added as a parameter to copy instruction .note that we pass in dest len but not real len as you patch fixes.
copy instruction was changed by the bad commit with diff removed for brevity figure example e mail snippet from the linux kernelmailing list.
it discusses a patch that fixes a kernel freeze.here the fix is performed by updating the rip address byadding lento the realvalue during the copying loop.
code tokens are labelled by the authors using the patches as con text and rendered using monospace.
whereadv real string literal wasverb addedverbasadpadetparameternountoadp copy instructionmethod name ..notenounthatadp wepronpassverbinadp dest len string literal butconjnotadv real len string literal asadpyoupron patchverbfixesnoun.. copy instructionmethod name wasverbchangedverbbyadpthedetbadadj commitnounwithadp .
figure2 posit soutputfromwhichtasknav extractsthe tasks passin dest len and passin real len .weshowthe pos ast tags as superscript and mark tokens with if they are identified as code.
posit spots the two mention roles of code tokens as string literal s. manychallengestotoolsthataimtohelpdevelopersmakesenseofthesedocumentsautomatically.anexampleistasknav atool that supports task based navigation of software documentation by automaticallyextractingtaskphrasesfromadocumentationcorpusandbysurfacingthesetaskphrasesinaninteractiveauto complete interface.
for the extraction of task phrases tasknav relies on grammatical dependencies between tokens in software documenta tionthat inturn reliesoncorrectparsingoftheunderlyingtext.tohandletheuniquecharacteristicsofsoftwaredocumentationcaused by themix of codeand natural language the tasknav developers hand craftedanumberofregularexpressionstodetectcodetokens as wellas anumberof heuristicsfor sentence completion such as adding this method atthe beginningofsentences withmissing subject.theseheuristicsarespecifictoaprogramminglanguage pythonintasknav scase andaparticularkindofdocument such as api documentation dominated by method descriptions.
1349posit has the potential to augment tools such as tasknav to reliablyextracttaskphrasesfromanydocumentthatmixescode and natural language.
as an example in figure we can see an e mail excerpt from the lkml2.
tasknav only manages to extract trivialtaskphrasesfromthisexcerpt e.g.
patchfixes andmisses task phrases related to the code tokens of dest real andlen due to incorrect parsing of the sentence beginning with note that... .afteraugmentingtasknavwithposit thenewversion which we call tasknav manages to extract two additional task phrases pass in dest len and pass in real len we present posit s output on this sentence in figure .
these additional task phrases extracted with the help of posit will help developers find resourcesrelevanttothetaskstheyareworkingon e.g.
whenthey are searching for resources explaining which parameters to use in whichscenario.wediscusstheperformanceoftasknav inmore detail in section .
.
mixed text tagging tags are the non terminals that produce terminals in a language s grammar.
given mixed text with knatural languages and lformal languages let a token s tag map bind the token to a tag for each of thek llanguages.weconsideraformallanguagetobeonewhich to a first approximation has a context free grammar.
the mixed texttaggingproblem isthentheproblemofbuildingatoken stag map.forexample inthesentence lieben meansloveingerman lieben is a subject in the frame language english and a verb in german.
moving to a coding example in a sentence such as i fooed the string bar .
we observe foo to be a verb in english and a method name of an object of type string .
ageneralsolutionproducesalistofpairs part of speechtagsfor each of the knatural languages together with the natural language for which we have the tag and ast tags for each of the lformal languagestogether withthe languagewithinwhich wehave the asttag.wealsoconsidertwospecialtags and thatarefresh relativetothesetofalltagswithinallnaturalandformallanguages.
weuse toindicatethataparticularlanguagehasnocandidatetag while ispairedwiththeoriginlanguage answeringthefirsttask ofourproblem.in thefirstexampleabove lieben stagmap is de verb de noun en c if we consider english german andc.inthecodeexample foo s tagmapis c de verb en method name c .inmultilingualscenarios atokenmight have a tag candidate for every language.
the mixed text tagging problem is context sensitive.
we argue below that determining the token s origin language is contextsensitivefor asingletoken code switch.the proofrestson aseries of definitions from linguistics which we state next.
to bootstrap a morpheme is an atomic unit of meaning in a language.
morphemes differ from words in that they may or may not be free or stand alone.
we source these definitions from poplack .
code switching isthealternationoftwolanguagesinasinglediscourse sentenceorconstituent.... ... wascharacterised accordingthedegreeofintegrationofitemsfromonelanguage l1 tothephonological morphological andsyntacticpatternsofthe other l2 .weuse l1torefertotheframelanguageand thesecondrestrictionisthatcode switchingoccursatpointswhere juxtapositions between l1andl2do not violate the syntactic rules ofeitherlanguage.code switchingallowsintegratingitemsfrom l2intol1alonganyoneofphonological morphological orsyntacticaxis butnotallthreesimultaneously.thislastcaseisconsidered to be mono lingual l1.
adaptation occurs when an item from l2changes when used in l1toobeyl1 srules.adaptationhasthreeforms morphological phonological and syntactical.
morphological adaptation represents modifyingthespellingof l2itemstofit l1patterns.
phonological adaptation represents changing the pronunciation of an l2item in anl1context.syntactic adaptation represents modifying l2items embeddedinadiscourse sentence orconstituentin l1toobeyl1 s syntax.
finally l2items can be used in l1without adaptation .i n this case these items often reference the code entity by name and are used as a noun in l1.
wenowconsiderthreecases i l2itemsaremorphologically adapted to l1 ii l2items are syntactically adapted to l1 and iii no adaptationof l2items occursbefore their use in l1.w ed on o t consider phonological adaptation of l2items into l1as that is not observable in text.
casei morphologicaladaptation.
considerusing affixationto convertfoo class tofoo ify verbto denote the action of converting to the class foo.
in this case foo ifybehaves as a bona fide word in l1.
such examples obey the free morpheme restriction mentioned above.
this enables it to be a separate stand alone morpheme item within l1.
the juxtaposition restriction further ensures that this parses within l1.
lacking a context to indicate foo s origin a parsers would need to assume that it is from l1.
case ii syntactic adaptation.
this case manifests similarly to morphological adaptation such as tense agreement or potentially aswordorderrestrictions.
ifspellingchangesdooccur thiscase reprises themorphological adaptationcase.
ifthe onlyadaptation iswordorder thenthetaskbecomesspottinga l2tokenthathas stayedunchangedina l1sentenceorconstituent.thisisimpossible in general if the two language s vocabularies overlap.
case iii no adaptation.
if no adaptation occurs then the formaltokenoccursin l1.thisreducestothesecondsubcaseofthe syntactic adaptation case.
posit posit starts from the bilstm crf model presented in huang et al.
augmentsittohaveacharacter levelencodingasseen inwinata et al.
andaddstwolearningtargetsasinsotoand hirschberg .figure4 presentstheresultingnetwork.
thenetwork architecture employed by posit is capable of learning to provide a language tag for any k llanguages considered.
this model iscapable ofconsidering thecontext inthe inputusing the lstms it can bias its subsequence choices as it predicts tags based on the predictions made thus far and the character level encoding allows it to learn token morphology features beyond those that we may expose to it directly as a feature vector.
a character level embeddings b feature vector embeddings figure computation of embeddings at the character level and from coding naming and spelling convention features.
in the bottom most layer the circles represent an embedding operation on characters or features to a high dimensional space.
the middle layer represents the forward lstm and the top most layer the backward lstm.
at the word level character and feature vector embeddings are represented by the concatenation of the final states of the forward and backward lstmsrepresented by in the diagrams above.adv adp det noun raw identifier equal numeric constant .
raw identifier equal numeric constant .
raw identifier equal numeric constantso for the values x y z 0forwardbackward figure a representation of the neural network used forpredicting english pos tags together with compiler derived asttags.theshadedcellsrepresentlstmcells arrowsrepresenttheflowofinformationinthenetwork.thetoplayerrepresentsalinearconditionalrandomfield crf andthetransition probabilities are used together with a viterbi de codetoobtainthefinaloutput.thefirstlayerisrepresentedby equation and converts the tokenised sentences into vector representations.
feature space.
we rely on source code attributes to separate codefromnaturallanguagewhiletaggingbothsimultaneously.wederive vector embeddings for individual characters to model subtle variations in how natural language is used within source code snippets.
examples of such variations are numbered variables such asi1ori2that often index axes during multi dimensional array operations.
another such variation arises in the naming of loop control variables where the iterator could be referred to in diverse but related ways as i itoriter.
these variations create out ofvocabulary oov wordswhichinhibitmodellingofthemixedtext.
theconfoundingeffectsofspellingmistakesandinconsistencies in the nlp literature have been independently observed by winata et al.
.theyproposedabilingualcharacterbidirectionalrnntomodel oov words.
posit uses this approach to capture character level information and address diversity in identifier names.
additionally weconsiderthestructuralmorphologyofthetokens.codetokensarerepresenteddifferentlytonaturallanguage tokens.
this is due to coding conventions in naming variables.
we utilise these norms in developing a representation for the token.
specifically we encode common conventions and spelling features intoafeaturevector.werecordifthetokenis uppercase title case lower case camelcase snake case orif any character other than the first one is upper case is a digit or isasymbol.itmaysurpriseyouthatfont whileoftenused by humans to segment mixed text is not in our token morphology featurevector.wedidnotuseitasitisnotavailableinourdatasets.forthepurposesofcodereuse weuseasequentialmodeloverthis vector as well similar to the character level vector although there is no inherent sequentiality to this data.
by ablating the high level modelfeatures wefoundthatthistokenmorphologyfeaturevector did not significantly improve model performance section .
.
encoding and architecture.
at a glance our network which we present diagrammatically in figure works as follows x t h t f wx t uh t y t g vh t .
in equation we have three sources of information characterlevel encodings fc wt token level encodings fw wt and a feature vector over token morphology ff wt .
each captures properties at a different level of granularity.
to preserve information we embed each source independently into a vector space represented bythethree ffunctions.forboththefeaturevectorandthecharacters withina word we computea representationby passingthem as sequences through the bilstm network in figure .
this figure representstheinternalsof fc wt andff wt fromequation and allows the model to learn patterns within sequences of characters as well as coding naming or spelling conventions cooccurrence 1351patterns.theresultsofthesetwobilstmstogetherwithaword embedding function fware concatenated to become the input to the mainbilstm x t in equation .
thisenables thenetwork tolearn basedonacorpus semanticsforeachtoken.thisvector represents the input cells in our full network overview in figure which is enclosed in the box.
wepasstheinputvector x t throughabilstm.thebilstm considers both left and right tokens when predicting tags.
each cell performs the actions of equation and equation with the remark that the backwards lstm has the index reversed.
this allows the network to consider context up to sentence boundaries.
we then make use of the standard softmax function softmax z j ezj k k 1ezkforj ... k which allows us to generate output probabilities over our learning targets as such p tagt tagt softmax y t p lidt lidt softmax 2lp h t equation represents language id transition probabilities andequation tag transition probabilities.
in equation 2lp represents a layer multi layer perceptron.
we make use of these transition probabilities in the crf layer to output language ids andtagsforeachtokenwhileconsideringpredictionsmadethusfar.
thetrainedeyemayrecogniseinequation andequation the transition probabilities of two markov chains.
indeed we obtain the optimal output sequence by viterbi decoding .
while equation mayseemtoindicatethatonlysingletagscanbeoutputby thisarchitecture thisisnottrue.givenenoughdata wecanmap tuples of tags to new fresh tags and decode at output time.
this may not be as efficient as performing multi tag output directly.
to train the network we use the negative log likelihood of the actual sequence being decoded from the crf network and we backpropagate this through the network.
since we have two traininggoals wecombinetheminthelossfunctionbyperforming a weighted sum of the negative log likelihood losses for each individual task then train the network to perform both tasks jointly.
whendeployed positmakesuseoftheclanglexerpythonport to generate the token input required by fw fc andff.
evaluation for each token posit makes two predictions language ids andpos ast tags.
the former task represents correctly identifying where to add code tags.
this measures how well posit segments english and code.
section .
reports posit s performance onthistask ontheevaluationset.forpos ast tagprediction we focus on posit s ability to provide tags describing the function of tokens for both modalities reliably.
to measure posit s performancehere weconsiderhowwellthemodelpredictsthetagsfora withheldevaluationdataset whichsection5.
.presentsalongwith the english code segmentation result.
posit implements the network discussed in section in tensorflow .itusestheadaptivemomentestimation adam optimiser to assign the weights in the network.
we trained it upto epochs or until we did not observe improvement in three consecutiveepochs.weusedmicro batchesof64 alearningrateof and learning decay rate of .
.
we use a dimensional word embedding space and a dimensional embedding space for characters.
the lstm hidden state is dimensional for the word representation 48dimensionalforcharactersand4forthetoken morphologyfeaturevector.theoutputofthetagcrfistheconcatenationofallfinalbilstmstates.weusea2layerperceptron with64and8dimensionalhiddenlayersforlanguageidprediction.
we apply a dropout of .
.
section .
uses this implementation for validation andsection .3uses itfor ablation.the model ssource code is available at allpositruns trainingandevaluation wereperformedona high end laptop using an intel i7 8750h cpu clocked at .9ghz .
gb of ram and a nvidia gpu with gb of vram.
thestate of the arttoolstormed whichweuseforcomparison isavailableasawebservice whichweusebyaugmentingthedemo files made available at .
corpus construction for our evaluation we make use of two corpora.
we use both totrain posit and we evaluate on each to see the performance in twoimportantuse cases anaturallanguageframelanguagewith embedded code and the reverse.
table presents their statistics.
the first corpus is the stack overflow data dump that stack overflow makes available online as an xml file.
it contains thehtml of stack overflow posts with code tokens marked using code aswellas pre class code tags.thesetagsenable us to construct a ground truth for the english code segmentation task.toobtainthepostagsforenglishtokens weusethetokeniser andstandardstanford part of speec htaggerpresentinnltk .
forast tags we usea pythonport of theclang lexerand label tokensusingafrequencytablebuiltfromthesecond codecomment corpus.
this additionally ensures that both corpora have the same setofasttags.weallowmatchesuptoalevensteindistanceof three for them we choose three from spot checking the results of variousdistances afterthree thelists were longand noisy.we address the internal threat introduced by our corpus labelling in section .
.
webuiltthesecond codecommentcorpus fromtheclang compilationof11nativelibrariesfromtheandroidopensource project aosp boringssl libcxx libjpeg turbo libmpeg2 libpcap libpng netcat netperf opencv tcpdumpandzlib.wechosethese libraries in a manner that diversifies across application areas such as codecs network utilities productivity and graphics.
we wrote a clang compiler plugin to harvest all comments and the snippets in the source code around those comments.
our compiler pass furtherharveststokenasttagsforindividualtokensinthesource codesnippets.in linecommentsareoftenpureenglish however documentation strings before the snippets with which they areassociated contain