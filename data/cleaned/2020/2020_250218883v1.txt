towards more trustworthy deep code models by enabling out of distribution detection yanfu yan viet duong huajie shao denys poshyvanyk department of computer science william mary williamsburg virginia usa email yyan09 vqduong hshao dposhyvanyk wm.edu abstract numerous machine learning ml models have been developed including those for software engineering se tasks under the assumption that training and testing data come from the same distribution.
however training and testing distributions often differ as training datasets rarely encompass the entire distribution while testing distribution tends to shift over time.
hence when confronted with out of distribution ood instances that differ from the training data a reliable and trustworthy se ml model must be capable of detecting them to either abstain from making predictions or potentially forward these oods to appropriate models handling other categories or tasks.
in this paper we develop two types of se specific ood detection models unsupervised and weakly supervised ood detection for code.
the unsupervised ood detection approach is trained solely on in distribution samples while the weakly supervised approach utilizes a tiny number of ood samples to further enhance the detection performance in various ood scenarios.
extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting ood samples from four different scenarios simultaneously and also positively impact a main code understanding task.
index terms ood detection trustworthy ml code models contrastive learning i. i ntroduction extensive ml models have been developed under the assumption that training and testing data come from the same distribution i.e.
closed world assumption .
however this assumption is often violated in practice where deployed models may frequently encounter out of distribution ood instances that are not seen in training .
for instance a model trained on high quality code may struggle to comprehend buggy code.
adapting ml models to distribution shifts is possible but challenging and costly due to the constantly evolving data .
moreover even if the training data is up to date models will still encounter unforeseen scenarios under the open world setting.
failure to recognize an ood sample and consequently to produce incorrect predictions significantly compromises the reliability of a model.
a reliable and trustworthy ml model should not only achieve high performance on samples from known distributions i.e.
in distribution id data but also accurately detect ood samples which can then either abstain from making predictions or potentially be forwarded to appropriate models handling other distributions or tasks.
ood detection has been extensively studied in computer vision cv and natural language processing nlp equal contribution.across a range of tasks e.g.
image sentiment classification question answering .
existing ood detectors typically design a scoring function to derive confidence id scores enabling the detection of ood samples based on a predefined threshold.
these ood detectors serve as an auxiliary function to the original ml models and ensures a high proportion e.g.
of id data finally retained based on the threshold.
this is crucial to prevent the ood auxiliary scoring from adversely affecting ml models performance on their main image language related tasks.
current ood detection approaches are proposed in supervised unsupervised and weakly supervised regimes depending on the availability of ood data.
supervised approaches learn a classical binary classifier based on both id and ood data but in practice it is hard to assume the presence of a large dataset that captures everything different from the id data.
unsupervised ones only utilize id data for training but are likely to suffer from poor performance.
recent studies have demonstrated that weak supervision can remarkably outperform unsupervised learning methods for anomaly ood detection.
some weakly supervised approaches generate pseudo labeled oods by partially corrupting id data based on output attention mappings while others leverage a tiny collection of labeled oods e.g.
of id data to detect specific ood types in the applications where access to ood samples is limited and pseudo ood generation is challenging .
however none of these ml approaches have been applied in the context of se for code related tasks.
existing ood detection research in se primarily focuses on anomaly detection or software defect detection.
anomaly detection techniques are designed to detect anomalous system states e.g.
failed processes availability issues security incidents during system running based on monitoring data e.g.
logs traces but they still cannot been applied to the code context.
there also exists a body of research dedicated to detecting suspicious defects in source code e.g.
vulnerability detection neural bug detection .
although defective source code represents a type of distribution shifts from normal code current defect detection techniques are not sufficient to cover a broad range of unseen scenarios considered by ood detection.
therefore the goal of this work is to address the ood detection problem in the context of se for code related tasks.
while transformer based nl pl programmingarxiv .18883v1 feb 2025language models have shown remarkable success in code understanding and generation by utilizing bimodal data i.e.
comment and code they often assume training and testing examples belong to the same distribution.
thus these models may not guarantee the robustness against ood instances in the open world as evidenced by for nl transformers .
for instance a code search engine which is trained on github comment based queries and code is likely to fail in user questions and code answers from stackoverflow.
in this paper we systematically investigate the ability of pre trained nl pl models in detecting ood instances and the impact of ood detection on a downstream code task i.e.
code search .
while nlp ood detection techniques show promise for adaptation to nl pl models due to the similarity between nl and pl they can only detect textual oods from uni modal data.
however in the se context for code related tasks distribution shifts can occur in either modality comment or code or both of them.
an effective ood code detector should be able to detect ood from comments code or both modalities by utilizing multimodal nl pl pairs.
several multi modal approaches have been proposed for vision ood detection utilizing information from both images and their textual descriptions but they are still designed to detect only visual oods.
to overcome these challenges we develop two types of multi modal ood detection models to equip nl pl models with ood code detection capability.
the first one is unsupervised coined as cood which fine tunes the nl pl models to closely align nl pl representations solely from id data based on the multi modal contrastive learning and then uses their prediction confidences as ood scores.
the contrastive learning objective is expected to effectively capture high level alignment information within nl pl pairs to detect oods.
to further enhance the ood detection performance we propose a weakly supervised ood detection model cood which utilizes a tiny collection of ood samples e.g.
during model training.
current techniques in ml typically considered unsupervised contrastive learning or outlier exposure in conjunction with a scoring function limiting their ability to detect oods from just one modality.
in contrast our cood integrates an improved contrastive learning module with a binary ood rejection module in order to effectively detect oods from nl pl or both modalities.
ood samples are then identified by a combination of two different scoring functions the confidence scores produced by the contrastive learning module and the prediction probabilities of the binary ood rejection module.
due to the lack of evaluation benchmarks for ood code detection we create a new benchmark tailored for code context following the construction principles in ml but containing more ood scenarios aligned nl pl pairs collected from a new domain e.g.
from stackoverflow rather than github misaligned nl pl pairs the presence of syntactic errors in nl descriptions and buggy source code.
we first evaluate the proposed models on two real world datasets codesearchnet java and codesearchnet python and establish a range of unsupervised and weaklysupervised baselines for comparison.
experimental results show that both cood and cood models significantly outperform the best unsupervised and weakly supervised baselines respectively.
specifically our unsupervised cood is moderately capable of detecting oods from three scenarios but does not perform well across all four scenarios.
by integrating two modules our cood model effectively detects oods from all scenarios simultaneously.
furthermore we apply our approaches to improve the robustness of existing nl pl models for the code search task under the four ood scenarios described above.
by corrupting of the testing dataset with ood examples we demonstrate that nl pl models actually are not robust to ood samples.
specifically the performance of a fine tuned graphcodebert code search model drops by around due to the presence of oods.
subsequently we filter the corrupted testing dataset with our cood cood and show that our detectors successfully recover this performance loss and also improve the code search performance compared to the original testing set.
in summary the contributions of this paper are a novel ood benchmark specifically designed for code contexts encompassing multiple ood scenarios the first work to address ood detection for code across four distinct scenarios a multi modal ood detection framework for nl pl pretrained models leveraging contrastive learning in both unsupervised and weakly supervised settings a comprehensive evaluation showcasing the superior performance of our cood and cood frameworks in detecting ood samples across four scenarios an online appendix providing the full codebase and experimental infrastructure of our approaches .
ii.
r elated work we review the related work on ood detection in various fields such as computer vision cv natural language processing nlp and software engineering se and then point out unique characteristics of our approach.
a. ood detection in se to ensure the reliability and safety of large scale software systems extensive work has been conducted on anomaly detection to identify anomalous system state e.g.
failed processes availability issues security incidents during system running based on monitoring data not in code format .
specifically monitoring data includes logs metrics e.g.
response time cpu usage traces etc.
while some approaches utilize supervised learning techniques others employ unsupervised or semi supervised learning due to insufficient anomaly labels.
however none of these anomaly detection techniques target code based ood detection the main focus of our work.
we mention this research line here since some existing oodrelated work in ml use the terms anomaly detection and generalized ood detection interchangeably but anomaly detection in se has distinct characteristics as described above.
additionally current defect detection techniques in se typically identify defects by analyzing source code with code semantic features extracted.
research in vulnerability detection focuses on security related defects such as buffer overflows and use after free.
compared to conventional static tools dl based techniques utilize graph neural networks gnns or transformers to learn implicit vulnerability patterns from source code.
additionally bug detection techniques also fall under the umbrella of defect detection but typically address semantically incorrect code e.g.
wrong binary operators variable misuse which is not necessarily security related and probably syntactically feasible.
although our focus is also on source code defective code is only considered as one scenario within the scope of our ood detection problem.
more recently several research has explored the robustness and generalization of source code models to different ood scenarios .
hu et al.
introduced a benchmark dataset to assess the performance of code models under distribution shifts while others investigated fine tuning strategies like low rank adaptation and continual learning for enhanced generalization on ood data.
however these studies did not specifically tackle ood detection and existing unsupervised ood detectors have shown limited effectiveness for source code data .
in short unlike prior work our study directly aims to improve the ood detection performance of existing code related models ensuring greater robustness and trustworthiness in the open world where many unseen ood scenarios may be encountered.
b. ood detection in cv and nlp in the ml community ood detection has been extensively studied over the years leading to a betterdefined and formulated task.
the primary objective of ood detection here is to design an auxiliary id ood classifier derived from neural based visual and or textual models based on ood scores.
given that correctly predicted instances tend to have greater maximum softmax probabilities msp than incorrectly predicted and ood instances msp based ood scoring function weree initially utilized to identify ood samples.
subsequently energy and distance based scores have also been utilized to derive ood scores.
for visual ood data existing techniques often aim for multi class classification tasks e.g.
image classification and learn a k classifier assuming that the unseen space is included in the additional class .
the ood data utilized for evaluation is typically constructed from a completely different dataset out domain data or by holding out a subset of classes in a categorized dataset where one category is considered normal and the remaining categories are treated as ood.
in the context of textual data ood detection techniques are applied to both classification tasks e.g.
sentiment topic classification and selective prediction tasks e.g.
question answering semantic equivalence judgments .
these techniques rely on various algorithmic solutions including outlier exposure data augmentation contrastive learning etc.. compared to traditional neuralbased language models pre trained transformer based models exhibit greater robustness to distributional shifts and are more effective in identifying ood instances .
besides the out domain data text based ood detection also consider syntactic ood data due to the intrinsic characteristics of sentences.
syntactic ood and id data come from the same domain but the syntactic ood data has its word order shuffled which allows for the measurement of ood detectors sensitivity to underlying syntactic information while preserving word frequency.
some studies have explored the incorporation of multi modal data into neural based models to improve ood detection accuracy.
recently clip based methods have emerged as a promising approach for ood detection by leveraging vision language bimodal data exhibiting superior performance over uni modal data only.
the main intuition behind these approaches is to take advantage of the alignment between visual classes or concepts and their textual descriptions.
for instance ming et al.
detect visual ood in an unsupervised manner by matching visual features with known id concepts in the corresponding textual descriptions.
however these studies typically focus on detecting ood data from at most two scenarios i.e.
out domain and shuffledtext oods within a single modality.
even multi modal approaches are often limited to detecting only visual oods by additionally considering accompanying textual descriptions.
our proposed approach aims to effectively identify ood samples from four distinct scenarios across two modalities i.e.
nl and pl .
to achieve this we utilize a combination of different scoring functions from two different modules cosine similarities of a contrastive learning module and prediction probabilities of a binary ood classifier.
iii.
p reliminaries below we review some background knowledge and techniques that will be used in the proposed approach.
a. code representation learning representation learning refers to the process of learning a parametric mapping from the raw input data domain to a low dimensional latent space aiming to extract more abstract and useful concepts that can improve performance on a range of downstream tasks.
recently self supervised representation learning has become prominent in the ml community due to the success of large pre trained models.
the similarity between nl and pl has led to the development of numerous nl pl pre trained models resulting in significant improvements across various code related tasks such as code search generation and clone detection.
codebert stands as the first transformer based nl pl pre trained model while graphcodebert further enhances its performance by considering the inherent structure of code i.e.
data flow 3during pre training.
subsequent models like unixcoder and contrabert further enhance the code understanding capabilities.
in our research study we are concerned with extending self supervised nl pl representation learning to the ood detection task which is outside the scope of existing pretrained models but is crucial for ensuring the trustworthiness of these models in real world applications.
b. multimodal contrastive learning contrastive learning is an emerging technique that learns discriminative representations from data that are organized into similar dissimilar pairs.
it has been developed over multiple sub fields such as nlp cv and se .
recently researchers have developed multi modal approaches that combine contrastive learning with multiple data modalities achieving superior performance over uni modal modals in various tasks .
due to the prowess of contrastive learning in discriminative tasks e.g.
classification and information retrieval it naturally fits the ood detection domain as shown by studies on both uni modal and multimodal data .
inspired by this we apply contrastive learning to nl pl data to learn representative and discriminate features for both id and ood instances and transfer this knowledge to train our ood detector.
iv.
a pproach in this section we first formally define the ood code detection problem for nl pl models sec.
iv a then introduce the overall proposed framework sec.
iv b and finally present details of unsupervised cood and weaklysupervised cood in sec.
iv c and sec.
iv d respectively.
a. problem statement since current state of the art code related models typically extract code semantics by capturing the semantic connection between nl i.e.
comment and pl i.e.
code modalities we formally defined ood samples involving these two modalities in the se context by following the convention in ml .
consider a dataset comprising training samples t1 c1 y1 t2 c2 y2 ...from the joint distributionp t c y over the space t c y and a neuralbased code model is trained to learn this distribution.
here t1 c1 y1 represents the first input pair of comment code along with its ground truth prediction in the training corpus.
t candyare random variables on an input comment code space t c and a output semantic space y respectively.
ood code samples refer to instances that typically deviate from the overall training distribution due to distribution shifts.
the concept of distribution shift is very broad and can occur in either the marginal distribution p t c or both p y andp t c .
we then formally define the ood code detection task following as follows.
given a main code related task e.g.
clone detection code search etc.
the objective here is to develop an auxiliary scoring function g t c r that assigns higher scores to normal instanceswhere t c y p t c y and lower scores to ood instances where t c y p t c y .
based on whether to use ood instances during the main task training of pretrained nl pl models we define ood for code in two settings namely unsupervised and weakly supervised learning.
for the unsupervised setting only normal data is used in the main task training.
conversely weakly supervised approaches utilize id and a tiny collection of ood data e.g.
of id data in training.
in this context the output space yis typically a binary set indicating normal or abnormal which is probably unknown during inference.
due to the small number of training ood data the ood samples required by our cood and other existing weakly supervised approaches in ml can be generated at minimal cost and feasibly verified by human experts when necessary.
b. overview overall there are two versions of our cood approach unsupervised cood and weakly supervised cood .
given a multi modal nl pl input the unsupervised cood learns distinct representations based on a contrastive learning module by utilizing a pre trained transformer based code representation model i.e.
graphcodebert .
then these representations are mapped to distance based ood detection scores in order to indicate whether the test samples are oods during inference.
the weakly supervised cood further integrates a improved contrastive learning module with a binary ood rejection module to enhance the detection performance by using a very tiny number of ood data during model training.
the ood samples are then identified by the detection scores produced by the contrastive learning module as well as the prediction probabilities of the binary ood rejection module.
c. unsupervised cood our unsupervised cood approach consists of a contrastive learning cl module trained only on id samples.
specifically given comment code pairs as input we fine tune a comment encoder and a code encoder through a contrastive objective to learn discriminative features which are expected to help identify ood samples based on a scoring function.
the comment code pairs are first converted into the comment and code representations which are processed by the comment and code encoder respectively.
we use the pretrained graphcodebert model as the encoder architecture i.e.
backbone .
graphcodebert is a transformer based model pre trained on six pls by taking the comment code pairs as well as the data flow graph of the code as input which has shown superior performance on code understanding and generation tasks.
all the representations of the last hidden states of the graphcodebert encoder are averaged to obtain the sequence level features of comment and code.
contrastive learning module.
to achieve the contrastive learning objective we fine tune the base graphcodebert encoders with the infonce loss .
the comment and code encoders follow the siamese architecture since they are designed to be identical subnetworks with the same 4binary oodrejectionl!
idprobabilitiesl similaritymatrix idoodcontrastive learningunsupervised coodweakly supervised cood multi headattentionnormmlpnorm codeencodermulti headattentionnormmlpnorm commentencodergraphcodebert graphcodebertsharedcodepublicstaticbooleanisequal objects1 objects2 returns1 s2 s1!
null s1.equals s2 commentaconveniencefunctionto checkiftwo objects areequal.fig.
the overview of our proposed cood and cood approaches for ood detection graphcodebert backbones in which their parameters i.e.
weights and biases are shared during fine tuning.
parameter sharing can reduce the model size and has shown state ofthe art performance for the code search task .
to extract discriminative features for comment code pairs we organize them into functionally similar positive pairs and dissimilar negative unpaired pairs.
through a contrastive objective positive pairs are drawn together while unpaired comment and code are pulled apart.
specifically for each positive comment code pair ti ci in the batch the code in each of other pairs and tiare constructed as in batch negatives similarly for the comment side.
the loss function then formulates the contrastive learning as a classification task which maximizes the probability of selecting positives along the diagonal of the similarity matrix as shown in fig.
by taking the softmax of projected embedding similarities across the batch.
the loss function can be summarized as follows lcl 2n nx n 1logesim vti vci pn j 1esim vti vcj nx n 1logesim vti vci pn j 1esim vtj vci where vtiandvcirepresent the extracted features of the comment tiand the code ci.
is the temperature hyperparameter which is set to .
following previous work .
sim vci vti andsim vti vcj sim vtj vci represent the cosine similarities between comment and code features for positive and negative pairs respectively.
nis the number of input pairs in the batch.
infonce loss is designed for self supervised learning and learns to distinguish positive pairs from in batch negatives.
compared to other contrastive losses it can take advantage of large batch size to automatically construct many diverse in batch negatives for robustness representation learning which is more effective to capture the alignment information between comment and code.scoring function.
existing ood detection techniques in ml derive scoring functions based on model s output which typically map the learned class probabilistic distributions to ood detection scores for testing samples.
maximum softmax probability msp is commonly used for ood scoring.
this method uses the maximum classification probability max l lsoftmax f vt vc where f vt vc is the output of the classification model with low scores indicating low likelihoods of being ood.
however nl pl code search models typically utilize the similarity retrieval scores of nl pl output representations to make predictions.
therefore to enable simultaneous similarity and ood inference we alternatively extract cosine similarity scores of testing nl pl pairs as ood detection scores denoted as pcl sim vc vt .
the underlying intuition behind this scoring metric is that ood testing samples should receive low retrieval confidence from the model fine tuned on id data which establishes a closer relationship between id comment code pairs.
hence this scoring function also assigns higher scores to id data and lower scores to ood data similar to previous scoring methods.
d. weakly supervised cood to further enhance the performance of unsupervised cood we extend it to a weakly supervised detection model called cood which takes advantage of a few ood examples.
inspired by our cood combines an improved contrastive learning cl and a binary ood rejection classifier bc .
the improved cl module adopts a margin based loss which enforces a margin of difference between the cosine similarities of aligned and unaligned comment code pairs and constrains the cosine similarities of ood pairs below another margin.
the bc module integrates features from both comments and code to calculate the probabilities of ood pairs.
the ood scoring function is then designed by combining the cosine similarity scores from the cl module and the prediction probabilities from the bc module.
below we detail each component of our weakly supervised cood .
5improved contrastive learning cl module.
given a batch of ninput pairs comprising n kid pairs and k ood pairs the latent representations are first obtained from the comment and code encoders.
then the margin based loss is leveraged in the cl module to distinguish representations of id and ood data by constraining the cosine similarity.
specifically the margin based contrastive loss is first applied ton kid code to maximize the difference between aligned comment code pairs and incorrect pairs for each batch lid n kx i nnx j j imax m s vt i vc i s vt j vc i s vt i vc i represents the cosine similarity of representations between each aligned id pair from all the n kaligned pairs and s vt j vc i represents the cosine similarity of representations between each id code and all the other n comments i.e.
the comment is either not aligned with the id code or from ood comments .
thus this margin based loss encourages the difference between the aligned pairs and the incorrect pairs greater than margin m. regarding the kood code we enforce a constraint on the cosine similarity between each ood code and all the comments ensuring that the similarity remains below a margin m. this constraint is necessary because each ood code should not align with its corresponding comment nor with any of the other k 1ood comments and the n kid comments.
the loss function is denoted as follows lood kx k nnx i 1max m sim t j c k !
where sim t j c k represents the cosine similarity between each of the kood code and all ncomments.
finally the overall loss for the contrastive module can be expressed as lcl n lid lood .
binary ood rejection bc module.
besides the cl module we also introduce a classification module under weakly supervision for identifying ood samples.
inspired by the replaced token detection rtd objective utilized in we bypass the generation phase since our ood data are generated prior to training.
therefore we directly train a rejection network responsible for determining whether comment code pairs are ood or not which can be framed as a binary classification problem.
our binary ood rejection network comprises a layer fully connected neural network with tanh activation and the input is based on the concatenation of features from the comment and code encoders vi vti vci vti vci vti vci .apart from utilizing the comment and code features we also incorporate feature subtraction vti vciand aggregation vti vci.
additionally we apply the sigmoid function to the output layer producing a prediction probability that indicates whether the sample is ood.
we then use binary cross entropy loss for this module lbc nn kx i yilogp vi yi log p vi where p vi is the output probability of the bc module and yi is the ground truth label.
yi 1indicates the input sample is an inlier while yi signifies it is an outlier.
hence for weakly supervised cood we combine the objectives of the cl and the bc modules to jointly train our model where is a weight used to balance the loss functions l lcl lbc.
combined scoring function.
similar to the unsupervised cood approach we utilize the diagonals of the similarity matrix as the ood detection scores obtained from the cl module.
to further improve the detection performance of the weakly supervised version we combine these pclscores with the output probabilities of the bc module denoted as pbc.
here we convert cosine similarity scores into probabilities using the sigmoid function pcl sim vc vt then use multiplication to create the overall scoring function yielding pid pcl pbc.
we anticipate that higher scores will be assigned to id pairs while lower scores will be assigned to ood pairs.
this combined scoring function aims to enhance the discrimination between inliers and outliers leading to more effective ood detection.
v. e mpirical evaluation design to evaluate the performance of the proposed approaches in four scenarios we investigate the following research questions rq1 how effective is our unsupervised cood when compared to unsupervised baselines?
rq2 how effective is our weakly supervised cood when compared to weakly supervised baselines?
rq3 how effective is our weakly supervised cood when using different modules or encode backbone?
rq4 is the main task code search performance affected by our cood cood auxiliary and to what extent?
a. datasets in our experiments we rely on two benchmark datasets codesearchnet csn and tlcs .
csn contains bimodal data points consisting of code paired with functionlevel nl descriptions i.e.
first lines of documentation comments in six pls e.g.
python java collected from github repositories.
while csn was originally created for a specific downstream task i.e.
code search it has since been widely adopted by large nl pl models for pre training due to the informative nature of bimodal instances.
large nlpl models are first pre trained across allsix languages and then further fine tuned for a specific pl for some downstream task to enhance performance.
for code search the goal is to retrieve the most relevant code given a nl query where csn is widely used to further fine tune a pl specific code search model .
6salza et al.
used training samples from csn for pretraining and created a new dataset sourced from stackoverflow so for fine tuning the code search model involving only three pls java python and javascript.
specifically they leverage so user questions as search queries and accepted answers as retrieved code snippets which differ from github comments and the corresponding code in csn.
we refer to this new dataset as tlcs.
existing work investigated code clones between so and github demonstrating there exists only code reuse.
besides code user questions in so are typically formulated before code answers without concrete knowledge of what code answers will be and are mostly written by end users.
conversely in github method docstrings i.e.
comments are often written following code snippets and are mostly written by developers.
these distinctions cause performance shortfall when directly applying models trained on csn to tlcs without further fine tuning or transfer learning .
b. ood scenarios we design four distinct ood scenarios using the datasets described above with csn java and csn python as inliers due to their common use for the pre training of code models.
scenario out domain.
following existing ml work we create an out of domain setting by choosing ood samples from a different dataset than the training data.
thus samples from tlcs java or tlcs python are treated as outliers accordingly.
inliers and their corresponding outliers belong to the same pl to ensure approaches don t identify oods based on syntax differences between pls but on data domains github vs. so.
prior studies show that csn queries are longer than so questions on average so we sampled tlcs questions and answers to match the length distribution of csn comments and code to avoid ood approaches exploiting spurious cues of query length differences.
we didn t consider other code search datasets because they either contain only one of the pls python or java or have a smaller dataset size.
scenario misaligned.
in this scenario we shuffle normal nl pl pairs so that each code doesn t match its nl description.
although the nl modality sourced from attached comments in code are typically aligned with the pl modality documentation errors may still occur and not effectively filtered by handcrafted rules .
scenario shuffled comment.
for comment code pairs we modify the syntactic information in each comment by shuffling of selected tokens using a seeded random algorithm with positions of stopwords and punctuations unchanged.
no changes are made to the code for this scenario.
this scenario is inspired by .
discovered that nl pre trained models are insensitive to permuted sentences which contrasts with human behavior as humans struggle to understand ungrammatical sentences or interpret a completely different meaning from a few changes in word order.
further introduces syntactic shuffling outliers into nl pre training corpora to enhance ood robustness and nl understanding performance.
scenario buggy code.
we create buggy code using a semantic mutation algorithm which injects more natural and realistic bugs into code than other traditional loose strict mutators .
this simulates buggy programs that the model may encounter during testing typically absent from the training dataset and should be taken into account by ood code detectors according to the ood definition .
we avoid using real bug vulnerability datasets due to limitations like the absence of paired comments lack of support for python or java introduction to a new dataset domain etc.. we generate buggy code for each code in csn java and csnpython using to serve as outliers ensuring the inliers and outliers are from the same dataset domain with the only difference being normal vs. buggy code.
we focus on variablemisuse bugs as only this mutation algorithm is available for both python and java in .
variable misuses occur when a variable name is used but another was meant in scope and often remain undetected after compilation and regarded as hard to detect by recent bug detection techniques .
comments remain unchanged for this scenario.
c. model configurations for the weakly supervised cood we experiment with either the contrastive learning module cood cl or the binary ood rejection module cood bc to compare against the combined model.
all models are trained using the adam optimizer with a learning rate of 1e and a linear schedule with warmup steps.
the batch size is set to and the number of training epochs is .
for the cood cl and cood the margins in the margin based loss are set to .
.
the balancing value is set to .2after a grid search.
the hidden layer size in the binary ood rejection module for cood is .
we also explore the robustness and agnosticism of our cood approach to different nlpl models by replacing the graphcodebert encoder with codebert unixcoder and contrabert .
d. ood detection model training and measurement for unsupervised cood we use only id data for model training thus involving all training data from csn python and csn java with randomly sampled for validation.
we avoid using the csn development dataset for validation due to its smaller size.
for weakly supervised cood we randomly select of the training data and replaced them with ood samples generated for each scenario following resulting in a total of ood samples and id samples for training.
during inference both cood and cood utilize the same ratio for inliers and outliers from each scenario which is more convincing than using an imbalanced dataset i.e.
tiny number of ood data .
detailed dataset statistics are provided in our online appendix .
since all outliers are randomly selected we report average ood detection results across fiverandom seeds of the test dataset to ensure evaluation reliability and reproducibility.
7following prior work in ml we use two standard metrics to measure the effectiveness of our cood cood models the area under the receiver operating characteristic curve auroc and the false positive rate at fpr95 .
auroc is threshold independent calculating the area under the roc curve over a range of threshold values representing the trade off between true positive rate and false positive rate.
it quantifies the probability that a positive example id sample receives a higher score than a negative one ood sample .
higher auroc indicates better performance.
additionally fpr95 corresponds to the false positive rate fpr when the true positive rate of id samples is .
fpr95 is thresholddependent where oods are identified by setting a threshold withpood pid so that a high fraction of id data is above the threshold.
it measures the proportion of ood samples that are mistakenly classified when of id samples are correctly recalled based on the threshold.
lower fpr95 indicates better performance.
e. baselines we compare our cood cood against various ood detection baselines including adaptations of existing unsupervised nlp ood approaches on nl pl encoders weakly supervised approaches based on outlier exposure and neural bug detection techniques .
since unsupervised approaches rely on classification outputs for ood scoring we reformulate code search as binary classification to fine tune the encoders similarly to .
is supervised for code search but unsupervised for ood detection.
for weakly supervised baselines we use the same number of ood samples as cood for a fair comparison.
note that the encoder backbone of is also graphcodebert.
are specifically designed for neural bug detection thus not requiring other encoder backbone for ood detection.
supervised contrastive learning for classification scl .
this method fine tunes transformer based classification models by maximizing similarity of input pairs if they are from the same class and minimize it otherwise.
following we adopts msp energy and mahalanobis ood scoring algorithms for ood detection.
margin based contrastive learning for classification mcl .
this approach fine tunes transformer based classification models by minimizing the l2 distances between instances from the same class and encouraging the l2 distances between instances of different classes to exceed a margin.
we also detect oods by applying msp energy and mahalanobis ood scoring algorithms.
energy based outlier exposure eoe .
this approach uses a few auxiliary ood data to fine tune the classification model with an energy based margin loss and then utilize energy scores for ood detection.
cubert .
cubert is pre trained on a large code corpus using masked language modeling then fine tuned for bug detection and repair.
we adapt cubert for ood classification by alternatively fine tuning it on our datasetswith comments appended to their corresponding code as cubert only accepts single instance inputs.
2p cubert .
this method enhances cubert s bug detection accuracy with a two phase fine tuning approach.
the first phase utilizes contrastive learning on generated synthetic buggy code .
for the second phase we alternatively fine tune cubert to detect ood using our datasets.
results are reported only for csn python due to the lack of java bug generation algorithms in .
f .
main task performance analysis an effective ood detector serving as an auxiliary component should identify and reject ood samples without negatively impacting the original model s performance on the main downstream task with id data .
consequently we validate the effectiveness of our cood cood auxiliary on the code search task using the official evaluation benchmark by calculating the mean reciprocal rank mrr for each pair of comment code data over distractor codes in the testing code corpus.
specifically we first measure the performance of original graphcodebert code search model on both id and ood data whose performance is expected to be negatively affected with the presence of ood samples.
then we utilize our cood cood auxiliary to filter the testing dataset by setting a threshold to retain of id instances with higher scores following existing ml work and the fpr95 definition as real world deployment typically involves few oods.
finally we directly use the fine tuned encoder in cood cood to perform code search but on the retained id instances and compare this performance with that on the ground truth id instances.
if the performance loss is recovered by using cood cood we actually enhance the trustworthiness and robustness of the original code search model as shown in sec.
vi d .
here trustworthiness and robustness mean that predictions of code models become more reliable when encountering ood data in real world deployment.
note that the dataset used for cood cood training is the same as that used for pl specific training of existing sota code search models.
vi.
e xperimental results a. rq1 unsupervised cood performance in this subsection we analyze the experimental results to assess the detection performance of our unsupervised cood model compared with the unsupervised baselines.
according to table i and ii we can observe that cood outperforms all unsupervised baselines on both csn python and csn java.
notably cood effectively detect out domain andmisaligned ood testing samples while other unsupervised approaches only work for the out domain scenario.
this is because cood effectively captures alignment information within comment code pairs through a multi modal contrastive learning objective with infonce loss and uses similarity scores between comments and code to detect oods.
specifically cood outputs low similarity scores for the out domain data from 8table i effectiveness of our cood and cood models compared with the baselines on the csn python dataset.
approachesout domain id misaligned id shuffled comment id buggy code id overall all oods id auroc fpr95 auroc fpr95 auroc fpr95 auroc fpr95 auroc fpr95 unsupervised scl msp .
.
.
.
.
.
.
.
.
.
scl energy .
.
.
.
.
.
.
.
.
.
scl maha .
.
.
.
.
.
.
.
.
.
mcl msp .
.
.
.
.
.
.
.
.
.
mcl energy .
.
.
.
.
.
.
.
.
.
mcl maha .
.
.
.
.
.
.
.
.
.
cood .
.
.
.
.
.
.
.
.
.
weakly supervised eoe .
.
.
.
.
.
.
.
.
.
cubert .
.
.
.
.
.
.
.
.
.
2p cubert .
.
.
.
.
.
.
.
.
.
cood .
.
.
.
.
.
.
.
.
.
cood cl .
.
.
.
.
.
.
.
.
.
cood bc .
.
.
.
.
.
.
.
.
.
table ii effectiveness of our cood and cood models compared with the baselines on the csn java dataset.
approachesout domain id misaligned id shuffled comment id buggy code id overall all oods id auroc fpr95 auroc fpr95 auroc fpr95 auroc fpr95 auroc fpr95 unsupervised scl msp .
.
.
.
.
.
.
.
.
.
scl energy .
.
.
.
.
.
.
.
.
.
scl maha .
.
.
.
.
.
.
.
.
.
mcl msp .
.
.
.
.
.
.
.
.
.
mcl energy .
.
.
.
.
.
.
.
.
.
mcl maha .
.
.
.
.
.
.
.
.
.
cood .
.
.
.
.
.
.
.
.
.
weakly supervised eoe .
.
.
.
.
.
.
.
.
.
cubert .
.
.
.
.
.
.
.
.
.
cood .
.
.
.
.
.
.
.
.
.
cood cl .
.
.
.
.
.
.
.
.
.
cood bc .
.
.
.
.
.
.
.
.
.
tlcs by additionally considering the knowledge gap difference in comment code pairs between id and out domain data.
also as the misaligned scenario involves misaligned comment code pairs their similarity scores are naturally low.
in contrast the unsupervised baselines aggregate misaligned information into classification logits and rely on the confidence of the aligned class to detect oods.
as previously discussed in sec.
iv c the contrastive losses used by them are not as effective for learning alignment information leading to inferior performance.
additionally detecting token level ood in shuffled comment and buggy code scenarios proves challenging without seeing ood samples during training as all unsupervised methods fail to detect these oods.
b. rq2 weakly supervised cood performance we further investigate the performance of our weaklysupervised cood method against several weakly supervised baselines on csn python and csn java.
table i shows that weak supervision on a tiny amount of ood data enables cood and eoe to not only address unsupervised cood s shortcomings in detecting finer grained shuffled comment and buggy code oods but also enhance performance for the outdomain scenario for csn python.
this improvement aligns with previous research which enhances ood detection by complementing the downstream task objective with an complementary discriminator operating to distinguishids from external oods.
while eoe slightly outperforms cood for the out domain andshuffled comment scenarios by utilizing the prediction probabilities from one classification module our cood which combines the bc and cl modules delivers consistently high performance across all four scenarios resulting in superior overall performance.
in addition the bc module can be directly adapted to the overall cood framework without modifying the underlying learning objective but the outlier exposure based methods e.g.
eoe typically require additional engineering e.g.
determining class probabilistic distributions boundaries for energy scores to equip ml models with ood detection abilities.
besides the bug detection method 2p cubert can reasonably detect oods but its performance for the buggycode scenario is negatively impacted by the limited amount of training ood examples.
on the csn java dataset our cood also achieves the best overall performance compared to all baselines despite trailing slightly behind eoe for out domain and shuffled comment oods.
while eoe has higher auroc score than that of cood for the buggy code scenario it suffers from a high fpr95 indicating a higher margin of error for ood inference using a threshold of id recall.
moreover similar to csnpython cubert fails to detect oods effectively on csnjava either likely due to the lack of training examples.
in summary the superior performance of our cood model results 9table iii our cood model with different encoders.
encoderscsn java csn python auroc fpr95 auroc fpr95 graphcodebert .
.
.
.
codebert .
.
.
.
unixcoder .
.
.
.
contrabert .
.
.
.
from the interplay between the cl and bl modules where contrastive learning captures high level alignment between nl pl input pairs that is naturally suitable for out domain and misaligned oods while the ood rejection classifier targets lower level ood information from shuffled comment andbuggy code samples.
furthermore by utilizing a weaklysupervised contrastive learning objective that jointly optimizes for ood detection and the code search task our method enables effective deployment of the code search model in ood environments which will be further studied in sec.
vi d. c. rq3 weakly supervised cood performance with different model components and encoder backbone in this subsection we evaluate the effect of using only the cl cood cl or the bc module cood bc against the proposed combined cood model to illustrate how cood generalizes in four ood scenarios.
as shown in table i and ii cood cl performs well in the outdomain andmisaligned scenarios which is due to its ability to effectively capture high level comment code alignment information.
cood bc excels in the out domain shuffledcomment and buggy code scenarios since it can learn lowerlevel features from these types of ood samples.
while cood bc maintains acceptable ood detection performance with high auroc and low fpr95 the cl module remains crucial for overall performance since without it the overall performance of cood will drop below the eoe baseline.
moreover removing the bc module has a more negative impact on the ood detection as cood loses the ability to capture the necessary lower level ood information for detecting shuffled comment and buggy code oods.
note that the standalone cl module performs better than the unsupervised cood overall demonstrating that our proposed modification to the original cl objective enhance ood detection by leveraging the margin based loss.
thus the combined model s superior performance validates our design choices.
that is the combined scoring function cosine similarities from cl and the prediction probabilities from bc is thoughtfully designed to leverage the advantage of each module for high detection accuracy.
moreover we compare the detection performance of our cood with various underlying nl pl pre trained encoder.
specifically we compare our choice of graphcodebert against other nl pl encoders from the literature including its predecessor codebert and more recent ones such as unixcoder and contrabert .
as shown in table iii all encoders perform within a difference indicating that our cood framework is robust across different encoders.table iv code search performance under the impact of ood detection.
higher numbers represent better performance dataset testing subset gcb eoe cood cood csn pythonorigin .
.
.
.
outliers .
.
.
.
filtered gt .
.
.
.
filtered ood model .
.
.
csn javaorigin .
.
.
.
outliers .
.
.
.
filtered gt .
.
.
.
filtered ood model .
.
.
this demonstrates our framework s flexibility and effectiveness in detecting oods when deploying various nl pl encoders for code related tasks.
furthermore we investigate key hyperparameters in cood such as mfor margin based contrastive loss and in the overall loss function.
the detailed results are available in our online appendix .
d. rq4 main task performance we present the code search performance under the impact of ood instances by using graphcodebert gcb cood cood and the closest competitor eoe in table iv.
as described in sec.
v f we use the official metric mrr and follow the same testing scheme as the original graphcodebert code search model for evaluation.
from table iv we first observe that our cood cood achieves performance comparable to graphcodebert while the eoe suffers from a significant reduction in performance as it reformulates code search as binary classification to gain ood detection ability.
this reveals a critical trade off between ood detection and downstream task performance.
to further validate the importance of ood detection for code search we construct outliers based on the csn java and python testing dataset respectively.
given that code search aims to retrieve the most aligned code from a code corpus given an nl query the outliers are only sampled from three ood scenarios outdomain shuffled comment and buggy code each replacing id data of the original testing set.
we then show the results when the dataset contains ood samples i.e.
outliers discard ood samples by filtering the testing set by ground truth labels i.e.
filtered gt or using various ood detection models i.e.
filtered ood model .
note that the filtered gt dataset is the original csn s subset with of id samples removed.
according to table iv the performance of the original graphcodebert code search model drops by .
and .
.
.
.
mrr when outliers are present in csn python and java respectively.
as a solution to this issue our cood cood detector recover the performance losses by identifying and filtering out the ood samples without negatively impacting the model s code understanding ability in code search.
specifically the code search performance of cood cood on the filtered cood cood dataset .
.
and .
.
on csn python and java respectively is comparable to or even better than graphcodebert on the filtered gt dataset .
and .
10on csn python and java respectively .
this slight improvement is probably because our detectors filter out additional lower quality testing samples that resemble outliers.
thus our cood cood enhance the trustworthiness and robustness of the graphcodebert since the model s predictions become more reliable when encountering ood data.
note that the original graphcodebert is not equipped with the ood detection ability so its corresponding cells for the filteredood model in table iv are left blank.
vii.
d iscussions analysis of the overconfidence of msp with conformal prediction.
dnn models pre trained on id data are prone to misclassify ood samples as id due to overconfident msp scores .
this issue arises from spurious correlations between ood and id features such as entities or syntactic patterns in nl and pl data .
for example ood inputs with common id syntactic patterns may receive high id scores.
to overcome overconfident predictions previous work in ml explored techniques like temperature scaling and confidence calibration using adversarial samples .
in contrast cood leverages weakly supervised contrastive learning with a small set of ood samples to prevent the alignment of ood pairs and adopts a binary ood rejection module to better differentiate ood and id representations.
we further verify cood s ability to address overconfidence using conformal prediction cp .
post hoc cp converts ood scores into prediction sets with a high user specified coverage e.g.
correcting overconfident thresholds during calibration.
this ensures prediction sets conform statistically to the desired coverage.
we experimentally demonstrated that cood achieves near optimal prediction set sizes with coverage effectively identifying true oods with statistical guarantees.
in contrast mcl msp the best msp based method still struggles with overconfident oods.
therefore cood can effectively overcome the overconfidence issue of msp.
full details of the experiments and result analysis are in the online appendix .
ood detection with large language models llms .
it s worth noting that transformer based code models e.g.
graphcodebert and llms share the same underlying transformer architecture.
scaling up transformer based code models and training them on vast amounts of code data allows llms to perform a wide range of coderelated tasks making coding less labor intensive and more accessible to end users.
since llms are transformer based they are also vulnerable to ood data with potentially worse performance degradation due to error accumulation during auto regressive inference.
thus identifying ood samples is crucial to knowing when to trust llm outputs.
our proposed ood code framework techniques can be applied to these larger transformer based code models similarly as demonstrated in our experiments with different encoders in table iii.
generalization of cood cood to other coderelated tasks.
during software development developers often write comments following code snippets methods functions .therefore from a realistic perspective our framework is generalizable to many code understanding tasks beyond code search such as clone detection and defect detection that use comment code pairs as input.
all that is needed is identifying the id dataset and out domain data as the four ood scenarios outlined in this paper are relevant to most tasks.
for instance in clone detection comment code pairs could first be processed by our framework to identify ood samples before checking for clones.
unfortunately since existing clone and defect detection datasets typically lack comments we haven t applied our framework to these tasks.
however the framework has strong potential to enhance these tasks as more realistic bi modal datasets become available in the future.
viii.
t hreats to validity l imitations construct validity our cood cood framework uses data driven techniques to synthesize ood samples which may not fully reflect real world se scenarios.
while we include diverse ood scenarios a pilot study with developers is necessary.
additionally the reliability of our ood benchmark depends heavily on the quality of the ood datasets used.
internal validity hyperparameter tuning impacts ml performance.
for model fine tuning we kept the graphcodebert architecture unchanged due to feasibility reasons but conducted ablation studies with various model components encoder backbones and key hyperparameters.
external validity we conduct ood detection experiments on two large scale code search datasets.
although our focus on python and java limits generalizability experiments on these two languages partially demonstrate that our approach is pl agnostic.
ix.
c onclusion we proposed two multi modal ood detection aproaches for code related pre trained ml models namely unsupervised cood and weakly supervised cood .
the cood merely leveraged unsupervised contrastive learning to identify ood samples.
as an extension of cood cood combined contrastive learning and a binary classifier for ood detection using a small number of labelled ood samples.
to reap the benefits of these two modules we also devised a novel scoring metric to fuse their prediction results.
the evaluation results demonstrated that the integration of the rejection network and contrastive learning can achieve superior performance in detecting all four ood scenarios for multi modal nl pl data.
additionally our models can be applied to the downstream se task achieving comparable performance to existing coderelated models.
x. a cknowledgement this research has been supported in part by the nsf ccf cns ccf ccf and cns .
we also gratefully acknowledge support from cisco systems.
the opinions findings and conclusions expressed in this work are solely those of the authors and do not necessarily reflect the views of the sponsors.
11references a. torralba and a. a. efros unbiased look at dataset bias in cvpr .
ieee pp.
.
x. liu c. yoo f. xing h. oh g. el fakhri j. w. kang j. woo et al.
deep unsupervised domain adaptation a review of recent advances and perspectives apsipa trans.
signal inf.
proc.
vol.
no.
.
j. yang k. zhou y .
li and z. liu generalized out of distribution detection a survey ijcv .
x. li m. liu s. gao and w. buntine a survey on out of distribution evaluation of neural nlp models in ijcai pp.
.
y .
ming z. cai j. gu y .
sun w. li and y .
li delving into out ofdistribution detection with vision language representations neurips vol.
pp.
.
d. hendrycks m. mazeika and t. dietterich deep anomaly detection with outlier exposure arxiv .
.
k. t. mai t. davies and l. d. griffin self supervised losses for one class textual anomaly detection arxiv .
.
w. zhou f. liu and m. chen contrastive out of distribution detection for pretrained transformers in emnlp pp.
.
y .
tian g. maicas l. z. c. t. pu r. singh j. w. verjans and g. carneiro few shot anomaly detection for polyp frames from colonoscopy in miccai .
springer pp.
.
s. majhi s. das f. br emond r. dash and p. k. sa weaklysupervised joint anomaly detection and classification in fg.
ieee pp.
.
j. kim s. t. kong d. na and k. h. jung key feature replacement of in distribution samples for out of distribution detection in aaai vol.
no.
pp.
.
j. kim k. jung d. na s. jang e. park and s. choi pseudo outlier exposure for out of distribution detection using pretrained transformers in acl pp.
.
j. yoo t. zhao and l. akoglu data augmentation is a hyperparameter cherry picked self supervision for unsupervised anomaly detection is creating the illusion of success tmlr .
v .
h. le and h. zhang log based anomaly detection with deep learning how far are we?
in icse pp.
.
x. wang j. song x. zhang j. tang w. gao and q. lin logonline a semi supervised log based anomaly detector aided with online learning mechanism in ase.
ieee pp.
.
x. guo x. peng h. wang w. li h. jiang d. ding t. xie and l. su graph based trace analysis for microservice architecture understanding and problem diagnosis in esec fse p. .
d. liu c. he x. peng f. lin c. zhang s. gong z. li j. ou and z. wu microhecl high efficient root cause localization in large scale microservice systems in icse seip pp.
.
a. sejfia s. das s. shafiq and n. medvidovi c toward improved deep learning based vulnerability detection in icse pp.
.
b. steenhoek h. gao and w. le dataflow analysis inspired deep learning for efficient vulnerability detection in icse pp.
.
s. cao x. sun x. wu d. lo l. bo b. li and w. liu coca improving and explaining graph neural network based vulnerability detection systems in icse pp.
.
m. allamanis h. jackson flux and m. brockschmidt selfsupervised bug detection and repair neurips vol.
pp.
.
j. he l. beurer kellner and m. vechev on distribution shift in learning based bug detectors in icml .
pmlr pp.
.
a. vaswani n. shazeer n. parmar j. uszkoreit l. jones a. n. gomez .
kaiser and i. polosukhin attention is all you need neurips vol.
.
d. guo s. ren s. lu z. feng d. tang s. liu l. zhou n. duan a. svyatkovskiy s. fu et al.
graphcodebert pre training code representations with data flow in iclr .
d. guo s. lu n. duan y .
wang m. zhou and j. yin unixcoder unified cross modal pre training for code representation arxiv .
.
.
available unifiedcross modalpre trainingforcoderepresentation y .
yan n. cooper k. moran g. bavota d. poshyvanyk and s. rich enhancing code understanding for impact analysis by combining transformers and program dependence graphs proc.
acm softw.
eng.
no.
fse .
d. hendrycks x. liu e. wallace a. dziedzic r. krishnan and d. song pretrained transformers improve out of distribution robustness arxiv .
.
s. liu b. wu x. xie g. meng and y .
liu contrabert enhancing code pre trained models via contrastive learning in icse p. .
s. esmaeilpour b. liu e. robertson and l. shu zero shot out ofdistribution detection based on the pretrained model clip in aaai .
h. husain h. h. wu t. gazit m. allamanis and m. brockschmidt codesearchnet challenge evaluating the state of semantic code search arxiv .
.
a. v. d. oord y .
li and o. vinyals representation learning with contrastive predictive coding arxiv .
.
w. liu x. wang j. owens and y .
li energy based out ofdistribution detection neurips vol.
pp.
.
y .
yan v .
duong h. shao and d. poshyvanyk cood online appendix .
.
available cood b. yu j. yao q. fu z. zhong h. xie y .
wu y .
ma and p. he deep learning or classical machine learning?
an empirical study on log based anomaly detection in icse pp.
.
c. zhang t. jia g. shen p. zhu and y .
li metalog generalizable cross system anomaly detection from logs with meta learning in icse .
ieee computer society pp.
.
g. zerveas s. jayaraman d. patel a. bhamidipaty and c. eickhoff a transformer based framework for multivariate time series representation learning in kdd p. .
x. zhang y .
xu q. lin b. qiao h. zhang y .
dang c. xie x. yang q. cheng z. li et al.
robust log based anomaly detection on unstable log data in esec fse pp.
.
s. lu x. wei y .
li and l. wang detecting anomaly in big data system logs using convolutional neural network in dasc picom datacom cyberscitech .
ieee pp.
.
a. farzad and t. a. gulliver unsupervised log message anomaly detection ict express vol.
no.
pp.
.
l. yang j. chen z. wang w. wang j. jiang x. dong and w. zhang semi supervised log based anomaly detection via probabilistic label estimation in icse .
ieee pp.
.
c. lee t. yang z. chen y .
su y .
yang and m. r. lyu heterogeneous anomaly detection for software systems via semi supervised cross modal attention in icse p. .
s. lu d. guo s. ren j. huang a. svyatkovskiy a. blanco c. clement d. drain d. jiang d. tang et al.
codexglue a machine learning benchmark dataset for code understanding and generation arxiv .
.
codeql .
.
available checkmarx .
.
available z. liu z. tang j. zhang x. xia and x. yang pre training by predicting program dependencies for vulnerability analysis tasks in icse pp.
.
a. kanade p. maniatis g. balakrishnan and k. shi learning and evaluating contextual embedding of source code in icml .
pmlr pp.
.
z. chen v .
j. hellendoorn p. lamblin p. maniatis p. a. manzagol d. tarlow and s. moitra plur a unifying graph based view of program learning understanding and repair neurips vol.
pp.
.
q. hu y .
guo x. xie m. cordy m. papadakis l. ma and y .
le traon codes towards code model generalization under distribution shift in icse nier .
ieee pp.
.
m. weyssow x. zhou k. kim d. lo and h. sahraoui on the usage of continual learning for out of distribution generalization in pretrained language models of code arxiv .
.
h. hajipour n. yu c. a. staicu and m. fritz simscood systematic analysis of out of distribution generalization in fine tuned source code models in naacl pp.
.
d. hendrycks and k. gimpel a baseline for detecting misclassified and out of distribution examples in neural networks in iclr .
m. salehi h. mirzaei d. hendrycks y .
li m. h. rohban and m. sabokrou a unified survey on anomaly novelty open set and out of distribution detection solutions and future challenges tmlr .
m. hein m. andriushchenko and j. bitterwolf why relu networks yield high confidence predictions far away from the training data and how to mitigate the problem in cvpr pp.
.
y .
sun y .
ming x. zhu and y .
li out of distribution detection with deep nearest neighbors in icml .
y .
hu and l. khan uncertainty aware reliable text classification in kdd p. .
p. liznerski l. ruff r. a. vandermeulen b. j. franks k. r. m uller and m. kloft exposing outlier exposure what can be learned from few one and zero outlier images arxiv .
.
a. kamath r. jia and p. liang selective question answering under domain shift in acl pp.
.
n. varshney s. mishra and c. baral investigating selective prediction approaches across several tasks in iid ood and adversarial settings in acl pp.
.
j. xin r. tang y .
yu and j. lin the art of abstention selective prediction and error regularization for natural language processing in acl ijcnlp pp.
.
z. zeng h. xu k. he y .
yan s. liu z. liu and w. xu adversarial generative distance based classifier for robust out of domain detection inicassp .
ieee pp.
.
y .
zheng g. chen and m. huang out of domain detection for natural language understanding in dialog systems ieee acm transactions on audio speech and language processing vol.
pp.
.
l. m. zhan h. liang b. liu l. fan x. m. wu and a. y .
lam out of scope intent detection with self supervision and discriminative training in acl pp.
.
d. jin s. gao s. kim y .
liu and d. hakkani t ur towards textual out of domain detection without in domain labels ieee acm trans.
audio speech and lang.
proc.
vol.
p. .
k. xu t. ren s. zhang y .
feng and c. xiong unsupervised outof domain detection via pre trained transformers in acl pp.
.
l. sun k. yang x. hu w. hu and k. wang real time fusion network for rgb d semantic segmentation incorporating unexpected obstacle detection for road driving images ieee robot.
autom.
lett.
vol.
no.
pp.
.
l. wang s. giebenhain c. anklam and b. goldluecke radar ghost target detection via multimodal transformers ieee robot.
autom.
lett.
vol.
no.
pp.
.
s. fort j. ren and b. lakshminarayanan exploring the limits of out of distribution detection neurips vol.
pp.
.
z. feng d. guo d. tang n. duan x. feng m. gong l. shou b. qin t. liu d. jiang et al.
codebert a pre trained model for programming and natural languages arxiv .
.
r. hadsell s. chopra and y .
lecun dimensionality reduction by learning an invariant mapping in cvpr vol.
pp.
.
p. khosla p. teterwak c. wang a. sarna y .
tian p. isola a. maschinot c. liu and d. krishnan supervised contrastive learning neurips vol.
pp.
.
t. chen s. kornblith m. norouzi and g. hinton a simple framework for contrastive learning of visual representations in icml .
pmlr pp.
.
p. jain a. jain t. zhang p. abbeel j. e. gonzalez and i. stoica contrastive code representation learning arxiv .
.
a. radford j. w. kim c. hallacy a. ramesh g. goh s. agarwal g. sastry a. askell p. mishkin j. clark et al.
learning transferable visual models from natural language supervision in icml .
pmlr pp.
.
y .
qiu t. misu and c. busso unsupervised scalable multimodal driving anomaly detection ieee tiv .
y .
c. hsu y .
shen h. jin and z. kira generalized odin detecting out of distribution image without learning from out of distribution data in cvpr pp.
.
l. ruff r. a. vandermeulen n. g ornitz a. binder e. m uller k. r. m uller and m. kloft deep semi supervised anomaly detection in iclr .
e. shi y .
wang w. gu l. du h. zhang s. han d. zhang and h. sun cocosoda effective contrastive learning for code search in icse pp.
.
d. hendrycks and k. gimpel a baseline for detecting misclassified and out of distribution examples in neural networks arxiv .
.
v .
duong q. wu z. zhou e. zavesky j. chen x. liu w. l. hsu and h. shao general purpose multi modal ood detection framework tmlr .
h. xue q. yang and s. chen svm support vector machines in the top ten algorithms in data mining .
chapman and hall crc pp.
.
p. salza c. schwizer j. gu and h. c. gall on the effectiveness of transfer learning for code search tse .
d. yang p. martins v .
saini and c. lopes stack overflow in github any snippets there?
in msr pp.
.
a. lotter s. a. licorish b. t. r. savarimuthu and s. meldrum code reuse in stack overflow and popular open source java projects inaswec pp.
.
r. abdalkareem e. shihab and j. rilling on code reuse from stackoverflow an exploratory study on android apps inf.
softw.
technol.
vol.
pp.
.
j. huang d. tang l. shou m. gong k. xu d. jiang m. zhou and n. duan cosqa web queries for code search and question answering arxiv .
.
a. podolskiy d. lipin a. bout e. artemova and i. piontkovskaya revisiting mahalanobis distance for transformer based out of domain detection in aaai vol.
no.
pp.
.
c. wang z. nong c. gao z. li j. zeng z. xing and y .
liu enriching query semantics for code search with reinforcement learning neural netw.
vol.
pp.
.
z. yao d. s. weld w. p. chen and h. sun staqc a systematically mined question code dataset from stack overflow in www pp.
.
r. li g. hu and m. peng hierarchical embedding for code search in software q a sites in ijcnn .
ieee pp.
.
n. rao c. bansal and j. guan search4code code search intent classification using weak supervision in msr pp.
.
k. sinha r. jia d. hupkes j. pineau a. williams and d. kiela masked language modeling and the distributional hypothesis order word matters pre training for little in emnlp pp.
.
k. sinha p. parthasarathi j. pineau and a. williams unnatural language inference arxiv .
.
c. richter and h. wehrheim learning realistic mutations bug creation for neural bug detectors in icst pp.
.
r. just d. jalali and m. d. ernst defects4j a database of existing faults to enable controlled testing studies for java programs in issta pp.
.
y .
zhou s. liu j. siow x. du and y .
liu devign effective vulnerability identification by learning comprehensive program semantics via graph neural networks neurips vol.
.
r. widyasari s. q. sim c. lok h. qi j. phan q. tay c. tan f. wee j. e. tan y .
yieh et al.
bugsinpy a database of existing bugs in python programs to enable controlled testing and debugging studies in esec fse pp.
.
c. richter and h. wehrheim how to train your neural bug detector artificial vs real bugs in ase.
ieee computer society pp.
.
d. hendrycks m. mazeika s. kadavath and d. song using selfsupervised learning can improve model robustness and uncertainty neurips vol.
.
k. lee h. lee k. lee and j. shin training confidence calibrated classifiers for detecting out of distribution samples arxiv .
.
s. liang y .
li and r. srikant enhancing the reliability of out ofdistribution image detection in neural networks arxiv .
.
y .
wu k. he y .
yan q. gao z. zeng f. zheng l. zhao h. jiang w. wu and w. xu revisit overconfidence for ood detection reassigned contrastive learning with adaptive class dependent threshold innaacl hlt pp.
.
j. bitterwolf a. meinke and m. hein certifiably adversarially robust detection of out of distribution data neurips vol.
pp.
.
a. n. angelopoulos s. bates et al.
conformal prediction a gentle introduction found.
trends mach.
learn.
vol.
no.
pp.
.
b. roziere j. gehring f. gloeckle s. sootla i. gat x. e. tan y .
adi j. liu t. remez j. rapin et al.
code llama open foundation models for code arxiv .
.