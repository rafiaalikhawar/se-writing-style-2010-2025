on the naturalness of hardware descriptions jaeseong lee ut austin usa jason.lee27 utexas.edupengyu nie ut austin usa pynie utexas.edujunyi jessy li ut austin usa jessy austin.utexas.edumilos gligoric ut austin usa gligoric utexas.edu abstract mining software repositories msr has been shown effective for extracting data used to improve various software engineering tasks including code completion code repair code search and code summarization.
despite a large body of work on msr researchers have focused almost exclusively on repositories that contain code written in imperative programming languages such as java and c c .
unlike prior work in this paper we focus on mining publicly available hardware descriptions hds written in hardware description languages hdls such as vhdl.
hdls have unique syntax and semantics compared to popular imperative languages and learningbased tools available to hardware designers are well behind those used in other application domains.
we assembled large hd corpora consisting of source code written in several hdls and report on their characteristics.
our language model evaluation reveals that hds possess a high level of naturalness similar to software written in imperative languages.
further by utilizing our corpora we built several deep learning models for automated code completion in vhdl our models take into account unique characteristics of hdls including similarities of nearby concurrent signal assignment statements in built concurrency and the frequently used signal types.
these characteristics led to more effective neural models achieving a bleu score of .
an point improvement over rule based and neural baselines.
ccs concepts hardware hardware description languages and compilation software and its engineering software maintenance tools .
keywords naturalness hardware description languages vhdl verilog systemverilog natural language processing code completion acm reference format jaeseong lee pengyu nie junyi jessy li and milos gligoric.
.
on the naturalness of hardware descriptions.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
.
the first two authors led this work with equal contributions.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
introduction mining software repositories msr has been an active area of research for over a decade.
researchers and practitioners have been mining various artifacts available in or associated with repositories e.g.
code license files images natural language elements pull requests open issues and have shown that msr can be effective for extracting data that is valuable for numerous software engineering tasks e.g.
.
recently the extracted data from software repositories was used to show that source code is natural i.e.
it is repetitive and predictable.
the pioneering paper on the topic found that the level of naturalness in java code is even higher than that of english and built a simple code completion tool based on an n gram language model.
a follow up work revisited the original results and explored naturalness of code written in several programming languages including c c java python ruby and scala.
the naturalness of source code has led to the advancement of powerful statistical models useful in a wide range of tasks including code completion code repair code search and code summarization .
however despite some great progress on msr in general and the naturalness of software in particular researchers have focused almost exclusively on general purpose imperative programming languages e.g.
java and c c .
in this paper we present the first work on mining hardware descriptions hds 1written in one of the three popular hardware description languages hdls vhdl verilog systemverilog.
hdls have unique syntax and semantics compared to popular imperative languages e.g.
they are data flow programming languages where many statements are executed simultaneously.
moreover hds have unique properties e.g.
most of the signals are logic bits or vectors of logic bits and the similarity of nearby statements is high.
we hope that our work will inspire more research on improving engineering practices and tools used by hardware designers a different target audience than developers using traditional imperative programming languages.
this improvement in development practices and tools is very much needed the state of the art in hardware description is well behind practices and tools used by software engineers.
for example even the most advanced integrated development environment ide sigasi is well behind ides for java in terms of adopting learning based code completion and code checking techniques.
thus hardware designers need help right now to speed up their development enforce coding conventions and ensure correctness of their hds.
we make several contributions to start off the research in this direction.
first we mine publicly available repositories on github to extract the most popular in terms of the number of stars repositories that contain hds for vhdl verilog and systemverilog.
we then 1hardware descriptions software written in hardware description languages they are also frequently referred to as hardware designs or hardware design models.esec fse november virtual event usa jaeseong lee pengyu nie junyi jessy li and milos gligoric analyze these repositories to extract various metrics of the code that characterize the languages.
the creation of this large corpus of hdls stands to motivate other researchers to study and improve available hds.
second we conduct a comparative evaluation of the naturalness of hds written in aforementioned languages against software written in imperative languages by building language models over the mined corpora and report standard cross entropy measures on the statistical regularity of hds.
our results show that hds have a high level of naturalness similar to software written in imperative languages like java which should motivate researchers and practitioners to develop code automation techniques based on deep learning for hdls.
finally we develop the first learning based models for hdls utilizing the extracted data.
we design several deep learning models for automatically completing the right hand side of a concurrent signal assignment statement concurrent assignment for short in vhdl concurrent assignments are among the most common components for data flow and structural descriptions in hds.
our models are designed to exploit unique properties of hds and hdls including similarities of nearby concurrent assignments in build concurrency and a relatively small number of frequently used signal types.
we reveal that these properties contribute to the performance of our most advanced model measured in terms of accuracy and bleu which are metrics commonly used in the related literature .
in summary we make the following key contributions we release a large corpus for learning over hds mined from github.
this corpus can be used by researchers practitioners and tool developers to improve engineering practices in this important domain.
we perform the first study of the naturalness of hds written in three popular languages vhdl verilog systemverilog.
by training and testing statistical language models we find that hds written in vhdl are more natural than software written in java and hds written in other hdls also have a high level of natural regularities in code.
we design and implement deep learning models for predicting the right hand side of concurrent assignments in vhdl.
we extensively evaluate our models using popular repositories.
our best model achieves .
accuracy and .
bleu which outperforms rule based and neural baselines by bleu.
our code and data is available on github engineeringsoftware hdlp.
background this section provides a brief background on hardware description languages hdls and introduces several vhdl constructs via an example.
we focus on vhdl due to our familiarity with the language.
because it is not feasible to describe all aspects of vhdl we focus on the constructs required in later sections.
hardware designers use hdls to write hardware descriptions hds that describe logic circuits on the register transfer level rtl rtl is a design abstraction of the flow of digital signals between hardware registers.
an hd is commonly tested and debugged in an event driven simulation program.
once the hd is1 entity fpga64 sid iec is port ... clk32 instd logic uart txd out std logic uart rts out std logic uart dtr out std logic uart ri out out std logic uart dcd out out std logic end fpga64 sid iec architecture rtl offpga64 sid iec is... signal cia2 pao unsigned downto signal cia2 pbo unsigned downto signal vicaddr unsigned downto ... begin setting local signals etc.
not shown due to space constraints process clk32 begin ifrising edge clk32 then ifsyscycle syscycledef high then syscycle syscycledef low elsif syscycle cycle cpu6 then syscycle cycle cpu8 else syscycle syscycledef succ syscycle end if end if end process iec data o cia2 pao iec clk o cia2 pao iec atn o cia2 pao uart txd cia2 pao uart rts cia2 pbo uart dtr cia2 pbo uart ri out cia2 pbo uart dcd out cia2 pbo vicaddr not cia2 pao vicaddr not cia2 pao end architecture concurrent assignments figure an example vhdl code snippet from misterdevel c64 mister repository which is a part of our corpus.
completed it is processed by a synthesis program and translated onto a programmable logic device e.g.
fpga.
we show an example of an hd written in vhdl in figure .
syntax of vhdl is the closest to the ada programming language.
we extracted this example from mister devel c64 mister repository which aims to recreate commodore using modern hardware .
this project is publicly available on github sha 0efb9e1b and is part of our corpus that is used in later sections.
in vhdl each hd consists of one or more modules only one module is shown in figure .
for each module there is code that describes its interface i.e.
what comes into the module and comes out of the module and behavior i.e.
how values on the output depend on the inputs .
aninterface starts with the keyword entity line followed by the name of the module fpga64 sid iec the interface ends with the keyword endfollowed by that module name line .
an interface defines ports lines available on the module.
each port has a name e.g.
uart txd the direction that information is allowed to flow through the port in out orinout and the type of the port e.g.
std logic that defines the set of values that can flow through the port.
abehavior starts with the keyword architecture line followed by the name for the architecture rtl the keyword of and then the name of the module for which the behavior is specified a sequence of keywords end architecture line closes theon the naturalness of hardware descriptions esec fse november virtual event usa behavior section.
at the beginning of each architecture lines a designer can define an arbitrary number of local signals that can be helpful when defining the behavior each signal definition starts with keyword signal followed by the signal s name e.g.
cia2 pao and type e.g.
unsigned downto which is an unsigned bit value .
between begin andend architecture lines there can be an arbitrary number of concurrent processes.
in our example there is one explicit process lines .
this process is executed whenever the input clk32 signal changes.
the statements within the process lines are executed sequentially.
the process in our example is followed by several concurrent assignments lines .
each concurrent assignment is an implicit process i.e.
w o the process keyword with a single statement.
all processes execute in parallel .
each concurrent assignment is executed whenever any of the signals on its right hand side is changed.
for example on line whenever cia2 paois changed the concurrent assignment is executed and a new value foriec data ois computed.
vhdl is strongly typed.
unlike vhdl verilog is a weakly typed language and its syntax is inspired by c and fortran.
however the base structure and language constructs available in verilog are similar to those in vhdl.
it is commonly accepted that vhdl is more verbose than verilog.
systemverilog was initially introduced as an extension of verilog with object oriented features and the goal to improve verification of hds the most common usage of systemverilog is to write code for verifying hds written in vhdl and verilog.
we do not show code snippets for verilog and systemverilog as that would take too much space.
in section we mine github to find and characterize available hds written in vhdl verilog and systemverilog.
we then in section study the naturalness of hds written in hdls which have substantially different syntax and semantics than imperative programming languages studied in the past.
finally in section we develop deep learning models to predict the right hand side of concurrent assignments in vhdl e.g.
on line in figure we want to predict cia2 pao .
we focus on vhdl but our models are generalizable to other hdls.
our learning based models are designed based on our intuition that nearby assignment statements are similar and provide important local context types of ports and local signals provide important global context and the order of concurrent assignments based on the parallel execution semantics is irrelevant.
we will show that these observations contribute to the performance of our models.
hardware description corpora in this section we describe the procedure that we followed to assemble the corpora of hds these hds are used in later sections to study naturalness section and train evaluate models for completing the right hand side of concurrent assignments section .
moreover we provide statistics for several features unique to hdls in the assembled corpora which could motivate other research directions.
in addition to the hd corpora we obtained two java corpora in order to compare the naturalness of hds with that of java code we followed prior work on software naturalness to obtain the java corpora.
we obtained all repositories from github which is the most popular repository hosting service.
we downloaded all the repositories on oct .hd corpora .
first we created a corpus for each hdl using the top repositories from github excluding forked repositories and ranked by the number of stars we used the number of stars as a proxy for projects popularity following recent literature .
the number of stars for the vhdl repositories in our corpus is on average .
and ranges from to the number of stars for the verilog repositories in our corpus is on average .
and ranges from to the number of stars for the systemverilog repositories in our corpus is on average .
and ranges from to .
second we processed all the files in all the repositories to find the set of files that can be parsed by the open source parsers generated using the antlr parser generator .
we observed that some files that have a correct file extension e.g.
.vhd are not parsable because they do not actually contain valid code but rather binary data e.g.
rsa keys and data blocks for example the file blk mem gen v8 4 vhsyn rfs.vhd from xilinx pynq dl project or they use latest language standards not supported by available tools.
we also filtered out duplicate files.
duplicate files are present in repositories with imperative languages too and they can introduce noise in experimental results .
java corpora .
in order to compare our findings with prior work that studied java repositories we use two java corpora java naturalness which contains exactly the same repositories and revisions as hindle et al.
s study and java popular which contains the most popular repositories at the time of our experiments.
we used only repositories in the java popular corpus in order to match the number of repositories used in the java naturalness corpus and to remain close to the number of tokens in the hdls corpora.
we used the same procedure as for hdls to clean duplicate files from the java corpora we used eclipse jdt core version .
.
to process java files.
statistics .
table shows the statistics of the hd and java corpora.
the first column shows the corpus name and the second column shows the number of repositories in each corpus.
columns show the number of parsable files the percentage of duplicate files and the number of unique files.
columns show the statistics of the unique files of each corpus including lines of code loc total number of tokens and vocabulary size i.e.
number of unique tokens .
we also use box plots to show the distribution of lines of code for the files in each corpus in figure 2a the distribution of number of tokens in figure 2b and the distribution of vocabulary size in figure 2c.
from the table and figures we can observe that hds written in vhdl are more verbose than verilog as commonly accepted in the community although the difference is not statistically significant2.
also we can see that code per file written in systemverilog is somewhat shorter though not significantly than code written in vhdl and verilog because systemverilog is commonly used for writing test benches to verify hds.
finally we can observe that the number of total and unique tokens taking into account the number of repositories is substantially higher in java repositories than in hdl repositories and the number of tokens in systemverilog is significantly smaller than that of vhdl and verilog.
2under significance level p .05using the bootstrap method by berg kirkpatrick et al.
.
the same method applies to other significance tests in this paper.esec fse november virtual event usa jaeseong lee pengyu nie junyi jessy li and milos gligoric table statistics of our hd corpora and java corpora number of repositories number of parsable files percentage of duplicate files number of unique files lines of code loc number of tokens vocabulary size.
corpus reposfilesloc tokens vocab.
size parsable duplicate unique vhdl .
verilog .
systemverilog .
java popular .
java naturalness .
vhdlverilog systemverilog java popular java naturalness 050100150200250300350loc a loc vhdlverilog systemverilog java popular java naturalness tokens b number of all tokens vhdlverilog systemverilog java popular java naturalness unique tokens c number of unique tokens figure lines of code loc per file and number of tokens per file distributions in our hd corpora and java corpora.
table statistics of hdl code elements in our corpora number of entities modules number of functions number of processes always blocks number of input ports number of output ports and number of inout ports.
avg columns are the numbers per file and sum columns are the total number for all files from all repositories.
corpus entity module function process always input port output port inout port avg sum avg sum avg sum avg sum avg sum avg sum vhdl .
.
.
.
.
.
verilog .
.
.
.
.
.
systemverilog .
.
.
.
.
.
table shows the statistics of the numbers of several hdl code elements in our corpora.
the first column shows the corpus name.
columns show the number of entities in vhdl or modules in verilog and systemverilog per file avg and total in each corpus sum .
note that number of modules per file for systemverilog could be 1because some files may consist of only program blocks but not modules.
columns show the number of functions.
columns show the number of processes in vhdl or always blocks in verilog and systemverilog .
finally columns show the number of input ports output ports and inout ports.
although we do not report the details about each of these code elements we find that a large number of them is available in popular open source repositories which can be exploited by various learning based techniques to build better software tools for hdls.
naturalness of hds this section presents an assessment of the naturalness of hds in comparison with software written in java using our corpora.
to make sure the results are compatible with existing literature we follow prior work on the naturalness of software and use n gram language models for this analysis.
we reveal that hdls have comparable levels of naturalness to java software.
.
methodology language modeling and naturalness .
at a high level language models are generative models that capture statistical regularities in terms of style and vocabulary in a corpus.
their learned parameters in turn can be used to determine whether a document dfits in the corpus by estimating the probability p d of generatingdunderon the naturalness of hardware descriptions esec fse november virtual event usa n23456789cross entropylanguage vhdl verilog systemverilog java popular java naturalness a repository level n23456789cross entropylanguage vhdl verilog systemverilog java popular java naturalness b language level figure line plots of the cross entropy measurements on our hd and java corpora.
a trained model.
following we use the document s cross entropy h d as a fitness measure also called selfcrossentropy in musfiqur et al.
cross entropy is the power term of perplexity which is the standard measure for language model evaluation h d len d log2p d ppl d 2h d where len d is the number of tokens in d. these are informationtheoretic measures that capture the length normalized inverse probability ofdin the corpus thus lower values entail more fit note thatdshould be a test document that does not appear in the training set of the language model.
as cross entropy and perplexity are monotonically correlated by definition we report only cross entropy as in prior work .
the concept of naturalness captures predictability and repetitiveness and is an aggregated version of cross entropy over the whole corpus.
namely we iteratively partition the corpus into training and testing sets the lower the cross entropy over all partitions the more natural the corpus is.
to obtain the aggregated measures we perform fold validation through the following steps reimplementing hindle et al.
randomly partition the corpus into equally sized folds this is done by first shuffling the files in the corpus then splitting into folds with equal number of lines of code.
train a language model on folds and apply it on the remaining fold repeat this step for each fold.
report the average cross entropy as a measurement of naturalness for the corpus.
n gram language model .
the probability of document dis modeled by the joint probability of its token sequence p d p wm m i 1p wi wi in an n gram language model the markov assumption is applied for the conditional probabilities for a given n p wi wi p wi wi i n therefore the model directly counts the frequencies of token sequences of lengths nandn .
since our goal in this section is a comparative analysis of naturalness rather than building the best language model using n grams here makes sure that our results are comparable to prior work in addition compared to neural language models n gram models are transparent and straightforward in their probability estimates.
this is particularly appealing given a large vocabulary and it is also easy to vary the length of the sequence n an analysis that led to important conclusions of source code naturalness which we also show below.
we used software language processing library slp to build the n gram language model.
the library had been used for building language models for imperative languages such as java and we modified it to accept hdls.
to handle zero probabilities of n grams we used jelinek mercer smoothing as hellendoorn and devanbu found it to be the most appropriate for source code.
repository level and language level naturalness .
hindle et al.
measured naturalness on the repository level they considered each java repository as a mini corpus computed naturalness measures on each repository and reported the average among all repositories as the naturalness for java software.
in contrast musfiqur et al.
reported language level naturalness they considered all repositories of one programming language as a single corpus.
our work reports naturalness on both levels to gain insights into regularities in each language as a whole while accounting for variabilities across different repositories written in the same hdl.
.
analysis figure shows the line plots of cross entropy for our hd corpora and java corpora with 3a and 3b showing the repository and language level measures respectively.
in each line plot the x axis is the value of nfor the n gram language model and the y axis is the cross entropy and each line corresponds to one corpus.
for all the corpora the cross entropy monotonically drops as nincreases which indicates that a higher order n gram language model larger n is better at capturing statistical regularity in bothesec fse november virtual event usa jaeseong lee pengyu nie junyi jessy li and milos gligoric hdls and java.
the decline of cross entropy saturates around 4grams for hds a similar finding as in hindle et al.
for java software.
at the repository level code written in all hdls show lower cross entropy values than java software at lower n. with larger n vhdl code has the lowest cross entropy values among hdls similar to that of java popular corpus while code written in verilog and systemverilog have similar cross entropy values compared to that of java naturalness corpus.
at the language level vhdl code has the lowest cross entropy compared to other language corpora.
systemverilog code shows lower cross entropy than verilog code with values similar to the two java corpora.
however verilog code has more regularities within repositories than at the language level.
our results indicate that verilog code is more diverse across repositories compared to vhdl code.
we verify whether the differences between the cross entropy of the languages are statistically significant or not by performing wilcoxon rank sum tests under significance level p .
on each pair of the languages at both repository and language level when n .
at the repository level the cross entropy differences between vhdl java popular and systemverilog java naturalness pairs and only these pairs are not statistical significant.
at the language level the cross entropy differences between verilog systemverilog systemverilog java popular and systemverilog java naturalness pairs and only these pairs are not statistical significant.
from these observations we conclude that similar to software written in imperative languages hds also show clear properties of naturalness i.e.
predictability and repetitiveness as captured by n gram language models with n .
vhdl code has the highest naturalness among the hdls we study and is higher than that of java software at the repository level.
this is due to vhdl being more verbose thus more repetitive.
the findings for verilog may be attributed to a larger vocabulary used in a smaller corpus compared to vhdl .
the cross entropy of java naturalness at the repository level matches that reported in hindle et al.
while at the language level this corpus has higher cross entropy values.
compared to that the java popular corpus which represents recent and trending repositories has lower cross entropy values at both repository and language levels.
assignment completion for vhdl the high level of naturalness of hds that we revealed over our large corpora entails that many learning based code automation techniques can be built to support hardware designers including automated code completion code search etc.
in this section we start off the research in this direction by designing and implementing the first technique for an automated code completion task in vhdl.
our technique leverages unique semantics of vhdl.
.
task we tackle code completion in concurrent signal assignment statements concurrent assignments for short .
namely given a left hand side of a concurrent assignment we predict the value on the right hand side to be assigned.
example line in figure vicaddr ?
our task is inspired by prior work on code synthesis that looked at completing the right hand side for imperative languages .
in a personal communication we also confirmed with a sigasi ide developer the relevance of the task.
we focus on vhdl because of its high level of naturalness among hdls we have studied the abundance of data on github and our familiarity with the language.
a concurrent assignment can be divided into its left hand side lhs and right hand side rhs .
the lhs consists of a signal name or an array indexing expression which specifies the signal to be assigned to.
the rhs consists of some operations over signals and literals which is to be the new value of the lhs.
our task is to design anassignment completion technique that automatically completes rhs code fragments conditioned on the lhs and other context.
.
neural architecture the underlying framework of our models is a sequence to sequence architecture that encodes a sequence into a deep representation and predicts a target sequence.
originally used in natural language generation such as machine translation this type of model has widely been used in language code tasks such as code summarization comment generation and code generation .
in contrast to traditional approaches which usually consider little context beyond the tokens that are to be encoded with a single encoder the unique aspects e.g.
concurrent assignments of hdls motivate us to design a richer context driven model.
namely our model utilizes the context in the source code before the concurrent assignment under consideration assuming that the code is written in top to bottom order this setting mimics how in practice developers would use a code completion tool.
for example when predicting the rhs of the concurrent assignment at line in figure our model can utilize all the previous context in the file including the previous concurrent assignments at lines and thesignal type declarations at lines and lines .
below we first introduce our base sequence to sequence model then describe our extensions that capture hdl specific characteristics a multi source architecture i.e.
having multiple encoders rather than one utilizing type embeddings and ensembling multiple sequence to sequence models.
figure depicts the sequence to sequence model with the extensions and .
base architecture .
sequence to sequence models are designed specifically for transduction tasks i.e.
given an input token sequence x x1 x2 ... x m the model predicts a target token sequence y y1 y2 ... y n. this is achieved by an encoder usually a recurrent neural network rnn which encodes the input into a deep semantic vector representation z encoder x and a decoder another rnn predicts the target y x decoder z .
the entire architecture is trained end to end using back propagation through time maximizing the conditional log likelihood logp y x n i 1logp yi yi z over the training set.
the output is predicted via a decoding algorithm such as beam search rather than predicting tokens one at a time as in language models.
our work uses bidirectional gru an advanced rnn often used for sequence data for both the encoder and the decoder.on the naturalness of hardware descriptions esec fse november virtual event usa ... ... sos input input eos lhsprevassign 1prevassign n ... ... sig2 vec sig3 vec...vecvec slv ... ... sig1 vec fc leaky relu slv slv slvslv slv sub token type sub token typesub token type figure neural architecture for our assignment rhs prediction model which is a multi source sequence to sequence model with type embedding concatenated with sub token embedding.
the left part shows several encoders for lhs or previous concurrent assignments each encoder is a bidirectional gru and the input at each time step is concatenation of the sub token embedding and the type embedding slv std logic vector .
the right part shows the decoder which predicts the sub tokens in the output sequence rhs .
attention mechanism is not shown in this figure to avoid unnecessary complexity but described in the main text.
to improve the model s ability in capturing long range dependencies among its tokens we incorporate an attention mechanism using the global attention model which is commonly used with sequence to sequence models.
when predicting a target token yi the attention mechanism informs the model how much information should be pulled from each input token xj j .. m which is then aggregated via a learned weighted sum over the encoder representation zconstituting another layer over the decoder.
sub tokenization .
in vhdl identifier names e.g.
signal names and function names are usually composed of multiple sub tokens separated by underscores .
camelcase is rarely used because vhdl is case insensitive.
we split the lhs and previous concurrent assignments into sub tokens normalize them to lower case and use the obtained sequence of sub tokens as the inputs to the models.
sub tokenization helps the model to capture more semantics in the identifier names and to generalize across different repositories.
multi source sequence to sequence model .
as described in section one of the characteristics of hdls that is very different from imperative languages is that the processes including concurrent assignments are executed in parallel despite the fact that they are still presented sequentially in a text editor.
in addition concurrent assignments that are similar to each other are often written close together.
with this in mind we design a multi source architecture such that prior context and the fact that concurrent assignments can be freely shuffled is taken into account.
we denote each input sequence with a superscript i.e.
x1 ... xk forksequences.
the multi source encoder is formulated as zj encoderj xj j ... k z merge z1 ... zk frequencystd logic vector std logic t in std logic vector out std logic out std logic vector in std logic unsigned std ulogic boolean signed inout std logic vector inout std logic out startaddr array typetype44377 179figure histogram of vhdl signal types in our dataset.
each bar represents a type that we use for the type embedding and its value is the frequency of that type.
themerge function consists of a fully connected layer followed by a leaky relu regularization.
the input sequences are concatenated for the attention mechanism in the decoder.
in our preliminary study the closest previous concurrent assignments showed the most similar context of the times measured by jaccard similarity among up to previous concurrent assignments .
thus we experiment with using previous concurrent assignments.
this leads to encoders one for lhs and others for previous concurrent assignments .
type embedding .
clearly the expressions on the rhs have to match the type of the signals on the lhs.
thus we condition on type information in the encoder s such that the model learns what expressions would be appropriate for the type of the lhs3.
therefore in addition to the local context in lhs and previous concurrent assignments we utilize the types of signals as global context as they are extracted from the port and local signal declarations at the beginning of entity architecture definitions.
this idea aligns with the recent work that uses types for code completion in imperative languages .
there are many possible types in vhdl but only few of them are frequently used.
to reduce sparsity and improve the generalizability of the model we focus on types that are most frequently used in concurrent assignments.
we replace all other types with a special token t .
figure shows the histogram of the signal types where each bar represents one signal type or t and its value shows the frequency of the type in our dataset.
in our model types are represented as a one hot bit vector where each bit represents one of the plus t .
we concatenate this type embedding to each sub token of the lhs and previous concurrent assignments.
if the sub token is a part of a signal the respective bit in the type embedding corresponding to the signal type is set to one otherwise the type embedding is a zero vector.
ensembling .
our preliminary experiments with the multi source sequence to sequence models revealed that utilizing different previous statements e.g.
lhs 1st previous concurrent assignment lhs 2nd previous concurrent assignment etc.
led to complementary model behavior.
in particular they make better worse predictions for different portions of the development set of the dataset and comparable performance on the entire development 3an alternative approach would be to perform an extensive static analysis and ensure that our model never suggests rhs with an incorrect type but we wanted to focus on an end to end neural architecture in this work.esec fse november virtual event usa jaeseong lee pengyu nie junyi jessy li and milos gligoric table statistics of our collected assignments dataset.
statistic all train dev test assignments avg.
lhs length .
.
.
.
avg.
rhs length .
.
.
.
set.
furthermore we also observed the rhs of the current concurrent assignment can be frequently the same as the rhs of a previous concurrent assignment especially when the lhs of the two concurrent assignments are similar.
thus the rule based models that copy rhs from 1st 5th concurrent assignments could complement the aforementioned sequence to sequence models.
because of the parallelism of hdls the order in which previous concurrent assignments are encoded does not matter.
thus we ensemble these sequence to sequence and rule based models to leverage their complementary prediction powers given a set of inputs we use all the models to predict the output sequence individually which gives us a confidence score from each sequenceto sequence model.
we train a regression model using the data from development set that assigns a score on each prediction.
the input features include the similarity of each prediction to the rhs of each concurrent assignment from 1st 5th concurrent assignments the similarity of the lhs of current concurrent assignment with the lhs of each concurrent assignment from 1st 5th concurrent assignments and the confidence scores of each sequence to sequence model.
the similarity of two sequences of sub tokens is measured as one minus the jaccard distance between the sets of their bag of sub tokens.
we then rerank the predictions and select the one with the highest score as predicted by the regression model.
.
data we extracted all concurrent assignments from our vhdl corpus.
we extracted each assignment s lhs and rhs and the type declarations in each entity architecture and performed post processing to obtain the type for each signal in the concurrent assignments.
table shows the statistics of our collected data.
we collected concurrent assignments across files in total where lhs has .
sub tokens on average and rhs has .
sub tokens on average.
note that .
and .
of lhs and rhs sequences in our dataset have lengths within the range of one standard deviation.
this means that long lhs and rhs sequences are rare.
we observed that all long rhs sequences contain a number of logic operators.
to obtain the training development and testing sets we randomly shuffle the list of all files and then take enough files to obtain of assignments for the testing set and of assignments for the development set assignments from other files go into the training set.
duplicates that contain same assignment and same context are removed.
.
baselines models and training details rule based baseline .
we compare to a baseline model which outputs the rhs of the 1st previous concurrent assignment or an empty sequence if there is no previous concurrent assignment.
language model baseline .
this model utilizes the inputs as existing tokens t1 ... t m and repeatedly predicts the next token usingthe language model lmasti lm t1 ... t i until the predicted token is the end of statement i.e.
a semicolon .
the model outputs all the predicted tokens except end of statement as the prediction for rhs.
we use rnn language model because prior work showed its power on code completion tasks.
we use three variants rnnlm is the rnn language model trained with the concurrent assignment lhs rhs sentences in rnnlm pa each sentence is a concurrent assignment with its 1st previous concurrent assignment in rnnlm pa each sentence is a concurrent assignment with its 1st 5th previous concurrent assignments.
for example the sentence extracted from the concurrent assignment on line in figure for rnnlm is vicaddr not cia2 pao the sentence for the same assignment for rnnlm pa is vicaddr not cia2 pao vicaddr not cia2 pao and for rnnlm pa is uart rts cia2 pbo uart dtr cia2 pbo uart ri out cia2 pbo uart dcd out cia2 pbo vicaddr not cia2 pao vicaddr not cia2 pao we also experimented with n gram language model for completeness.
similar to rnn language model we use three variants for n gram language model 10gramlm is trained with the concurrent assignment lhs rhs sentences in 10gramlm pa each sentence is a concurrent assignment with its 1st previous concurrent assignment in 10gramlm pa each sentence is a concurrent assignment with its 1st 5th previous concurrent assignments.
sequence to sequence models .
the s2smodel is the sequenceto sequence model with only lhs as input.
s2s pa utilizes the 1st previous concurrent assignment as additional input using multisource architecture.
s2s pa type adds type embedding based ons2s pa .s2s pa k type utilizes the 1st kth previous concurrent assignments e.g.
s2s pa type utilizes 1st 4th previous concurrent assignments using the multi source architecture with type embedding.
s2s pa ensemb type utilizes 1st 5th previous concurrent assignments by ensembling models and also uses type embedding.
s2s pa concat type uses only the basic sequence to sequence architecture and utilizes the 1st 5th previous assignments by concatenating them with the lhs as one input sequence and it also uses type embedding.
all the sequence tosequence models have attention mechanism enabled.
hyper parameters .
we set hyper parameters of all neural models by tuning on the development set.
for sequence to sequence models both encoder and decoder bidirectional gru have layers and hidden state dimensions of the sub token embedding dimension is randomly initialized.
we limit the lengths of input sub tokens sequences to be at most .
we train each model with batch size for at most steps one batch per step and use an early stop mechanism which stops the training if the validation loss does not improve for the subsequent checkpoints one checkpoint every steps .
during training time we train the sequence to sequence model with teacher forcing during inference time the model performs beam search with beam size .on the naturalness of hardware descriptions esec fse november virtual event usa table bleu accuracy acc and exact match accuracy xmatch scores of assignment completion models.
all improvements are statistically significant.
model bleu acc xmatch rule based baseline .
.
.
rnnlm pa .
.
.
s2s pa ensemb type .
.
.
table bleu accuracy acc and exact match accuracy xmatch scores of the rnn language model variants.
model bleu acc xmatch rnnlm .
.
.
rnnlm pa .
.
.
rnnlm pa .
.
.
the rnn language models have the same hyper parameters except that they use single directional gru.
our models are implemented using pytorch and opennmt .
.
results evaluation metrics .
we use three automatic metrics to evaluate our models bleu originally proposed for machine translation bleu is now widely used in language code tasks .
this metric calculates the percentage of n grams in the predicted output that also appear in human written rhs averaging across n and using a brevity penalty to eliminate the impact of the number of tokens predicted.
the range of values is .
following settings from code related tasks e.g.
we use sentence level bleu implementation in nltk library with the smoothing method proposed by lin and och .
accuracy acc we report the accuracy of sub tokens averaged across the testing set.
the accuracy for each concurrent assignment is calculated using the following formula accuracy len i pred tgt max len pred len tgt where pred is the sequence of sub tokens in the predicted rhs tgtis the sequence of sub tokens in the human written rhs and lenfunction calculates the length of a sequence or size of a set.
exact match accuracy xmatch we also report the accuracy in statement level i.e.
the number of predicted rhs that exactly match the human written rhs divided by total number of concurrent assignment across the testing set.
we perform statistical significance testing to compare the metrics between models under significance level p .05using the bootstrapping method .
quantitative results .
table compares the performance of the baselines and our best model evaluated on the testing set.
we ran each model times and show the averaged metrics each time we use a different random seed for initializing the model as well as for splitting the training development testing sets .
the best model iss2s pa ensemb type with .
bleu .
accuracy andtable bleu accuracy acc and exact match accuracy xmatch scores of n gram language modeling based assignment completion models.
model bleu acc xmatch 10gramlm .
.
.
10gramlm pa .
.
.
10gramlm pa .
.
.
table ablation study.
the best scores are in bold text and statistically significantly outperform all others.
model bleu acc xmatch s2s pa ensemb type .
.
.
s2s pa type .
.
.
s2s pa type .
.
.
s2s pa type .
.
.
s2s pa type .
.
.
s2s pa type .
.
.
s2s pa .
.
.
s2s .
.
.
s2s pa concat type .
.
.
.
exact match accuracy.
this is the model that ensembles multisource sequence to sequence models for previous assignments.
it significantly outperforms the rule based and language model baselines.
table compares the three variants of the rnn language model.
none of the language models outperform the rule based baseline.
rnnlm pa is better than rnnlm pa despite the latter one having more context as inputs.
this is likely because the language model was trained with too much context that is less relevant to the current assignment thus distracts the model from the task of assignment completion.
we also observed that the language models usually cannot predict the end of statement token correctly and end up predicting very long sequence.
table compares the three variants of the n gram language model where n 10because it gives the best results among n ... on the development set.
both models 10gramlm and 10gramlm pa are worse than the rule based baseline although 10gramlm pa achieves a reasonable performance .
accuracy and .
bleu .
moreover 10gramlm pa has much worse performance despite the fact that higher order n grams consistently shows lower cross entropy values i.e.
higher naturalness .
this again shows that using too much context to train language model is not helpful and may distract the model from the task and compared to rnn language model n gram model is more likely to be distracted.
ablation study .
table compares the variants of sequence tosequence models all of which except for the basic s2smodel are better than the rnn language model baseline.
the best model s2s pa ensemb type significantly outperforms all sequenceto sequence models on all computed metrics.
in preliminary experiments we tried using previous concurrent assignments butesec fse november virtual event usa jaeseong lee pengyu nie junyi jessy li and milos gligoric found them significantly decreasing the performance likely due to the low similarity of those assignments to the current assignment.
notably all the multi source models significantly outperform the models without multi source architecture s2sands2s pa concat1 type which shows that multi source architecture is effective.
for type embedding comparing s2s pa ands2s pa type there is an improvement although not statistically significant.
the ensembling model s2s pa ensemb type has .
bleu .
accuracy and .
exact match accuracy which is significantly better than s2s not including any context and significantly better than s2s pa type which uses one multi source architecture to include all previous concurrent assignments.
from these observations we conclude that the using of ensembling model to include multiple previous concurrent assignments local context and the using of type embedding global context have positive effect on the performance of the models.
language model is not a good option for this task because it cannot effectively utilize the context information such as those available in previous concurrent assignments.
qualitative analysis .
we showcase two examples in table from our corpus to illustrate our models.
for each example we show its inputs lhs previous concurrent assignments and relevant signal types its expected output rhs and the predictions of rnnlm pa s2s s2s pa type and s2s pa ensemb type models.
in the first example the five previous concurrent assignments have the same pattern m 7seg ?
disp 7seg segment ?
where ?
represents a missing character or number.
the input lhs is similar to the lhs of previous concurrent assignments thus the output rhs should also be similar to the rhs of previous concurrent assignments and follow the pattern.
rnnlm pa ands2s pa type models learn the disp 7seg segment ?
pattern but cannot predict the correct number in the parenthesis.
the s2s pa ensemb type detects the pattern where the number in the parenthesis is ascending and predicts the correct number following the pattern.
due to the lack of longer local context rnnlm pa guesses a wrong pattern to repeat the numbers and s2s pa type guesses a wrong pattern to decrease the number.
due to lack of context and especially global context s2spredicts an irrelevant variable.
in the second example there are several patterns in the rhs of previous assignments but no clear pattern in the lhs.
rnnlm pa s2s pa type and s2s pa ensemb type are still able to detect the right pattern sseg edu cathode out ?
for rhs and the latter two models predicts the right number in the parenthesis.
without any context the s2smodel could not learn this pattern and predicts both a wrong variable and value.
threats to validity internal .
as our goal was to compare different characteristics of the models under the same environment we did not fine tune the hyper parameters for each assignment completion model individually.
the models might achieve better performance if their hyper parameters are fine tuned but we do not expect changes to our reported observations.
we plan to evaluate the benefit of such fine tuning in future work.
our scripts and implementation may contain bugs.
to mitigate this threat at least two authors reviewed each script and the outputtable example predictions of our models.
repository f32c f32c lhs m 7seg f previous concurrent assignments 1st m 7seg e disp 7seg segment 2nd m 7seg d disp 7seg segment 3rd m 7seg c disp 7seg segment 4th m 7seg b disp 7seg segment 5th m 7seg a disp 7seg segment types m 7seg f out std logic m 7seg 2 out std logic m 7seg d out std logic m 7seg c out std logic m 7seg b out std logic m 7seg a out std logic disp 7seg segment std logic vector expected output rhs disp 7seg segment predictions rnnlm pa disp 7seg segment s2s state cur s2s pa type disp 7seg segment s2s pa ensemb type disp 7seg segment repository fpga logi logi projects lhs pmod3 previous concurrent assignments 1st pmod2 sseg edu cathode out 2nd pmod2 sseg edu cathode out 3rd sys sda z 4th sys scl z 5th vga clk clk 50mhz types pmod3 inout std logic vector pmod2 inout std logic vector sseg edu cathode out std logic vector sys sda inout std logic sys scl inout std logic vga clk std logic clk 50mhz std logic expected output rhs sseg edu cathode out predictions rnnlm pa sseg edu cathode out s2s sseg edu anode out s2s pa type sseg edu cathode out s2s pa ensemb type sseg edu cathode out logs.
we also automated each step of the process to remove a chance for human errors while executing the experiments.
we used only parsable files in our hd corpora and experiments.
to parse the files we initially generated parsers using the antlr parser generator framework and publicly available grammars that appeared to be the most complete .
however those grammars are somewhat obsolete and do not support the latest versions of hdls.
to mitigate this threat we extended these grammars in several ways to increase the number of files that can be parsed by supporting more recent language standards.
of all files in vhdlon the naturalness of hardware descriptions esec fse november virtual event usa table vhdl parsing errors categories.
error count mismatched input keyword extraneous input keyword unrecognized token not interpretable syntax missing input keyword corpus still cannot be parsed.
table shows the error categories and the count for each of them note that there are multiple errors per file.
note that parsing one file could generate multiple parsing errors.
we observed five types of errors mismatched input keyword when the parser is expecting a keyword but got a different one extraneous input keyword when the parser is not expecting a keyword but got one unrecognized token when the file contains binary data that the parser can not handle not interpretable syntax when the parser could not find a valid parsing rule for a code snippet and missing input keyword when the parser is expecting a keyword but got none due to end of file .
external .
we constructed our dataset from hdl repositories available on github.
we chose github because of its popularity and our prior experience with mining the repositories available on this repository hosting service.
as many hds are not publicly available the hds used in our study might not be representative of proprietary hds.
we mitigate this threat by ranking the repositories on github by the number of stars which might indicate that these repositories are also widely used by various companies.
additionally nowadays many hds are made publicly available on github by hardware companies.
related work we briefly discuss related work on the naturalness of software software engineering for hds and code completion.
the naturalness of software .
hindle et al.
were the first to study the naturalness of software written in java and c. recently musfiqur et al.
replicated the study of naturalness for several other imperative languages and on much larger corpora.
our section replicates the study of naturalness for hds and we have discussed our similarities and differences to these two works throughout the text.
hellendoorn et al.
studied the naturalness of proofs written in coq and hol and found that proofs are also repetitive and predictable.
unlike prior work we studied the naturalness of hds.
software engineering for hds .
clarke et al.
developed a program slicing technique for vhdl based on mapping its operational semantics to imperative languages.
sudakrishnan et al.
analyzed the bug fix history of four hd repositories written in verilog and grouped them into bug fix patterns.
they found that of the bug fix pattern instances involve assignment statements which reflects the need for automating completion for assignment statements to reduce a chance for manual errors.
duley et al.
developed vdiff a program differencing tool for verilog that finds syntactic differences of two versions of code.
notably their differencing algorithm is position independent to robustly handlelanguage constructs whose relative orderings do not matter e.g.
concurrent processes.
uemura et al.
developed a clone detection tool for verilog by first converting verilog modules into pseudo c code then applying code clone detectors for c .
schkufza et al.
developed the first just in time compiler for verilog to speed up program execution on fpgas.
these software engineering techniques for hds have not considered using learning based approaches.
our study reveals the naturalness of hds which stands to motivate other researchers and practitioners to improve previously studied techniques based on deep learning for hdls.
we presented the first work in this direction by designing and implementing an assignment right hand side completion technique for vhdl.
code completion .
code completion is a task that recommends upcoming code elements given the code context.
hindle et al.
developed a code completion tool using n gram language models.
proksch et al.
used bayesian networks for code completion which can utilize global context from methods class etc.
li et al.
developed a neural code completion technique with attention mechanism and pointer networks to better handle out ofvocabulary words.
raychev et al.
proposed an rnn language model for completing holes in partial programs with the most likely sequences of api method calls.
svyatkovskiy et al.
proposed a neural code completion model that incorporates type information instantiated as a tool called pythia for python.
hellendoorn et al.
applied several code completion tools on real world data and analyzed the real world efficacy of those tools.
hu et al.
proposed lstm based code completion tool by inducing tokens at character and token levels thereby reducing vocabulary size.
sun et al.
developed neural code generation model that implements attention mechanism and tree based ast reader.
these prior efforts targeted imperative languages.
we introduced the first code completion technique for concurrent assignments in vhdl utilizing its unique semantics.
furthermore unlike prior work on code completion we used a multi source sequence to sequence model with type embedding which we found suitable for our task.
conclusion we assembled large hd corpora consisting of source code written in vhdl verilog and systemverilog and reported on their characteristics.
we studied the naturalness of hds and our language model evaluation reveals that hds possess a high level of naturalness similar to software written in imperative languages.
further we built several deep learning models for automated code completion in vhdl utilizing unique characteristics of hdls e.g.
semantics of concurrent signal assignment statements .
these characteristics led to effective neural models achieving a bleu score of .
.
our study stands to motivate other researchers and practitioners to develop code automation techniques based on deep learning for hdls to provide powerful tools to hardware designers.