iopv on inconsistent option performance variations jinfu chen jinfuchen whu.edu.cn wuhan university wuhan hubei chinazishuo ding zishuo.ding uwaterloo.ca university of waterloo waterloo on canadayiming tang yxtvse rit.edu rochester institute of technology rochester ny usamohammed sayagh mohammed.sayagh etsmtl.ca ets quebec university montreal qc canada heng li heng.li polymtl.ca polytechnique montr al montreal qc canadabram adams bram.adams queensu.ca queen s university kingston on canadaweiyi shang wshang uwaterloo.ca university of waterloo waterloo on canada abstract maintaining a good performance of a software system is a primordial task when evolving a software system.
the performance regression issues are among the dominant problems that large software systems face.
in addition these large systems tend to be highly configurable which allows users to change the behaviour of these systems by simply altering the values of certain configuration options.
however such flexibility comes with a cost.
such software systems suffer throughout their evolution from what we refer to as inconsistent option performance variation iopv .
an iopv indicates for a given commit that the performance regression or improvement of different values of the same configuration option is inconsistent compared to the prior commit.
for instance a new change might not suffer from any performance regression under the default configuration i.e.
when all the options are set to their default values while altering one option s value manifests a regression which we refer to as a hidden regression as it is not manifested under the default configuration.
similarly when developers improve the performance of their systems performance regression might be manifested under a subset of the existing configurations.
unfortunately such hidden regressions are harmful as they can go unseen to the production environment.
in this paper we first quantify how prevalent in consistent performance regression or improvement is among the values of an option.
in particular we study over hadoop and cassandra commits for which we execute a total of and tests respectively amounting to machine hours of testing.
we observe that iopv is a common problem that is difficult to manually predict.
and of the hadoop andcassandra commits have at least one configuration that hides a performance regression.
worse most of the commits have different options or tests leading to iopv and hiding performance regressions.
therefore we propose a prediction model that identifies whether a given combination of commit test and option cto manifests an iopv.
our evaluation for different models shows that random forest is the best performing classifier with a median auc of .
and .
for hadoop andcassandra respectively.
our paper defines and provides scientific evidence about the iopv problem and corresponding author.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
prevalence which can be explored by future work.
in addition we provide an initial machine learning model for predicting iopv.
ccs concepts software and its engineering software performance .
keywords software performance performance variation configurable software systems acm reference format jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang.
.
iopv on inconsistent option performance variations.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction modern large scale software systems tend to have a large number of configuration options which can hide performance issues.
these options are used to customize the behaviour of a software system without changing its source code.
although these options add flexibility to a software system they make testing software performance a challenging task.
for example in theory one has to run210tests for a software system with just boolean configuration options while a highly configurable software system such as hadoop can have as many as available options .
while there are constraints between configuration options bringing down the total number of configurations in practice this still amounts to a too large set of configurations to test exhaustively especially for long running performance tests.
prior study found that more than of performance bugs are related to misconfiguration implying that configuration tuning is crucial for system performance.
most of the configurations directly impact performance such as the timeout family configurations like ipc.client.connect.timeout that directly affects latency in hadoop .
for another example a prior study found that the configuration option native transport max threads in the cassandra project can lead to up to .3x latency i.e.
performance regression under two different configuration settings i.e.
two different values for the option native transport max threads .esec fse december san francisco ca usa jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang performancev1v2commitc1c2 a c2v1v2abv1v2c1commit b c1c2v1v2abv1v2performancecommit c c1c2v1v2abv1v2commit d figure the definition of iopv a approaches that do not consider the historical evaluation b an option with a consistent performance variation a b c an option with an inconsistent performance variation a!
b c2 has a performance regression compared to c1 and d an option with an inconsistent performance variation a!
b c2 has a performance improvement compared to c1.
v1 and v2 are two different values of the same configuration option.
c1 and c2 are two revisions.
a smaller performance metric value e.g.
cpu usage indicates a better performance.
performance related configuration is a non trivial problem since configuration options are hard to understand and poorly documented .
in particular the default configuration may have no performance regression perceived by the end user however userspecific configuration settings may lead to a significant performance regression.
for example a real life performance issue is shown in mysql under the configuration option query cache type in versions .
.
.
.
and .
.
.
such an issue is reported in the mysql bug tracking system with bug id mysql .
the option query cache type is used to set the query cache type.
the possible values of the option query cache type include off on and demand defaulting to off.
in general turning on query cache type leads to somewhat better performance since mysql can cache results in memory.
in this bug we see that with setting query cache type to off the performance among the three versions remains the same.
however when turning on query cache type the performance in version .
.
is worse than in versions .
.
and .
.
.
traditionally prior work studied the difference in system performance caused by different values of the same option without considering how the performance impact of an option evolves due to code changes .
for instance traditional approaches compare different values of a configuration option based on their raw performance values as illustrated in figure 1a.
however such comparison is subjective as the option s value v2with worse performance might not necessarily be problematic but might as an example just enable the execution of some extra features.
conversely even if an option s value has a good performance compared to other values those differences in performance might start to vary when comparing to the performance of the same option valuein the prior commit.
normally one would expect to see the situation in figure 1b which shows that both option values have a consistent variation in performance in this case a similar increase regression in the performance metric.
in reality one can observe cases such as in the example in figure 1c where v1still shows better performance compared to v2after commit c2 but it faces a significantly larger performance regression compared to the prior commit than v2.
in figure 1d the v2value still has a worse performance after the new commit but its performance improved much more significantly compared to the prior commit than v1.
therefore different values of an option can have an inconsistent variation in terms of performance compared to the prior commit which we refer to as inconsistent option performance variation a.k.a iopv .iopv is a difficult and complicated issue for developers to identify and fix.
we find there is a lack of reported iopv issues in the issue tracking systems as developers all too often do not test different values of options across versions.
however theiopv might be problematic as it can hide a performance regression that is manifested only when altering configuration options.
such regressions can unfortunately go as unseen to the production environment.
the iopv may directly affect the user experience increase the resources cost of the system and lead to reputational and financial repercussions.
in this paper we perform a case study on two large scale opensource software systems hadoop andcassandra .
we first conduct a preliminary study to quantify the prevalence of iopv in practice.
we observe that of the commits have at least one option manifesting an iopv issue.
we also observe that manually identifying such issues is challenging as commits do not share the same options that manifest an iopv.
that motivates us to propose an automated model that predicts if the combination of a commit a test and an option cto would exhibit an iopv issue.
we evaluate our prediction model using the following two research questions rq1.
can we accurately learn iopv issues in the studied systems?
our prediction model reaches an area under the receiver operating characteristic curve auc up to .
and .
for predictingiopv forhadoop andcassandra respectively.
auc measures our models ability to discriminate the cto cases into iopv and non iopv cases.
we observe that random forest is the most performing model for four and three out of five performance measures i.e.
response time cpu memory i o read and i o write for cassandra andhadoop respectively.
rq2.
what are the most important metrics for predicting iopv issues?
we observe that all four dimensions of metrics considered in our study namely the code structure code change code token and configuration options metrics have a statistically significant impact in predicting iopv.
the dimensions that are related to the configuration options and the tokens of the changed code are the most important dimensions for both case studies.
background software configuration is a mechanism used to customize the behaviour of a software system without changing the source code.
the configuration options are often stored in configuration files as a set of key value pairs where the key represents an option s name and the value represents a default or user chosen value for thatiopv on inconsistent option performance variations esec fse december san francisco ca usa option.
we define a configuration as one particular assignment of a value to all existing options.
table lists the definition of these terms.
for example a 1andb 2is one possible configuration for a software system with the two integer options aandb.
configuration options enable users to adapt the execution of their software systems by simply modifying the values of certain configuration options without re compilation.
for example a user can change the directory that stores the cache for cassandra by changing the value of the saved caches directory configuration option.
table our definition of configuration option and value.
term definition example option a typed configurable item that allows users to set different values.a value a specific assignment of a value for an option.
a configuration an assignment of values to all options by a user.
a b although configuration introduces large flexibility for users considering all the possible configurations during testing is impossible.
a software system with boolean configuration options requires testing 210configurations.
in fact configuration problems are among the dominant problems in software engineering .
in particular a software system can suffer from what we refer to as inconsistent option performance variation a.k.a iopv .
this occurs when for a given commit c the performance of a subset of an option s values evolved differently relative to their performance in the commit prior to c. considering the example in figure when comparing the raw performance of the two option values v1andv2 figure 1a we observe that v1shows a better performance than v2.
however that might not be problematic as v2 might just enable an extra feature such as logging a transaction.
in fact figure 1c shows that even if v2does not show any significant performance variation from the prior commit v1suffers from a performance regression.
similarly in figure 1d the performance ofv2is improved compared to the prior commit while that improvement does not manifest under option value v1.
the iopv may directly affect the user experience increase the resources cost of the system and lead to reputational repercussions.
a performance variation is calculated as the difference between the performance variation of each option s value after and before each commit which is illustrated in figure by a b .
data collection in this section we present our subject systems and our approach to collect performance regressions and configuration data.
.
subject systems in this paper we consider hadoop and cassandra as two subject systems.
we choose these two subject systems due to the following reasons their performance is critical for the users the two systems are highly configurable the two systems have been studied in prior research on software performance and we are familiar with these two systems.
the overview of our subject systems is shown in table .
.
data gathering we follow the approach summarized in figure to collect data.table our studied dataset.
subjects studied releaseslast release commits configuration options tests hadoop .
.
cassandra .
.
.
.
filtering commits.
since we study performance variation across different versions of a software system we only consider source code related changes.
hadoop andcassandra are both java systems.
therefore we filter out commits without any java source code changes.
furthermore developers can commit multiple changes toward fixing the same issue which is defined in the issue tracking system.
as hadoop andcassandra use jira as their issue tracking system and have an explicit mapping between commits and issues we use the issue id mentioned in the commit messages to identify the commits that belong to the same issue.
if multiple commits are associated with the same issue we only consider the last commit.
this is important as developers can initially introduce a regression but then fix it before releasing the code changes related to the issue.
.
.
extracting options.
in the second step we extract configuration options and their corresponding values for each subject system i.e.
and configuration options in the last studied releases of hadoop andcassandra respectively .
we obtain option names and default values by crawling the documentation of hadoop and cassandra by extracting the configuration file that is shipped with the project s releases.
finally we manually classified the extracted options based on their expected data types e.g.
boolean when the default value is true orfalse .
.
.
identifying impacted tests.
we automatically create a mapping between the changed source code in each commit and the existing unit tests.
we derive such commit test mapping based on the automatically generated method level code coverage results similar to a prior study by chen et al.
.
for each commit we use eclipse jdt to automatically add logging instrumentation to each method that will print log messages that indicate the execution of the method at runtime.
we then run each test for the commit.
a test is considered impacted by the commit if any instrumented logging is output.
afterwards we only run the tests that execute the changed source code for a given commit since executing all the existing tests of a software system for each commit and each possible configuration is practically infeasible.
in addition running those tests that are not impacted by the code change of a commit is not likely to detect performance variations regressions or improvements under some values of an option .
.
.
identifying impacted options.
similar to identifying impacted tests we would like to identify which configuration options are impacted while running the tests.
the configuration option values are accessed using getters based on our research of two well known projects hadoop andcassandra e.g.
databasedescriptor.java to access the cassandra s options .
we keep track of method invocations to the getters that involve configuration options.
if such method invocations occur during test execution the relevant options are considered impacted by the commit.
note that we only execute the tests that cover the changed methods in each commit.esec fse december san francisco ca usa jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang git1.
filtering commitscode commit3.
identifying impacted tests .
evaluating performanceperformance evaluation results6.
statistical analyses on performance evaluation resultsfor each test and each option identified tests with performance regression regarding optionsrepository process datacode commit and options with impacted tests .
extracting optionsoption documentationoptions with values4.
identifying impacted options .
discretizing cto into iopv and non iopv iopv data figure an overview of our approach to collect data.
cto is a combination of a commit a test and an option.
.
.
evaluating performance.
after obtaining which tests and which options are impacted by each commit we exercise the test on each commit and its parent commit i.e.
the previous commit to evaluate their respective performances.
we first execute each test with all the configuration options set at their default values.
then we alter the value of one configuration option at a time.
for the configuration options with boolean values we alter the configuration option to the value that is not the default.
for example if the default value is true we would alter the value to be false .
for the numeric type option we alter the configuration option once to the value that is double the default value and once to half of the default value.
for example if a configuration option has a default value of we would run the test altering the value to then run the same test altering the value to .
for enumeration typed options we alter to each of the possible values.
our performance evaluation environment uses the google compute engine with 8gb memory and cores cpu.
in order to generate statistically rigorous performance results we adopt the practice of repetitive measurements to evaluate performance.
conservatively we executed each test times independently which is larger than prior work that repeats a test only to times .
to measure the performance that is associated with each test we use a performance monitoring tool named psutil python system and process utilities .
psutil can capture detailed performance metrics and has been widely used in prior research .
we collect both domain level and physical level performance metrics.
in our execution we collect five performance metrics during the execution i.e.
response time cpu usage memory usage i o read and i o write.
to minimize the performance noise we first control the execution environment strictly in each instance i.e every instance is with the same hardware setup and every instance only runs the small scale test process.
second all the performance measures related to a given version of a given project were done in a limited time period so the variation within that scope should not have been impacted by vm provisioning contention.
finally we repeat the unit test times.
the first time of execution is to warm up the junit process.
the remaining times of execution are taken into consideration in our statistical analysis.
.
.
statistical analyses on performance evaluation results.
to identify the iopv we statistically compare the performance of a given test and a configuration option value before and after eachcommit using the mann whitney u test i.e.
.
and cliff s delta which measures the magnitude of performance regressions.
we choose mann whitney u test since it does not have any assumption on the distribution of the data.
researchers have shown that reporting only the statistical significance may lead to erroneous results i.e.
if the sample size is very large p value can indicate statistical significance even if the difference is trivial .
thus we also use cliff s delta to quantify the magnitude of the differences a.k.a.
effect sizes .
cliff s delta measures the effect size statistically and has been used in prior engineering studies .
cliff s delta ranges from to where a value of indicates two identical distributions.
for each combination of commit test and option value we obtain a cliff s delta value.
we then calculate the differences between the maximum and minimum cliff s delta for each option s different values which the next subsection uses to categorize a combination of commit test and option as iopv or non iopv.
we also consider a test to be a performance regression when the value of the effect size is positive and has either medium .
cliff s delta .
or large .
cliff s delta magnitude.
on the other hand we consider a test to manifest a performance improvement if the value of the effect size is negative and has a medium .
cliff s delta .
or large .
cliff s delta magnitude.
note that we perform this statistical analysis for each performance metric i.e.
response time cpu usage memory usage i o read and i o write separately.
for example a commit may show a cpu regression or improvement but not show any difference for the response time.
.
.
discretizing cto intoiopv and non iopv .in the final step we categorize each commit test and option cto into iopv or noniopv based on an automatically determined threshold.
our intuition is that the maximum difference values a b in figure would be concentrated in either small values i.e.
when adjusting an option does not make a difference or large values i.e.
when adjusting an option does make a difference which is demonstrated in figure .
specifically we use ckmeans.1d.dp a one dimensional clustering algorithm to find a threshold that separates the maximum difference values of all cto s into two groups i.e.
iopv and noniopv.
note that the option variation ranges between when there is no variation and when the effect size cf.
section .
.
is for one option value and for another value of the same option.iopv on inconsistent option performance variations esec fse december san francisco ca usa to further investigate our collected data we manually examine the issue reports related to each commit with an iopv issue based on our quantitative analysis of our preliminary analysis .
in particular we first extract the issue id from the commit message.
then we write a script to search in the issue report for the name of each configuration option as the keywords.
finally if there are commits or issue reports containing the searched keywords we manually examine whether the issue is reporting any performance regression.
we observe that out of and ctos without regression under the default option value but with regression under other option values only and cto inhadoop andcassandra respectively have reported configuration related performance regression in the commit messages or issue reports.
such results show that performance regression may be hidden from developers since performance regression such as cpu or memory regression do not have a direct impact on developers but rather on users.
in addition we infer that developers do not test other configuration option values except the default ones.
preliminary study through this preliminary study we quantify the existence of the iopv problem in large and highly configurable software systems as well as how difficult it is to identify the iopv.
this preliminary analysis will also motivate the need for an approach that automatically identifies the iopv.
pq1.
are iopv issues common in the studied systems?
motivation.
the goal of this preliminary research question is to quantify and provide scientific evidence on how often a configuration option can suffer from instances of the iopv issue.
while a new code change might not show any performance regression under the default configuration another configuration can hide a performance regression that can go as unseen to the production environment.
this is an important problem as performance issues often lead to serious monetary losses .
similarly a configuration improvement might not be manifested under all the configurations.
one may only compare different values of a given configuration option rather than identifying the iopv problem.
however only comparing different values of a given option cannot know whether the performance variation is due to the configuration error or other reasons such as a new feature.
approach.
we first collect performance measurements for each cto combination of a commit test and option and label each cto asiopv or a non iopv.
then we identify for each commit and unit test the number of configurations under which the performance is statistically significantly worse a.k.a.
performance regression or better a.k.a.
performance improvement than the performance of the same test and configuration in the prior commit.
finally we quantify for each commit the number of tests that show a performance regression or a performance improvement under just a subset of the existing configurations.
in the studied hadoop andcassandra releases there are and cto respectively.
we also evaluate whether the interactions between the combinations of configuration options would influence the manifestation of iopv issues.
unfortunately measuring all the possible interactionof configuration options is practically unfeasible.
an estimation of what would be the cost of evaluating all the combination of just wise configuration options sums up to and test executions for hadoop andcassandra respectively which would take more than machine years to finish the experiment.
instead we select a statistically representative confidence level and confidence interval random sample of t wise cto from the population of all possible t wise tranges from two to five in our study combinations of options which comprises and 800t wise combinations for hadoop andcassandra respectively.
then we measure the performance of each of our t wise based cto similar to single option based cto.
intuitively if each occurrence of an iopv issue for a combination of options coincides with the occurrence of an iopv for at least one of the individual options in practice one would be able to rely on only the analysis of iopv for individual options.
in other words if none of the individual options would manifest iopv issues the combination of options would not manifest iopv issues either and if the combination of options manifests iopv issues at least one individual option would manifest iopv issue.
therefore we also evaluate the number of t wise cto that have results contradicting with the single option results.
table number of cto collected from the subject systems.
no regression under the default option value but with regression under other option values.
subject ctoany response time cpu memory i o read i o write metric large med large med large med large med large med hadoop cassandra improvement under the default option value but with regression under other option values.
subject ctoany response time cpu memory i o read i o write metric large med large med large med large med large med hadoop cassandra regression in default value and non regression improvement in other values.
subject ctoany response time cpu memory i o read i o write metric large med large med large med large med large med hadoop cassandra any metric means the union cto of five performance metrics.
med large means the effect size cliff s delta of performance regression is medium large.
result.
the iopv is a common problem as and of our studied cto inhadoop andcassandra suffer from the iopv problem in at least one performance metric.
in addition each hadoop andcassandra commit has a median percentage of and of the pairs of tests and options that manifest at least one iopv across releases.
although a small percentage of hadoop tests and options suffers from an iopv in each performance metric e.g.
response time there is a large percentage of cto suffering from iopv when considering five performance metrics.
on the other hand the percentage of pairs of tests and options that suffer from aniopv is larger than the percentage of pairs of tests and options that do not face an iopv forcassandra across all the performance metrics.
the result of high percentage is relative to the number of values of each cto.
for instance cassandra has a larger percentage ofcto that suffer from iopv.
we find that there are a lot of values of each option in cassandra .esec fse december san francisco ca usa jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang noted from table out of cto inhadoop and out of cto incassandra show a performance regression on at least one performance metric when the default configuration does not show any performance regression.
for instance in terms of response time we observe a performance regression on and out of and hadoop andcassandra cto respectively when the default configuration does not show any regression as shown in table .
as shown in the same table the performance metric that suffers the most from the iopv problem are the i o write in hadoop and cpu usage in cassandra .
in addition these are not minor regression differences as of the regressions are large based on our effect size analysis.
in almost all cases an iopv result for a t wise combination of options correlates with an iopv for at least one of the individual options for all evaluated performance metrics.
for instance and out of a total of t wise cto show such a correlation in terms of response time for hadoop andcassandra .
similarly the iopv of and hadoopt wise cto correlate with iopv of their constituent individual options in terms of cpu memory i o read and i o write respectively.
we find similar numbers for cassandra .
such results imply that if there is aniopv issue manifested by a combination of options i.e.
a t wise combination in practice the same iopv issue will be exhibited by one of the individual options.
on the other hand if none of the individual option manifests an iopv issue their combination is not likely to manifest any iopv issue either.
for this reason the rest of the paper focuses on the individual cto.
pq2.
how difficult is it to manually identify iopv issues?
motivation.
the goal of this preliminary question is to understand how difficult the manual prediction of iopv i.e.
identification of iopv without running the tests is.
for instance the higher the number of options that manifest an iopv in a large number of pairs of commits and tests the more difficult the identification of iopv is as it indicates that an iopv can occur in an unexpected way and any option can be responsible for such a problem.
the lower the number of options that suffer from an iopv the easier it is to test all of these iopv responsible options.
approach.
to investigate the difficulty of identifying an iopv we first study the prevalence of iopv in different granularity i.e.
commit test and option.
second we calculate the intersection of the test option iopv triplets between each pair of commits using the jaccard similarity defined as follows j c1 c2 ctoc1 ctoc2 ctoc1 ctoc2 wherec1andc2refer to every pair of commits both consecutive and non consecutive commits .
ctoc1 ctoc2 is the number of cto that share the same test option iopv i.e.
the intersection .
ctoc1 ctoc2 is the total number of unique test option iopv in commits c1andc2 i.e.
the union .
the jaccard distance ranges between and where a value of means that the pair of commits share the same test option iopv triplets while indicates that the pair of commits does not share any test option iopv triplet.result.
iopv problems are hard to manually predict.
the results of the prevalence of iopv in different granularity are shown in table .
in particular out of commits in hadoop and out of commits in cassandra show at least one cto with an iopv in at least one performance metric.
similarly out of options in hadoop and out of options in cassandra suffer at least once from an iopv through the studied commits.
table shows more details about how common are iopv for the studied commits tests and options.
in summary our results indicate that the iopv problem is not limited to a small set of commits tests or options which makes it challenging to predict which cto would have an iopv.
table number of unique commits tests and options with iopv problems.
commit test option total iopv total iopv total iopv hadoopres.
time cpu memory i o read i o write any metric cassandrares.
time cpu memory i o read i o write any metric even if most of the commits show at least one iopv it is not easy to predict which test and option may suffer from the iopv.
figure shows the pairwise jaccard distance between the test option iopv triplets of the studied commits in the hadoop and cassandra systems respectively.
the figures indicate that most of the commits do not share any test option iopv i.e.
with dark cells especially for the cassandra system i.e.
more dark cells .
therefore it is difficult for developers to manually identify which tests and options that they need to run and configure to verify the existence of iopv.
a hadoop response time b cassandra response time figure pairwise jaccard distance between the test option iopv triplets of the studied commits of the hadoop andcassandra system.
the x axis andy axis show the studied commits.
each cell refers to the jaccard distance of any pair of commits the darker the color is the larger the distance is.iopv on inconsistent option performance variations esec fse december san francisco ca usa to understand why different commits show inconsistent test option iopv triplets we manually analyze some commits that show the largest jaccard distance from other commits.
in particular there are two and six commits with large jaccard distance .
to all other commits in hadoop and cassandra respectively.
for hadoop we pick up one out of the two commits ofhadoop to manually examine the impacted tests configuration option and the commit changes.
we find that the studied options that cause iopv are related to connection time such as the options dfs.ha.fencing.ssh.connect timeout andfs.s3a.connection.timeout .
by examining the code in the test testkms.java we find that testkms.java loads the connection timeout configuration options.
and the code changes in this commit trigger the test case in the test testkms.java .
thus the commits that impact such connection time related options and the test may lead to iopv problems while other commits may not lead to the same iopv.
for cassandra we select one commit with the largest jaccard distance to other commits.
our results show that two tests named embeddedcassandraservicetest anddebuggablescheduledthreadpoolexecutortest manifest the largest performance regression regarding options max hints file size in mb and memtable heap space in mb respectively.
by manually examining the commit changes covered by the tests we find that there exist code changes in the method start within the java file embeddedcassandraservice.java .
such code changes trigger the test cases to load and initialize options in the impacted tests embeddedcassandraservicetest anddebuggablescheduledthreadpoolexecutortest which lead to performance regression.
in particular of commits inhadoop and of commits in cassandra have a jaccard distance more than .
.
such results imply that different commits may lead to different options and tests that exhibit iopv problems.
summary of preliminary study iopv is a common problem in our studied systems and it is difficult to manually identify iopv without exhaustively running the tests.
our results suggest the need for an approach that identifies which cto manifests an iopv.
predicting iopv problems rq1.
can we accurately learn iopv issues in the studied systems?
motivation.
this research question is to evaluate different classification approaches on predicting for which cto one has to check multiple option s values.
in our preliminary study we observe that theiopv is common and hard to manually predict which indicates that developers need to test different values for each option.
however as there are typically a large number of configuration options e.g.
hadoop version .
.
has configuration options with different possible values exhaustively experimenting with all different options for each test in performance testing is time and resourceconsuming.
in this rq we aim to reduce the effort of conducting configuration aware performance testing by predicting the need for testing with different values for a given configuration option when a code change is made i.e.
for a cto .
specifically our approach predicts whether a cto manifests an iopv such that developerscan make an informed decision on whether they should consider different values for that option in their performance testing.
approach.
in this rq we follow the detailed steps to build ml models to predict whether a cto manifests an iopv.
step .
data preparation.
our target variable is a binary variable that indicates whether a cto manifests an iopv which we obtained following the approach discussed in section .
we consider four dimensions of software metrics that are related to the likelihood of a configuration option impacting the performance testing of a code commit for each test i.e.
of a cto .
table lists the detailed metrics used in our models.
chen et al.
find that code structure and code change dimensions are important for predicting performance regressions however they did not consider the impact of different configurations on the manifestation of performance regressions.
therefore we use the prior dimensions as well as an additional dimension about the configuration options.
next we pre process the features.
the code token metrics include thousands of unique code tokens.
thus we need to preprocess such metrics into a numeric representation.
we consider three different approaches to pre process the code token metrics.
term frequency inverse document frequency tf idf tfidf generates a feature for each unique token.
the value of a feature for a commit is the term frequency of the corresponding token i.e.
tf t c ft c whereft cis the number of times a token tappears in commit c times the inverse frequency of the commits that contain the token idf t log n nt wherenis the total number of commits while ntis the number of commits containing the tokent.
principal component analysis pca using tf idf generates a large number of features that may lead to very complex models.
therefore we apply pca on the features resulting from tf idf to reduce the number of features.
word embeddings we use word2vec to code each token into a vector of numerical values.
specifically we pre train the embeddings from a large code base then apply the pre trained embeddings on the tokens in our data.
step .
model construction.
we build machine learning models to predict whether a configuration option suffers from an iopv on a given cto.
for the generalization of our results we consider five different types of models including random forest rf logistic regression lr xgboost xg neural network nn and convolutional neural network cnn .
a random forest is a classifier consisting of a collection of decision tree classifiers .
logistic regression is a statistical model that uses a logit function to model a binary variable as a linear combination of the independent variables which is widely used in software analytics .
xgboost is an efficient and accurate implementation of the gradient boosting algorithm .
the neural network model used in our study consists of four layers and is trained with batch size and epochs.
the cnn model in our study consists of five layers and are trained with batch size and epochs.
step .
model evaluation.
we use fold cross validation to evaluate the performance of our models.
in each repetition of the fold cross validation the whole data set is randomly partitioned into sets of roughly equal size.
one subset is used as the testing set i.e.
the held out set and the other nine subsets are used as the training set.
we train our models using the training set and evaluate the performance of our models on the held out set.
in each foldesec fse december san francisco ca usa jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang table overview of our selected metrics.
dimension metric rationale code changenumber of modified subsystems the more subsystems are changed the higher risk the change may be .
number of modified directories changing more directories may more likely introduce performance regressions .
number of modified files changing many source files are more likely to cause performance regressions .
modified code across files scattered changes are more possible to cause performance regressions .
number of modified methods changes altering many methods are more likely to introduce performance regressions .
number of lines soc in tests program with more lines is more likely to suffer from performance regressions .
lines of code added the more lines of code added the higher risk that the program will suffer from performance regressions .
lines of code deleted the more lines of code deleted the higher risk of performance regression is introduced .
code structurenumber of methods in impacted test program with a large number of methods is more likely to suffer from performance regressions.
mccabe cyclomatic complexity program with higher complexity is more likely to suffer from performance regressions .
number of called subprograms large called subprograms will amplify the regressions if there exist regressions in the called program .
number of calling subprograms large calling subprograms will amplify the regressions if there exist regressions in the called program .
code token code tokens of the changed code some code tokens may be more related to performance than other tokens.
option token split configuration option names the name components of a configuration option may be related to a specific performance metric.
of the cross validation we use precision recall and area under the receiver operating characteristic curve auc to measure the performance of our models.
result.
our models can effectively predict when a cto is manifesting an iopv for all of our five studied performance measures as shown in table .
our best models i.e.
as indicated by the bold italic values achieve an auc of .
to .
on the hadoop project and .
to .
on the cassandra project for different performance metrics.
for the hadoop project rf is the best model for four out of the five performance metrics achieving an auc of .
to .
.
grebhahn et al.
find that random forest performs better compared to other prediction models.
even if xg shows the best auc performance for the fifth performance metric i.e.
response time the difference between rf and xg is only .
.
for the cassandra project rf shows the best performance on three out of five performance metrics.
nn shows the best performance on also three performance metrics memory and i o read have the same performance as the rf model .
the average auc of the best nn model is .
while the average auc of the best rf model is .
.
note that nn on the other side requires a large amount of resources to train and test a model while the improvements it shows over rf is trivial.
cnn shows the best performance on only one performance metric i.e.
with an auc of .
for the response time .
however the average auc of the best cnn model is .
lower than that of rf.
in summary we suggest that developers consider the rf model for predicting when a cto has an iopv problem.
the choice of representation of the code tokens significantly impacts the performance of our models.
for the traditional models rf lr and xg using code embeddings to represent the code tokens often achieves the best performance while using pca usually results in the worst performance.
for example for the hadoop project the rf model achieves an auc of .
to .
using code embeddings .
to .
using tf idf and only .
to .
using pca.
the reason for the poor performance of the models using pca might be that pca significantly reduced the information in the tokens through dimension reduction even though we considered the principal components that account for of the variance intable results of modeling whether a cto manifests iopv .
hadoop rf with tf idf rf with pca rf with code embedding xg with tf idf pre.
recall auc pre.
recall auc pre.
recall auc auc res.
time .
.
.
.
.
.
.
.
.
.
cpu .
.
.
.
.
.
.
.
.
.
memory .
.
.
.
.
.
.
.
.
.
i o read .
.
.
.
.
.
.
.
.
.
i o write .
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
cassandra rf with tf idf rf with pca rf with code embedding nn with pca cnn with pca pre.
recall auc pre.
recall auc pre.
recall auc auc auc res.
time .
.
.
.
.
.
.
.
.
.
.
cpu .
.
.
.
.
.
.
.
.
.
.
memory .
.
.
.
.
.
.
.
.
.
.
i o read .
.
.
.
.
.
.
.
.
.
.
i o write .
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
only full results of the random forest models and the auc values of models with at least one best result are presented.
the original variables.
in contrast for the deep neural network models nn and cnn using pca to represent the code tokens may achieve better results than the other two representations.
for example for the cassandra project the cnn model combined with pca achieves the best auc for two out of the five performance metrics across all different models.
the reason might be that there are a larger number options in our studied systems in the deep neural network models while using pca could significantly reduce the number of options to be trained.
summary of rq1 our models can effectively predict whether a cto manifests an iopv problem.
random forest based on code embedding shows the best performance on predicting iopv for most of the performance measures.iopv on inconsistent option performance variations esec fse december san francisco ca usa rq2.
what are the most important metrics for predicting iopv issues?
motivation.
the goal of this research question is to analyze the models of rq1 that predict the iopv to understand the factors that play the most important role in determining whether an option could manifest an iopv.
in particular we focus on the random forest model with code embeddings as it shows the best performance in predicting iopv.
our results can help practitioners understand and identify the scenarios where they need to adjust their configuration parameters during their performance tests.
approach.
to analyze the most important metrics for predicting iopv we analyze the impact of each dimension.
different projects have different features as the code tokens and configuration options are different.
to make our results more generalizable we measure the important features at the dimension level instead of feature level.
in particular we consider the following experiments measuring the importance of each dimension of metrics by removing the dimension from the model.
in order to study the importance of each dimension of metrics we build a model with all dimensions and compare it to a model with one dropped dimension at a time.
that comparison consists of statistically comparing both models auc values.
the larger the difference is for a dimension the more important that dimension is.
measuring the importance of each dimension of metrics by only keeping the dimension in the model.
since metrics from different dimensions can be correlated we also consider comparing models that are built using one dimension at a time.
for example some tokens from the code token dimension can be correlated with tokens from the configuration dimension.
therefore we build a model using one dimension at a time which results in four models.
finally for each model with different dimensions of metrics we have ten values of auc since we use fold cross validation.
we compare all these models based on their respective auc values.
in particular we use the mann whitney u test to examine whether there is a statistically significant difference between the original model with all dimensions of metrics and other models with partial metrics.
result.
every dimension of metrics plays a statistically significant role in predicting iopv cases.
table shows the results of using the mann whitney u test to compare the complete rf model with the rf model that uses only one dimension of metrics or that excludes one dimension of metrics.
a p value that is smaller than .
indicates a statistically significant difference.
table shows that when only keeping one dimension of metrics all the resulting models show a statistically significant different worse performance.
when excluding each dimension of metrics the resulting models show a statistically significant different worse performance in most of the cases in out of the combinations of the four metric dimensions and the five performance measures forhadoop and in out of the combinations for cassandra .
our results highlight that one should consider all the four dimensions of metrics together when building a model to predict which cto manifests an iopv.
the code token and configuration dimensions show the best performance among the four dimensions of metrics.
for both hadoop andcassandra for all the performance measures usingtable the results p values of using the mann whitney u test to statistically compare the auc of rf with the complete set of metrics vs. with a subset of metrics.
hadoop without cc without cs without ct without con res.
time .
.
.
.
cpu .
.
.
.
memory .
.
.
.
i o read .
.
.
.
i o write .
.
.
.
only cc only cs only ct only con res.
time .
.
.
.
cpu .
.
.
.
memory .
.
.
.
i o read .
.
.
.
i o write .
.
.
.
cassandra without cc without cs without ct without con res.
time .
.
.
.
cpu .
.
.
.
memory .
.
.
.
i o read .
.
.
.
i o write .
.
.
.
only cc only cs only ct only con res.
time .
.
.
.
cpu .
.
.
.
memory .
.
.
.
i o read .
.
.
.
i o write .
.
.
.
cc is code change cs is code structure ct is code token and con is configuration.
only the code token metrics or the configuration metrics in the model achieves a better auc than using other single dimension of metrics except that the configuration dimension leads to a relatively worse performance for the i o write measure of cassandra .
the results indicate that the context of the change as well as the goal of configuration options expressed through their tokens are the most important predictors for iopv.
however when we exclude one dimension of metrics from the model the resulting differences are less significant and removing the code tokens and the configuration dimensions in fact does not lead to the worst performance.
for example removing the code change dimension from the model for the response time measure of hadoop actually leads to worse performance than removing the code tokens dimension.
this is because the different dimensions of metrics are correlated even after correlation analysis thus the impact of removing one dimension of metrics may be partially mitigated by other dimensions of metrics.
summary of rq2 every dimension of metrics plays a statistically significant role in predicting whether a cto manifests an iopv problem.
the most important dimensions are related to code tokens and configurations.esec fse december san francisco ca usa jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang discussion in this section we discuss the learned lessons during the implementation of our approaches the generalizability of our study and the application of our approach for predicting iopv in interaction of configuration options.
.
generalizability of our study adding more case studies can benefit the generalizability but it still may not address the generalizability issue.
below we discuss some aspects that may impact the generalizability of our study.
option quantity our approach aims to predict the inconsistent option performance variation issue automatically.
we find that there are and configuration options in hadoop andcassandra respectively.
if the number of configuration options in a system is small e.g.
less than ten options our approach may not have a practical impact for practitioners as practitioners can examine the small limited number of configuration options manually.
test and option coverage test and option coverage.
our approach depends on the readily available small scale tests in the software systems.
if the tests cannot cover the source changes and impacted options our approach may fail to predict iopv.
since our approach works at the commit level only the changed methods need to be covered by the test.
we find that for all the changed methods in all commits and are covered by the tests in hadoop andcassandra respectively.
such changed methods cover a total of and options in hadoop andcassandra respectively.
such high coverage ensures the success of our approach.
this also implies that in order to adopt our approach practitioners may first evaluate whether the source code that is likely to be changed is covered by tests.
test quality we use the existing available small scale tests to evaluate the performance variations.
prior research study the use of performance unit tests to increase performance awareness.
if the existing test is written with a sub optimal quality the performance results may be biased.
for example the test failures in the flaky test may introduce noise and require extra running time to achieve stable performance results.
recent research discusses the reasons for tests not suitable for performance evaluation which can be leveraged to know how well other projects can adopt our approach.
.
the application of our approach for predicting iopv in interaction of configuration options based on the findings from the study performance bugs are often related to configurations.
the results from study show that the majority of parameter configuration bugs is related to only one option about of studied configuration bugs involve two or more configuration options.
therefore on the one hand our approach can be directly used to predict the majority of configuration aware performance issues.
on the other hand our measured data and our approach can be also partially toned to predict a combination of configuration aware performance issues.
we have executed cto instances.
even if our measured data cannot represent all the interactions our measured performance data covers part of interactions of options like two way and n wayoptions.
for example we assume that there are two options o1 and o2.
the possible values of o1 are defaulting to and the possible values of o2 are true and false defaulting to false.
our performance data covers the following pairwise option values between o1 and o2 false false false true .
on the other hand our existing performance data misses the following pairwise option values true and true .
n way testing is a kind of combinational test that requires that every combination of any n options in the software must be tested at least once.
threats to validity in this section we discuss the threats to the validity of our study.
external validity.
due to the expensive computing resources needed we spent around machine hours collecting performance data we conducted our evaluation on two open source software systems i.e.
hadoop and cassandra .
our findings may not generalize to other software systems.
however we found motivating results on the prevalence of iopv and the performance of our prediction model which can be replicated by future studies on other software systems.
internal validity.
in our approach we collect five popular performance metrics i.e.
response time cpu memory i o read and write while other performance metrics such as throughput can still be explored by future research.
we do not consider the combination of configuration options as that will require a huge cost and the goal of our study is to identify and define the iopv issues.
on the one hand prior work mentions that of the performance issues are due to a single option so our paper covers the most common cause of performance issues.
on the other hand one can use covering arrays to conduct n way testing for functional tests with very low number of cases .
however for performance testing the approach likely does not work since we want to isolate each combination s performance impact from others.
we encourage future studies to extend our work by considering the interaction of configuration options.
construct validity.
the stability of the cloud based testing environment may cause testing noise.
to minimize the noise we capture the performance of the corresponding linux processes of the running tests.
furthermore for each test we repeat the execution times independently.
finally we run all of our experiments in the same environments.
there may still exist extreme values as outliers that should not be considered by our approach.
to mitigate this threat we remove the outlier data using the mean standard deviation std as an indicator of outliers.
related work in this section we discuss prior works along three dimensions performance regression detection performance model for configurable system and identifying optimal configuration for performance.
.
performance regression detection performance regression detection techniques can be divided into two categories measurement based and model based detection.
measurement based approaches compare performance metrics e.g.
iopv on inconsistent option performance variations esec fse december san francisco ca usa cpu usage between two consecutive versions to detect performance regressions.
for example nguyen et al .
leveraged control charts to identify performance regressions.
foo et al .
proposed an approach that compares a test s performance metrics to historical performance metrics.
model based approach builds a machine learning model with a set of performance metrics to detect performance regressions.
bodik et al .
leveraged a logistic regression model to model system users behavior to improve cohen et al .
s model .
foo et al .
proposed an approach that uses ensembles of models to detect performance regressions in heterogeneous environments.
our work complements this line of research in the sense that we consider the configuration aspect of configurable software systems.
this paper sheds light on the iopv problem by first quantifying the existence of inconsistent performance variations then proposing a prediction model that identifies the commits tests and options that exhibit the iopv problem.
.
performance model for configurable system many prior research has been conducted on predicting performance for configurable software systems.
m hlbauer et al.
build performance model to identify performance changes in software performance evolution.
unlike the latter work which analyzes a software system as a whole our work considers the impact of configurability on individual test cases implying that our work is more fine grained.
because our focus is on test cases it may aid in attributing performance variation not only to configuration options but also to tested functionality.
such prior study provides an evidence that performance changes during software evolution which motivates our study.
jamshidi et al.
employ transfer learning to learn performance model across environments.
deepperf uses a deep feed forward neural network to model configurable software systems.
the existing studies use the historical revisions performance or the sampled configuration s performance to build a model to estimate the performance of future revisions.
different from prior studies our prediction model does not require training on the historical performance data but rather identifies inconsistent option performance variations as software evolves.
in general our study is orthogonal to the above approaches and our measured performance data can be used in future research on performance.
.
identifying optimal configuration for performance a large body of research has been conducted on performance optimization by finding optimal configurations .
siegmund et al.
build mathematical models that describe the impact of a configuration on software performance based on each option s value.
raghavachari et al.
propose an iterative approach to identify an optimal configuration in terms of performance.
guo et al.
leverage non linear regression to suggest an optimal configuration.
nair et al.
conduct several studies to find well performing configurations using rank based and sequential model based approach.
oh et al.
propose a truly random sampling to search configurations recursively to find near optimal configurations without building a performance model.
kim et al.
present a lightweight tool to prune unnecessary configurations for test execution but they only take into account boolean options.
other efforts identified the optimal configuration options in terms of performance by leveraging existing optimization approaches i.e.
iterative search multi objective optimization and smart hill climbing .
our goal is neither to identify optimal configurations nor to debug configuration related performance issues.
in particular we focus on understanding whether a performance improvement or regression is consistent through all the values of an option.
that is important as one can improve the performance of a software system or release new changes that do not impact the performance under one configuration when other configurations hide a performance regression.
furthermore prior work on this line of research compares the absolute performance between two values for the same option while this can be subjective as discussed earlier.
one option s value can naturally consume performance as it enables the execution of additional features.
however performance comparison need also considers historical performance data.
conclusion the performance improvement or regression of a software change might not be equally manifested through all the possible configuration options values which we refer to as the problem of inconsistent option performance variation iopv .
in this paper we observe that iopv is a common problem which is difficult to manually identify without running exhaustive tests because most of the commits do not share similar options or tests that may lead to iopv and hide performance regressions.
we also observed that predictive models e.g.
rf can effectively predict the iopv problems using four dimension of metrics that are related to code changes code structures code tokens and configurations.
our findings highlight the importance of considering different configurations when performing performance regression detection and that leveraging predictive models can mitigate the difficulty of exhaustively considering all configurations of a system during such a process.
we expect that our study inspires a wide spectrum of future studies on configuration aware performance regression detection.
data availability the link to our replication package can be found here .esec fse december san francisco ca usa jinfu chen zishuo ding yiming tang mohammed sayagh heng li bram adams and weiyi shang