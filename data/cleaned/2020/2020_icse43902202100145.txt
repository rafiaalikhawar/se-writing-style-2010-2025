pyart python api recommendation in real time xincheng he lei xu z xiangyu zhangy rui hao yang feng and baowen xu state key laboratory for novel software technology nanjing university china ypurdue university usa xinchenghe2016 gmail.com xlei nju.edu.cn xyzhang cs.purdue.edu rui.hao.gm gmail.com fengyang nju.edu.cn bwxu nju.edu.cn abstract api recommendation in real time is challenging for dynamic languages like python.
many existing api recommendation techniques are highly effective but they mainly support static languages.
a few python ides provide api recommendation functionalities based on type inference and training on a large corpus of python libraries and third party libraries.
as such they may fail to recommend or make poor recommendations when type information is missing or target apis are projectspecific.
in this paper we propose a novel approach pyart to recommending apis for python programs in real time.
it features a light weight analysis to derive so called optimistic data flow which is neither sound nor complete but simulates the local data flow information humans can derive.
it extracts three kinds of features data flow token similarity and token co occurrence in the context of the program point where a recommendation is solicited.
a predictive model is trained on these features using the random forest algorithm.
evaluation on popular python projects demonstrates that pyart can provide effective api recommendations.
when historic commits can be leveraged which is the target scenario of a state of theart tool arirec our average top accuracy is over and average top accuracy over outperforming apirec and intellicode i.e.
the recommendation component in visual studio by .
.
for top accuracy and .
.
for top accuracy.
in other applications such as when historic comments are not available and cross project recommendation pyart also shows better overall performance.
the time to make a recommendation is less than a second on average satisfying the real time requirement.
index terms api recommendation context analysis data flow analysis real time recommendation python i. i ntroduction apis are widely used in modern software development to simplify the process of software implementation and maintenance.
recently many approaches have been proposed to provide intelligent api recommendation.
however most existing api recommendation approaches are mainly for statically typed languages such as java.
few can provide effective and efficient api recommendations for dynamic languages such as python and javascript due to the difficulties in handling their dynamic features.
python is one of the most popular programming languages1.
many popular machine learning and deep learning programming platforms such as tensorflow and pytorch support applications written in python.
as such an effective api recommendation solution for python is of importance.
zcorresponding author it is challenging to achieve the goal.
python applications are dynamically typed.
the type of a variable is not explicitly declared and hardly known until runtime.
traditional type inference techniques may not be effective for python because the type of a variable at a given program point may change along different paths and or with different inputs as python allows changing types attributes and methods of objects in an arbitrary fashion.
the lack of type information greatly degrades the accuracy of traditional static analysis on python and increases the uncertainty of api recommendation which heavily relies on these analyses.
for example in order to handle path sensitivity traditional static analysis typically merges results along different paths even if many of them are bogus.
as such a variable may have a long list of types and thus the ide integrated development environment has to provide an over sized list of recommendations to cover all these possible types.
besides type inference many traditional static analyses such as data flow analysis and alias analysis have difficulty handling python too.
many existing api recommending approaches are built on these analyses and hence do not support python.
for example data flow features are widely used in existing api recommendation approaches .
however it s challenging to extract accurate data flow for python programs.
existing python data flow analyses tools produce substantial false dependencies .
hence as we show in section iv they lead to poor recommendation accuracy.
furthermore providing real time api recommendation poses additional challenges.
recommendations are solicited during development where the syntax and semantics of the current context of a recommendation point i.e.
an api invocation point where the developer expects the ide to fill in are incomplete.
hence it is challenging to perform sophisticated static analysis.
developers often expect the ide to make instant recommendations which preclude any heavyweight online analysis.
a few python ides provide api recommendation functionalities.
for example pycharm relies on python skeletons2and typeshed3to make api recommendations.
visual studio intellicode4leverages machine learning to learn programming ieee acm 43rd international conference on software engineering icse .
ieee patterns from a huge repository of python projects.
as we will show in section ii they still have various drawbacks caused by the aforementioned challenges.
for example intellicode focuses on learning apis of standard libraries and popular third party libraries it can hardly handle apis defined within the same project.
a state of the art api recommendation research prototype for java is apirec .
given an invocation for which the developer wants api recommendation apirec first extracts a bag of fine grained atomic code changes and a set of code tokens that precede the current editing location.
then it computes the likelihood score for each candidate api by looking up the co occurrence frequency of code changes and tokens in a large corpus.
it heavily relies on fine grained code changes that happen on abstract syntax tree ast nodes and hence the accuracy of recommendation results is heavily dependent on the accuracy of ast differencing tools and the quality of code change histories.
in this paper we ported apirec to python and use it as a baseline.
our results show that it is not as effective as on java due to the inherent challenges of analyzing python programs.
in this paper we propose a new real time api recommendation approach for python called pyart py thon a pi recommendation in real t ime .
compared to existing solutions pyart can recommend both library apis and apis defined within the same project it does not rely on any third party tools it is lightweight and provides recommendations without noticeable delay and it delivers recommendations with good accuracy.
the key to the success of many existing api recommendation projects lies in the use of high quality data flow information .
they largely utilize data flow analyses in mature compilers analysisinfrastructures which tend to be conservative as they were designed to ensure safety in code transformation.
such conservativeness is substantially aggravated by the uncertainty in dynamic languages.
we observe that the conservativeness is not necessary in our context because the essence of api recommendation is to precisely model the joint distribution of apis and their surrounding syntactic entities and semantic properties.
the semantic properties may not need to be as accurate and complete as in the traditional application scenarios e.g.
code transformation and bug finding as long as the approximate ones and the apis have regularity in their joint distribution.
as such we develop a technique to extract so called optimistic data flow from the context preceding a recommendation point.
this data flow is derived in a way similar to how humans reason about data flow heavily relying on local variables function ids syntactic structures and ignoring global effects such as those by aliasing.
specifically our technique first collects a comprehensive list of candidate apis without relying on type inference tools like intellicode does.
it then extracts and encodes three kinds of features in the context of the recommendation point optimistic data flow token similarity along data flow that measures if the dataflow paths reaching the recommendation point involve some token similar to a candidate api and token co occurrence thatmodels the joint distribution of a candidate api with a token in its neighborhood.
during training these features are used to construct a predictive model based on random forest .
during deployment these features are provided to the trained model to generate a ranked list of recommendations.
our main contributions are summarized as follows we propose to derive optimistic data flow that is neither sound nor complete but sufficient for api recommendation and cost effective to collect.
we propose to use three kinds of features data flow token similarity and token co occurrence.
we also develop a method to encode them to feature vectors.
we develop a prototype pyart based on the proposed idea and evaluate it on real world projects.
the evaluation results demonstrate that pyart can provide effective api recommendations.
when historic commits can be leveraged which is the target scenario of a state of theart tool arirec our average top accuracy is over and average top accuracy over outperforming apirec and intellicode by .
.
for top accuracy and .
.
for top accuracy.
in other applications such as when historic comments are not available and cross project recommendation pyart also shows better overall performance.
our datasets and source code are available on github5.
the rest of this paper is organized as follows the motivating example is presented in section ii.
the technical details of pyart are described in section iii.
the evaluation for our approach is shown in section iv.
related work and conclusions are in section v and section vi.
ii.
m otivating example api recommendation approaches based on machine learning tend to have better performance for python than most traditional methods .
however existing machine learning based approaches have some limitations.
we take the recommendation results of visual studio intellicode one of the state of the art recommendation tools for python as an example to demonstrate these limitations figure .
observation .
the ability to recommend apis is heavily dependent on the type inference result for the object of the api call.
most recommendation tools collect candidate apis according to the type of the object at the recommendation point.
for instance type of aina.f .
however due to the dynamic nature of python it is hard to precisely infer object type.
if the type cannot be inferred intellicode cannot produce any recommendation results.
for example at the recommendation point kwargs.
in figure a .
that is assume the developer has keyed in kwargs.
and he she wants intellicode to recommend the api that is supposed to call which shall be get ... as shown in the final form of the code .
however intellicode 1635fig.
.
motivating example cannot provide any recommendation due to the lack of type information about kwargs .
observation .
even if the object type can be successfully inferred it is possible that the recommendation list contains too many candidates that do not have an order better than the alphabetical order.
the problem typically occurs at recommendation points for apis beyond standard libraries.
intellicode is trained based on massive data from github to learn the most frequent programming patterns.
it does very well in recommending frequently called apis but not the project specific ones.
for example at the recommendation point self.
in figure b the target api is one of the methods defined in a class of the current project.
intellicode provides all of the callable methods of the class in the alphabetical order as the recommendation.
although the list contains the right answer crawl it ranks too low to be found.
the main reason is that intellicode is largely machine learning based.
it is very likely that it has not seen or trained on many project specific apis.
as such its recommendation cannot be more informative than listing all callable methods.
observation .
even if intellicode manages to type the object and determines a nontrivial order with some candidates labeled with ?
to indicate high confidence the recommendations can nonetheless be wrong.
the problem occurs due to the uncertainty in machine learning.
that is even though intellicode has seen a similar api invocation context the api that it learned may not be the one intended for the current recommendation point.
for example at the recommendation point pkg resources.
in figure c intellicode provides a list of specially recommended apis labeled by ?
in which the top answers are not expected but are more frequently used in the training corpus than the right api.
we propose a novel api recommendation technique specifically designed for python.
it was shown that data flowis extremely useful in improving api recommendation results .
however existing techniques heavily rely on some precise data flow analysis engine which is very difficult for a dynamic language like python.
traditional data flow analyses tend to be conservative due to their application context i.e.
compilation .
for example if they cannot determine if a read and a preceding write must access different objects they conservatively assume that there is data flow between the two.
as such they tend to generate substantial false positives for dynamic languages where type inference and points to analysis are much more challenging.
this substantially degrades the recommendation results.
the key observation is api recommendation can be formulated as a distribution modeling problem that does not require using conservative analysis .
in other words we aim to determine the probabilities of different api candidates at a recommendation point based on various kinds of hints that do not have to be sound or complete as long as the joint distribution of the hints and the apis can be precisely modeled.
our method leverages three kinds of hints features optimistic data flow token similarity along data flow and token co occurrence and uses random forest to learn the joint distribution of api functions to recommend and these hints.
the learned model is then used to make recommendation.
optimistic data flow.
optimistic data flow is data flow that appears to be true.
the derivation of such data flow is not through conservative standard data flow analysis but rather in a way similar to how humans derive data flow.
specifically a set of data flow derivation rules are defined for various syntactic structures such as function calls and loops.
we say these rules are optimistic as they do not consider the possible disruption injection of data flow induced by aliasing.
instead they derive data flow that appears to be true from variable ids and control flow structure .
for example in figure b pyart derives the optimistic data flow at the recommendation point as self!targetapi!statusjnewtasksjresult .
here the arrows denote the data flow direction and targetapi denotes the api we want to predict.
observe that it only includes the 1636data flow that can be derived before the api invocation to simulate the real time api recommendation scenario where only the context before recommendation point is available.
observe that the data flow is optimistic as it may not be true.
for example there may not be any data flow from self to any of the result variable.
neither is it complete.
similarly for figure c pyart extracts the optimistic data flow pkg resources!targetapi!entry point .
optimistic data flow paths are universally extracted at all statements and linked together if possible.
pyart then leverages the similarity of the data flow paths at the recommendation point with those that it has seen in other places during training to help make prediction.
token similarity along data flow.
pyart also assumes that an api should have token similarity with some variable function along an optimistic data flow path extracted above.
as such for each candidate api it measures the similarity scores between the candidate and tokens in the dataflow paths.
for example in figure c the similarity score between the target api iter entry points and its neighbor token in the data flow entry point is higher than other candidate apis which provides strong hint for the recommendation.
token co occurrence.
in the third kind of hints pyart leverages co occurences of tokens.
that is it predicts an api based on the tokens preceding the recommendation point in the same source file .
this includes the enclosing function preceding variables preceding function invocations etc.
note that these tokens may not have data flow with the target api.
for example for a recommendation point with open ... as f f. candidates read andwrite are more likely to be invoked due to the high co occurrence frequency with token open .
in figure a pyart computes the co occurrence of candidate api with each token in the bag fdef crawl self url none track kwargs if g. and in figure b pyart computes the co occurrence of candidate api and each token in the bag fdef test 10notstatus status newtasks result self g. here a bag is a set of tokens before the recommendation point.
these three kinds of hints are encoded as feature vectors and used to train a predictive model using random forest.
iii.
a pproach a. overview the workflow of pyart is shown in figure .
basically pyart takes python code context before the recommendation point as an input which consists of statements preceding the recommendation point in the same file and outputs a ranked list of recommended apis to developers.
it is composed of five main components api recommendation point locator data extractor data encoder model constructor and api recommender .
given a python code context the api recommendation point locator first identifies the recommendation point in the form of .targetapi .
the candidate generator in the data extractor first performs static type inference for the object of the target api i.e.
with anexisting light weight type inference tool pytype6.
if the type inference is successful the candidate generator takes all the callable methods of the inferred type as the api candidates.
otherwise it collects api candidates from the following three sources standard libraries imported third party libraries and all the callable methods declared in the current scope.
as such pyart would not yield an empty list even if the type inference is not successful.
for each recommendation point the feature collector in the data extractor analyzes the source code before the recommendation point to collects three kinds of hints i.e.
optimistic data flow token similarity along dataflow andtoken co occurrence .
then all the hints features are encoded to feature vectors by the data encoder.
each feature vector is a tuple of t t1 t2 t3 t4 witht1representing the optimistic data flow hint t2the token similarity along data flow hint and t3andt4the token co occurrence hints specifically t3the co occurrence of an api with the object that invokes the api and t4the co occurrence with other tokens.
in model training by the model constructor feature vectors are extracted from the projects in the training corpus and used to compose positive and negative samples.
positive samples are constructed with apis and their feature vectors and negative samples are composed of feature vectors with the api candidates other than the true positives identified by the candidate generator .
then a real time recommendation model is trained using the random forest algorithm.
it essentially learns a joint distribution of the hints and the apis.
in the process of api recommendation the api recommender uses the pretrained model to compute the probability piof each candidate api and ranks all these candidates according to pi.
due to the demand of real time api recommendation these steps have to be lightweight.
in the following we discuss a number of the important steps in details.
b. feature collector feature collection needs to address two main challenges due to the dynamic nature of python there are very few static analysis tools that are capable of providing accurate results for python applications in a cost effective manner and in the context of api recommendation it is mandatory to deal with partial code.
therefore pyart features an efficient way of extracting optimistic data flow around the recommendation point.
as mentioned in section ii such data flow is neither sound nor complete it just appears to be correct.
the idea is to simulate how humans reason about data flow.
it is supposed to be concise and largely precise.
our hypothesis is that it is sufficient to build a joint distribution for api prediction.
specifically humans infer data flow largely based on local symbolic information such as surrounding variables and program structures and rarely and in most case unable to reason about aliasing that is obscure and global.
specifically we define rules to derive optimistic data flow from five basic abstract syntax units aus assignment loop 1637python code context api recommendation point locatordata encodercandidate genetator feature collectorsample labeling training api recommender ranked apis developersdata extractormodel constructor23 61fig.
.
workflow of pyart table i optimistic data flow extraction rules rule name data flow derivation example assign v e f8u2vm e u!vg dfs v e.g.v e for v e f8u2vm e u!vg dfs v e.g.for v in e invoke u v fu!vg dfs v e.g.u v access v e f8u2vm e u!vg dfs v e.g.v para f e e e1 e n f8ei2e 8ui vm ei ui!fg dfs f e.g.f e1 e n aggregate ui ujprecedes 8v2ui uj dfs v ui v uj dfs v e.g.for v inu f e x propagation 12pred 8v2vm dfs v dfs v preservation 8v2vmnkill dfs v dfs v object attribute access invocation container access and function parameter passing.
as mentioned earlier optimistic dataflow does not have to be complete such that these rules are not intended to be comprehensive but rather model the intuitive methods humans use to infer data flow.
table i presents these rules.
in the table label 2lrepresents the location of each unit e2erepresents an expression fu v x yg vm represents a variable object or a method object f2frepresents a function in the code and ui uj2au represent instances of the abstract syntax units.
note that python considers any type an object including a method.
as such we can consider there is data flow between an object and the api it invokes.
the notations and indicate the program points immediately before and after the point labelled respectively.
for example vm e represents all the variable and method objects in expressionebefore the location labelled anddfs v contains all the data flow paths involving object vafter the location labelled .
the first column of the table presents the rule names the second column presents the corresponding data flow derivation and the last column presents examples for the corresponding aus.
in the first rule about assignment for any variable and method object uin the right hand side operand ebefore a location there is optimistic data flow from uto the lefthand operand v. the second rule for v e specifies data flow extraction for a loop there is data flow from anyvariable or method object in the iterator eto the loop variablev.
the third rule invoke u v is about attribute loading invocation if an object uaccesses a field attribute or invokes a method attribute v there is data flow between u andv.
rule access v e specifies data flow for container accesses if a container v such as a list a set or a class is accessed through an index e there is data flow from any objectuineto the container object v. rule para f e is for function parameter passing.
it specifies that any variable involved in any of the parameters e1 ... enhas data flow tof.
since the five units may occur in combinations rule aggregation aggregates the data flow relations derived from individual units.
note that they may form paths.
specifically for a variable or method object voccurring in both units uianduj the data flow of vafter location will contain both the data flow relations generated by uiand byuj before the location .
note that the subscripts denote the units which derive the relations.
for example for an expression for v inu f e x there are four units including the for loop attribute invocation and container access and parameter passing.
the feature collector combines results of each unit and outputs a long data flow sequence ejy!x !f!u!v.
in addition it models the effect of control flow following thepropagation rule for any location after the dataflow relations right before contain all those after .
the preservation rule retains all the data flow relations whose 1638variable or method objects are not affected by a unit and hence should be killed i.e.
removed .
for example the value of a global variable awill not be changed by any statement in a local block when the key word global is not given.
in this case data flow relations that point to ashould be killed.
these rules are strictly syntax and variable name driven and do not consider aliasing.
as such they can be performed locally whereas aliasing requires global analysis and even on partial code.
these are critical for real time recommendation.
according to our experiment in section iv these rules allow us to produce data flow with both high precision i.e.
.
.
and high recall i.e.
.
.
.
in comparison an existing analysis engine pysonar2 has very good precision up to but extremely low recall only .
.
meaning that it can hardly be used to make api recommendations.
our results using the optimistic dataflow show that having concise and largely accurate data flow is sufficient for achieving good recommendation outcomes.
in addition the feature collector also collects the tokens involved in the data flow relations for the later token similarity feature encoding and the tokens within individual source code files for the later co occurrence feature encoding .
they are elided due to simplicity.
c. data encoder the data encoder encodes the data extracted by the data extractor including the api candidates and the features.
as such the encoded information can be used for training during model construction and for recommendation during deployment.
for a candidate api the encoder generates a feature vector t t1 t2 t3 t4 that includes the three aforementioned features i.e.
optimistic data flow token similarity along dataflow and token co occurrence.
in the following we discuss how to encode individual features.
optimistic data flow encoding.
data flow propagates important information for api recommendation.
intuitively assume we have two objects aandband there is an assignment b a that induces data flow between the two.
according to python semantics binherits all the method attributes of a. in other words if we want to recommend an api for b e.g.
b we can get hints from what we should recommend for a and vice versa.
therefore in this part of encoding pyart aims to quantify the similarity between the data flow path leading to the recommendation point and some paths it has seen from the corpus.
specifically pyart encodes optimistic data flow as follows.
for a data flow path x0!x1!
!
api!
!
xnthat contains an api pyart collects the corresponding token sequence x0 x1 api x n and learns the order of tokens using a common statistical language model ngram.
with a fully pre trained n gram model built on plenty of data flow paths containing apis pyart is able to learn the implicit regularity between data flow and apis.
for instance the n gram model can predict an api from the preceding dataflow sub path.
however in pyart the model is not directly used to make recommendation but rather to encode feature.during deployment upon a recommendation request pyart encodes the features for each candidate api.
it first extracts all the data flow paths related to the current recommendation location through the data extractor.
since pyart focuses on real time recommendation we assume it only extracts dataflow before the current recommendation point.
after that the encoder appends the api to the extracted data flow which includes the tokens along all the data flow paths sorted by their distances to construct an input to the trained n gram model.
the model then outputs a log probability score p which is taken as the first element t1of the feature vector tof the api.
example.
for instance given a recommendation point for k v in dict.
pyart extracts two data flow paths ending at k andv respectively denoted as dict!tagetapi!kjvand then appends each candidate api to the list fk v dicgto construct an input to the n gram model.
among the inputs from different candidates the list denoting the true positive dict!items!kjvgets the highest log probability .
higher than the probabilities of others i.e.
from .
to .
.
the probability .
is used as the first element of the feature vector for the candidate api items .
encoding token similarity along data flow.
intuitively pyart considers that a candidate api is likely the intended one if it z sxcqdv1gfhweybsjq1 q v has similarity to some token along a data flow path reaching the recommendation point.
specifically for a data flow path x0!x1!
!
api!
!xn pyart produces a set of triples in the form of xi api d in whichxirepresents a token in the path and drepresents the distance between xiand api with i n d n. for a triple xi api d the encoder measures the similarity between api and tokevcn xi.
it also assigns a xicloser to api a larger weight for their similarity score .
for example in figure c pyart acquires a data flow path pkg resources!targetapi!entry point in which the true positive recommendation iter entry points fortargetapi shares a common sub string with the token entry point and is thus given a larger similarity score.
more precisely for a token xialong some data flow path to the recommendation point the encoder computes the similarity score between xiand a candidate api as follows.
sim xi api jlcsk xi api j d jxij japij note thatlcsk x api computes the longest common token sub sequence of xiandapi .
upon a recommendation request the encoder computes the total similarity score tosim dfs api between each candidateapi and the tokens in all the data flow paths that reach the recommendation point denoted by a set dfs as the second element t2of the feature vector tofapi using the following equation tosim dfs api p x2dfssim x api jdfsj 1639encoding token co occurrence.
for each candidate api in a recommendation request the encoder derives and encodes the token co occurrence feature in two aspects cooccurrence between the object whose api needs recommendation and the candidate called object api co occurrence co occurrence between tokens in the current context all the code up to the recommendation point and the candidate called context api co occurrence .
intuitively the first kind leverages the observation that the object and the api are the closest couple and their co occurrence distribution provides strong indication whereas the second kind aims to leverage a broader set of information.
for object api co occurrences the encoder replaces the object name with its type if available for better generality.
some example type api patterns are shown as follows list.append string.join and file.read .
they are frequently used in python projects.
more precisely the encoder computes the object api cooccurrence frequency of an object xand a candidate api as the third element t3of the feature vector tofapi with the following equation confidence x!api p apijx n api x n x n x n x heren x represents the number of occurrences of object x andn api x represents the number of co occurrences i.e.
x api .
for context api co occurrences the encoder considers all the tokens in the current context before the recommendation point.
for example the built in function open that aims to open a file always appears with some specific tokens like with as f since the statement is in the form of with open ... as f is widely used in software development.
in particular the encoder computes the co occurrence frequency of a token x and a candidate api with the following equation.
confidence x!api p apijx n api x n x n x n x heren x represents the number of files in which xoccurs andn api x represents the number of files in which xand api co occur.
for a set of tokens s of the current context before the recommendation point s fx0 x1 x ng the encoder computes the overall co occurrence frequency of sand a candidate api as the fourth element t4of the feature vector t with the following equation.
totalcnfd s api jsjx xiconfidence xi!api dist xi api heredist xi api computes the distance between xiand api .
recall a closer xihas a larger weight for their cooccurrence score.d.
training and recommendation model constructor.
the model constructor performs supervised learning using random forest.
the model learns to predict api from the feature vectors.
the training feature vectors are divided into two categories positive cases feature vectors generated from the true positive apis labeled as1 and negative cases feature vectors generated from other candidate apis labeled as .
api recommender.
for a recommendation request .
pyart first collects all the candidate apis.
for each candidate api it computes the corresponding feature vector.
then the api recommender takes all the candidate feature vectors as test cases and provides them to the recommendation model that is pre trained.
for each candidate vector the recommendation model produces a probability pithat the label of the vector is .
finally it ranks all the candidates according topiand outputs the ranked list to developers.
iv.
e valuation a. research questions to evaluate pyart we propose the following research questions.
rq1 how effective is pyart in data flow analysis compared with one of the state of art approaches pysonar2?
rq2 in comparison with the state of the art approaches visual studio intellicode and py apirec how effective is pyart in recommending apis within a project?
rq3 in comparison with the state of the art approaches visual studio intellicode and py apirec how effective is pyart in recommending api calls across projects?
rq4 how efficient is pyart in recommending apis in real time?
rq5 what is the impact of each kind of features?
b. baselines to evaluate the effectiveness and efficiency of pyart we choose a state of the art pysonar2 as the baseline for data flow analysis evaluation rq1 and two state of the art approaches visual studio intellicode and py apirec as the baselines for the api recommendation evaluation rq2 .
pysonar2.
pysonar27is an advanced static analyzer for python which performs costly whole project interprocedural analysis to infer types.
visual studio intellicode.
visual studio intellicode is an experimental set of ai assisted developer productivity tools trained from thousands of open source projects on github with high star ratings8.
for an api recommendation task intellicode places the most relevant ones at the top of the recommendation list and labels them with a star.
py apirec.
we reproduce a state of art api recommendation method for java named apirec and port it to python.
since py apirec relies heavily on fine grained code changes that happen on ast nodes the accuracy of recommendation 1640results is hence heavily dependent on the accuracy of ast differencing tools and code change histories.
following the original paper we diff the consecutive commits to identify code changes.
we validate the correctness of the reproduced apirec before porting it to python by comparing the results acquired by our reproduced system with the ones published in the original paper.
the error is within .
c. implementation we make our evaluation on linux ubuntu .
.
generic with intel r xeon r gold cpu .30ghz.
all approaches including py apirec and pyart are realized in python .
in the process of porting apirec we use the state of the art ast diff tool gumtree9to extract fine grained code changes.
we learn two parameters wcandwtfor pyapirec from a training set using the hill climbing adaptive learning algorithm and use the following settings we set the value of fold number kas we set the step size in hill climbing as .
we set the maximum number of iterations in adaptive learning as .
besides in order to provide a simple static type inference for pyart we use a type inference tool pytype10.
d. evaluation setup for fair comparison with py apirec and intellicode we set up three scenarios about the corpora used in evaluation.
project edition pe .
we randomly collect python projects from github that have a long development history with commits and files in total as a corpus called project edition .project edition was defined in which uses the oldest of project commits for training and the most recent commits for recommendation.
in particular we train pyapirec and pyart on the first commits of each project and test py apirec intellicode and pyart on the remaining commits.
since py apirec can only recommend apis in specific kinds of code changes i.e.
add that inserts a node into an ast methodinvocation that changes a method invocation and apiname that denotes the name of a method and cannot deal with apis in other kinds of changes we select code changes that py apirec can identify and recommend in the remaining commits as the recommendation points for a fair comparison.
the results of pe are used to answer rq2 intra project effectiveness .
moreover we record the time of each recommendation of pyart to answer rq4.
intra project edition ipe .
to evaluate the intra project effectiveness of pyart when historic commits are not available we prepare another set as follows.
we call it the intra project edition ipe .
specifically we collect the last commit of each project in pe to form the ipe corpus consisting of python files and locs.
we then divide the source files of each project in folds.
we train pyart on of the folds and test it on the .
the fold evaluation is performed five times ii data set statistics for pe and ce pe ce total projects total source files total slocs number of commits total changed files total ast nodes of changed files total changed ast nodes total detected changes total detected changes with apis on each project.
the average results are shown in table v as part of the answer to rq2.
community edition ce .
apirec collects a large corpus called community edition .
it trains its prediction model on this corpus ad then tests it on a different and smaller corpus to evaluate the cross project recommendation effectiveness.
we conduct a similar experiment.
we collect the top forked python projects from github with long development histories i.e.
commits and files as the ce corpus.
we train py apirec on all the commits of the projects and train pyart only on the last commit of the projects for the sake of cost saving.
then we use py apirec intellicode and pyart to make recommendations for code changes in all the commits of the projects in pe to answer rq3.
in addition we train pyart with different subsets of feature vectors from ce and evaluate the trained model on the projects from pe to study how the features impact accuracy rq5 .
the statistics for the pe and ce sets are shown in table ii.
e. metrics and settings data flow analysis evaluation.
we evaluate the effectiveness of data flow analysis in pyart and also in pysonar2 for comparison using precision recall and f1 score.
in order to construct a good baseline like the ground truth two authors that have more than three years of python coding experience inspect a subset of source files in ipe random files per project with each file having loc and manually recognize the data flow in these files.
here we look at pairwise data flow relations i.e.
definition and use .
we use manual inspection as the ground truth because there is not an existing tool that can report sound and complete data flow information.
we do recognize that humans may not extract global data flow and hence the human results may be biased.
note that we do not claim that our data flow analysis is accurate and complete.
instead optimistic data flow analysis is to simulate how humans reason about data flow and hence we argue the human study can serve as a reasonable baseline.
api recommendation evaluation.
we evaluate pyart pyapirec and intellicode using top k accuracy .
for a list of possible apis recommended with the length of l we search for the correct answer among the first kelements of the list.
we set the value of kto and respectively.
for a 1641more intuitive and objective evaluation of pyart and the other baselines we also use mrr as an evaluation measure.
mrr mean reciprocal rank evaluates the process that produces a list of possible apis ordered by the probability of correctness and computes the average of the reciprocal ranks of results with equation mrr jqjjqjx i rank i qrefers to a sample of queries and rank irefers to the rank of the first relevant api for the i th query.
intuitively a larger mrr value means a more accurate recommendation.
f .
result analysis data flow analysis evaluation rq1 .
according to table iii pyart s data flow results align well with the human baseline .
.
precision and .
.
recall .
in contrast pysonar2 has very good precision up to but extremely low recall only .
.
.
the reason is that pysonar2 aims to avoid false dependences caused by aliasing.
as such when it is not sure it does not report.
note that traditional data flow analyses conservatively assume there is data flow when they are not sure for safety .
such sparse data flow information generated by pysonar2 can hardly be used to make api recommendations.
observe it has runtime failures during analysis in a few cases too.
effectiveness within projects rq2 .
according to table iv pyart achieves better accuracy and mrr score than the baselines py apirec and intellicode on almost all projects.
the top1 accuracy of pyart is generally high i.e.
up to .
which supports the effectiveness of pyart on recommending apis within projects.
in addition we conduct an experiment using the ipe set i.e.
no historic commits .
we train the model on of the source files and evaluate it on the remaining files.
the recommendation points are selected in the same way as before.
note that py apirec is not applicable in this scenario.
the results are shown in table v. observe that pyart is still highly effective even it does not get to learn from other commits.
it substantially outperforms intellicode in majority of the cases.
effectiveness evaluation across projects rq3 .
according to table vi the average accuracy and mrr of pyart are higher than those of py apirec and intellicode which supports the effectiveness of pyart across projects.
although the top k accuracies and mrr of pyart are generally higher a small part of projects show inferior results than intellicode .
this is acceptable since intellicode is trained based on thousands of python code on github while pyart is only trained on projects.
efficiency evaluation rq4 .
it is important for recommendation tools to respond in a short period of time.
according to figure the average api recommendation time of pyart for a query in each project is generally lower than one second hardly noticeable.
fig.
.
recommendation time of pyart feature importance analysis rq5 .
we evaluate each kind of feature by subtracting one kind at a time to construct a new random forest model.
figure shows the effects of each kind of feature on top accuracy top accuracy top accuracy and the mrr value.
according to the results we observe that each kind has a non trivial positive contribution to the accuracy.
for out of the projects accuracy is most heavily affected by the optimistic data flow feature.
g. threats to validity internal validity.
our evaluation is performed on a limited set of python projects.
the performance of pyart may vary for different programs.
the ground truth of the data flow evaluation and the results of api recommendation by using intellicode are both provided manually which may lead to biases.
to mitigate such risk two authors performed the same experiments independently and cross checked afterwards.
another author was employed to resolve any conflict.
the conflict rate is lower than and all of them were resolved.
for validation and reproduction purpose we release our code and datasets.
external validity.
there are often run time errors when using gumtree and parser on python programs.
this is different from the case for java programs.
such errors may lead to biases.
v. r elated work api recommendation for natural language queries.
researchers use embeddings to model and translate among natural language queries application documentation and api functionalities.
others use graphs for representation and recommendation .
these techniques aim to answer queries in natural languages and differ from our real time scenario.
method recommendation based on code corpus and context.
nguyen et al.
proposed a graph based language model gralan learned from a source code corpus.
based on the model an api suggestion engine and an ast based language model are constructed to make recommendation.
nguyen et al.
proposed to learn from a corpus the regularity between fine grained code changes and apis and use that to make recommendations.
liu et al.
proposed a method independent of historical commits by ranking the top recommendations of gralan to achieve better 1642table iii data flow analysis results for python compared to the human extracted baseline model metrics cornice pyspider bs4 httpbin allennlp gitsome simplejson flask pysonar2precision failure failure failure recall .
.
.
.
failure failure .
failure f1 score .
.
.
.
failure failure .
failure pyartprecision .
.
.
.
.
.
.
.
recall .
.
.
.
.
.
.
.
f1 score .
.
.
.
.
.
.
.
a top accuracy b top accuracy c top accuracy d mrr fig.
.
impact of each kind of features on accuracy and mrr top accuracy than apirec .
nguyen et al.
mine open source software oss repositories to provide developers with api function calls and usage patterns by analyzing how apis are used in projects similar to the current project using a 3d matrix.
although existing works achieve very good results most of them aim at recommending apis for static languages such as java.
api recommendation for dynamic languages in real time poses many new challenges.
sch afer et al.
proposed an effective smart completion method for javascript by combining a static alias analysis enhanced with support for usage based property inference and a fully automatic dynamic analysis which infersapi models based on the framework s test suite.
d souza et al.
leveraged api use patterns from open source repositories to order python api recommendations.
however it does not focus on addressing the prominent challenges in dynamic languages such as type dynamics path sensitivity and thirdparty libraries.
there are some ides such as intellicode that provide real time recommendations for python apis based on artificial intelligence technologies.
however they are often dependent on type information and cannot make accurate recommendations for project specific apis.
1643table iv recommendaiton results within projects model top1 top2 top3 top4 top5 top10 mmr allennlppy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
bs4py apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
cornicepy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
flaskpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
gitsomepy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
httpbinpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
pyspiderpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
simplejsonpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
avgpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
table v results of intra project evaluation model top1 top2 top3 top4 top5 top10 mrr allennlpintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
bs4intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
corniceintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
flaskintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
gitsomeintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
httpbinintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
pyspiderintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
simplejsonintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
avgintellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
vi.
c onclusion we propose a novel approach pyart to recommending apis in real time for python.
it overcomes the challenges of handling dynamic language features by extracting so called optimistic data flow which is neither sound nor complete table vi recommendaiton results across projects model top1 top2 top3 top4 top5 top10 mmr allennlppy apirec .
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
bs4py apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
cornicepy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
flaskpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
gitsomepy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
httpbinpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
pyspiderpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
simplejsonpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
avgpy apirec .
.
.
.
.
.
.
intellicode .
.
.
.
.
.
.
pyart .
.
.
.
.
.
.
but resembles the data flow information humans derive.
it also extracts token similarity and token co occurrences as additional features and encodes them together to numerical vectors.
random forest is then used to train a model from a corpus of python projects.
our results show that our technique substantially outperforms a state of the art research prototype and the api recommendation technique in a mainstream python ide.
acknowledgment we thank the anonymous reviewers for their constructive comments.
this research was supported in part by nsfc jiangsu postgraduate innovation program kycx20 cooperation fund of huawei nanjing university next generation programming innovation lab no.
ybn2019105178sw11 nsf and onr n000141712045 n000141410468 and n000141712947.
any opinions findings and conclusion in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.
1644references q. huang x. xia z. xing d. lo and x. wang api method recommendation without worrying about the task api knowledge gap inproceedings of the 33rd acm ieee international conference on automated software engineering .
acm pp.
.
w. xiong z. lu b. li b. hang and z. wu automating smart recommendation from natural language api descriptions via representation learning future generation computer systems vol.
pp.
.
m. m. rahman c. k. roy and d. lo rack automatic api recommendation using crowdsourced knowledge in ieee 23rd international conference on software analysis evolution and reengineering saner vol.
.
ieee pp.
.
x. sun c. xu b. li y .
duan and x. lu enabling feature location for api method recommendation and usage location ieee access vol.
pp.
.
l. qi q. he f. chen w. dou s. wan x. zhang and x. xu finding all you need web apis recommendation in web of things through keywords search ieee transactions on computational social systems .
w. yuan h. h. nguyen l. jiang y .
chen j. zhao and h. yu api recommendation for event driven android application development information and software technology vol.
pp.
.
c. y .
ling y .
z. zou z. q. lin and b. xie graph embedding based api graph search and recommendation journal of computer science and technology vol.
no.
pp.
.
c. chen x. peng j. sun z. xing x. wang y .
zhao h. zhang and w. zhao generative api usage code recommendation with parameter concretization science china information sciences vol.
no.
p. .
a. t. nguyen m. hilton m. codoban h. a. nguyen l. mast e. rademacher t. n. nguyen and d. dig api code recommendation using statistical learning from fine grained changes in proceedings of the 24th acm sigsoft international symposium on foundations of software engineering .
acm pp.
.
a. t. nguyen and t. n. nguyen graph based statistical language model for code in ieee acm 37th ieee international conference on software engineering vol.
.
ieee pp.
.
x. liu l. huang and v .
ng effective api recommendation without historical software repositories in proceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
p. t. nguyen j. di rocco d. di ruscio l. ochoa t. degueule and m. di penta focus a recommender system for mining api function calls and usage patterns in proceedings of the 41st international conference on software engineering .
ieee press pp.
.
c. chen z. xing y .
liu and k. l. x. ong mining likely analogical apis across third party libraries via large scale unsupervised api semantics embedding ieee transactions on software engineering pp.
.
x. ren j. sun z. xing x. xia and j. sun demystify official api usage directives with crowdsourced api misuse scenarios erroneous code examples and patches in ieee acm 42th ieee international conference on software engineering .
ieee pp.
.
a. r. d souza d. yang and c. v .
lopes collective intelligence for smarter api recommendations in python in ieee 16th international working conference on source code analysis and manipulation scam .
ieee pp.
.
r. xie x. kong l. wang y .
zhou and b. li hirec api recommendation using hierarchical context in ieee 30th international symposium on software reliability engineering issre .
ieee pp.
.
m. gorbovitski y .
a. liu s. d. stoller t. rothamel and t. k. tekle alias analysis for optimization of dynamic languages in proceedings of the 6th symposium on dynamic languages pp.
.
l. fritz and j. hage cost versus precision for approximate typing for python in proceedings of the acm sigplan workshop on partial evaluation and program manipulation pp.
.
z. xu x. zhang l. chen k. pei and b. xu python probabilistic type inference with natural language support in proceedings of the 24th acm sigsoft international symposium on foundations of software engineering pp.
.
m. salib starkiller a static type inferencer and compiler for python ph.d. dissertation massachusetts institute of technology .
a. svyatkovskiy s. k. deng s. fu and n. sundaresan intellicode compose code generation using transformer arxiv preprint arxiv .
.
a. svyatkovskiy y .
zhao s. fu and n. sundaresan pythia ai assisted code completion system in proceedings of the 25th acm sigkdd international conference on knowledge discovery data mining pp.
.
m. asaduzzaman c. k. roy k. a. schneider and d. hou cscc simple efficient context sensitive code completion in ieee international conference on software maintenance and evolution .
ieee pp.
.
m. bruch m. monperrus and m. mezini learning from examples to improve code completion systems in proceedings of the 7th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering pp.
.
g. a. kildall a unified approach to global program optimization in proceedings of the 1st annual acm sigact sigplan symposium on principles of programming languages pp.
.
m. rapoport o. lhot ak and f. tip precise data flow analysis in the presence of correlated method calls in international static analysis symposium .
springer pp.
.
k. d. cooper t. j. harvey and k. kennedy iterative data flow analysis revisited tech.
rep. .
y .
bengio a. courville and p. vincent representation learning a review and new perspectives ieee transactions on pattern analysis and machine intelligence vol.
no.
pp.
.
a. coates a. ng and h. lee an analysis of single layer networks in unsupervised feature learning in proceedings of the fourteenth international conference on artificial intelligence and statistics pp.
.
w. hamilton z. ying and j. leskovec inductive representation learning on large graphs in advances in neural information processing systems pp.
.
y .
li l. xu f. tian l. jiang x. zhong and e. chen word embedding revisited a new representation learning and explicit matrix factorization perspective in twenty fourth international joint conference on artificial intelligence pp.
.
o. levy and y .
goldberg neural word embedding as implicit matrix factorization in advances in neural information processing systems pp.
.
y .
y .
lee h. ke t. y .
yen h. h. huang and h. h. chen combining and learning word embedding with wordnet for semantic relatedness and similarity measurement journal of the association for information science and technology vol.
no.
pp.
.
s. negara m. codoban d. dig and r. e. johnson mining fine grained code changes to detect unknown change patterns in proceedings of the 36th international conference on software engineering pp.
.
m. dias a. bacchelli g. gousios d. cassou and s. ducasse untangling fine grained code changes in ieee 22nd international conference on software analysis evolution and reengineering saner .
ieee pp.
.
b. fitzgerald the transformation of open source software mis quarterly pp.
.
d. spinellis z. kotti k. kravvaritis g. theodorou and p. louridas a dataset of enterprise driven open source software arxiv preprint arxiv .
.
m. sch afer m. sridharan j. dolby and f. tip effective smart completion for javascript technical report rc25359 .
p. fegade and c. wimmer scalable pointer analysis of data structures using semantic models in proceedings of the 29th international conference on compiler construction pp.
.
m. hind pointer analysis haven t we solved this problem yet?
inproceedings of the acm sigplan sigsoft workshop on program analysis for software tools and engineering pp.
.