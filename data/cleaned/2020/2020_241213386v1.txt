an exploratory study of ml sketches and visual code assistants lu s f. gomes software and societal systems dept.
carnegie mellon university pittsburgh usa lfgomes andrew.cmu.eduvincent j. hellendoorn software and societal systems dept.
carnegie mellon university pittsburgh usa vhellendoorn cmu.edu jonathan aldrich software and societal systems dept.
carnegie mellon university pittsburgh usa jonathan.aldrich cs.cmu.edurui abreu inesc id faculty of engineering university of porto porto portugal rui computer.org abstract this paper explores the integration of visual code assistants in integrated development environments ides .
in software engineering whiteboard sketching is often the initial step before coding serving as a crucial collaboration tool for developers.
previous studies have investigated patterns in se sketches and how they are used in practice yet methods for directly using these sketches for code generation remain limited.
the emergence of visually equipped large language models presents an opportunity to bridge this gap which is the focus of our research.
in this paper we built a first prototype of a visual code assistant to get user feedback regarding in ide sketch to code tools.
we conduct an experiment with data scientists most of whom regularly sketch as part of their job.
we investigate developers mental models by analyzing patterns commonly observed in their sketches when developing an ml workflow.
analysis indicates that diagrams were the preferred organizational component .
often accompanied by lists .
and numbered points .
.
our tool converts their sketches into a python notebook by querying an llm.
we use an llm as judge setup to score the quality of the generated code finding that even brief sketching can effectively generate useful code outlines.
we also find a positive correlation between sketch time and the quality of the generated code.
we conclude the study by conducting extensive interviews to assess the tool s usefulness explore potential use cases and understand developers needs.
as noted by participants promising applications for these assistants include education prototyping and collaborative settings.
our findings signal promise for the next generation of code assistants to integrate visual information both to improve code generation and to better leverage developers existing sketching practices.
index terms ai4se code generation visual programming sketching machine learning tool development human ai interaction i. i ntroduction a. motivation developing software is a collaborative process that relies on the collective creativity and problem solving skills of a team .
it often begins with brainstorming sessions where developers gather around a whiteboard to sketch out initial ideas and concepts .
this collaborative sketching helps inaligning everyone s understanding creating a more solid foundation for the subsequent stages of development.
developers also use sketches as a form of planning their own code communicating and expressing their mental models through sketches as in the typical whiteboard meetings .
drawings and diagrams thus serve as visual aids to express and refine mental models of the code structure .
this ties into a recent development in artifical intelligence research large language models llms have evolved towards multi modality which includes strong vision capabilities .
while the use of natural language to generate code text to code is already widespread and a common focus of research with many code llms being developed using image to code techniques has almost exclusively been explored for tasks such as front end development where the sketch directly captures the visual layout that the code should provide.
this is orthogonal to sketches that capture the structure of a software system.
in this work we bridge the gap between existing research on whiteboard sketches and their use to generate code focusing specifically on data science ds systems.
the demand for ds software has recently increased steeply .
such systems typically involve a pipeline that transforms data in various ways generating models and other artifacts e.g.
visualizations along the way.
as shown in this study data scientists frequently sketch their systems and such sketches tend to be fairly rich in detail.
more generally our work explores the concept of visual code assistants .
complementing the typical text based assistants a visual code assistant is capable of understanding developers sketches helping them when they express their ideas visually.
b. contributions this paper makes contributions to both the research community and developers by offering guidance on developing generative code tools and providing insights into the mentalarxiv .13386v1 dec 2024models of ml developers.
we implement a visual assistant by building an in ide sketch to code tool which converts drawings of ds pipelines into ipython jupyter notebooks.
we conducted a user study with data scientists the majority of whom regularly sketch.
the study is composed of two parts sketching andprogramming .
in the first part a task is given to participants and they are asked to behave as in a regular team meeting or code interview using the whiteboard to explain how they would solve it.
in the second part we use the visual code assistant to generate an initial code skeleton and the participant iteratively edits it to achieve the final solution.
both parts follow the think aloud protocol .
the study is guided by three research questions rq1 what are the prevalent sketching patterns of ml developers?
rq2 to what extent can current sketch to code technologies support ml developers?
rq3 what are developers perspectives on the value and potential of in ide visual code assistants?
we observe that real drawings are highly varied frequently mixing diagrams and textual content.
using a separate llm to score the generated code against a set of requirements we find that popular llms can generate valid outlines the high level structure of the desired notebooks with accuracy and implement of the requirements correctly from sketches alone.
longer sketch times correlate positively with outline quality but not significantly with implementation details in part because sketches often do not provide all details required to implement the code correctly.
we collect rich qualitative insights from interviews conducted after the study.
the contributions of our work are as follows we develop an in ide visual code assistant prototype that converts sketches of ml related tasks into complete jupyter notebooks.
we conduct the first exploratory study on sketching for ml related tasks and the integration of vision capabilities in code assistants.
we carry out the first user study to evaluate the quality of code generated by llms based solely on images.
we provided guidance for future visual code assistant development by reporting on developers needs and identifying potential use cases where this technology can be most effectively applied.
ii.
r elated work programming is all about understanding tasks and creating code to execute them.
to effectively work on software development programmers must create a clear understanding of both the current code structure and any new code they need to write.
this involves constructing mental models that help them visualize how the code works and how different components interact with each other.
a. sketching in se in a survey with software engineers latoza et al.
studied this phenomenon in arguing how mental modelsof code are expensive to create and maintain.
yet they found that these mental models are only kept in developers heads or in transient forms such as sketches on a whiteboard .
furthermore they pointed out how paper and whiteboards were perceived most effective while designing software.
they suggested reducing the cost of using design documents by linking them to the code or building tools that capture informal whiteboard or paper designs .
building on latoza et al.
s findings many researchers studied sketching practices in se.
cherubini et al.
showed that developers often express their code in temporary drawings that are later lost due to the cost of translating them into electronic renderings.
the transient nature of whiteboard sketches can be reduced by visual coding assistants.
when uploading their sketches to generate code developers make them persistent as a side effect.
branham et al.
built reboard which captures whiteboard drawings.
they studied how whiteboard content is reused identifying issues to be addressed when building augmentation systems.
similarly mangano et al.
identified three basic needs of developers when sketching frequently shifting focus using low detailed models and using a mix of notations.
they presented positive results with calico a tool to address these issues.
we find that the flexibility of current ai vision models makes them suitable for working with this mix of notations low detailed models and informal sketches.
as developers frequently shift their focus these models should be integrated into the ide.
walny et al.
followed the lifecycle of sketches and diagrams reporting on their transitions and commenting on how informal sketches can usefully serve as memory aids and for communication purposes.
in a second study they observed whiteboard sketches concluding that participants have an inventive capability to create representation of whatever problems processes or data they wanted to discuss .
they also found that words were often used as primary objects and not only as labels.
this highlights that developers have the ability to sketch different problems and drawing is a natural way of expression.
in sketches and diagrams in practice baltes et al.
indicated that the majority of sketches were created informally on analog media and were found to be valuable resources for understanding source code artifacts.
they also pointed out that sketches usually represent higher levels of abstraction.
they propose two new tools to bridge analog and digital artifacts sketchlink links code to sketches using a web app to capture the drawings and an ide plugin to visualize them and livelysketches supports the iterative process of sketching versioning and linking drawings.
more recently almeida et al.
interviewed software architects followed by a survey with participants.
their findings contradict prior research affirming that of software architects actually document their whiteboard meetings with photos of the whiteboard among the five most common documentation approaches.
however they also observe that in whiteboard meetings there is not sufficient information about the problem to design the solution and that certain aspects of the solution are over simplified which makes the solution mismatch the original sketch.
this matches the rate and characteristics of sketching in our work.
these studies suggest that visual assistants should expect to mainly extract big picture information from sketches rather than low level implementation details.
b. images to code the use of sketches as a specification for code generation has been explored previously especially when the code is highly related to visuals.
this is the case in graphic interface design where the use of hand drawn mock ups is common and its translation to html code is very useful.
pix2code explored this problem and uses convolutional and recurrent neural networks to generate code from designers mockups.
ellis et al.
convert drawings into latex programs representing the intended graphics.
although not generating code psdoodle uses sketches as the input to perform an app screen search.
on ml code generation sethi et al.
present dlpaper2code that generates python code from images of deep learning architectures.
with the advent of multi modal llms a range of innovative use cases are being successfully explored.
notable contributions include mathvista and mmmu which investigate the integration of multi modal capabilities in mathematics and college level reasoning respectively.
in the domain of frontend development si et al.
examine the performance of gpt and gemini in generating accurate web pages from images.
additionally p adurean and singla analyze visual programming generation performance tailored for elementarylevel students.
plot2code extends the application of llms to code generation from scientific plots while mmcode focuses on multi modal coding for evaluating algorithmic problem solving skills in visually rich contexts.
d amorim et al.
proposed the concept of sketch to code applied to the domain of ds arguing that it is a highly visual discipline.
we build on their vision aiming for the broader goal of operating on complete data workflows and generating jupyter notebooks based on whiteboard drawings.
in this paper we introduce the first benchmark specifically designed to assess the performance of leading llms in interpreting and generating code from whiteboard sketches.
iii.
v isual coding assistant to study these phenomena we developed a vscode plugin where users can open a snapshot of a whiteboard sketch and have a jupyter notebook generated from it.
this tool acts as part of the experimental materials and is used to test part of our theory regarding the usefulness of visual coding assistants.
in this section we provide details about the prototype implementation and a motivating example.
interaction.
we want participants to focus on sketching and the conceptualization of in ide visual coding assistants.
the interaction that we seek to study is the naturalness of information exchange between the developer and the ai using the drawing in the same way a developer would do with other humans.
we are not interested in studying the plug inuser interface interactions leaving this factor open to future research.
therefore our vscode extension simplifies the user interaction to a single button that generates code.
when a picture is opened in the ide the plugin detects the image and allows the user to click on the generate code button.
otherwise a warning will tell the user that the open file is not an image and therefore cannot be used to generate code.
once the user clicks on the button a new panel is opened to the right of the active window which displays the generated jupyter notebook.
code generation the code generation is delegated to openai s gpt 4o with vision capabilities by using their web api.
at the time the study was conducted the model performs better than any other regarding vision capabilities according to the benchmarks updated daily at huggingface .
gpt4 is preferred both for the convenience of using the api and its performance.
the prompt used in the final version of the prototype and the user study is the following you are an expert code generation system and you are specialist in python.
you will receive an image representing a ml workflow and you will reply with the correspondent steps in json list format.
the output for each step must have exactly two elements code field with python code and markdown field containing the explanation.
markdown explanations must be short and easy to understand.
no introductory text is allowed in the output.
retrieve only the list format e.g.
code ... markdown ... code ... markdown ... create the code for the following diagram.
assembling the notebook after receiving the code generated from the model the plugin gathers all the identified steps and assembles them into a jupyter notebook.
each step in the image is then translated into a pair of cells a markdown cell that provides a brief explanation followed by the code cell that contains the code to be executed.
a. motivating example this study recreates situations where developers want to communicate their ideas using the whiteboard.
we give a task to each participant and ask them to behave as in a regular meeting where they explain how to solve it.
after sketching they use our prototype to generate code.
figure shows an example of one of our participants usage of the prototype during the study.
in this example the participant explains how they want to load three csv files test.csv target.csv and test.csv visualize age with a histogram map sex male female to numerical variables train a random forest model and use a confusion matrix to see the results.
the participant uses a diagram to represent the precedence of each operation.
in the center we have a data table that needs to be loaded from the train.csv file.
most of the other vlm leaderboard accessed may 27th 2024fig.
.
example of usage of our prototype.
left editor tab the whiteboard sketch uploaded by the participant to be used by the code assistant to generate code.
right editor tab generated jupyter notebook with code and markdown explanations.
operations depend on this one so it must occur before the others.
causal dependencies are indicated with arrows.
the participant represents the singular operations using a mix of visual icons such as the representation of the histogram and many short textual annotations such as male .
the user uploads a picture of their whiteboard sketch to the repository they are working on and after clicking the generate code button a jupyter notebook is created and opened next to the image.
using the drawing as the only source for code generation we see the code assistant preserving the order of operations while maintaining a coherent succession of steps.
at the same time it understands most of the participant intentions such as the correct mapping the use of a histogram function or the split of the variable survived from the dataset.
on the other hand some inaccuracies are visible such as the arguments to the histogram function or splitting the train dataset into train and test.
these are things the developer may want to change or remove according to their initial intention.
in real scenarios even with the inherent inaccuracies this is useful to quickly generate a first code skeleton and discuss ideas at the code level during the meeting.
it saves time by reducing the time on programming activities and making the design clearer to everyone in the meeting.
iv.
m ethodology the goal of our research study is to investigate developers behavior while using the whiteboard and visual code assistants.
our population of interest is individuals whose needs encompass ml and ds tasks independent of their level of knowledge or expertise in this domain.
our sample comprises individuals from academia and industry who have previously worked with python and understand ml and ds concepts.a.
recruitment we recruited participants in two ways.
the first was by identifying researchers and developers from personal contacts and contacting them directly via email.
the candidates were selected taking into account previous experience.
the second was by employing snowball sampling asking previous participants to nominate new people who meet the criteria and they believed would accept to take part in the study.
each participant was compensated with a amazon gift card.
we recruited participants female male non binary from different institutions in industry participants meta ansys tripadvisor bandora and academia participants cmu nyu univ.
of porto univ.
of lisbon univ.
of waterloo brown univ.
tu delft bow college colby college bucknell university .
regarding the educational levels in our sample participants were pursuing or had completed a doctorate five a master s degree and four a bachelor s degree.
regarding their current occupation four are working in ml or ds as their main activity nine in software engineering or other programming intensive fields and six in other fields.
b. protocol our irb approved protocol encompasses five stages after the participant agrees to take part in the study pre task survey sketching coding interview and posttask survey.
we designed the study to take a maximum of minutes per participant.
stages and occur before and after the study respectively.
the study was conducted in person or virtually depending on the participant s availability.
a pilot study with five participants was conducted to improve and validate the protocol and the tasks used in the final study.
in this section we detail each of the parts.pre task survey.
participants complete a survey collecting demographic information and details about their sketching and coding habits.
this information is crucial for understanding the diverse backgrounds and experiences of the participants which can influence their interactions and measurements during the study.
the survey is hosted at google forms and a link is sent to the participant before the study.
sketching.
each participant is presented with an ml task and given a few minutes to get familiarized with the task and dataset.
we allow the participant to ask clarifying questions about the task before starting the sketch.
when participants confirm that they understand the task they are asked to impersonate the team member responsible for solving the problem and simulate a whiteboard meeting where they explain their intended solution to an ml developer.
at this point a timer is started to capture sketching time and audio is recorded to capture their thought process think aloud .
participants were instructed to draw until they felt their explanation was complete.
at this point the timer and recording were stopped and we captured a picture of the final sketch.
coding a break is made before proceeding to the coding stage.
the break allows participants to switch to a new context since now they impersonate an ml developer and simulate a programming session.
in the meantime we set up the in browser vs code ide using github codespaces.
this workspace includes the visual coding assistant prototype presented in section iii and common ml libraries that can be useful to solve the tasks.
the drawing is uploaded to this workspace allowing the user to generate code from it.
their sketch is used to generate code using the prototype and a jupyter notebook with the same base name as the image is generated.
the participant renames the file to generated.ipynb and creates a copy of it calling it solution.ipynb .
the participant can make changes to the solution file until they are ready to submit their final solution.
during the code session a timer is started and audio and screen are recorded through zoom for further analysis.
the think aloud protocol is used and the participant is encouraged to speak while they do the task.
during the coding session the participant can utilize all tools that they would use in a real situation such as google or chatgpt.
interactions with such tools are registered as well.
to ensure the user study remains within the minute time slot participants are assigned minutes to complete this step.
to capture real scenario patterns participants are not informed of this time limit beforehand preventing any bias towards utilizing the entire minutes.
interview questions.
we conclude the user study with a semi structured interview focusing on three key areas to understand developers needs regarding visual coding assistants.
the first area addresses sketching discussing the intuitiveness of expressing coding ideas as sketches and how sketching patterns may influence code generation.
the second area involves the participant reconstructing their work environment to identify potential use cases where they could benefit from this technology.
the final area explores future directions for developing coding assistants with vision capabilitiesand gathering participant feedback on potential features and improvements.
post task survey.
participants are asked to fill out a survey designed to collect feedback on various aspects of the visual coding assistant tool.
the google forms link is sent to the participant after the user study.
the survey begins by assessing participants initial confidence in the tool s ability to generate accurate code from their sketches.
participants rate the performance of the tool providing their overall satisfaction with the accuracy of the generated code and the frequency of discrepancies between their intentions and the code that was generated.
additionally participants evaluate the tool s usefulness and provide their perceptions of how code generation from sketching could impact their productivity.
c. tasks we designed the tasks to simulate realistic ds scenarios in which developers want to express their ideas visually before coding e.g.
during whiteboard meetings.
the tasks are similar to those encountered in ml course assignments and coding challenges.
to increase the generalizability of the results we designed three ml tasks that center around typical modeling settings binary classification regression and image classification.
each task has the same types and number of subtasks such as data loading and visualization.
table i shows a summary of the three tasks that are used in the user study.
table i tasks and subtasks subtask task1 task2 task3 loadload one csv file.load three csv files.load two csv files.
transformsplit target.
split train testsplit target.
variable encodingreshape image plot1 boxplot histogram grid plot plot2 scatter plot confusion matrix line plot model linear regression random forest cnn each task was designed by starting from a kaggle dataset and defining the five subtasks afterward.
the subtasks were chosen taking into account common ds and ml routines.
the datasets were modified to guarantee that no extra complexity was added to the task and no extra execution time was needed making it feasible to complete it in the stipulated time frame hour .
for task we use the house prices dataset datasets vikrishnan boston house prices for task the titanic dataset and for task the digit recognition dataset competitions digit recognizer data .
tasks were assigned randomly for each participant following a round robin approach with assignments made upon participant acceptance into the study.
this procedure ensures a balanced distribution of tasks with an identical number of participants per task.d.
data and artifacts during the user study we collect both qualitative and quantitative data.
for privacy reasons all the recordings and transcripts are not publicly available.
this includes the screen recording of the coding sessions and the audio recordings with the corresponding transcripts of the sketch sessions coding sessions and interviews.
in the publicly available data alongside the original sketches and the generated jupyter notebooks we include coding session annotations interview thematic analysis and sketching thematic analysis documents.
the raw data of pre task and post task surveys is also available after anonymizing participants identities.
all data and scripts can be found at e. evaluation our evaluation involves both qualitative and quantitative methods.
in this exploratory study we follow a constructivist paradigm where we collect data and draw conclusions from it in an inductive manner.
this section details the methodology used to answer each rq in this study.
sketch analysis in our study we employed conceptual modeling alongside the think aloud protocol to gather ds sketches.
participants were asked to simulate a whiteboard meeting where they explained their approach to a given task using a whiteboard paper or a tablet.
to analyze these sketches we first applied open coding to identify and categorize recurring characteristics.
this was followed by axial coding which allowed us to consolidate the findings into two main categories detailed in the results section.
due to some inherent subjectivity in interpreting the sketches our analysis was further supported by the audio recordings and transcripts from the sketching sessions.
this additional data enabled us to trace the connection between specific subtasks and corresponding parts of the drawings providing a clearer understanding of the sketching process and enhancing the reliability of our findings.
code generation sketching is inherently subjective which makes the development of metrics for evaluating code generated from another person s drawings harder.
given the inherent limitations of visual representations which often omit crucial details it is unrealistic to expect the llm to perfectly interpret and translate all aspects of a sketch into accurate code.
to address this challenge we established a controlled environment where participants follow specific tasks to guide their sketches.
this setup enables us to create metrics based on these tasks and by extension infer the accuracy of the generated and submitted code.
we rely on one key assumption participants correctly comprehend and execute the provided tasks in their sketches.
this assumption is supported by audio recordings and transcripts of the sketching sessions which are used during the sketch analysis.
llm as a judge to calculate the metrics we employ a code analysis strategy termed llm as a judge.
this approach leverages a large language model llm to evaluate code based on predefined criteria mirroring how a humanevaluator would grade notebooks.
the primary motivation for using llm as a judge rather than manual grading is to ensure consistent and efficient evaluation across the dataset particularly as new notebooks are added.
manual grading is time intensive and lacks scalability becoming increasingly impractical for larger datasets.
in prior work llm as judge approaches achieved agreement levels comparable to human evaluators in previous work and are widely used in settings like chatbot arena .
we used gpt as the judge for pointwise grading as it has been reported to perform well in such task .
a simplified example of the evaluation process is illustrated in figure .
an evaluation of this judge in our setting is provided below.
although this is not as precise as formal program analysis it is more flexible in task presentation and evaluation.
participants in our study may use a wide range of libraries e.g.
pandas csv numpy or dask for file loading.
implementing formal analyzers to check the correctness of pipelines would be prohibitively expensive without strict restrictions on the libraries and functions participants can use which would reduce the realism of our study.
execution level metrics are another common choice but require fully functional notebooks.
notebooks generated from sketches are virtually never fully functional nor is it feasible to specify complete test cases for them.
given this we opted for the flexibility of using an llm as a judge combined with detailed specifications.
during the development of the llm judge ground truth jupyter notebooks for each task were used to verify how well the model performed against a perfect solution.
for each subtask specific parts of the code were manually removed to ensure the llm correctly flagged the corresponding questions as false .
this process allowed us to curate a better evaluator improving both the prompt and the grading questions.
the final version of the prompt used in the evaluator was the following you are a programming teacher responsible for grading data science coding assignments.
you receive a jupyter notebook to be graded and a set of questions that you should answer with true or false.
you should return the same list of questions answered with the corresponding answer true or false .
for example the output should be question1 true n question2 false n etc.
judge validation.
during the study a second evaluation was performed manually grading and comparing the results with the judge of a subsample of the notebooks submitted by study participants.
to avoid confusion with the results presented in section v we refer to judgment metrics as the ones that are used to evaluate how well the llm as a judge is capable of assessing the correctness of a jupyter notebook.
these are intrinsic to the specific llm that is used as the judge.
the outline evaluation demonstrated a judgment accuracy of .
and a judgment precision of .
.fig.
.
overview of our benchmarking using llm as a judge.
to evaluate a jupyter notebook generated from a sketch the judge has access to knowledge elements related to the original drawing a task description that relates to users coding intentions depicted on the drawing and the grading criteria for the outline and instantiation metrics and .
the instantiation evaluation showed a judgment accuracy of .
and a judgment precision of .
.
these results suggest that the llm as judge methodology is reliable for this use case as it demonstrates high levels of agreement with human judgment.
thegeneration metrics are specific to each input notebook as shown in the example of figure .
these are reported in section v. our quantitative benchmark evaluates jupyter notebooks based on the following metrics.
outline accuracy the percentage of correctly identified high level steps in the notebook according to the sketch outline.
it focuses on whether the notebook includes logic for all major sub tasks regardless of their specific implementation details.
instantiation accuracy the percentage of correctly implemented details within the notebook based on the sketch.
this captures whether the specific elements or parameters match those sketched by the user.
the instantiation accuracy is a sub specification of the outline metric and they are correlated.
models that perform well in the instantiation metric will perform well in the outline but the reverse may not be true.
the reason for having two different metrics is to understand the degree of detail captured by the vision capabilities of llms.
this interacts with the sketches themselves they virtually always contain the details required to construct the outline but may omit details required to implement the code completely such as the input file name.
hence we typically do not expect a perfect instantiation accuracy.
while our main study uses gpt 4o we conduct a retrospective analysis of three models on the collected sketches claude .
sonnet gemini pro .
and openai s gpt 4o.for each model we feed the same set of sketches generating a jupyter notebook per sketch.
we compare both the outline and instantiation performance.
please note that gpt 4o serves as both the judge and a participant in this experiment.
while this might introduce bias the judge only has access to the generated notebook the task and the grading criteria not the model name so we expect that the risk of bias is slim.
surveys and regression analysis we use our benchmark to evaluate the code generated from sketches analyzing the relationship between sketching time and code accuracy through regression analysis.
this analysis considers participants programming experience and sketching expertise which we collected from our pre task survey.
we also control for various levels of programming knowledge and sketching expertise to understand their impact on code accuracy.
other variables from the pre task survey were used primarily for demographic reporting as detailed in the recruitment section.
similarly data from the post task survey provides additional evidence that supports our qualitative results and complements the insights gathered from the interviews.
interview analysis the approach to analyzing interview data closely follows the methodology employed in sketch analysis.
initially open coding is applied to the transcripts to identify and categorize the raw data.
this preliminary step involves breaking down the text into discrete units and labeling them with codes that represent significant observations.
subsequent to open coding axial coding is utilized to discern and establish relationships between these codes.
this process helps to organize the data into coherent categories that address specific themes such as emotional responses sketching practices coding behaviors and future perspectives.building upon these categories an inductive thematic analysis is conducted.
by identifying recurring patterns and thematic elements this approach helps to summarize the findings into a comprehensive report that reflects the participants viewpoints.
to measure the robustness of the findings the percentage of interviewees mentioning each theme is calculated.
this quantitative measure provides insight into the preponderance of certain perspectives among participants highlighting the strength of our conclusions.
the audio and video recordings of the sketch and coding session are used as support during the analysis of the sketches and as confirmation of the coding session annotations such as the number of times participants assessed chatgpt.
v. r esults a. how do data scientists sketch?
to effectively design and implement visual methods in code assistants understanding prevalent whiteboard sketching practices is essential.
this section explores the findings that address rq1 what are the prevalent sketching patterns of ml developers?
sketching practices quantitative findings.
in our pretask survey we assessed the last time participants used sketching to express their mental models during work activities.
the results indicate that .
of participants used sketching within the last week .
within the last month and .
within the last three months.
additionally .
had either not used sketching in over a year or did not recall their last use.
these results suggest that sketching is actively used by a substantial portion of participants with .
engaging in it within the last month alone.
sketching practices qualitative insights.
this finding aligns with interview insights that illustrate varied attitudes toward sketching.
participants who do not sketch often expressed a preference for having control over their code directly or using text prompts.
p1 mentioned is something that you don t do very frequently you don t need to do it when you have a data set that you want to look at .
p13 and p17 show their preference for text prompts i would be tempted to use the text more often.
and i don t trust a computer to recognize sketches.
so i prefer chatgpt.
respectively.
conversely participants who sketch frequently cited its utility in clarifying and structuring their thoughts.
p7 stated sometimes i do the drawings for my benefit so that i don t get as confused and p12 elaborated when you write down you have a whole structure when you are the sketches it s like you already run the whole thing in your mind.
.
this suggests that sketching serves as a cognitive aid helping developers to visualize and organize their ideas before coding as an integral part of their problemsolving process.
along the same lines some participants expressed their preference for the new possibility presented during the study of using sketch prompts to generate code.
p9 observed that is faster than writing the code for sure and p11 found sketching to be more efficient compared to detailed prompt explanations noting alsorequires you more time to explain.
so just being able to sketch some stuff and generate the code was interesting to me.
this preference aligns with the survey data showing that a significant portion of participants use sketching regularly suggesting it is valued for its speed and clarity in coding tasks.
finding sketching as a development tool developers commonly leverage sketching as a cognitive tool to visualize and organize ideas during the problemsolving process with .
of developers engaging in sketching within the past month.
while some prefer traditional text based prompts or direct coding approaches there is a significant demand for tools that can directly translate sketches into code .
sketch styles.
the first finding tells us why developers sketch.
to understand how they sketch we analyzed the sketches acquired during the study.
as our focus is on people working in ds the patterns we observed are linked to ml and data related tasks.
while it is expected given the nature of the tasks a notable finding from our analysis is the strong emphasis developers place on maintaining order in their drawings.
participants exhibited clear mental models for structuring task dependencies which were primarily expressed through visual representations.
the following structural representations were found in the sketches diagrams the most common approach .
was to use diagrams to represent relationships.
participants typically employed arrows to indicate dependencies data flow or the sequence of operations.
these diagrams often exhibited a spatial orientation such as left to right or top to bottom progression.
nodes within these diagrams represented components objects entities or groups of operations.
sequential lists while less frequent than diagrams .
some participants used sequential lists to outline task order.
these lists relied on spatial arrangement e.g.
top to bottom to imply ordering.
numbering in some cases .
participants used numbers to clarify the order of operations freeform occasionally sketches lacked explicit structural elements suggesting that participants were focusing on capturing ideas without immediate concern for order.
from the sketch analysis we further identified patterns regarding how participants represented the operations within each subtask.
developers combine the following five elements boxes participants frequently employed boxes or other enclosed shapes to delineate distinct operational units within their sketches.
these boxes are often combined with labels or icons.
iconography symbolic or graphical representations of data or concepts.
annotations short text elements providing essential context for understanding the intended operations.
2note that multiple constructs can be combined on the same sketch so the percentages add up to more than .table ii usage of element by subtask .
textual annotations are the most commonly used element in all tasks .
load plot1 transform model plot2 box iconography annotation description code descriptions longer and more descriptive textual explanations of operations often using verbs.
code python code pseudo code or code like expressions.
as illustrated in table ii textual annotations were the most frequently employed element across all subtask types indicating a strong preference for textual descriptions of operations.
while both plot1 and plot2 subtasks exhibited a pronounced reliance on iconography suggesting a potential advantage of visual representations for data visualization the creation of machine learning models was primarily conveyed through detailed textual descriptions.
this was also mentioned during interviews.
for instance p4 mentioned is more intuitive than having to look up function calls on the apis there are different variations and it s not exactly what i wanted visually.
this also highlights that image and text based code generation are not mutually exclusive.
finding structure text and icons developers use the whiteboard to organize their ideas by maintaining an order of operations.
they rely on diverse types of elements including textual annotations for details and icons for visual tasks.
effective visual assistants must prioritize accurate text recognition while also considering spatial layout and visual symbols for enhanced comprehension.
b. sketch to code generation in this section we examine the results regarding rq2 to what extent can current sketch to code technologies support ml developers?
our focus is on understanding the complexities of transforming whiteboard sketches into executable code exploring developers expectations for such tools and assessing the capabilities of current large language models in this domain.
results.
on average participants spent minutes sketching min max and minutes programming min max .
this average sketch yields code with an outline accuracy of and an instantiation accuracy of .
the median number of lines changed starting from the generated code in the final solution is .
where the median solution has lines in total.
given that sketching reduced the amount of written lines of code by on average.
3an outlier we instated the minute coding time cap mentioned in the methodology after one participant spent minutes on the coding portion.table iii summary of regression models predicting code generation outcomes based on sketch time last sketching activity and programming experience .
reported p values are after benjamini hochberg correction .
instantiation outline predictor coeff.
p value coeff.
p value intercept .
.
.
.
sketch time .
.
.
.
last sketching .
.
.
.
experience .
.
.
.
r20.
.
adjr2 .
.
p values p .
p .
p .
adjusted p values for multiple testing b h correction to support this analysis a manual verification of the results obtained from the submitted notebooks was conducted with each subtask evaluated as either correct or incorrect.
subtasks were considered correct if they were executed successfully and produced the expected output.
in total out of participants successfully solved their assigned challenge while an additional participants completed out of subtasks correctly.
individual differences.
to investigate the relationship between sketch characteristics and code generation outcomes we conducted linear regression analyses to predict outline and instantiation accuracies.
three key variables were considered.
first we examined sketch time the duration of the whiteboard sketching process measured in minutes.
second we explored last sketching a proxy for sketching proficiency representing the time elapsed since the participant s last sketching activity.
this variable was included under the assumption that participants who had recently engaged in sketching activities would likely exhibit better drawing skills.
finally we controlled for programming experience measured in years.
table iii presents the results of the linear regression analyses predicting code instantiation and outline generation.
the pvalues are adjusted using the benjamini hochberg correction to control the false discovery rate and account for multiple hypothesis testing.
while neither sketch time last sketching activity nor programming experience significantly influenced code instantiation accuracy the model for outline generation revealed a positive relationship between sketch time and generation accuracy.
specifically each additional minute spent sketching was associated with a .
percentage point increase in outline accuracy controlling for other variables.
the pvalue for this variable .
lies slightly outside the standard confidence intervals .
which indicates that a larger sample size is needed to better estimate the true coefficient of this variable.
this corresponds to an average outline accuracy of ca.
for sketches completed in minutes to ca.
for ones completed in minutes.
worth noting that the intercept in the outline generation model indicates a baseline outline accuracy of .
when the other predictors are zero.
this does not imply that a completely blank sketch would generate any code.
rather it suggests thatgemini .
pro claude .
sonnet gpt 4o020406080100accuracy metric instantiation outlinefig.
.
performance comparison of top visual llms on the notebooks.
no statistically significant difference was detected in either dimension in our study.
minimal sketching efforts could lead to a reasonably accurate code outline.
for instance one participant s sketch made in under four minutes reached an outline accuracy of .
finding sketch duration and accuracy even brief sketching periods have the potential to generate useful code outlines.
longer sketch durations are associated with improved outline accuracy.
this suggests that code generation tools can be effectively adapted to user needs based on the desired level of code generation detail.
model comparison.
in our prototype we used gpt 4o as the underlying model.
we evaluate the performance of other leading commercial alternatives with similar functionalities including vision capabilities and public apis.
the alternatives that we compare are google s gemini .
pro anthropic s claude .
sonnet and openai s gpt 4o.
figure shows the results of the three models on our two metrics automatically calculated using the llm as a judge approach.
outline accuracy for gpt 4o claude and gemini is and respectively.
similarly gpt 4o leads in instantiation accuracy with a score of followed by claude at and gemini at .
while statistical significance was not found in code generation performance among the models there may still be some evidence of differences worth further exploration with a larger dataset.
finding generation across models there is no significant difference in accuracy between gpt 4o gemini .
pro and claude .
sonnet.
all three models performed substantially better at generating code skeletons outline compared to handling detailed code implementations instantiation .c.
perspectives on visual code assistants to maximize the utility of these models it s crucial to integrate them into tools that assist developers in real world scenarios.
the concept of a visual code assistant involves embedding vision capabilities into existing code assistants like github copilot.
to understand the potential of this approach we investigate developers needs and expectations through rq3 what are developers perspectives on the value and potential of in ide visual code assistants?
participants expressed a desire for a few key characteristics in visual code assistants.
explainability and guidelines were frequently mentioned with users seeking to understand the rationale behind generated code and how to effectively sketch and utilize the tool.
some participants show their contentment about the generated markdown explanations was the most impressing part because already give these parts here that are not code but just information p3 .
on the other side some participants wished they had more information like p7 who captured the sentiment if there are parts that don t require as much detail i think those are important guidelines for the users.
like this much detail helps the model generate better code .
iterative and interactive code generation was identified as another important aspect.
participants emphasized the dynamic nature of the programming process highlighting the need to modify both sketches and code iteratively.
as p10 stated to change the code i would have gone and like updated my drawing.
similarly p11 noted the importance of misunderstandings from the schematic and the code easily fixed with some kind of back and forth interaction.
observational data verifies these findings with .
of participants revisiting their sketches at least once during the coding process with an average of three