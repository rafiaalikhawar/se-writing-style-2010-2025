navigating the testing of evolving deep learning systems an exploratory interview study hanmo you college of intelligence and computing tianjin university tianjin china youhanmo tju.edu.cnzan wang college of intelligence and computing tianjin university tianjin china wangzan tju.edu.cn bin lin hangzhou dianzi university hangzhou china b.lin live.comjunjie chen college of intelligence and computing tianjin university tianjin china junjiechen tju.edu.cn abstract deep learning dl systems have been widely adopted across various industrial domains such as autonomous driving and intelligent healthcare.
as with traditional software dl systems also need to constantly evolve to meet ever changing user requirements.
however ensuring the quality of these continuously evolving systems presents significant challenges especially in the context of testing.
understanding how industry developers address these challenges and what extra obstacles they are facing could provide valuable insights for further safeguarding the quality of dl systems.
to reach this goal we conducted semi structured interviews with dl developers from diverse domains and backgrounds.
more specifically our study focuses on exploring the challenges developers encounter in testing evolving dl systems the practical solutions they employ and their expectations for extra support.
our results highlight the difficulties in testing evolving dl systems e.g.
regression faults online offline differences and test data collection and identify the best practices for dl developers to address these challenges.
additionally we pinpoint potential future research directions to enhance testing effectiveness in evolving dl systems.
index terms deep learning software evolution testing interview study i. i ntroduction deep learning dl systems have been widely integrated into products of different sectors such as autonomous vehicles medical diagnostics and software engineering .
like traditional software dl systems also need to continuously evolve to adapt to new requirements and evergrowing demands of users.
for example during the covid pandemic apple updated face id a built in facial recognition system which enables users to unlock devices and authenticate purchases while wearing masks bringing huge convenience to users .
however the evolution of dl systems sometimes may also pose challenges or side effects.
for example the chatbot of the parcel delivery firm dpd was found to swear at users after an update in january leading to substantial corresponding author.reputation damage .
similarly chatgpt has faced criticism from users for providing incorrect or overly simplified answers after updates resulting in accusations of it becoming dumber and lazier .
this has significantly impacted user experience and dented user confidence in openai .
these cases underscore the potential risks associated with the evolution of dl systems.
therefore it is particularly important to conduct rigorous testing during the system evolution.
the academic community has conducted extensive research on dl testing and various approaches have been proposed to reveal bugs of dl systems such as test case generation and mutation testing .
however these studies often focus on testing a specific version of the dl system neglecting its evolving nature which encompasses continuous development refinement and enhancement over time.
in practice many aspects of dl systems may change along with the system evolution such as training data or model structure.
the complexity of these systems also increases making it more challenging to effectively test the entire system .
currently there remains a notable gap in the knowledge of testing evolving dl systems.
understanding the difficulties faced by developers and the strategies they adopt to handle these difficulties can provide valuable insights for further improving the effectiveness of the testing process in dl systems.
in this study we interviewed dl system developers from different companies in different sectors and countries aiming to explore various aspects of testing evolving dl systems including the specific challenges they face the effective solutions they adopt to address these challenges and what kind of additional support they yearn for.
our results lead to six different types of obstacles developers face e.g.
high costs for test case collection annotation best practices for addressing these challenges e.g.
adopting large language models to assist in data labeling and types of concrete support developers would like to have e.g.
tools that can effectively detect errors in annotated data .the main contributions of our paper are as follows.
by consulting experts in the field we summarize the challenges encountered in testing evolving dl systems.
we extract the best practices of current dl system developers in testing evolving dl systems.
these strategies serve as invaluable