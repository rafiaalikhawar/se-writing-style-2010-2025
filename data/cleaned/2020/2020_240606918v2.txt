humanevo an evolution aware benchmark for more realistic evaluation of repository level code generation dewu zheng1 yanlin wang1 ensheng shi2 ruikai zhang3 yuchi ma3 hongyu zhang4 zibin zheng1 1sun yat sen university zhuhai china zhengdw5 mail2.sysu.edu.cn wangylin36 zhzibin mail.sysu.edu.cn 2huawei cloud computing technologies co. ltd. beijng china shiensheng huawei.com 3huawei cloud computing technologies co. ltd. shenzhen china zhangruikai1 mayuchi1 huawei.com 4chongqing university hyzhang cqu.edu.cn 5zhuhai key laboratory of trusted large language models abstract to evaluate the repository level code generation capabilities of large language models llms in complex realworld software development scenarios many evaluation methods have been developed.
these methods typically leverage contextual code from the latest version of a project to assist llms in accurately generating the desired function.
however such evaluation methods fail to consider the dynamic evolution of software projects over time which we refer to as evolution ignored settings.
this in turn results in inaccurate evaluation of llms performance.
in this paper we conduct an empirical study to deeply understand llms code generation performance within settings that reflect the evolution nature of software development.
to achieve this we first construct an evolution aware repository level code generation dataset namely humanevo equipped with an automated execution based evaluation tool.
second we manually categorize humanevo according to dependency levels to more comprehensively analyze the model s performance in generating functions with different dependency levels.
third we conduct extensive experiments on humanevo with seven representative and diverse llms to verify the effectiveness of the proposed benchmark.
we obtain several important findings through our experimental study.
for example we find that previous evolutionignored evaluation methods result in inflated performance of llms with performance overestimations ranging from .
to .
under different context acquisition methods compared to the evolution aware evaluation approach.
based on the findings we give actionable suggestions for more realistic evaluation of llms on code generation.
we also build a shared evolutionaware code generation toolbox to facilitate future research.
the replication package including source code and datasets is available at i. i ntroduction in recent years the llm based code generation task has drawn widespread attention .
many code llms are being employed to empower programming assistants which are playing a vital role in practical yanlin wang is the corresponding author.software development .
recently several repositorylevel code generation benchmarks such as codereval repobench evocodebench and repoeval have emerged to better simulate real world development scenarios.
they sample functions from real world projects as programming tasks aiming to reflect the performance of llms in actual development by prompting them to generate these functions according to the target function description and project context.
despite the fact that real world projects are inherently dynamic and evolve over time current code generation benchmarks often overlook this critical aspect.
they tend to treat the latest version of the repository as context source for evaluation a situation we term as the evolution ignored phenomenon.
this evaluation approach fails to accurately capture true nature of real software development scenarios where projects continuously evolve over time which means that the project context that developers face when writing different code changes constantly.
therefore to accurately assess the repository level coding capabilities of llms it is imperative to factor in this evolution aspect.
specifically when prompting llms to generate different target functions the project context provided should mirror the one available to developers at the time of the target function s creation.
unfortunately evolutionignored evaluation in previous benchmarks provides llms with an inaccurate and potentially misleading programming environment leading to serious issues.
we have found concrete instances of issues that stem from the evolution ignored setting in real github projects which are detailed in section ii.
benchmark humanevo.
to fill this gap we construct humanevo a novel evolution aware repository level code generation benchmark which better simulates real world development processes and reflects the evolution nature of projects over time.
the evaluation process of humanevo is as follows to ensure that the context we provide to llms mirrors thearxiv .06918v2 mar 2025context available to the programmer when writing the target function neither including additional code introduced by subsequent version changes nor omitting code that may have been updated or deleted due to version change we roll back the entire repository to the state before the target code was committed and evaluate the code generation performance of llms at function granularity on the rolled back repository.
we conduct a rigorous data construction pipeline to construct humanevo.
we start with selecting high quality projects and then collecting a large number of pull requests prs from these projects.
then we perform an initial filtering on the crawled prs to meet three attributes to guarantee the quality of the newly added functions in prs.
additionally to ensure the reliability of the humanevo s test suite we establish a runtime environment for each pr and execute the corresponding project s testing framework to verify that the newly added functions are covered by the test suite.
finally we obtain task instances for python and for java.
for each task instance in humanevo we record its corresponding pr s metadata to roll back the repository to the state prior to the target function s commit.
empirical study.
based on the new benchmark humanevo we perform the first study to reveal llms repository level code generation capability in the evolution aware setting.
in particular we conduct an extensive evaluation of mainstream llms including both open source ones codellama 7b codellama 13b codellama 34b deepseekcoder .7b deepseekcoder 33b and closed source ones gpt4 and gpt .
turbo on humanevo.
according to the empirical results we have the following findings 1 we find that all the studied llms show much worse performance on our evolution aware setting.
previous evolution ignored evaluation would lead to the inflated performance of llms ranging from .
to .
revealing how previous evaluation methods provided the models with a deviated programming scenario from reality.
2 we find that the impact of the evolution ignoring setting is more severe for target code with more complex dependencies.
specifically compared to the evolution ignored setting the success rate of llms in generating functions with intra class dependencies under the evolution aware setting has been observed to decrease by approximately .
on average while for functions with inter class dependencies the decline is .
on average.
3 we investigate how the performance of llms change as the repository evolve.
experimental results show that in the evolution ignored setting in general models performance gradually deviates from the real performance as the project evolves.
4 we find that under the evolution ignored setting utilizing docstrings in different styles as input consistently leads to inflated performance.
experimental results indicate that llms code generation performance improves when given a detailed docstring compared to a brief one.
this work makes the following key contributions we have identified a common flaw in previous repositorylevel code generation benchmarks they are evolutioncommittedonsep2 evolving ignoredrepository base prompttemplategeneratethemethod body for similarity basedretrievaldef register instance lookup self lookup lookup name none if lookup name is none lookup name lookup.lookup nameif instance lookups not in self.
dict self.instance lookups self.instance lookups lookupreturn lookuphere are some context that may help you input def register lookup cls lookup lookup name none committed on jan retrievedcontent input retrievedcontent future context leakage!!!
if lookup name is none ... codeomittedhere llmgeneration llm generates correct code withthehelpof future information!!!fig.
.
example future context leakage.
ignored causing inaccurate simulations of real world development scenarios.
we introduce a new benchmark humanevo an evolutionaware repository level code generation benchmark that addresses the evolution ignored issue to conduct a comprehensive empirical study on the performance of llms in repository level code generation.
through extensive experimentation we substantiate the evolution ignored situation leads to inflated performance of the models with llms showing a performance inflation ranging from .
to .
.
for convenient usage of humanevo we release all data code and the evaluation platform with docker images that provide runtime environments for all projects facilitating automated execution of all test suites.
the replication package is provided at science r humanevo .
ii.
m otivating examples in this section we firstly elaborate on how real repository evolves over time.
then we present two motivating examples1 that showcase the specific issues that can emerge under the evolution ignored setting.
in real world development projects undergo continuous evolution over time .
during the development phase of a project programmers need to develop the project i.e.
by adding new features removing existing code adding test code etc based on the project requirements.
in the maintenance phase programmers continuously monitor the project and promptly fix any bugs that arise.
these activities result in 1the motivating examples are from the real world project com django djangoclass migration models.model ...name models.charfield max length applied models.datetimefield default now ... codeomitteddeletedonjan7 deleted context committed on may evolving ignoredrepository base prompttemplategeneratethemethod body for similarity basedretrievaldef applied migrations self if self.has table ... code omitted here are some context that may help you input def migration cls committed on jan retrievedcontent input retrievedcontent if cls.
migration class is none ... code omittedllmgenerationnot useful context for target function useful context missing!!!
llm generates wrong code without the help of useful context!!!fig.
.
example useful context missing.
varying degrees of modifications to the project causing the project to evolve over time .
for a realistic and accurate evaluation of llms code generation capabilities this evolution nature should be considered.
otherwise severe issues under the evolution ignored setting would emerge.
we have identified concrete instances of issues which we refer to asfuture context leakage anduse context missing which we will illustrate with the following two examples.
example .
figure illustrates the future context leakage issue that can arise from the evolution ignored setting.
this example involves a target function that was originally committed on jan .
in the canonical rag code generation pipeline a retriever finds similar code from the corresponding repository which is the latest version in the evolution ignored setting to help llms generate the target code.
however the retriever yields a function committed on sep a date that is later than the target function .
this process is illogical because it is implausible for a developer to access code that was not yet written at the time of the creation of the target function.
therefore providing this code snippet as context to the model is a future context leak.
while in this example the llm generates the correct code the reliability is questionable as it is uncertain whether it can still generate correctly without the help of this future code.
example .
figure illustrates the useful context missing issue that may arise in the evolution ignored setting.
in this example the function committed on jan is the target function.
in the same rag code generation pipeline as in example the retrieved code snippet is not particularly useful function declarationdef get nonflat cls cls kls optional none optional detailed docstringfind the corresponding non flat class.
the class bases are searched recursively.
parameters kls class astropy.cosmology.cosmology class or none optional if none default this class is searched instead of kls .
raises typeerror if more than one non flat class is found at the same level of the inheritance.
this is similar to the error normally raised by python for an inconsistent method resolution order.
returns type a class cosmology subclass this class inherits from that is not a class flatcosmologymixin subclass.
basecommit2ae987ba1160486e36eb315ee0e9d85cdbea5844brief docstringfind the corresponding non flat class.
instanceidowner repoversion5feceb66ffc8astropy astropy5.1dependencylevelstandalonetaskinstanceoverviewfig.
.
task instance overview.
and llm fails to generate the correct code.
however when the developer wrote that code there was actually a piece of code in the repository that was highly similar to the target code and could have been retrieved as context which would have likely helped generate the correct code.
but this code snippet was deleted during the evolution process.
as a result under the evolution ignored setting this piece of code did not exist in repository context.
while in this example the llm generates wrong code it is possible that providing llm with this useful context might have enhanced its performance.
iii.
h uman evobenchmark in this section we introduce our new benchmark humanevo in detail.
we present the benchmark overview section iii a benchmark construction pipeline section iii b and benchmark characteristics section iii c .
a. benchmark overview humanevo consists of a total of task instances comprising python programming tasks and java programming tasks respectively.
figure shows an example of a task instance overview.
each task instance is associated with a unique identification number instance id .
the owner repo field helps identify the github repository from which the selected function originates while the version field indicates the version of the repository according to the implementation time of this function.
base commit refers to the last commit on the branch before the target function was committed.
we use it to restore the repository to the state before the target function was implemented.
the function declaration of the target function will be provided to the llm as part of the prompt along with its correspondingdocstring.
since the docstring styles of functions in pragmatic projects vary with some adopting line by line comments and others even lacking corresponding comments we think it is necessary to rewrite the docstring for each selected function.
as depicted in figure we provide two styles of docstrings.
thebrief docstring provides a concise summary of the functionality of the target function while the detailed docstring provides a thorough description of the function s functionality inputs and outputs.
b. benchmark construction pipeline as depicted in figure the construction pipeline of humanevo consists of six steps namely project selection pull request crawling attribute based filtering execution based filtering function selection and human annotation.
project selection.
in order to make humanevo more representative we adopt a strict approach to selecting functions from various open source projects.
for python we select high quality real world projects from a list of the top most downloaded pypi libraries2.
subsequently we deploy the selected projects and run their testing frameworks to ensure each of them can run correctly and stably.
we then proceed to collect prs from these repositories through the github developer api3.
for java as widely recognized projects typically indicate extensive documentation structured opensource development guidelines and functional well formatted code our approach involves searching for the top starred projects on github.
then we deploy every project on the list to select projects that can be successfully deployed and pass the corresponding test suite.
finally we retain real world projects covering over common development domains which guarantees that the functions selected for evaluation in humanevo could represent real world programming scenarios.
pull request crawling.
we crawl a large number of initial prs from the popular open source repositories selected in the procedure above.
specifically for each project we do not crawl all prs as we find that early versions of many projects are difficult to deploy successfully.
therefore for python we crawl prs created after september the release time of python .
for java we collect prs created after september the release time of java .
finally we obtain over prs and meticulously record metadata pertaining to these prs for subsequent filtering processes.
attribute based filtering.
in this step we preliminarily filter the crawled prs based on the following three attributes.
the status of the pr is merged .
a merged status indicates that the code changes associated with the pr were accepted and incorporated into its parent repository.
the pr must introduce new functions.
these newly added functions are typically intended to develop the software project by adding new features improvements or fulfilling general development objectives which can be selected as programming task.
pr must introduce at least one or more new tests and these tests must cover the selected function.
all these attributes aim to ensure that the subsequently selected functions are of high quality have undergone rigorous review and are covered by the project s testing framework.
execution based filtering.
in this step we perform execution based filtering for the prs filtered in the attributes filtering step.
note that although both the project selection step and the execution based filtering step have the successful installation criterion their purposes are different.
the main purpose of requiring successful installation in the project selection step is to quickly filter projects while in this step it ensures that all versions associated with different prs can run successfully as prs may correspond to multiple versions of a project.
firstly we prepare the execution environment for each candidate task instance in humanevo.
for each task instance we first determine the version of the project corresponding to the pr then manually identify the required dependency environment for running the project at that version.
for example for a python task instance we confirm the required python version and the types and versions of third party libraries it depends on.
after obtaining this dependency environment information we create a virtual execution environment for each version of the project.
task instances that can not install the dependency environment properly are filtered out during the virtual execution environment setup process.
after creating the execution environment we execute the project s testing framework in the corresponding environment.
secondly for each selected function we execute the project s test framework in its required environment to ensure that the selected function is covered by the project s test framework.
specifically we split the pr s patch file which is also known as the code diff file into two parts namely the production code patch and the test code patch.
for each candidate task we apply the pr s test patch and log the associated test results before and after applying the pr s production patch.
then we filter out task instances with no test where its status changes from a fail to pass which indicates the newly added functions are covered by the test suite.
after filtering out instances without a fail to pass transition we obtain prs.
function selection.
in this step we manually select functions from the prs that have passed the filtering process above to serve as programming tasks.
after completing the above filtering we confirm that the remaining prs meet our requirements.
therefore we select newly added functions from these prs as programming tasks.
during the selection process to avoid choosing basic functions we filter out initialization functions constructors and destructors to ensure the quality of the selected programming tasks.
in addition we ensure that the selected target functions are covered by the project s test framework through manual review .
finally we acquire a total of programming tasks.
human annotation.
in this step we recruit six annotators with more than five years of programming experiencehigh quality projects pypi downloading leaderboardproject selection top highly staredexecution based filtering .passes all tests .
installs successfully pull request crawlingmore than prsabout prs400 prshuman annotation .
rewritesrequirement1.
dependency levelclassify attribute based filtering2.adds new functions .
merged status3.
contributes testsfunction selection programming tasksfig.
.
humanevo construction pipeline.
to manually rewrite two different styles of docstrings for the selected functions and label each function with a dependency level.
we rewrite the docstring for each instance in humanevo to ensure the quality of the benchmark dataset and mitigate data leakage issues as much as possible.
specifically in the process of data construction we find that the docstring styles of actual projects can be divided into two categories the detail and the brief.
docstrings are used as input for models so variations in docstring style may significantly influence model evaluation.
aiming to determine whether the evolution ignored setting leads to unreal llms performance evaluations across different input styles we provide two types of the docstring for each task instance.
the detailed docstring describes the function of the target function in detail as well as the types and meanings of its inputs and outputs.
in contrast a brief docstring only briefly describes what the target function does.
after the annotators complete writing the docstrings we conduct a cross validation review requiring each task instance to be verified and confirmed by two developers to ensure accuracy.
if the annotators disagree with a docstring written by the previous annotator during the review process they will discuss the issue together until all annotators reach an agreement.
in order to facilitate the evaluation of the ability of llms in generating functions with different dependency levels we further categorize all task instances in humanevo into three categories standalone intra class and inter class functions.
a dependency denotes the invocation of elements defined within projects and the dependency level is defined based on the position of the target function s dependencies.
standalone functions refer to those that invoke or access only built in functions and standard libraries.
intra class functions rely on other functions within the same class.
inter class functions involve calling functions within the same file or functions in other files.
c. benchmark characteristics evolution aware benchmark humanevo is the pioneering repository level code generation benchmark to introduce the evolution aware concept.
we highlight a significant oversight in prior research the developmental trajectory of a project is an evolution process altering the project context available to programmers as the project advances through various stages.
previous studies have overlooked this temporal dimension resulting in llms being fed with code that had not yet been implemented in the project when certain functions were developed.
consequently it also misses some context that should have been present but might have been deleted orupdated during the project s evolution.
this undoubtedly leads to the model facing an unrealistic development scenario.
as a result the performance of llms in real world software development tasks remains unknown.
therefore we are committed to enhancing humanevo to serve as a benchmark that better simulates real world software code development scenarios aiming to more accurately reflect the true performance of llms applied to pragmatic development tasks.
strict data filtering process we ensure the quality of the selected function from several aspects.
first we ensure that the selected projects are of high quality because high quality projects generally have excellent maintenance comprehensive testing frameworks and good coding styles.
for python we choose the projects with the highest download count in pypi.
for java we select projects from the top projects on github based on the number of stars with an average of around 24k stars.
previous benchmarks have ensured the functionality of target functions by writing unit tests for each function separately.
however we believe that in a project with complex dependencies adding a new function may have certain impacts on other parts of the project.
relying solely on unit tests for the target function is insufficient to ensure that the project can still function properly and pass the project s test framework after inserting the function.
therefore in this work we directly run the project s test framework to ensure the correctness of the target function s functionality while ensuring that inserting the generated function does not have a negative impact on other parts of the project.
in addition we also verify that our selected functions are covered by the project s test framework through executionbased validation.
first we decompose the code diff in the pr corresponding to the target function into a source code patch and a test code patch and directly apply the test code patch.
then we validate whether the target function is covered by the project s test framework by executing the project s test framework twice.
in the first execution we run the project s test framework without applying the source code patch.
in the second execution we apply the source code patch and then run the test framework.
if there is a test function in the project s test framework that targets the target function and the execution of this test function requires calling the target function the first execution based on execution will fail because the target function call is missing.
in the second execution after applying the source code patch the test framework will correctly call the target function and run as expected.
therefore if the test framework covers the target function the results of the two executions should show a fail table i dependency level in human evo.
dependency level humanevo python humanevo java standalone .
.
intra class .
.
inter class .
.
to pass transition.
we perform execution based validation for all selected functions to ensure that the humanevo test suite is reliable.
human annotation we manually rewrite two styles of docstrings for all task instances and label each of them with a dependency level.
during the dataset construction process we observe different docstring styles among these high quality projects which can be generally categorized into the detailed and the brief.
detailed docstring typically provides comprehensive descriptions of a function s functionality elucidates input and output parameter types and annotates potential error scenarios.
conversely a brief docstring succinctly outlines the function s functionality in a few sentences.
to enhance the diversity of our benchmark and better align with realworld development scenarios we manually crafted two types of docstring for each function detailed and brief.
this provides users with more options for subsequent usage.
furthermore to facilitate the assessment of llms capability in generating functions with varying levels of dependency we categorize the selected functions based on their dependency levels including standalone intra class and inter class.
standalone functions can be implemented without relying on other functions within the project.
intra class functions require dependencies on other functions within the same class.
interclass functions imply more complex dependencies with most of the dependent functions being located in other files within the project.
manual categorization of dependency levels aids in analyzing the performance of llms in generating functions of varying complexity facilitating researchers in improving the application of llms in practical development scenarios based on different contexts.
moreover as indicated in table i the majority of our benchmark consists of functions with deep dependency levels.
this suggests that our benchmark presents a sufficiently challenging scenario.
note that we use prs to construct humanevo primarily for the following three reasons.
first since prompting llms to generate repository level functions can be viewed as an incremental development process and prs for adding new features exactly mirror the incremental development process in the real world we choose to instruct llms generate the newly added functions in prs to simulate real world development scenarios.
second the quality of functions in prs is guaranteed.
before merging the newly added code in prs into the project repository administrators conduct a review on it which allows us to select high quality task instances directly.
third the base commit in prs helps us roll back the project to the state before the target function was committed.
finally by processing prs appropriately we can further determinewhether the target functions are covered by the project s testing framework.
iv.
e xperimental design in this section we introduce the models used in the experiments outline the research questions that the experiments aim to address and provide a detailed description of the experimental setups.
a. studied llms we select the mainstream llms both open source and closed source ones that have been widely used in recent code generation work .
for open source llms we select the codellama series including codellama 7b codellama 13b and codellama 34b and the deepseekcoder series including deepseekcoder .7b and deepseekcoder 33b .
the codellama and deepseekcoder series are popular llms that perform well in code generation.
for closed source llms we choose the commonly used commercial models gpt .
turbo and gpt with gpt demonstrating excellent performance in previous benchmarks.
b. studied context acquisition strategies due to the large size of modern software repositories it is not feasible to feed the entire repository to the model.
thus previous works mainly obtain project context that might be beneficial for generating target functions through two strategies retrieval based and static analysis based methods.
in the following experiments we cover both of these technical routes to demonstrate that the evolution ignored situation leads to the deviation of the llms performance from reality.
retrieval based context acquisition.
we adopt the standard retrieval augmented generation pipeline for repositorylevel code generation .
in the evolution aware setting we roll back the repository to the state before the target code was committed for each task instance.
on the contrary in the evolution ignored settings we directly utilize the latest project version as the context source.
then we construct a corresponding context retrieval database by partitioning the source code files from the repository into a collection of code snippets.
subsequently we utilize the docstring of the target code as the query for retrieval and compute the jaccard similarity between the query and all code snippets in the context retrieval database.
after sorting all the code snippets by similarity score we select project contexts with higher similarity scores as project context for target function generation.
static analysis based context acquisition.
similar to the static analysis based repository level code generation work we obtain code files in the repository that might help generate the target code through static analysis.
first we roll back the repository to the corresponding state according to evolutionignored or evolution aware settings.
then we extract the source code from these files as project context to feed to the model.
there are four types of context sources local file thefile where the target code resides import file files imported by the local file in the project sibling file files located in the same folder as the local file and similar file files in the project with names similar to the local file .
we utilize four prompting strategies from local extracting context solely from the local file local import extracting code from both local and import files local sibling extracting code from both local and sibling files local sim extracting code from both local and similar files.
c. research questions our experiments intend to answer the following research questions rqs rq1 how effective is humanevo in benchmarking the performance of llms in repository level code generation task?
we conduct experiments under two different settings evolution aware and evolution ignored to explore the differences in model performance between these two settings and reveal the actual performance of the models in real development scenarios.
rq2 how does the code generation performance change as repository evolves?
we treat multiple versions of project as context sources to explore how the model s performance change as the project evolves.
rq3 how different code descriptions in the dataset affect code generation performance?
we conduct experiments to compare the performance of the model under different docstring styles.
d. experimental settings we assess the functionality correctness of code generated by llms with execution based evaluation and utilize the commonly used metric pass for evaluation.
we set the generation temperature of llms to .
top p to .
and the context window length to .
note that to mitigate issues stemming from the randomness of model generation the experimental results presented in this paper are obtained by conducting three repeated experiments and averaging the results.
the experiment was conducted on a server running ubuntu .
.
lts and equipped with intel r xeon r platinum 8336c .30ghze cpus and nvidia a800 with 80gb memory.
v. e valuation results a. rq1 how effective is humanevo in benchmarking the performance of llms in repository level code generation task?
overall performance.
table ii and table iii present the performance of seven mainstream llms under two different experimental settings evolution ignored where the context source is the latest version of the project and evolutionaware humanevo where the context source is the project before the target function was committed .
table ii shows the performance of retrieval based project context acquisition approach and table iii shows the performance of statictable ii retrieval based context acquisition approach model performance under evolution aware human evo and evolution ignored ei settings .
modelpython java ei humanevo ei humanevo codellama 7b .
.
.
.
.
.
codellama 13b .
.
.
.
.
.
codellama 34b .
.
.
.
.
.
deepseekcoder .7b .
.
.
.
.
.
deepseekcoder 33b .
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
gpt .
.
.
.
.
.
analysis based project context acquisition approaches.
all the experimental results in table ii and table iii show a significant decrease in performance under the evolution aware setting compared to the evolution ignored setting.
this suggests that ignoring the evolution of the project would lead to inflated performance of llms failing to accurately reflect their real code generation performance.
break down analysis.
we further analyze the generation results to investigate whether evolution ignored setting always leads to over estimated performance of llms.
table iv illustrates the overlap of successful task instances between evolution aware setting and evolution ignored setting.
since the results from retrieval based project context acquisition approach and static analysis based project context acquisition approaches are similar subsequent analysis primarily focuses on the experimental results of the retrieval based approach.
as introduced above evolution ignored setting would raise issues such as the future context leakage issue which would lead to inflated performance.
meanwhile evolution ignored setting can raise issues such as the useful context leakage issue which would lead to underestimated performance.
however the results in table ii reflect that models overall performance only exhibits an inflated trend driving us to delve deeper into whether issues such as useful context leakage has occurred.
we investigate the overlap between task instances that llms successfully completed under evolution ignored and evolutionaware settings.
in table iv ea stands for the evolution aware setting and ei stands for the evolution ignored setting.
a means successful while a means failure.
so ea ei refers to the number of instances that pass under the evolutionaware setting but fail under the evolution ignored setting.
ea ei represents instances that fail under ea but pass under ei.
ea ei denotes the number of instances that pass under both settings.
from the experimental results it is evident that except for gpt in humanevo python which has a value of for ea ei situation all other llms exhibit deviations from reality caused by issues such as the useful context missing issue in a certain quantity showing that the issues which are raised by the evolution ignored setting and would lead to underestimated performance have impact on evolution ignored setting.
however due to the significantlytable iii static analysis based project context acquisition approaches model performance under evolution aware human evo and evolution ignored ei settings .
data modellocal local import local sibling local sim ei humanevo ei humanevo ei humanevo ei humanevo humanevo pythoncodellama 7b .
.
.
.
.
.
.
.
.
.
.
.
codellama 13b .
.
.
.
.
.
.
.
.
.
.
.
codellama 34b .
.
.
.
.
.
.
.
.
.
.
.
deepseekcoder .7b .
.
.
.
.
.
.
.
.
.
.
.
deepseekcoder 33b .
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
humanevo javacodellama 7b .
.
.
.
.
.
.
.
.
.
.
.
codellama 13b .
.
.
.
.
.
.
.
.
.
.
.
codellama 34b .
.
.
.
.
.
.
.
.
.
.
.
deepseekcoder .7b .
.
.
.
.
.
.
.
.
.
.
.
deepseekcoder 33b .
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
.
.
.
.
evo0evo1evo2evo3evo4evo5evo6evo7evo8evo9evo103040506070numbers of successful instances444650545657 5658codellama 7b evo0evo1evo2evo3evo4evo5evo6evo7evo8evo9evo103040506070numbers of successful instances46 5758575861codellama 13b evo0evo1evo2evo3evo4evo5evo6evo7evo8evo9evo103040506070numbers of successful instances485056575862 6263codellama 34b evo0evo1evo2evo3evo4evo5evo6evo7evo8evo9evo103040506070numbers of successful instances464751535759 606160deepseek coder .7b evo0evo1evo2evo3evo4evo5evo6evo7evo8evo9evo103040506070numbers of successful instances505257 62deepseek coder 33b evo0evo1evo2evo3evo4evo5evo6evo7evo8evo9evo103040506070numbers of successful instances51556062 65gpt .
turbo fig.
.
llm s performance when using different project versions as context sources.
higher number in ea ei the primary influencing factor remains the future context leakage leading to exaggerated performance of llms.
dependency level.
functions with different dependency levels vary in their reliance on project context and the evolution ignored setting directly affects project context.
therefore in this experiment we examine the performance differences of llms in generating functions with deeper dependency levels focusing on non standalone functions in humanevo which encompass intra class and inter class functions across two settings evolution ignored and evolutionaware.
as shown in table v under the evolution aware setting the accuracy of llm in generating both intra class and inter class functions has noticeably declined.
the decline is more pronounced for inter class functions as these functions typically require cross file context which is often more scattered.
as the project evolves its various parts tend to become more comprehensive.
therefore under the evolution ignored setting the models can reference an increasing amount of context.
consequently when the evolution aware setting loses access to these contexts the model s performance significantly deteriorates.
rq1 summary experiments show an inflated trend in llm s performance under the evolution ignored setting.table iv break down analysis .
ea and eistand for evolution aware and evolution ignored respectively .
means success meas failure .
modelpython java ea ea ea ea ea ea ei ei ei ei ei ei codellama 7b codellama 13b codellama 34b deepseekcoder .7b deepseekcoder 33b gpt .
turbo gpt table v model performance in generating functions with deeper dependency levels .
modelpython java ei humanevo ei humanevo intra classcodellama 7b .
.
.
.
.
codellama 13b .
.
.
.
.
.
codellama 34b .
.
.
.
.
.
deepseekcoder .7b .
.
.
.
.
.
deepseekcoder 33b .
.
.
.
.
gpt .
turbo .
.
.
.
.
gpt .
.
.
.
.
.
inter classcodellama 7b .
.
.
.
.
.
codellama 13b .
.
.
.
.
.
codellama 34b .
.
.
.
.
.
deepseekcoder .7b .
.
.
.
.
.
deepseekcoder 33b .
.
.
.
.
gpt .
turbo .
.
.
.
.
.
gpt .
.
.
.
.
.
b. rq2 how does the code generation performance change as repository evolves?
to further demonstrate the impact of evolution ignored setting we treat different versions of the project as context sources to explore how the performance of llms change over time.
we start from the version right before the target code was committed and incrementally increase the project versions up to the latest version.
typically a project s version number consists of two digits in the format v1.
where the first digit denotes the major version and the second digit denotes the minor version.
in figure evo0 is the version right before the target code was committed standing for the evolutionaware setting.
for evo1 to evo10 the version numbers are incrementally increased.
for example if evo0 corresponds to the v1.
version of the project then evo1 will be v1.
.
as shown in figure we find that the performance of llms deviates from the actual performance in the evo0 evolution aware setting with this deviation increasing as the project evolves.
all studied llms regardless of being opensource or closed source and irrespective of parameter size consistently exhibit inflated performance and follow a similar trend in performance changes.
this result further confirms that ignoring project evolution can lead to unrealistic evaluation of llms.rq2 summary in the evolution ignored setting as the project evolves the performance of llms deviates from actual performance with this deviation increasing over time.
c. rq3 how different code descriptions in the dataset affect code generation performance?
during the benchmark construction process we notice that in real world development some projects tend to write detailed docstrings while the others prefer to describe the function s functionality using only several brief statements.
since these docstrings are used as input to models which may significantly affect model performance we explore whether the impact of the evolution ignored setting on model performance varies under different docstring styles.
to this end we manually write both brief and detailed docstrings for each task instance in humanevo.
in this experiment we evaluate the performance of the studied llms on the two types of manually written docstrings.
as shown in table vi experimental results indicate that under the evolution ignored setting using different styles of docstrings as input consistently leads to inflated performance and the style of the docstring does not significantly change the impact of the evolution ignored setting on the inflated performance evaluation.
additionally the majority of llms perform better on detailed docstrings compared to the brief docstrings.
we suspect that the detailed docstrings provide a more comprehensive description of the functionality of the target function and include the implementation logic of the target function.
in addition detailed docstrings also provide explanations for the input and output of the target function enabling llms to better understand the tasks to be addressed.
conversely brief docstrings only introduce the intention of the target function which may lead to llms struggling to understand user intent in certain complex scenarios ultimately resulting in decreased performance.
however the decline in llms performance is not significant.
we suspect that the reason could possibly be related to our use of a retrieval augmented generation approach by which we have retrieved a substantial amount of context from the project through similarity calculations.
these contexts contribute to a lengthy prompt.
however when llms deal with overly long prompts their attention gets dispersed leading to the neglect of some useful information by the model.
rq3 summary from the experimental results we do not observe any correlation between docstring styles and evolution settings.
compared to brief docstrings detailed docstrings provide more beneficial information to llms resulting in better performance.
however the performance improvement on detailed docstrings is not substantial.
vi.
t hreats to validity we have identified the following potential threats that may affect the validity of our study.
firstly due to limited computational resources our experiments were not able to cover all the available code llms thus not fully reflecting the performance of all llms in actual development scenarios which maytable vi model performance under different docstrings styles .
model docstringpython java ei humanevo ei humanevo codellama 7bdetailed .
.
.
.
brief .
.
.
.
codellama 13bdetailed .
.
.
.
brief .
.
.
.
codellama 34bdetailed .
.
.
.
brief .
.
.
.
deepseekcoder .7bdetailed .
.
.
.
brief .
.
.
.
deepseekcoder 33bdetailed .
.
.
.
brief .
.
.
.
gpt .
turbodetailed .
.
.
.
brief .
.
.
.
gpt 4detailed .
.
.
.
brief .
.
.
.
slightly affect the representativeness of our experiments.
secondly currently humanevo only includes two programming languages.
in the future we plan to address this limitation by continuously expanding humanevo to cover as many programming languages as possible thereby providing a more comprehensive evaluation.
thirdly in prompt format selection we ultimately chose the one that yielded the best results after trying several formats.
however since we did not cover all available prompt formats the one we selected may not be the optimal one.
lastly our benchmark includes github data before which might already be present in the pretraining dataset of the models we studied which poses a risk of data leakage.
to mitigate this risk we have implemented an automated benchmark construction pipeline that continuously updates the dataset and rewrited the docstrings for every task instance in humanevo.
vii.
r elated work a. code generation benchmarks previously the vast majority of benchmarks were constructed to evaluate the performance of llms in generating standalone functions with the most representative ones being humaneval and mbpp .
humaneval comprises manually crafted programming problems covering language comprehension reasoning algorithms and simple mathematics.
mbpp consists of short python function problems primarily designed for novice programmers.
finding previous benchmarks fail to reflect complex programming scenarios researchers propose codereval the first repository level code generation benchmark to evaluate the performance of existing llms in real world development by prompting them to generate functions selected in actual complex software.
codereval meticulously selects python and java programming tasks from popular real world open source projects evaluating the functional correctness of generated code based on execution.
evocodebench collects data from real world repositories.
it also ensuresthat the distribution of programming tasks with or without dependencies matches the distribution of real repositories.
repocoder proposed repoeval which consists of the latest and high quality real world repositories covering scenarios such as line completion api invocation and function body completion.
currently most benchmarks for llm code generation provide simple functions mostly targeting general functions rather than specific domains thus limiting practicality.
however biocoder a benchmark for bioinformatics code generation has been introduced.
biocoder carefully selects high quality projects related to bioinformatics then parses these repositories to generate asts and obtain relevant data.
a custom code context is written for each filtered function including necessary imports cross file dependencies and some test cases.
however all the benchmarks mentioned above have a common flaw they all overlook the evolution of projects over time.
these benchmarks take the latest version of the project as the context source and such an evolution ignored setting makes it difficult for them to accurately reflect the performance of llms in real world development scenarios.
therefore in this work we propose humanevo the first repository level evolution aware code generation benchmark.
before obtaining the project context for the target function we roll back the repository to the state before the target function was committed to ensure that the project context provided to llms does not suffer from issues such as future context leakage and missing useful context.
our objective is to offer llms a more realistic development scenario thus more accurately reflecting the actual performance of llms in real world development scenarios.
b. repository level code generation many technical studies have been proposed to better tackle repository level code generation tasks .
docprompting uses natural language nl intent to retrieve documents related to the target code and then sends nl intent and retrieved documents to llms.
cocomic develops a cross file context finder which is a static code analysis tool used to retrieve the most related crossfile context.
mgd leverages ides to assist in providing context.
ide is invoked to provide useful suggestions when reaching the pre defined trigger point.
rlpg analyses dependencies potentially existing in the target function then extracts project context that may be conducive to generating the target function.
repocoder first slices the source code files in the repository to construct a retrieval library then uses the generated problem as a query calculates similarity to obtain project context and feeds the project context and problem to the model to generate initial results.
after obtaining the initial results the previous generation results are used as queries for further querying to obtain more context.
this process is repeated to continuously optimize the generation results.
a3 codgen first constructs a repository knowledge base covering knowledge of third party libraries and extracts information on mobile and code context.
during the codegeneration process llm first generates an initial version of code based on nl intent then uses nl intent and the initial version of code to retrieve as much foundational knowledge as possible from the knowledge base for the llm.
finally local global and third party library information is integrated into the prompt to allow the model to generate the final results.
all the mentioned works treat the latest version of the project as the context source falling into the category of evolution ignored scenarios.
although in repocoder researchers begin to take notice of this issue by considering all code after the target function in the corresponding file as future context this still falls short of reality and introduces a lack of the context that should be present.
therefore we propose a novel repository level code generation benchmark humanevo an evolution aware pragmatic benchmark to better simulate real development scenarios for llms.
c. llms for code generation codellama is further trained based on llama2 .
it is capable of understanding and generating code in various programming languages including but not limited to python java javascript etc.
codellama not only generates complete code but also provides suggestions and optimization solutions for specific code snippets greatly assisting developers.
during the training of the base model weights are initialized with an equivalent parameter amount of the llama2 model followed by training on a dataset of billion tokens of code.
starcoder is derived from starcoderbase which is trained using trillion tokens from the stack a vast collection of licensed github repositories with inspection tools and selection exit procedures.
the deepseek coder takes into account the often intricate dependency relationships inherent in real world projects.
the model needs to fully consider these dependencies before completing programming tasks.
therefore to equip deepseek coder to handle real world complex projects the training data is structured to support repositorylevel comprehension enhancing the model s understanding of complex projects.
this enables the model to utilize cross file context effectively.
metagpt constructs a multi agent collaborative framework employing standardized operating procedures and involving multiple roles such as programmers and product managers to collaborate on project completion.
the results generated by agents are not saved in conversational form but in structured output formats such as charts files etc.
communication between agents is also conducted through structured files.
metagpt also implements an iterative development pipeline optimizing results through multiple rounds of iteration by recording information generated during the development process by different roles.
similarly chatdev adopts a multi agent framework allocating and assigning different ai agents to various job functions.
chatdev facilitates communication among different role agents forming a complete software development process where each agent performs its respective tasks while collaborating uniformly.
in addition to these models there are many other excellentmodels used in the field of code generation .
viii.
c onclusion in this paper we highlight a previously overlooked flaw in evaluating existing repository level code generation methods the evolution of projects over time.
to address it we introduce humanevo an evolution aware repository code generation benchmark and conduct a comprehensive empirical study to explore llms performance in real world development scenarios.
experimental results demonstrate that the evolutionignored situation leads to the inflated performance of llms.
by incorporating temporal considerations into the benchmark humanevo provides a more comprehensive assessment of model performance in pragmatic development scenarios.
we believe that our empirical results and humanevo will offer valuable insights for the future development of llms based repository level code generation methods and benchmarks.
ix.
a cknowledgements this work is supported by the national natural science foundation of china the guangdong basic and applied basic research foundation 2023a1515012292 and ccf huawei populus grove fund ccfhuaweise202403.