generating and visualizing trace link explanations yalin liu jinfeng lin oghenemaro anuyah ronald metoyer jane cleland huang university of notre dame notre dame in yliu26 jlin6 oanuyah rmetoyer janehuang nd.edu abstract recentbreakthroughsindeep learning dl approacheshaveresulted in the dynamic generation of trace links that are far moreaccurate than was previously possible.
however dl generated links lack clear explanations and therefore non experts in the domaincanfinditdifficulttounderstandtheunderlyingsemanticsof the link making it hard for them to evaluate the link s correctness or suitability for a specific software engineering task.
in this paper we present a novel nlp pipeline for generating and visualizing trace link explanations.
our approach identifies domain specificconcepts retrieves a corpus of concept related sentences mines concept definitions and usage examples and identifies relations be tweencross artifactconceptsinordertoexplainthelinks.itapplies apost processingsteptoprioritizethemostlikelyacronymsand definitions and to eliminate non relevant ones.
we evaluate ourapproach using project artifacts from three different domains of interstellar telescopes positive train control and electronic healthcaresystems and thenreportcoverage correctness andpotential utility of the generated definitions.
we design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface.
results show that the explanationspresented in the interface helped non experts to understand the underlyingsemanticsofatracelinkandimprovedtheirabilityto vet the correctness of the link.
keywords software traceability explanation interface concept mining acm reference format yalin liu jinfeng lin oghenemaro anuyah ronald metoyer jane clelandhuang .
.
generating and visualizing trace link explanations.
in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm new york ny usa pages.
https introduction software traceability establishes connections between related artifacts andthenutilizesthoselinkstosupportnumeroussoftware engineering tasks such as safety assurance impact analysis and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
.however giventhenon trivialcost and effort of manually creating trace links researchers have vested significant effort into automating the process using information retrieval ir machinelearning ml andmorerecently deep learning techniques dl .
in general a project stakeholder will issue a trace query generate a set of links and inspect the resulting links to accept or reject individual resultseither as a standalone vetting activity or at point of use.
tracelinksgeneratedusingirandmlapproachesareofteneasy to analyze but tend to deliver relatively low accuracy on large industrialdatasets .however recentadvancesindltracing techniqueshavereturnedfarhigher degreesofaccuracy.forexample in tracing from requirements to code lin et al.
showed that their tracebert approach which leveraged pretrained bert models and applied multi staged fine tuning delivered highly accurate trace results for three large open source systems achieving mean average precision map scores greater than .
across several large datasets .
unfortunately dl generated trace links can be difficult to interpret without supporting explanations of their underlying semantics.
forexampleconsiderarequirementstatingthat therobotshall movetothenextpositionintheorderspecifiedbythetaskplan and a corresponding design definition that the rcu shall publish an ackermanndrivestamped message to the robot s control topic .
despite having no meaningful common terms the artifacts are linked because the designsolution contributes towardssatisfying therequirement.ananalysisoftheconceptsshowsthat ackermanndrivestamped messages are closely associated with movements i.e.
carry velocity angles and timestamps and that task plans ofteninvolvemovement.adomainexpert inthiscase someone familiar with the robotic operating system ros could likely inspectthetwoartifacts applytheirinnateknowledgeofthedomain and intuitively understand the connection between the ackermanndrivestamped messages and the robot s movement.
however someone lacking domain expertise or knowledge of the specific project may have difficulty understanding the underlying concepts andconnectingthetwoartifacts.wethereforebelievethatexplanations are useful across a range of expertise levels including nonexperts e.g.
students andthosewithpartialdomainknowledge e.g.
onboarding team members .
this paper therefore addresses the challenge of trace link explainability defined as the ability to explain why two artifacts are relatedtoeachother.
traceexplanations canincludebothtextual information as well as visualizations and are designed to facilitate reasoningandunderstandingofsemanticrelations.theyareparticularly important when analysts or trace link users lack domain expertisetoindependentlyunderstandtheunderlyingsemanticsof a trace link especially as it has been shown that non experts often mistakingly discard correct links during the link vetting process ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa liu et al.
figure workflow of our method for extracting the explanation elements from scratch.
.ourworkaddressesacurrentgapintheliterature asprevious studies have focused on tracelink accuracy maintainability and efficiency while overlooking the importance of explainability.inpriorwork dick proposedtheuseoftrace rationales in which link creators or vetters would document the rationales behind a link.
however manually annotating links with rationales requires nontrivial effort thereby increasing the overall costandeffortofcreatinglinks.second aspreviouslystated dl approaches often produce links with underlying rationales that are obscure to non experts.
we therefore propose an approach for explainingtracelinksthataimtoautomaticallygenerateexplanations throughmining extracting and learningrationalesfrom external knowledge sources.
our nlp pipeline executes the following automated steps to generatealinkexplanation.first itextractsdomain specificconceptsfromtheartifacts andthenusestheseconceptswithinsearch queries to retrieve the broader context of each concept from diverseknowledgesources.theresultingdatasetconstitutesa context corpus for the target project.
next it applies natural language processing nlp techniques to extract various forms of structured knowledgefromthecontextcorpus andfinally incorporatesthis structured knowledge into a trace link explanation that includes bothtextualdescriptionsandvisualizationtechniques.weevaluate our approach against three industrial datasets reporting accuracy and coverage metrics.
further we conduct a controlled user study andreportresultsshowingthatutilizingthegenerateddescriptions as rationales within a trace link explanation interface helps nonexpertstounderstandartifactandlinksemanticsandtoperform the trace link vetting task more effectively.
our work makes three primary contributions.
first we propose implement andevaluate annlp pipelinefor automaticallyidentifying domain specific concepts and mining acronym expansions definitions andcontextualizedusageexamples.second weaddressthe challenging problem of data sparsity for specific project domainsbyintegratingbothtop downandbottom updatamining techniques so that we can adapt our approach to different domains withdifferentdatasources.finally weevaluatetheeffectiveness of our approach through designing and evaluating an explanation interface with non expert users.
the remainder of this paper isorganized as follows.
section provides additional backgroundinformation.
sections and describe the three datasets used in our study and present the nlp pipeline used to generate each part of our trace link explanation.
section takes a quantitative look at each technique and its utility across three datasets while section describes our design of the explanation interface and describes the controlleduser studythatwasconductedtoevaluateitseffectiveness.finallysections7to9discussthreatstovalidity relatedwork and present conclusions.
trace link explanations as previously explained we seek to generate explanations that explain the rationale for trace links in a way that is useful for nonexperts astheyaretheuserswhoexperiencethegreatestdifficulty in evaluating the correctness of a link.
.
the semantic gap various types of artifacts such as requirements and design are often written using different and potentially mismatched terminology.
this mismatch can make it challenging for non experts in the domaintounderstandwhytwoartifactsareconnectedbyatrace link.
a few researchers have explored ways to bridge this gap.
guoetal.proposedatechniqueforgeneratingtracelinkrationales however theirexplanationsweredeeplycoupledwithheuristics embeddedintheirunderlyingtrace linkgenerationalgorithms .
liu et al.
used the generalized vector space model to improve authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
generating and visualizing trace link explanations icse may pittsburgh pa usa table software project datasets used for explanatory and evaluation purposes are drawn from three distinct domains name domainsource target traceavailablecount artifact description count artifact description links cchit electronic health records world vista requirements cchit regulations cm1 nasa telescope low level requirements high level requirements ptc positive train control subsystem requirements system requirements proprietary the quality of generated trace links and used the higrowth algorithm toconstructahierarchicalmodelinwhichconcepts were linked through synonyms acronyms ancestors and siblings however they did not consider generating trace link explanations.
inthispaper weexplorethese andadditionaltechniques witha focusonexplainingthesemanticsofeachindividualartifactand the conceptual relationships across pairs of linked artifacts.
.
artifact semantics tracelinkexplanationsshouldfirstprovidedescriptionsofconcepts withinindividualartifacts.weidentifiedthreeimportantaspects ofartifactexplanations forenhancingtheunderstandabilityoftechnical artifacts.
these included acronym expansions concept definitions and contextualizedusageexamples.building upon ourpreviousexamplewecouldexpandtheinternalprojectacronym rcuto roboticcontrolunit provideadefinitionforan ackermanndrivestampedmessage as time stampeddrive commandfor robots with ackermann steering and provide an example context for the use of the term ackermanndrivestamped message.
.
link semantics theexplanationalsoneedstodescribethesemanticrelationship between two linked artifacts by identifying related concepts across source and target artifacts.
we explore two primary ways in which concept to concept associations could be explained.
first as a tripletwrittenas ci v c j wheretheconcepts ciandcjareconnectedwithadescriptivephrase vtoindicatetheircorrelation .
asliuetal.
demonstrated therearemultiplewaysinwhich two semantically related concepts can be connected over a relation path.
examples might include ackermanndrivestamped msg published to teleop topic o r teleop controls robot movement .
whilewewouldideallyusenaturallanguage togenerateexplanations dynamicallyconstructingclearandconcisesentencesisadifficultchallenge andthereforewefocusthispaperontheinitial challenge of discovering meaningful triplets that explain trace link relations.
.
proposed solution fig.
provides a high level overview of our approach.
in step we analyze project level artifacts to extract domain specific concepts focusing on noun phrases.
this step produces thousands of candidate phrases including both domain specific and commonly used phrases.
step then filters the list of concepts identified inthe project artifacts to remove general concepts e.g.
data struc ture userinterface .theremainingconceptsbecomethetargetsof our explanations first for individual artifacts and second as part of the trace link explanations.
in step we retrieve a domain corpus of sentences containing these concepts exploring two techniquesbasedontop downfilteringofalargedomaincorpus andbottom upsearch drivenbytheproject specificconcepts.thisproduces a large corpus of sentences each of which includes at least one targeted domain specific concept.
step applies a variety of nlptechniquestoexpandacronyms generatedefinitions discover context and to build relation triplets all of which are needed intheexplanation.instep5 webuildamachinelearningquality control modelwhich automatically filters non relevantsentences to improve the accuracy of our explanations.
finally in step theseexplanatoryelementsarevisualizedandpresentedtotheuser within the explanation interface.
project domains datasets throughout the remainder of this paper we focus explanations and experimental analysis on three target software engineering domainsofelectronichealth care aspacetelescope andpositive train control cf.
table .
the domains were selected for their diversity availability of project artifacts and because each one represented a technical domain with specific terminology and jargon.
twodatasetsarepubliclyavailable whilstoneisproprietaryand provided by our industrial collaborators.
cchit is from the domain of electronic health care records ehr andincludestracelinksbetween117requirementsfrom theusveteran sworldvistahealthcaresystem e.g.
thesystem shallallowevent delaycapabilityforpre admission discharge and transfer orders and regulatory requirements specified by the usa certification commission for health information technology cchit e.g.
thesystemshallprovidetheability tosendaqueryformedicationhistorytopbmorpharmacyto capture and display medication list from the ehr .
cm1includes low level requirements for a nasa spacecraft telescope e.g.
the tmali csc serves as an intermediate manager of event data supplied by the dci driver... traced to 23higherlevelones e.g.
thedpu tmalishallbecapableof makingdataavailablefromthedcitodpu dpa.dpu tmali will populate a ring buffer... .
this dataset is quite small and the project contains obscure technical jargon with limited online documentation.
ptcisfromthedomainofpositivetraincontrolandisprovided by our industry collaborator.
it traces subsystem requirements to system requirements.
we cannot provide examples due to the proprietary nature of this dataset.
mining explanation elements the concept detection step is designed to extract high quality phrases representing key domain concepts from a text corpus.
severalresearchershaveshownthebenefitsofaphrase basedapproach based on information retrieval taxonomy construction and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa liu et al.
topic modeling .
for example liu et al.
p r o posedatechniquethatintegratesphraseminingtechniqueswith phrasal segmentation and argued that their approach outperforms many other approaches.
it starts by identifying the most common n gramsandthenappliesqualitycriteriatoremovelow quality less commonconcepts butin technical phrases that are of particular relevance for explaining hard to understand trace links.
for example the previously discussed ackermanndrivestampedmessage wouldbeunlikelyto survivethefilteringprocess.theapproachrequiresuserstoprovide a small set of example phrases for training purposes.
shang proposed autophrase whichusedphrasesfromsection titles in repositories such as wikipedia to train their model to extractconceptphrasescomposedofnouns adjectives andadverbs.
asaprecursortotheworkwepresentinthispaper weconductedan initialseriesofexperimentsusingautophrasetrainedonwikipedia toproduceahugeconceptlistcoveringawiderangeofdomains.
however wefoundthatautophrasedidnotperformwellinourtargeted software engineering domains primarily because wikipedia lackedsufficienttrainingdataforourdomains.autophrasenotonly overlooked important concepts but also extracted phrases with incorrect grammar and or redundant adjectives.
we concluded that a key obstacle in using ml based models to identify core concepts for trace link explanations is the lack of sufficient training data for specific software engineering project domains.
.
adopting a dependency analysis approach previous research has noted the importance of noun phrases inthecreation andcomprehension oftracelinks .given theissuesweencounteredinextractingmeaningfuldomainconceptsusingmltechniques coupledwiththeimportanceofnoun phrases we opted to leverage stanford dependencies sd analysis and focused upon detecting meaningful noun phrases as domainconcepts.stanforddependencyanalysisidentifiesdirect relationsbetweentokenswithinasentencebycategorizingtheir relationships into pre defined types.
this technique is a rule based approachbuiltusingapre trainedphrasestructuregrammarparser.
the compound relationsinsdrefertoanouncompoundmodifier of an np pos tag that is used to annotate the head noun in a noun phrase .asillustratedinfig.
weusethistypeofdependency to determine the boundary of a noun phrase in this example applied to a sentence from the ehr domain.
.
filtering out general concepts analyzing project artifacts in this way tends to produce a large number of concept phrases some of which are domain specific phrases worthy of explanation while others are commonly occur ring phrases with well understood meaning.
as we do not wish to clutterourexplanationswithsuperfluousinformation wereduce theconceptlisttoincludeonlydomain specificones.weachieve this through generating a black list of general concepts.
this is achievedbyapplyingthestanforddependencyanalysistoamas sive domain independent corpus identifying themost commonly occurringconcepts andstoringthemastheblacklist.forpurposes of this study we used the umbc webbase corpus which was built using web scraping in by the stanford webbase project.the dataset contains english paragraphs with over three billion words.
it is 13gb in compressed tar file format and is 48g when uncompressed.afterapplyingtheconceptdetectiononthiscorpus we obtained noun concepts.
we ranked concepts by their frequency included concepts that appeared more than times in our black list.
the black list represented the top .
of the detectedconcepts.
foreach of ourthree datasets we removed any concept found in the black list.
examples of project specific concepts as well as blacklisted ones are listed in table .
.
constructing a concept domain corpus the next step in our pipeline focuses on building a domain corpusinwhicheachsentenceincludesatleastoneproject relevant domain concept.
sentences are mined from either an existing general corpus or through searching the internet for relevant text.however we observed a huge discrepancy in the amount of text available for different domains and this likely accounts for the different outcomes we report later in this paper for each of our three projects.
popular domains such as biomedical and finance have numerousrelatedwhitepapersandwebsitesdescribingthedomain and sometimes even a textually rich well organized corpus e.g.
ncbidiseasecorpus reuterscorpora collatedbydomain experts.however manysoftware projectshaveno previouslycollatedtextcorpus andfurthermore somerelativelyobscuredomains have very limited web presence.
to address these dual problems weapplyautomatedcorpuscollectiontechniquestobuildadomain corpusforeachtargetedprojectwhilstensuringthatretrieveddocuments have sufficient affinity to the targeted project.
our objective is to mine a focused corpus covering all concepts in the target project and we explored both top down and bottom up corpus collection strategies.
.
.
top downapproach thetop downapproachstartsfroma relatively large corpus and operates downwards to identify and retainrelevanttext whilsteliminatedallotherparts.thecorpus documentsarefirsttokenizedintosentences andthentheknuthmorris pratt kmp string matchingalgorithmis usedtoefficiently examine whether the sentence containsat least one ofthe identifiedprojectconcepts.matchingsentencesarethenmapped totheirassociatedconcepts.weutilizedthearxivrepository table examples of project and blacklisted concepts cchit cm1 patient health information eeprom filesystemhl7 astm continuity x ray sensitive ccd imagerhipaa risk assessment dpu task monitoringcardiovascular tests fsw tasksncpdp script dci error interrupt ptc blacklist wayside data user interface emp protocol team manager obu transitions family health class c protocol network operator train control functions useful results authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
generating and visualizing trace link explanations icse may pittsburgh pa usa a three criteria for a concept definition.
b handle the transformed sentence with a clause figure an example of using using stanford dependency based rules for concept definition and context extraction.
this approach is more robust than pattern matching on complex sentences whichincludesabstractsandthefull textofacademicpapersacross physics computerscience biologyandfourotherlargedomains and then downloaded 248gb of plain text data through an api provided by arxiv for this purpose.
.
.
bottom upapproach thebottom upapproachstartswith anemptydomaincorpusandthengraduallyaddsdatabysearchingexternalresourcesusingapublicsearchengine i.e.
bingfor thisstudy .
searchqueries areformulatedfrom thepreviouslyextracted project concepts and retrieved websites are scraped to find sentencescontainingthetargetedconcept.weinformallyexperimentedwith severalquerytemplatesand foundthatthe template what is inbody concept in domain name returned the most consistently relevant results using the bing search engine where concept referstoaprojectconceptthathasbeenautomatically detected and domain name i.e.
electronic health record positive train control and nasa helps the search engine to narrow the scope of the search and to disambiguate similar concepts across different domains.
once results were returned we apply the same technique as the top down approach to remove sentences that do not contain at least one project concept.
.
extracting explanation elements wethenapplyaseriesofnlptechniquestoextractthefollowing descriptions and definitions from the constructed corpus.
.
.
acronym descriptions we define a concept as an acronym if allthealphabeticcharactersintheconceptareupper case.wethen utilizetheschwartz hearstalgorithm tominetheacronyms from the collected corpus.
this algorithm leverages pattern matchingandheuristicrules e.g.phraselength todetectthelongand short forms of the acronyms and returns them as mapped pairs.
as reportedbytheauthors theschwartz hearstalgorithmachieved82 recallatprecisionof96 whenappliedtothebiomedicaldomain outperforming methods previously proposed by chang et al.
and pustejovsky et al.
.
.
.
definitionsandcontext.
giventhecorpusofconcept specific sentences weutilizestanforddependenciesheuristicrulestoextractdefinitionsandcontextdescriptions.forcontextextraction we first check whether a concept is the nominal subject of a sentenceandisconnectedtoanotherwordviaa nsubj dependency.
anominalsubjectisanounphrasewhichisthesyntacticsubject ofaclause andthereforeapplyingthisruleensuresthatthe sentence focuses on the target concept.
to identify definitions wealso check the verb connected to the target concept.
as the dependenciesformadirectedrelationgraphwelocatetheassociatedverbbysimplysearchingforverbsthatareatmosttwo hopsawayfromthethetargetconcepts whilesimultaneouslyconstrainingpathsto containonly nsubj and cop copula dependencies wherethe cop dependencyreferstoarelationbetweenacopularverbandits complements.asdefinitionsusuallyfollowasetofknownpatterns such as concept is are do something we only select the sentences whose verbs are either is are or with pos tags of vbz referringtopresenttenseverbs .weobservedthedependenciesbased approach to be more robust for handling complex sentences than a simple pattern matching approach as illustrated in fig.
whichshowshowrulescanbeusedtoextractadefinitionfor ccd from sentences with different levels of complexity.
.
.
filtering non domain acronyms definitions and descriptions theautomated extractionmethodsinevitably introduceimprecise resultsbyincludingsentencesandacronymsoutsidethescopeof thetargetprojectdomains.toaddressthisissue wedevelopeda deep learning topic model and leveraged it as a binary classifier to identifywhetheranacronym slongname adefinition oracontextsentencebelongstothetargetprojectornot.wetrainedthemodel in a weakly supervised manner by utilizing actual artifacts i.e.
requirements designdefinitions fromthetargetprojectaspositive authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa liu et al.
examplesandsentencesfromothersources e.g.artifactsfromother projects as negative examples.
we used these example to train the model to identify sentences belonging to each target project.
the trained model accepts a tokenized candidate sentence as input andleveragesabert basedlanguagemodel lm toanalyze the semantic meaning of the tokenized sentence.
it uses a small multi layerperceptron mlp topredictalikelihoodscorebetween and representing the affinity between each sentence and the targetedprojectdomain.inthisstudy weusedscibert asour languagemodel whichhas beenpre trained withmassivepapers in difference science domains.based on initial observations of the results wefiltered out allsentences scoringless than .
.further werankedallremainingdefinitions contextualinformation and acronymexplanationsaccordingtotheirscores andineachcase selected the one with the highest score.
we evaluated the accuracy achievedwithandwithoutthisfilteringsteptotestitsutilityonthe entirecorpusofretrievedsentencesforallthreeprojectsandfound that accuracy improved from .
to .
for the bottom upapproach and from .
to .
for the top down approach thereby demonstratingthe importance of this part of the pipeline.
.
.
relationdiscovery thepreviousstepshavefocusedondescribing concepts found in each individual artifact.
in this step we redirectourattentiontodescribingthelinkitselfbyapplyingthree techniques for identifying relations between concepts.
first we extract and formulate relations between concepts by exploring the simple subject verb object grammar in the corpus we have collected by leveraging stanford dependencies that incorporate verb tokens.
more precisely we use the nsubj and xsubj tags to find thesubjectoftheverband obj dependenciestoidentifytheobject ofthe verb andonlyaccept tripletsforwhichthe verbrepresents a hierarchical or equivalency relationship e.g.
includes as this impliesaparent childrelationwhichcanbeusedtobuildmeaningfulcross artifactrelations.forexample thetriplet navigational information includes operational hazards could help us to understand why a requirement stating that the obu shall transmit navigational information tothebackoffice islinkedtothederived requirementthat thewiushalldetect operational hazards .
we createdasetofeightseedverbsrepresentinghierarchicalrelations and then expanded this set by retrieving four additional synonyms fromwordnet .wethencombinealloftheretrievedtriplets intoaknowledgegraph inwhichthesubjectsandobjectsofthetriplet relations are used as vertices and their inter connecting verbs are used as edges.
given two concepts distributed across a pair of linked source and target artifacts we use dijkstra s algorithm tofindtheshortestpathbetweeneachcandidateconcept pair.thepath includingitsnodesandedges constitutesapotential explanation for an underlying trace link.
in addition we consider two concepts as equivalent when their lemmatized forms are identical or when one concept is a sub sequence of another e.g.
tmali versus tmali event queue in which case we consider the shorter concept to be a semantic abstraction of the longer one.
quantitative analysis in our first set of experiments we investigate the potential use fulness of artifact and trace link explanations by evaluating thetable number of acronyms exist in projects and the precision and recall score for explaining these acronyms top down bottom up acronym precision recall precision recall cchit .
.
.
.
cm1 .
.
.
.
ptc .
.
.
.
correctness of the generated explanation elements and the percentage of artifacts and trace links for which associated elements were mined.foreachexplanationelementandeachofthethreeprojects we addressed the following research questions rq1 what percentage of the generated explanation elements are correct with respect to the project domain?
rq2 what percentage of the identified elements i.e acronyms or domainconcepts inartifacts have correspondingcorrectexplanatory elements for use in trace link explanations?
to answer these questions three researchers evaluated the correctness of the generated acronyms definitions and context descriptions.
in some cases our domain knowledge was sufficient todirectlydeterminewhetheraconceptwas correct however in othercases werevieweddocumentationmanualsandwhitepapers to discover or confirm the correct meaning of the concept.
.
acronym evaluation table3reportsresultsforretrievingacronymdescriptionsforall three projects.
the top down approach generated descriptions for and69acronymsatprecisionsof and forfor cchit cm1 and ptc respectively.
this compares to only and acronym descriptions at and precision for cchit andptc andno acronymsfound forcm1.
thereare twoparticularly notable observations.
first the bottom up approach returned fewer butmoreaccurateresults.thisapproachlikelyperformed better because search queriesincluded more domain specific context than the top down approach.
second the pipeline completely failed to retrieve any correct acronym descriptions for the cm1 project.
while early phases of the pipeline produced acronyms the quality filter correctly eliminated of these as the definitions came from different domains.
there are several primary reasons forthisfailure.first theacronymsincm1 sdesignspecification refer to very low level architectural components second the cm1 domain contains more technical jargon than either cchit or ptc andthird thedomain ofinterstellarsatellites hasfar feweronline descriptions within white papers websites or other documents.
forall ofthesereasons theacronym expansionfailedin thecm1 project but produced useful results for cchit and ptc.
these results contrast clearly with prior claims that the schwartz hearst algorithm returned accuracy however those prior results focused on the mechanisms for extracting acronym descriptions from a document in which correct descriptions were available.
.
definition and context evaluation tables 4a and 4b report results from performing the manual evaluation for top ranked definitions and contexts for each concept.
the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
generating and visualizing trace link explanations icse may pittsburgh pa usa table4 coverageandaccuracyoftheextractedsentencesas concept definition and context top down bottom up extr.
correct acc.
extr.
correct acc.
cchit .
.
cm1 .
.
ptc .
.
total a numberandaccuracyofprojectconceptswhichhavedefinitions extracted from corpus top down bottom up extr.
correct acc.
extr.
correct acc.
cchit .
.
cm1 .
.
ptc .
.
total b numberandaccuracyofprojectconceptswhichhavecontext extracted from corpus top downapproachretrieved15 1and16definitionswithaccuraciesof80 and forcchit cm1 andptcrespectively whilethebottom upapproachretrieved12 and1definitionsat accuracies of and respectively.
in the case of contextual descriptions the top down approach retrieved and definitionsataccuraciesofapproximately65 and19 while the bottom up approach retrieved and definitions at accuracies of approximately and .
overall the bottom up approachgenerallyretrievedfewerbutmoreaccurateresults.again we observe low retrieval rates for cm1 with only one definition inthetop downapproachandonlythreeinbottom up however these were retrieved at accuracy.
the bottom up approach also underperformed for the ptc dataset only finding one correct acronym definition.
despite sending unique concept queries to the search engine only .
of them returned sentences with directconceptmatches.thiscomparedto16.
and36.
direct matches for cchit and cm1.
this coverage problem has been observed by previous researchers.
for example zeng et al.
reported that onlyabout half ofthe termsfound in themesh medical taxonomy appeared anywhere in the pubmed database of million paper abstracts .
false positives could likely be further reduced by providing a more diverse set of training examples.
.
concept relation evaluation to evaluate concept relations we examined the generated paths for coverage and correctness.
unfortunately the knowledge graph sufferedfromapathsparsityissueandthereforefewmeaningful pathswerefoundbetweenconceptsinpairedartifacts.thesparsitywasprimarilycausedbylimitingverbstothoserepresenting hierarchicalrelationships therebysignificantlyreducingthesizeof the triplet set.
while accepting a broader set of verbs creates a far largersetoftripletsandmanymorecross artifactrelations themajority of these paths do not produce meaningful explanations.
we thereforeoptedtofavorprecisionoverrecallandexcludedmultihop paths generated from the knowledge graph in our explanationinterface.however theheuristicrulesforequivalenciesandsubsequences along with the hop paths retrieved several interesting explanations such as icd used for billing resulting in and concept relation explanations for the three projects.
.
leveraging available project glossaries while our bottom up approach returned fairly accurate results there were a large number of artifacts for which no supporting definitions were retrieved.
we therefore decided to explore the combinationofboththebottom upandtop downtechniquealongside definitionsprovided byproject specificglossaries and subsequently identified and retrieved a glossary for each project.
the cchitprojecttracestherequirementsinwordvistaehrsystem to the cchit regulatory requirements and we used a glossary fromthewordvistaproject containing352acronymswiththeir expanded names and concepts with associated definitions and contextualexamples.theprovidedcm1glossary provideslong names for acronyms used in the project.
finally we derived the ptcglossaryfromthearchitecturespecificationdocument of interoperable train control network itcnet released by the meteorcomm company and containing acronyms with long names and concepts with definitions.
we checked for overlap of definitions.
results reported in fig.
bottom up green top down red approaches and glossaries purple show that different projectconcepts were provided by each of the three techniques.
addingthe definitions generated by the bottom up approach to each of theexistingglossariesincreasedexplanationelementsby forcchit forcm1 and150 forptc.ofcourseactualgainsare highlydependentuponthecompletenessofthebaselineglossary.
given the benefits of combining glossary and generated data and theaccuracyofthebottom upapproach weusedthesetwodata sourcesintheuserstudydescribedinthenextsectionofthispaper.
trace link explanation interface we designed and developed an explanation interface and used it to address the following two research questions rq4 does the trace link explanation interface help users to evaluate the similarity between two linked artifacts more effectively than without the explanation?rq5 which aspects of the explanation interface are most helpful to users?
.
designing the explanation interface weadoptedtheexplanationdesignframeworkintroducedbyanuyah etal.
toguidethedesignandevaluationofourexplanationinterface and engaged in a series of three participatory design sessions thatincludedsixmembersofourteamwithexpertiseinux design and orsoftwareengineering.throughthesesessionsweidentifiedsoftwareengineeringtasks suchas impactanalysis andcompliance verification thatwouldutilizetraceability andidentifiedmultiple typesofusers.ofthese wefocusedprimarilyonpeoplewithout domain expertise e.g.
project newcomers and people perform ing tasks across skill boundaries e.g.
a business analyst tracingfrom requirements to low level models or code where they may be exposed to terminology they are not familiar with.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa liu et al.
a cchit b cm1 c ptc figure number of the project concepts explained by project glossaries as well as our top down and bottom up approaches we then identified a set of tasks that a user would perform using the interface.
these included identifying artifact types analyzingartifactcontent judgingwhethertwoartifactsarerelated and providing feedback on the correctness of the link.
we then brainstormed different ways of presenting the link explanationsin support of these tasks and created a set of low fidelity prototypes.
each prototype was explored to understand its strengths andweaknesseswithrespecttosupportingthepotentialusertasksandleveraginghumanperceptualcapabilities .thisactivity informed several design decisions each of which is reflected in the ui presented in .
first our relation extraction and acronym expansion process resultsinsemanticrelationsamonglinkedconcepts.connection marksusealinetoshowapairwiserelationshipbetweentwoitemsandthusareappropriateforshowingtherelationshipbetweentwo concepts .
second concepts themselves are nominal types and colorencodingsareparticularlyeffectivefornominaldata.additionally usingthesamecolorsforthesameconceptsacrossthetwo artifacts creates a perceptual grouping .
finally every concept isaccompaniedbyaquantitative importancevalue .sizeencodings whilenotidealforshowingspecificvalues areappropriatefor comparison purposes and thus we chose to encode the importance of a concept using font size .
thecombinedvisualelementsareintendedtoprovideanoverview ofthetotalrelatedelementsbetweenthetwoartifacts howthey areconnected andhowstrongtheconnectionsare.weadoptan overview detailapproach wheretheuserisprovidedanat aglanceoverviewandcanobtaindetailsthroughmouse overinterac tionsontheconceptnodesorconceptwordsthemselvestogetdefi nitions orontheconnectionedgestoobtainasemanticrelationship description.
we iterated through medium fidelity mockups created infigma toexploretheeffectivenessofthevisualencodings andinteraction strategies.ourfinal prototypewas builtasa fully functionalwebapplicationforuseinourevaluationandaninteractivedemoisavailableat .
study design we conducted a controlled user study to evaluate the effectiveness ofourtracelinkexplanationinterface.wedesignedourstudytohave two treatment conditions showing explanations for the links tc versus hiding explanations tc .
our goal was to examine theeffect thatreceiving explanationswould havefor identifying correct and incorrect trace links.
.
.
participants.
we recruited eight participants with the requirement of having some prior experience working on a software engineering project.
we did not exclude participants accordingto the number of years of experience or their specific role in a project.
all of our participants are currently graduate students of a universityintheunitedstatesandwererecruitedthroughemail.
participants were tasked to perform trace vetting on links of which were correct links while the remaining were incorrect.
each participant evaluated links from each of the three projects.
furthermore of definitions and acronyms were from theprojectglossaries weregenerateddynamicallyusingthe bottom up approach and were found in both.
figure two experiment groups were formed by samplingan equal number of trace links from each of the threeprojects.
for each link we created a version with an expla nationandonewithoutanexplanation andforagivenlink group received the explanation whilst group did not.each group received half the links with explanations andhalf without meaning that the two groups received oppo sitetreatments.wethenassignedanequalnumberofstudyparticipantstoeachgroup.theorderoflinkswasrandomlyshuffled for each participant who was then asked to evalu ate the correctness of each presented link.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
generating and visualizing trace link explanations icse may pittsburgh pa usa figure snapshot of the trace link explanation user interface.
in part we used a text panel to display concept level explanations including long acronym names definitions context and inter concept relationships based upon the current cursor roll over position.
in part we visualize the links by highlighting the concepts as they appear in the source and target artifacts.inpart3 weenlargetheconceptsbasedontheirimportanceandunderscoretheonesthathaveavailableexplanations.
.
.
procedure.
webeganthestudybybriefingparticipantsabout the task that they would perform.
next we directed participants to our web based explanation interface using the url that we provided.
participants were asked to vet links as correct incorrect or don tknow iftheywereunabletoconfidentlymakeadetermination .
participants completed all of the vetting tasks in a random order.inthiscase linksfromthethreedomainsdiscussedinsection3werepresentedrandomly.duringthesession participants wereaskedtoverballyexplaintheirdecisiononvettingthelinks enablingustocollectbothqualitativeandquantitativedata.the study took about minutes to complete.
we exposed each participant to the two treatment conditions with the aim of understanding the extent to which explanations can guide them in differentiating between correct and incorrectlinks.
to address the potential bias of learning effects we used abetween subjectsexperimentaldesignputtingeachparticipant in one of two groups such that participants in one group were exposedtotheoppositetreatmentconditionforeachofthelinks than participants in the second group see figure .
further the orderinwhichlinkswerepresentedtouserswasrandomizedfor each person.
.
results we examined our data using a combination of quantitative and qualitativemeasurestounderstandtheimpactofourexplanation design on trace link vetting.
.
.
quantitative result outcomes.
overall our results show that there was a significant improvement in accurately vetting links when participants were provided with explanations .
p .
.
d .
.
this finding indicates that the explanations helpedtoguideourparticipantsforidentifyingcorrectandincorrect tracelinks.however whenweexaminedthedifferencesforeach domainindependently weobservedthattherewerenostatistical improvements ptc .
cm1 .
cchit .
.
we attribute this finding to the fact that the data collected in each domain was too small to arrive at a statistical conclusion.
.
.
qualitative insights.
all participants responded positively to the explanations and expressed how helpful they were for the vettingtasks.noneoftheparticipantswereexpertsinthedomainsunder study and were therefore unfamiliar with many of the domainspecific technical terms and acronyms in the artifact content.
this resulted in many participants selecting don t know when they werenotgivenexplanations.p4stated idon tknowwhatthisterm means.
i want to know what things mean.
p6 noted the explanations helped me to not just easily identify keywords but better understand what they mean.
participants also noted that receiving explanations reduced the mentaleffortittookforthemtocomprehendthecontentofthearti facts especiallywhenthelengthwaslong.mostoftheparticipantsstruggled to understand the content meaning when presented with long artifacts without explanations often resulting in the don t know response.
p8 for example when presented with a long artifactdescription stated thisartifactrequiresalotofthinking.there is too much text and too much function names and abbreviations without explanation.
several participants noted that the explanations helped them easily find common terms or keywords in the artifact content.
for links without explanations however participants often resorted authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa liu et al.
to using the find feature on their browser to identify common terms in the artifact content.
being able to easily identify these commontermsintheexplanationsmadeitpossibleforparticipants to more quickly make a decision.
p8 noted that because drug interactions was highlighted i identified what was specifically not to beincluded.contexthelpedmeknowwhatthesystemisexpecting.
p8 also commented about finding keywords in large content artifacts if the text is too huge highlighting the keywords was helpful.
i could easily jump to the important part.
while we did not collect timing information for participants tasks anecdotally participants made decisions more quickly when presented with the explanation.
wealsoobservedthatthesemanticconnectionsbetweenartifact concepts helped our participants in identifying if artifacts were linked.
while some participants struggled to determine links when the concept terms did not exactly match they noted that seeing the relationship path made it possible for them to not only see that artifacts were linked to examine the relationship more directly to buildconfidenceintheirunderstanding.forexample p3noted i caneasilyseethatthesearenotsemanticallythesame.
p7foundit ...helpful to see overlapping keywords and phrases.
whilethefeedbackwasoverwhelminglypositive participants alsonotedsomeareasforimprovement.forinstance theynoted that some concept terms were too ambiguous to understand even with definitions.
some of them also mentioned that they needed contextforfunctionsandotherartifactsthatwerereferencedinthe artifactcontent.p7stated idon tknowwhatcertificationisbeing referred to here.
i need some context.
threats to validity our study carries a few threats to validity.
with respect to internal validity weevaluatedourapproachonthreedifferentsystemsfromdiverse domains for which golden answer sets defining correct and incorrectlinks werealreadyprovided.
weuseddomain documentation to evaluate the correctness of the generated definitions and descriptions.
with respect to external validity we observed trends across the datasets such as the observation that the bottom upapproach returned more precise results than the top down one.
however we cannot draw general conclusions based only on three datasets.
for example while we have hypothesized that the rea son for cm1 s underperformance is that it is a highly scientificsystem with jargon filled project artifacts and therefore general domain documents failed to provide relevant concepts we cannot categorically support such generalizations at this time.
as with any nlp pipeline we made numerous design decisions and whilst we justified our decisions it is likely that different combinationsoftechniqueswouldreturndifferentresults.finally withrespecttoconstructvalidity weusedmetricstoshowthedegreeof coverage of the mined acronyms definitions and contextual explanations howev er coveragedoesnotmeasureusefulness.tothat endweconductedasmalluserstudy andwhilstourparticipants weregraduatestudentsandnotcurrentlyworkinginindustry they served as reasonable proxies for project newcomers and other non domainexpertsworkingonaproject.neverthelesstomorerigorously evaluate whether our approach is useful we need to apply it with real project stakeholders in an actual project context.
related work inadditiontothepreviouslydescribedrelatedwork webrieflysum marizeothercloselyrelatedworkinconceptminingandgenerating traceability rationales.
concept and relation mining numerous researchers have focused ontechniquesforminingconceptsandtheirrelationsfromtheweb.
angeli et.al proposed dependency analysis based method for triplet relation mining in .
our relation extraction approach is basedonthesameideawhilemodifyandsimplifytheheuristicrules to focus on the given noun phrases we detected in project artifacts.
taxonomyexpansionalgorithms focusonexpandinganexistingontologybyextractingnewconceptsfromlargeopencorpus and leverage deep learning models to insert the concept into the concepthierarchy.thelinkexplanationtaskcanbenefitfromthese methods when an initial domain concept is available.
in the software engineering domain researchers have proposed or evaluated techniques for ontology building in order to capture key project concepts however theseapproacheswereusedinthetrace link creation algorithms and not applied for link explanations.
traceabilityrationales hullanddickproposed richtraceability as a means ofexplicitly capturing satisfaction arguments between requirements and design thereby documenting the rationale fora link however performing this task manually is very time consuming.
other researchers such as balasabrumanian et al.
andzismanetal.
proposedspecifictraceabilitymeta models describing link semantics however while these techniques create semantically typed links they fail to explainthe purpose of individual links.
guo et al.
proposed a technique that utilized domainspecific knowledge bases to support trace link generation andthenaugmentedtheknowledgebasewithrationalepatternstoprovideaninitialexplanationforthelink.however theirapproachiscloselycoupledwithaheuristicapproachtotracelinkgeneration whereasourapproachisnotdependentuponaspecifictracingtechnique.finally afewresearchershaveexploredtheuseofontologies to generate semantically meaningful trace links .
conclusion in this paper we sought to extract acronym descriptions definitions contextual examples and cross artifact relations by applying annlppipelinetoprojectartifactsandlargegeneraldatacorpii.our goal was to use the generated descriptions within an interfacetoexplainwhytwoartifactswereconnectedthroughatrace link.generatingtracelinkexplanationsrepresentsarelativelynewresearchchallengedesignedtosupportrecentadvancesindltraceabilitymodelswhichhavesignificantlyimprovedtheaccuracyof generated trace links but lack clear explanations.
ourquantitativeanalysisshowedthatthegenerateddescriptions and definitions were quite precise and were able to augmentmanually created project glossaries.
while we were not able to generate definitionsforallidentifiedprojectconcepts ouruserstudyconductedwithnon expertsacrossthreedifferentsoftwaredomains demonstratedthattheexplanationsminedfromglossariesandaugmented with dynamically retrieved definitions and descriptions aidedusersinevaluatingtracelinksdespitelackingexpertiseinthe domain.infutureworkwewillfocusonenhancedtechniquesfor providing more comprehensive coverage of all domain concepts.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
generating and visualizing trace link explanations icse may pittsburgh pa usa we share all of the artifacts for cchit and cm1 datasets1and the associated code2into the public domain to empower other traceabilityresearcherstotakeupthechallengeofgeneratingcomplete correct andmeaningfultracelinkexplanationsinorderto make dl generated trace links more useful for a broader set of stakeholders.
acknowledgment theworkinthispaperisprimarilyfundedundertheusanational science foundation grant ccf .