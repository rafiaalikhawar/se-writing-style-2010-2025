learning to rank vsranking to learn strategies for regression testing in continuous integration antonia bertolino antonia.bertolino isti.cnr.it isti cnr pisa italyantonio guerriero antonio.guerriero unina.it universit di napoli federico ii napoli italybreno miranda bafm cin.ufpe.br federal university of pernambuco recife brazil roberto pietrantuono roberto.pietrantuono unina.it universit di napoli federico ii napoli italystefano russo stefano.russo unina.it universit di napoli federico ii napoli italy abstract in continuous integration ci regression testing is constrained by the time between commits.
this demands for careful selection and or prioritization of test cases within test suites too large to be run entirely.
to this aim some machine learning ml techniques have been proposed as an alternative to deterministic approaches.
two broad strategies for ml based prioritization are learning torank and what we call ranking to learn i.e.
reinforcement learning .
various ml algorithms can be applied in each strategy.
in this paper we introduce ten of such algorithms for adoption in ci practices and perform a comprehensive study comparing them against each other using subjects from the apache commons project.
we analyze the influence of several features of the code under test and of the test process.
the results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.
ccs concepts software and its engineering software testing and debugging.
keywords regression testing test selection test prioritization continuous integration machine learning acm reference format antonia bertolino antonio guerriero breno miranda roberto pietrantuono and stefano russo.
.
learning to rank vsranking to learn strategies for regression testing in continuous integration.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
org .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction continuous integration ci is widely practiced in the software industry for its benefits in terms of release time and productivity .
due to the frequent commits to the shared codebase the cost of continuously performing regression testing escalates .
regression testing has been investigated for decades .
however most techniques for reducing its cost in traditional development cannot be applied at the scale of modern ci practices .
scalability issues are due not only to the size of codebases and test suites but also to the dynamicity of such environments .
to address the needs of regression testing in the context of ci researchers actively chase lightweight and effective test selection and prioritization ts p techniques.
an ideal ts p technique for ci should be able to quickly identify a relevant subset of test cases that can safely andtimely detect any potential regression introduced by the latest committed changes.
solutions are investigated along two main directions heuristics to trade off precision and effort and fully automated approaches leveraging machine learning ml algorithms .
this paper features a ts p approach that first picks a subset of test cases using a coarse grained static selection approach as the one proposed in and then prioritizes them through ml.
prioritization is basically a ranking problem and naturally lends itself to be formulated as a learning task.
in fact several authors have investigated the use of ml for prioritization in ci as we discuss in the next section.
however we still lack criteria for choosing the most appropriate technique to be applied in a certain situation.
in this paper we address the gap by experimentally evaluating ten ml algorithms which may be adopted for ts p in ci and analyzing the influence of features of the code under test cut and of the test process.
nine of the ten ml algorithms have never been used before for test prioritization.
we consider two different learning strategies.
learning to rank ltr encompasses mainly supervised algorithms proved useful in information retrieval and natural language processing .
in software engineering ltr was successfully applied to defect prediction to rank modules based on their defectiveness .
in test prioritization ltr can be used to rank test targets e.g.
test cases or test classes based on a testing objective e.g.
the chance of exposing failures .
being usually formulated as a supervised learning problem ltr requires prior training.
when the operating context differs ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea bertolino guerriero miranda pietrantuono russo from the training one the model may no longer be representative and may loose its prediction ability.
this indeed may happen in ci.
an alternative strategy suited in dynamic contexts is reinforcement learning rl .
rl foresees an artificial agent that learns from theenvironment by observing its state and selects a proper action either from a learned policy or by random exploration of possible actions.
as a result of the action the agent receives feedback in form of rewards the goal is to take actions that maximize the reward.
the agent is usually implemented with shallow or deep neural networks mapping state action pairs to rewards.
we denote rl algorithms applied to ranking as ranking to learn as opposed to learning ro rank since they leverage the ranking at each step to improve the model s predictive ability.
we foresee a potential benefit in using rtl for test prioritization in ci due to its natural ability to adapt to changes in the test suites with removed and newly added tests at every ci cycle and to changes in the ci process.
in summary the original contributions of this paper are we present ways to formulate ts p as a ml problem we conduct the first experimental study comparing performance of ltr and rtl test prioritization algorithms in ci we identify and study features of the cut and of the ci process which may affect the effectiveness of algorithms we draw criteria for applying ml techniques to the ts p problem in ci environments thus supporting testers in selecting and tuning the strategy that best fits their context.
the paper is organized as follows.
section surveys related work.
section presents our ts p techniques.
section describes the experimental evaluation and section presents the results.
threats to validity and guidelines are discussed in section .
section provides concluding remarks.
related work research on ts p in ci environments is today very active.
a recent study analyzes change commits of almost one thousand github projects to understand the needs of regression test selection .
various techniques have been proposed to efficiently identify those test cases that exercise the changed code .
machine learning has been proposed for this purpose too pang et al.
demonstrate that simple unsupervised learning algorithms such as k means based on coverage information can be used with good results.
however at large scale it may be infeasible to collect coverage information in contrast static approaches use program analysis to identify the code parts potentially affected by a change.
legunsen et al.
observed that class level techniques may be as effective as dynamic ones.
following their conclusions this work adopts a static approach to test selection at class level see section .
test suite prioritization techniques are reviewed by khatibsyarbiniet al.
who classify primary studies between and .
they find that the three most used prioritization techniques are search based coverage based and fault based .
the survey includes the work by thomas et al.
using topic modeling a text mining method for similarity based test prioritization yet it does not identify an explicit class of ml prioritization techniques.
indeed those more closely related to our work i.e.
discussed below appeared only later.1the survey missed 1reference is dated november probably too late to appear in .though a work by tonella et al.
who apply the case based ranking algorithm to rank tests according to coverage complexity metrics and historical data when the ordering between two test cases cannot be decided the user is asked to manually rank them.
targeting ci environments we opt for the analysis of fully automated ml techniques.
these draw increasing interest today in many domains including several software engineering tasks as early surveyed by zhang and tsai in .
use of ml in software testing is the focus of a recent mapping study by durelli et al.
who noted a surge of research in this topic in the very last years.
they identify four primary studies that use ml for test prioritization in addition to the one by tonella et al.
already discussed they are .
lenz et al.
propose a ml strategy which leverages testrelated information to support various tasks including test prioritization they first use clustering of data derived by executing some example test cases to obtain groups of functionally related test cases these clusters are then used to train a ml classifier.
busjaeger and xie claim to be the first to reduce the problem of test prioritization to that of ltr their model learns from a training set made up by past changes and by the tests observed for each of them tests are binary labelled pass fail .
a feature vector is created for each change tests pair.
their model experimented on a real world dataset includes five features coverage data test file path similarity and test content similarity failure history and test age.
the approach based on a listwise ltr algorithm see section .
.
showed significantly better results than existing heuristics relying on single features.
we experiment a different listwise ltr algorithm never used before and compare it to other strategies.
lachman et al.
apply the ltr svm rank algorithm to blackbox prioritization starting from test cases and failure reports in natural language nl .
they derive a dictionary of terms from the test cases and collect further metadata based on history and requirements.
the evaluation shows that ml prioritization outperforms previously available manual approaches by experts.
as we target ci environments we do not consider tests and requirements in nl.
spieker et al.
observe that existing prioritization techniques using historical information cannot properly account for changes in the testing context on the one side test cases can be removed from or added to the test suite on the other the testing focus could vary based on external factors.
therefore they propose retecs the first ts p approach using rl with a shallow neural network agent.
as a lightweight approach it uses only failure history information.
the evaluation on a real world data set shows that performance comparable to deterministic approaches can be achieved after a learning stage of about ci cycles without training.
we perform a more comprehensive analysis of rl algorithms including multilayer perceptron and random forest besides a shallow network.
finally elbaum et al.
propose a history based ts p strategy for ci customized to the pre and post submit stages of the google ci process whereas our study does not assume any specific process.
approach in ci practices testing is a time constrained problem typically dealt with by proper test selection and or prioritization algorithms.
the goal of the former is to select only those tests exercising the 2learning to rank vsranking to learn strategies for regression testing in continuous integration icse may seoul republic of korea figure overview of the test selection and prioritization process code directly or indirectly affected by changes.
the latter reorders the entire test suite so that tests with higher priority are run first.
selection alone may be insufficient if time between commits is so short that not all selected tests can be run or when the goal is to avoid running many non failing tests.
prioritization without selection may be unsatisfactory too as it might act on non relevant tests.
clearly they can be combined by selecting a subset of tests then prioritizing them or by prioritizing the suite then selecting tests e.g.
by a temporal threshold excluding the lowest priority ones .
a process made up by a lightweight coarse grained selection followed by ml based prioritization is proposed here .
.
test selection we adopt a conservative criterion for test selection based on static class level dependency analysis.
static techniques are preferred over dynamic ones since the latter are often impractical in ci environments due to runtime overhead.
lightweight dynamic techniques like in could be applied but their reliance on runtime collection of dependencies is a hurdle in ci contexts as it is time consuming and for programs with non determinism the result may be unsafe since collected dependencies may not cover all possible traces .
class level is preferred to method level selection as it is faster and cheaper and even safer .
we exclude coverage based test selection and prioritization as well which in ci may be expensive due to the costs of instrumentation recording and maintaining coverage data per release and inaccurate the quick cycles and code changes make coverage data imprecise and obsolete .
in the first ci cycle we build the class level dependency graph at next commits we consider the changed classes and update it accordingly step in fig.
.
by querying the graph we obtain step all the classes that transitively depend on the changed classes along with the associated test classes which are our test targets.
these are then prioritized.
.
test prioritization the test targets prioritization criteria are fault detection andexecution time.
several ways of combining these two criteria can be envisaged depending on the testing objective.
as usual in test prioritization fault detection is assumed here as the primary criterion the optimal ranking list is the one that orders all failing test targets first and non failing targets after within each of these two sublists targets with shorter execution time are ranked first.
the features used to predict the ranking derived in step are listed in table .2the first three rows are code metrics of the 2we use scitools understand .
code and test metrics type metrics description program sizeavgline avglineblank avglinecode avglinecomment countdeclfunction countline countlineblank countlinecode countlinecodedecl countlinecodeexe countlinecomment countsemicolon countstmt countstmtdecl countstmtexe ratiocommenttocodemetrics related to the amount of lines of code declarations statements and files mccabe s cyclomatic complexityavgcyclomatic avgcyclomaticmodified avgcyclomaticstrict avgessential maxcyclomatic maxcyclomaticmodified maxcyclomaticstrict maxessential maxnesting sumcyclomatic sumcyclomaticmodified sumcyclomaticstrict sumessentialmetrics related to the control flow graph of functions and methods objectoriented metricscountdeclclass countdeclclassmethod countdeclclassvariable countdeclexecutableunit countdeclinstancemethod countdeclinstancevariable countdeclmethod countdeclmethoddefault countdeclmethodprivate countdeclmethodprotected countdeclmethodpublicmetrics based on object oriented constructs test historynumber of failed tests in the current commit number of failed tests per test class n commits before the current one n to total execution time of all the tests of the test class last time the test class was runmetrics based on the history of tests execution class es under test.
the last row refers to test metrics test execution time and failure history of the test target up to the previous four commits.3specifically we compute the difference between the metrics values of two consecutive commits.
this study analyzes the following ltr and rtl strategies for adoption in step .
.
.
ltr .training is done on a number of observations w depending on the amount of history available the resulting model is used to prioritize tests for next commits.
the model needs to be re trained from time to time this is preferred to online learning when training is expensive e.g.
for a large codebase .
the ltr strategy can use pointwise pairwise orlistwise algorithms .
pointwise ltr.
the ranking problem is transformed into classification regression or ordinal classification and solved with respective existing methods.
the training data are typically supervised learning data given a sample a test target the algorithm predicts the class label real number or grade label for the three cases.
for example in classification problems the score can be the probability of a test belonging to a class e.g.
high priority and low priority in a binary formulation in regression and ordinal classification problems a function of the testing objective yielding a real priority number or a grade.
the loss function in learning is pointwise in the sense that it is defined on a single object feature vector .
pairwise ltr.
ranking is transformed into a classification or regression problem where a sample is a pair of test targets a model can tell which test target has higher score than the other in a pair.
3a too long history may not make sense in ci as code changes and test outcomes generally change over time.
the choice of previous commits was first made in .
3icse may seoul republic of korea bertolino guerriero miranda pietrantuono russo the goal is to minimize the average number of inversions in ranking due to unordered pairs.
listwise ltr.
the problem is addressed in an intuitive way as ranking lists are taken directly as samples in both learning and prediction.
the approach trains a model able to assign scores to feature vectors and rank them accordingly.
the goal is to minimize the difference between the predicted and the actual ranking lists.
in classification based pointwise ltr we consider four classes derived from the two above mentioned prioritization criteria fault detection and execution time which are in decreasing priority class at least one failure is detected running the test target and theexecution time of the test target isshorter than a threshold computed as the median of execution times on the last wsamples class at least one failure is detected running the test target and its execution time is longer than the threshold class no failure is detected running the test target and its execution time is shorter than the threshold class no failure is detected running the test target and the execution time of the test target islonger than the threshold.
in regression problems the relevance of test targets needs to be defined for the ltr algorithm to assign a score rito the i th target.
with the objective of prioritizing the failing tests first and then the shortest ones the relevance function is defined as ri fi e ti fi e ti fi e ti where fi 1if the test target fails at least once 0otherwise and ti is the execution time of the target.
this ensures that a higher score is given to failing and shorter test targets compared to the others i.e.
failing longer non failing shorter and non failing longer having progressively lower scores .
.
.
rtl .training is done online namely when the agent updates its knowledge about state action reward.
the reward is implemented as in the classification based pointwise approach i.e.
with four classes.
here the states are the test targets to prioritize.
the policy is a function from states to actions which initially is a loose approximation of the optimal policy and is then refined over time by the gained experience.
an action is the assignment of a priority to a test target by using the policy preferring the actions rewarding more.
several approximators of the policy can be used neural networks are the commonly used ones .
in our formulation the approximator receives a state as input and outputs a probability vector of class membership for the four classes .
the probability is used as priority score to rank tests attributed to the same class.
to train the agent a mechanism called experience replay is exploited the past experience of the agent in terms of state action reward next state is stored separately in order to have the opportunity to reprocess it later and use for learning in different ways.
in an online setting as ours the replay memory keeps the experience information of the last ntime steps with nconstrained by the limited memory capacity.
when the capacity is reached oldest experiences get replaced first.
training exploits a batch of experience which can be sampled in many ways we sample it randomly giving the newer observations higher probability of selection than the older ones given the i th observation the bigger the older the selection probability is pi ipn j j.table algorithms strategy class algorithm ltr pointwise k nearest neighbor knn ltr pointwise random forest rf ltr pairwise lambdamart l mart ltr pairwise mart ltr pairwise rankboost ltr pairwise ranknet ltr listwise coordinate ascent ca rtl reinforcement learning shallow network rl rtl reinforcement learning multilayer perceptron rl mlp rtl reinforcement learning random forest rl rf we consider three rtl variants where the agent is i ashallow network as the one used in the spieker s model but preceded by the test selection described in section .
and with a different reward classification based ii amultilayer perceptron iii aclassification algorithm namely random forest .
evaluation .
algorithms this work evaluates the ten algorithms listed in table .
they can be further classified into ensemble andnon ensemble algorithms the former category includes rf rl rf rankboost mart l mart the others are non ensemble.
the weka4and knime5tools were used for pointwise algorithms and the ranklib library6for pairwise and listwise algorithms.
the number of samples used for training initially set to is subject to sensitivity analysis.
for supporting the independent verification and replication we make available the python implementation for the rtl strategy the code for test selection for java and the maven build system the algorithms settings as well as additional results not included here for the sake of space.
.
experimental factors we investigate what factors make some ml algorithms behave better than others for test prioritization in a ci context.
we focus on characteristics of the code under test and of the ci process .
as for the former we consider i thevariability of the code test metrics ii thefailure proneness of the code which causes more or less balanced datasets iii the code test metrics that can be used asfeatures for training and improving prediction.
as for the latter we consider i theinter commit time which determines the time available for performing ts p ii thecycle size i.e.
size of the sample or number of tests per commit which affects the length of the history available for learning.
.
research questions the study addresses the following research questions 4learning to rank vsranking to learn strategies for regression testing in continuous integration icse may seoul republic of korea rq1 .
how do the selected algorithms perform in a ci context in terms of prioritization effectiveness and cost?
rq1.
which algorithm performs better?
rq1.
which of the four strategies rtl pointwise pairwise and listwise ltr performs better?
rq1.
which category performs better between non ensemble andensemble algorithms?
rq2 what is the influence of code characteristics?
rq2.
what is the influence of cut variability?
rq2.
what is the influence of the code failure proneness?
rq2.
how many and which features are important?
rq3 what is the influence of ci process characteristics?
rq3.
what is the influence of the inter commit time?
rq3.
what is the influence of the cycle size?
.
subjects we ran experiments on six java subjects from the open source apache commons project which use maven as build system.
they have been selected based on their size 10kloc number of latest working commits9 and mean number of test targets per commit .
all the selected projects come equipped with the developer s test suite.
table lists the subjects and their characteristics.
table subjects subject sha commits kloc targets tests codec 5a9c79f .
compress 66338dd .
imaging 9c5dc5b .
io f9e08f8 .
lang ce0c082 .
math 71fd124 .
n.b.
sha kloc targets and tests refer to the first commit.
.
evaluation metrics to evaluate the ranking in an algorithm independent way e.g.
rlbased and ltr classification and regression based we leverage the fault percentile average fpa used in the ltr task for software defect prediction .
we define the rank percentile average rpa to adapt the fpa to the prioritization problem it is used to compute how much a predicted ranking is close to the actual ranking.
the metric can evaluate a ranking independently of the specific testing criteria e.g.
fault detection .
let us assume that priority scores are increasing integers from to k with kbeing the number of test targets to prioritize a higher score means higher priority.
let us denote with ri ithe actual true ranking score of test target i e.g.
r fork 5targets and their sum with r r1 r2 rk k k r 15in the example .
the prediction task produces a permutation of r e.g.
r working means not requiring deprecated libraries or old software versions e.g.
old java maven to build successfully.with the second and fourth elements swapped .
the following ratio represents the proportion of the actual ranking scores contained in the top mpredicted test targets with respect to r rkx i k m 1ri k k 2kx i k m 1ri.
for instance for the top elements of the predicted ranking r the proportion is over .
the actual ranking rscores over .
rpa is defined as the average of this proportion over the kelements rpa kkx m rkx i k m 1ri pk m 1pk i k m 1ri k2 k .
the higher the rpa the better.
the maximum value rpa m is reached when the predicted ranking is equal to the actual one ri i in the summation .
with few manipulations it can be shown that rpa m pk i k i k i k2 k which is less than .
we normalize rpa so that nrpa computing the normalized rank percentile average nrpa rpa rpa m. for our example the predicted ranking r hasrpa .
rpa m .
and nrpa .
.
.
.the n rpa metric has an intuitive interpretation representing the average of how much of the optimal ranking scores is contained in the top m predicted ranking s objects.
moreover the metric is more accurate than the spearman s correlation coefficient on partial lists.
for instance consider an optimal ranking of tests ri .
.
.
and a partial ranking evaluation of the first three tests assume we have predicted ranking and predicted ranking optimal ranking the spearman correlation coefficient is .
in both cases as it considers only the rank in both cases while rpa considers also the magnitude selecting number rather than as second test and gives respectively .
and .
.
to investigate differences in the performance of algorithms in time constrained scenarios we consider the cases in which not all the selected test targets can be run at each cycle e.g.
because the inter commit time is short compared to the tests execution time.
we adopted the same constraints used in previous work investigating the effect of time constraints on regression testing i.e.
and of the number of selected test targets.
in such cases the rpa metric makes no sense sublists of the optimal and the predicted rankings generally do not contain the same tests and cannot be compared.
considering the two prioritization criteria fault detection and test execution time at each commit we compute the difference between the predicted total tests execution time for the predicted ranking and the optimal total tests execution time for the optimal ranking the difference between the predicted number of failing tests for the predicted ranking and the optimal number of failing tests for the optimal ranking with reference to the and lists.
the larger these differences the worse the predicted ranking.
it should be noted that as fault detection metric we discard the well known apfd as it considers cumulative fault detection over time and is not appropriate in ci where the focus is on obtaining 5icse may seoul republic of korea bertolino guerriero miranda pietrantuono russo feedback on individual test cases rather than on a whole test suite .
additionally in ci changes in each release are very limited with respect to the codebase which makes the number of faults per release very small as in our dataset compared to traditional processes .
this would lead to undetermined apfd values.
to quantify the cost we compute in each cycle the time for test selection the time for prioritization made up of training time and ranking time and the time for the execution of the selected tests.
their sum is referred to as end to end time.
the above metrics are computed at every commit when tests are prioritized.
algorithms are run times on each subject.
results table shows the results of test selection.
the first two rows report thetotal number of selected targets test classes and total number of tests of the selected targets across all the commits and their average per commit.
rows and list the total number and percentage of failing targets and tests over selected ones.10the last two rows report the sum of the times for test selection including time to check for changes upon a commit to update the dependency graph and to extract the test classes and for execution of all selected tests.
the list of selected test targets at each commit along with their execution time the number and percentage of failed tests per target and the metrics per target cf.
with table correspond to an instance for the ml based prioritization algorithm completed by the actual during training or predicted in prioritization ranking.
table results of test selection subje cts codec compress imaging io lang math selected targets3 av.
.
av.
.
av.
.
av.
.
av.
.
av.
.
selected tests46 av.
.
av.
.
av.
.
av.
.
av.
.
av.
.
failing targets3 .
.
.
.
.
.
failing tests3 .
.
.
.
.
.
total sel.
time s av.
.
av.
.
av.
.
av.
.
av.
.
av.
.
total exec.
time s av.
.
av.
.
av.
.
av.
.
av.
.
av.
.
.
rq1 prioritization effectiveness and cost .
.
rq1.
algorithms comparison.
therpa values and ranking times per algorithm are computed after prioritization at each commit.
we consider the averages over commits of these values obtaining observations subjects x repetitions these are shown in the boxplots in fig.
.
the boxplots per subject are available in the supplemental material.
fig.
reports the total training times per algorithm in logarithmic scale averaged over subjects and repetitions.
rtl algorithms take much longer since training is repeated at each commit.
however rtl can rank test cases since the beginning the time to first prioritization tt fp is null whereas ltr must wait for training to finish tt fp equals the training time .
the test selection and 10note the small number of failing tests for all subjects causing highly unbalanced datasets.
sensitivity of algorithms to this aspect is assessed in section .
.
.
knn rf l mart mart rankboost ranknet ca rl rl mlp rl rf0.
.
.
.
.
a rpa per commit knn rf l mart mart rankboost ranknet ca rl rl mlp rl rf0102030405060 b ranking time ms per commit figure average rpa and ranking time per algorithm figure average training and ranking times per algorithm execution times plus the algorithm dependent training and ranking times for prioritization make the end to end time.
we run one way analysis of variance anova considering the algorithm as factor and rpa and ranking time as response variables.
the levels are grouped in order to show the differences as formulated in rq1.
to rq1.
.
to choose the statistical test for anova we checked the data for normality and for homoscedasticity by means of respectively the shapiro wilk test and the levene s test the two null hypotheses of data coming from a normal distribution and of variances being homogeneous are both rejected at p value .2e which is the minimum value of the r statistical tool that we used.
therefore we conducted a non parametric anova by means of the friedman test which is robust to non normality and heteroscedasticity with the iman and davemport extension .
the test detects if at least one factor s level significantly differs from another.
we then run a post hoc test to detect what levels are different by using the shaffer s static method a powerful method forall pairwise comparison exceeding nine algorithms .
the test confirms for the rpa and the ranking time p value .2e in both cases that there is at least one significant difference among the algorithms.
fig.
4a and fig.
4b report the pairwise comparison results by the ranking plot an adaptation of the nemenyi s test critical difference plot working with other tests algorithms with no significant difference are grouped together using a bold horizontal line the more distant two algorithms are the distance being the average ranking the smaller the p value for the null hypothesis of equal performance.
11the iman and davemport extension is a popular choice to improve the too conservative friedman s statistic suited to compare more than five algorithms .
6learning to rank vsranking to learn strategies for regression testing in continuous integration icse may seoul republic of korea mart l mart rf rl rf rankboostranknet knn rl ca rl mlp a rpa the leftmost one has biggest rpa knn rl rf rf mart l martrl mlp rl ranknet rankboost ca b ranking time the leftmost has longest time figure pairwise comparison of algorithms the algorithms performing better in terms of rpa are mart and its extension lambdamart followed by rf rl rf and rankboost with no significant differences .
their commonality is to be based on decision or regression trees while most of others are based on neural networks ranknet rl mlp rl .
the worst group includes rl mlp ca and rl although their average rpa values are above .
.
as for ranking time knn requires much longer followed by rl rf and rl.
the best one is by far ca.
summarizing mart and l mart have the best rpa and medium level ranking andtraining times ca performs poorer but is better forranking time with a medium level training time.
knn requires lesstraining time than others ltr but it takes long for ranking not justifiably paid off by the rpa.
rl algorithms require high training times being online learning schemes rl rf has high ranking times but good rpa.
rl and rl mlp have better times but poorer rpa.
.
.
rq1.
strategies comparison.
we compare the four strategies rtl ltr pointwise ltr pairwise ltr listwise grouping the results of the respective algorithms.
as the groups have an unequal number of algorithms we run the skillings mack test an adaptation of the friedman s statistic able of dealing with unbalanced designs .
the p value is .2e for both rpa and time.
fig.
and fig.
show the boxplots and the ranking plots respectively.
in terms of rpa the strategies differ significantly from each other with pairwise algorithms being the best ones.
in terms of ranking time pointwise are the worst because of knn and the listwise method ca is the best rl and pairwise are comparable.
pointwise and pairwise ltr exhibit both higher rpa than rl methods hence online learning by rl does not necessarily ensure better performance than static methods as seen in the analysis of single algorithms.
this may depend on other features related to the code variability as investigated in the next rqs.
.
.
rq1.
categories comparison.
we compare non ensemble vs ensemble algorithms.
the mann whitney wilcoxon test is suited in this two levels case.
again the statistical test reports a significant difference both in terms of rpa and prioritization time p value .2e for both the averages rpa are .
and .
for pointwise pairwise listwise rl0.
.
.
.
.
a rpa per commit pointwise pairwise listwise rl0102030405060 b ranking time per commit ms figure average performance of the four strategies pairwise pointwiserl listwise a rpa pointwise pairwiserl listwise b ranking time figure pairwise comparison of the four strategies ensemble andnon ensemble learning respectively and the average times are .
ms and .
ms. thus ensemble methods have both higher rpa and shorter ranking times.
looking at fig.
they also have shorter training times.
.
rq2 influence of code under test .
.
rq2.
cut variability.
fig.
reports the rpa across the cycles each rpa value is the average over repetitions for each subject algorithm.
for readability the commits with one test class that always gives rpa are removed from the plots hence less commits are shown than the actual ones and the average rpa at each commits is reported.
the top subplot for each subject shows thertl algorithms the bottom one the ltr ones in fact while the latter ones require a training phase during which no prediction is carried out the former ones do not require training and give predictions sooner being based on reinforcement learning.
the performance of learning algorithms depend on the representativeness of the learnt model in highly variable ci scenarios when the code changes a lot and failure proneness and or test execution time change too ml predictive performance may be poor.
looking at the plots in figure the performance are quite stable but some algorithms like rl rf and knn exhibit trends in some scenarios.
it is interesting to investigate if and to what extent the trends are related to code variability.
we proceed as follows.
we first run for each subject s dataset the principal component analysis pca over the features so as to remove their first order inter correlation and select a number of pcs necessary to keep the of the original variance for the subjects the number of pcs was .
then we combine the pcs in one time series to have a synthetic indicator of the trend capturing the variability of the code metrics to this aim we exploit the hotelling s multivariate control charts .
multivariate control charts are used to monitor two or more interrelated process variables in order to detect shifts in the mean or the relationship between several interrelated variables .
a hotelling chart computes a statistic 7icse may seoul republic of korea bertolino guerriero miranda pietrantuono russo a subject commons codec b subject commons compress c subject commons imaging d subject commons io e subject commons lang f subject commons math figure average rpa plots across cycles named t2 that combines the information from the dispersion and the mean of several related variables.
it can be used in a one phase setting i.e.
by using the same data that is being plotted on the control chart to characterize the normal behavior or two phase setting i.e.
using part of the data to characterize the normal behavior and the other part to be checked for deviations our aim falls in the one phase setting.
given the selected kpcs the statistic for observation iis t2 i yi y s yi y where yiis the vector of kmeasurements for observation i yis the vector of sample means of the kvariables s 1is the inverse of the sample covariance matrix which provides information regarding the relationship between different variables.
plots of t2are omitted for lack of space but they are made available separately.7using the t2time series as metrics variability indicator and the rpa time series as performance indicator we finally compute the transfer entropy t e between t2andrpa namely the amount of directed transfer of information between the two time series.
differently from correlation t einforms us about causality it actually generalizes the granger causality test computing how much information about the transition between two consecutive steps of therpa time series can be found in the past state of t2.
how far the past is depends on the user defined delay between the two time series d. we compute the t eunder different delays d 1to d 10and using the kraskov st gbauer grassberge ksg t eestimator .
to capture any possible causality relations we consider beside the average the maximum of the t evalues over the dvalues so as to identify the condition under which the t2affects more rpa.
table reports the maximum t eper algorithm and subject over the delays.
the last column summarizes which algorithm has been 8learning to rank vsranking to learn strategies for regression testing in continuous integration icse may seoul republic of korea table transfer entropy subje cts codec compress imaging io lang math all maxdtemaxdtemaxdtemaxdtemaxdtemaxdtea vg max d knn .
.
.
.
.
.
.
rf .
.
.
.
.
.
.
l mart .
.
.
.
.
.
.
mart .
.
.
.
.
.
.
ranknet .
.
.
.
.
.
.
rankboost .
.
.
.
.
.
.
ca .
.
.
.
.
.
.
rl .
.
.
.
.
.
.
rl mlp .
.
.
.
.
.
.
rl rf .
.
.
.
.
.
.
more sensitive to the variation of the t2time series the ranking is as follows ca rf knn ranknet lambdamart rankboost mart rl rf rl mlp rl all ltrapproaches are more affected by the t2 code variability indicator while all rl based algorithms are more robust to variations likely because of their online learning nature.
this suggests that in highly variable contexts with intense metrics changes rl based approaches can be preferred to avoid having to re train a static ltr algorithm too often.
in contrast static contexts can stress the good performances of ltrs.
.
.
rq2.
code failure proneness.
we created an artificial dataset from our original one by injection of failing test case outcomes.
in each commit a changed class is selected randomly along with its dependent classes.
we considered the corresponding test classes and altered the outcome of their tests form pass to fail with a given probability p we used p .
.
the so produced dataset is more balanced with percentages of of failing test targets .
results are plotted in fig.
.
table scenario with error injection subjects codec compress imaging io lang math failing targets388 failing tests3 figure comparison on failure proneness rpa the wilcoxon signed ranked test is run on each pair .
only out of algorithms turn out to be not affected by a greater number of failing tests.
those affected are in decreasing order oftable statistical analysis scenario with error injection.
difference between averages inj injection scenario orig original scenario with no injection and p value for the null hypothesis h the rpa of the two scenarios is the same algorithm rl mlp rl rl rf rf knn a vg inj avg orig .25e .39e .86e .73e .21e p value .46e .14e .67e .67e .42e algorithm mart ranknet rankboost ca l mart a vg inj avg orig .30e .87e .
.69e 52e p value .02e .21e .42e .17e .73e confidence lambdamart rf rl rf rankboost knn mart rl rl mlp.
these are the three rtl algorithms and the ltr ones using ensemble learning except knn.
as for rq1.
they perform generally better but are more sensitive to the dataset balancing.
non ensemble ltr algorithms ranknet and ca are less sensitive.
.
.
rq2.
feature selection.
when building a model redundant or irrelevant features increase computational costs and can result in poor predictive performance.
to identify the most important features in a ci context we used an unsupervised feature selection approach called principal feature analysis pfa .
basically pfa exploits the structure of the principal components of the original feature set to select a subset that keeps most of the essential information.
one advantage of pfa over other feature selection techniques is that it operates independently of any learning algorithm as it depends only on the original feature set.
our implementation of pfa decides how many features should be selected by traditional pca to identify the principal components required to keep at least of the cumulative explained variance.
for all our subjects features were selected out of the original hence of the features suffice to keep of the original data variance.
one feature among those listed in table was selected for all the subjects which is avgessential a cyclomatic complexity metric obtained after iteratively replacing all well structured control structures with single statements to account for any branches into or out of a loop or decision other six sizerelated features were selected for at least of our subjects avgcyclomatic countdeclfunction countdeclmethoddefault countline countlinecomment and countdeclmethod.
overall the selection was balanced across the different attributes types and features from theprogram size cyclomatic complexity and object oriented groups were selected and of the time respectively.
.
rq3 ci process characteristics .
.
rq3.
inter commit time.
rq1 investigated algorithms performance regardless of the time available to execute test cases.
if time limits do not allow to run all selected tests the tester might be interested in analyzing the algorithms performance under various time constraints.
based on a previous work studying regression testing under different time constraints we consider scenarios in which and of the number of selected tests can be run at each cycle and assess the impact on ranking effectiveness.
in large scale systems practitioners could look for more aggressive reductions deeper exploration of this aspect is left to future work.
9icse may seoul republic of korea bertolino guerriero miranda pietrantuono russo table differences between predicted and optimal rankings in tests execution times s and number of failing tests best values per columns are in bold worst values in italics time constrained scenarios algorithms time failures time failures time failures knn .
.
.
.
.
.
rf .
.
.
.
.
.
l mart .
.
.
.
.
.
mart .
.
.
.
.
.
rankboost .
.
.
.
.
.
ranknet .
.
.
.
.
.
ca .
.
.
.
.
.
rl .
.
.
.
.
.
rl mlp .
.
.
.
.
.
rl rf .
.
.
.
.
.
as discussed in section .
the rpa metric makes no sense in this case table reports the differences between the predicted and optimal rankings in terms of tests execution time average over all subjects and commits and total number of failing tests average over all subjects .
the former is always the latter always they are zero when the predicted ranking is equal to the optimal ranking in the top list of tests.
the bigger their absolute values the worse the performance of the algorithm.
data per subject are made available separately.
whenever results are in line with the rpa rq1 e.g.
l mart and mart on test execution times or rankboost on number of failures that are close to the optimal one it means that mis ranked tests had low impact e.g.
because their execution times or number of failures is not much different than those of the optimal ranking s tests .
when results are not in line with rpa e.g.
rl rf and ranknet on failures then either the algorithm performs well for one prioritization criterion and bad for the other and or the tests that were mis ranked even by a small amount had a high impact.
clearly short inter commit time may influence the choice of the algorithm to adopt more than the rpa metric.
.
.
rq3.
history length.
we investigate whether and to what extent the amount of history used for learning impacts performance of the algorithms.
for ltr this history is used just once at the beginning for training for rtl the history is in a sliding window the memory used to update the learning process online.
fig.
shows the average rpa andtraining time over all the commits and projects vsfour values of training sample size.
therpa is quite insensitive with respect to a training sample size bigger than observations for all the algorithms.
the average good performance of ltr even with smaller training sets mitigates the drawback of not having predictions during the training process.
contrarily the training time expectedly increases remarkably with training sample size the graph is in logarithmic scale the rtl algorithms are severely affected followed by pairwise and listwise and finally the least affected ones the pointwise algorithms.
training sample size0.
.
.
.951rpaknn rf l mart mart rankboost ranknet ca rl rl mlp rl rf training sample size101102103104105106training time ms log scalefigur e sensitivity of rpa and training time to training sample size discussion .
threats to validity threats to construct validity may descend from the adopted evaluation metrics section .
.
we used a new metric rpa to assess the algorithms performance with a different metric rq1 might have received different answers.
the same threat may affect the cost related analyses.
indeed there do not exist other studies comparing heterogeneous ml techniques for test prioritization so we could not rely on established metrics for a fair comparison.
another threat might be due to our identification of characteristics of cut rq2 or ci process rq3 if our experimental design does not properly capture the relevant features or their influence on regression testing performance our answers to rq2 and rq3 might not be valid.
to mitigate these risks other studies should be conducted.
as for threats to internal validity one risk may derive from inappropriate settings of the tools we used for the ml algorithms section .
.
different settings or also different choices for the values of parameters of the experiment might have produced different results.
to control this threat we performed sensitivity analyses to our feature selection choices however due to the complexity of our assessments we cannot exclude that some decisions in the experiment design might have biased our comparisons by impacting differently the algorithms.
another threat concerns our study about failure proneness in which we artificially injected failing test outcomes.
real faults in production might produce different effects however this is a procedure commonly used for reliability studies and we opted for this as we did not have real faults.
as for threats to external validity our experiments were run on only six java subjects from the apache library.
although these six projects are very active have a large contributors base and a long history they might not be representative of industrial practice.
they 10learning to rank vsranking to learn strategies for regression testing in continuous integration icse may seoul republic of korea table ml based prioritization requirements vsdecision variables.
the ranks are from 1st best to 10th worst prioritization effectiveness rpa ranking timetraining timeonline robustness to code variabilityrobustness to failing testsrobustness to training size rpa robustness to training size time time constrained effectiveness time failures pointwise 2nd4th1stno 8th 9th1st 6th2nd1st2nd 2nd pairwise 1st3th2ndno 4th 5th 6th 7th5th 7th 9th 10th3rd2nd1st 1st listwise 4th1th3rdno 10th2nd4th3rd4th 3rd rtl 3rd2nd4thyes 1th 2nd 3rd3rd 4th 8th1th4th3rd 4th non ensemble 2nd2nd2ndcan be 1th 3rd 4th 9th 10th1th 2nd 3rd 4th 6th2nd1th2nd 2nd ensemble 1th1th1thcan be 2nd 5th 6th 8th5th 7th 8th 9th 10th1th2nd1th 1th knn 7th10th1thno 8th6th4th1th3rd 5th rf 3rd8th2ndno 9th9th5th3rd7th 4th l mart 2nd6th3rdno 6th10th10th2nd2nd 7th mart 1th7th5thno 4th5th6th4th1th 3rd rankboost 5th2nd4thno 5th7th8th5th6th 1th ranknet 6th3rd7thno 7th1th7th7th5th 2nd ca 9th1th6thno 10th2nd9th6th10th 6th rl 8th4th9thyes 3rd4th1th9th8th 8th rl mlp 10th5th10thyes 2nd3rd3rd10th9th 9th rl rf 4th9th8thyes 1th8th2nd8th4th 10th are also not enough representative for generalizing our conclusions to programs developed with other languages or processes not open source .
to control this threat more studies should be conducted.
hopefully the fact that we explain in detail our experimental settings and make available code and data for replication will support other researchers in performing replication studies over different subjects and contexts even using different ml algorithms.
.
guidelines results led us to draw the following suggestions for applying machine learning algorithms to regression testing in ci a valuable approach under short inter commit times is to run a ml prioritization algorithm after selection so that it acts on a small problem size and on test cases relevant for that commit.
an incremental static selection approach like the one we experimented appears suited for the time requirements of ci.
the testing criteria should be identified first along with their relative weight within the ranking function they determine what theoptimal ranking is.
if a coarse grain ranking is enough then classification algorithms may work well otherwise regression approaches better manage fine grain ranking problems.
most of the experimented algorithms can work with both formulations.
the ml algorithm s input consists of the features more likely related to the identified testing criteria.
we targeted feature selection in an algorithm independent way unsupervised first defining relevant features and then choosing the algorithm based on other requirements this simplifies the problem.
an alternative is to run feature selection for each potential algorithm being considered so as to infer the best features for each of them this although more precise can be much more time consuming.
in the plethora of ml algorithms to choose from a suggestion is to first decide the learning strategy and possibly the category ensemble or non esemble and then opt for the specific algorithm.
the choice depends on the requirements for prioritization in terms of desired ranking effectiveness and efficiency rq1 tolerable sensitivity to code features rq2 and on the ci process features rq3 .
they are strongly dependent on the ci context.
table reports the non exhaustive list of requirements we have investigated in this article along with the values of the decision variables algorithm strategy category ranked from best to worst according to the reported results.
the lowest part of table suffices to drive the choice among the algorithms we investigated for other algorithms the first and second parts may be used.
the choice of the strategy algorithm depends on the relative importance given to the requirements in the specific ci context.
a final suggestion concerns the tuning of algorithm parameters.
in this study we adopted default parameter values for fair comparison but as hinted already in validity threats performance can vary significantly depending on tuning.
this is especially true for the individual algorithms while the choice of the strategy or category is less affected.
a tuning step with any existing methods e.g.
grid or randomized search may be in order.
conclusions continuous integration practices in large industrial settings pose specific requirements on test selection and prioritization for regression testing due to the frequent commits with short inter commit times and few changes with respect to the codebase size.
this study has presented a comprehensive evaluation of ten machine learning algorithms for test prioritization after selection in ci.
based on the results of controlled experiments with open source subjects including an analysis of features of the code under test and of the test process influencing algorithms performance guidelines have been devised for testers to select and tune the ml algorithms best fitting their needs.