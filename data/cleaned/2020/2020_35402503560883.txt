exploring the under explored terrain of non open source data for software engineering through the lens of federated learning shriram shanbhag risha lab dept.
of computer science engineering indian institute of technology tirupati india cs20s503 iittp.ac.insridhar chimalakonda risha lab dept.
of computer science engineering indian institute of technology tirupati india ch iittp.ac.in abstract the availability of open source projects on platforms like github has led to the wide use of the artifacts from these projects in software engineering research.
these publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools.
however these advancements have missed out on the artifacts from non open source projects due to the unavailability of the data.
a major cause for the unavailability of the data from non open source repositories is the issue concerning data privacy.
in this paper we propose using federated learning to address the issue of data privacy to enable the use of data from non open source to train ai models used in software engineering research.
we believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy.
we present the preliminary evaluation of the use of federated learning to train a classifier to label bug fix commits from an existing study to demonstrate its feasibility.
the federated approach achieved an f1 score of .
compared to a score of .
using the centralized approach.
we also present our vision of the potential implications of the use of federated learning in software engineering research.
ccs concepts software and its engineering collaboration in software development .
keywords software engineering research non open source data data privacy federated learning acm reference format shriram shanbhag and sridhar chimalakonda.
.
exploring the underexplored terrain of non open source data for software engineering through the lens of federated learning.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of contribution technical work writing contribution idea supervision writing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction open source repository mining is one of the most important sources of data for software engineering research.
a platform like github has over million repositories as of april with over million being public.
the artifacts such as source code discussion forums pull requests and so on from github projects have been used in a wide variety of research in the domain of software engineering .
due to the availability of huge amounts of data ai techniques are being applied to the data to arrive at insights about the projects the developers and their behaviour and also in building ai based tools that are aimed at helping the developers .
the availability of huge data has also introduced the scope for the development of ai tools aimed at helping the developers by automating some of the tasks of the software development process thereby enabling them to make intelligent decisions and increasing their efficiency .
examples include automated identification of non functional requirements effort estimation classification of issue reports automatic tagging of pull requests clone detection etc.
in addition machine learning models have also been used in large scale empirical studies based on github artifacts .
while these ai models have been used in tool development and empirical studies the data used to train these models largely comes from open source repositories.
due to various restrictions the data from non open source projects that include private github repositories and proprietary software projects by various organizations cannot be used for training.
this lack of access to non open source repositories prevents the researchers from leveraging a large amount of non open source data for software engineering research.
several factors contribute to the unavailability of the data from non open source repositories.
platforms such as github do not provide access to the data from private repositories.
the projects involving the development of proprietary software developed by various companies are also reluctant to share the data from their software projects due to the issues concerning the mindset of the industry trust confidentiality and intellectual property concerns .
collaborations between the industry and the researchers can help overcome this issue to an extent.
however several factors can hamper the prospects of such collaborations .
the concept of technology transfer discourages researchers that are esec fse november singapore singapore shriram shanbhag and sridhar chimalakonda focused on knowledge exchange from having collaborations with the industry .
the collaborations are generally backed by contracts with several terms and conditions .
the long term goals of the research often do not align with the short term goals of the industry which introduces an additional impediment to starting a collaboration.
even with collaboration access to sufficient data is an issue due to confidentiality and the terms of the contract .
it must be pointed out that software engineering research collaborations do exist between academia and the industry.
but largely remain restricted to a few software companies such as google microsoft meta etc.
missing out on the data from non open source projects and private repositories on github has major downsides in software engineering research.
the research based on github data misses out on a majority of the github repositories as only around million1of the over million repositories on github are public.
the results obtained from open source projects may not generalize well for industry projects.
many factors such as geography organization factors project characteristics etc.
influence software development .
as an example fendler found that the software engineering practices recommended for the western world did not fit well in the african context in namibia demonstrating how geographic and cultural factors influence software development.
the type of organization also influences software development practices.
for instance startups typically run on tight budgets high uncertainty lack of resources etc.
would have different practices and strategies compared to a large organization with adequate budget resources and teams.
research merely based on the data from publicly available projects may not be able to reflect these nuances.
from the industry perspective it may be more desirable for them to have the results that would include the data from their projects along with the open source.
a scenario where several companies can collaboratively use their data to support software engineering research may generate better results and insights.
in this paper we present the idea of leveraging federated learning to use the data from non open source software projects in software engineering research.
our vision is to tap the potential of the largely underexplored software artifacts and data available in non open source context to incorporate non open source data in existing software engineering research to gain insights that may either confirm or contradict existing results.
improve existing ai tools developed using models trained by only using open source data.
federated learning federated learning is an approach of distributed machine learning that enables training on decentralized data spread across devices.
in non distributed machine learning approaches the data from different devices is aggregated on a central server and is used for training the model.
in federated learning however the model from a central server is shipped to the device trained locally and the model parameters are sent back to the central server.
the updated parameters from different devices are used to update the model in the server using an appropriate aggregation algorithm.
since federated learning brings the model over to the device instead of sending the data over to the server the data never leaves the device.
thus it addresses the issues with data privacy and data ownership to an extent.
the potential implications of this have also led the researchers to study the software development life cycle of the federated learning systems .
the earliest use of federated learning was its application in google keyboard to collaboratively learn from different devices running the android operating system .
federated learning is being applied to areas where data privacy is critical such as healthcare banking smart homes etc.
however the application of federated learning in software engineering largely remains unexplored.
although wang et al.
have explored the use of federated learning for heterogeneous defect prediction their primary focus has been on model improvement motivating the need for our work.
since data privacy and confidentiality are a major concern of the non open source projects we see great potential for the use of federated learning in software engineering research.
preliminary evaluation in this section we present a preliminary evaluation of the use of federated learning to train a machine learning model using the data from software repositories.
the goal of our evaluation is to find out whether the use of federated learning in a model with the data from software repositories lets us achieve a performance similar to centralized learning.
we evaluated the use of federated learning to train a model to classify the commits as bug fix commit or not as proposed by zafar et al.
on the dataset shared by the authors.
we first give a brief description of the work by zafar et al.
and then we explain our federated learning approach.
zafar et al.
proposed the use of deep neural networks model called bert to automatically label commit messages with high accuracy.
they focused on labeling classifying commit messages as bug fix commit or not.
in the paper the authors proposed the use of bert that can understand the context of commit messages.
they first proposed rules for semantic interpretation of commit comments.
using the rules the authors labeled a dataset of commit messages as bug fix and non bug fix commits.
the authors then used this dataset to build a commit classification model by fine tuning the pre trained bert model.
the model achieved a significant improvement over existing approaches with an accuracy of .
on the test set.
figure describes the federated learning approach we used to train and evaluate the commit classification model.
we split the training dataset provided by zafar et al.
into smaller datasets d1 d2andd3.
the number of data points in d1 d2andd3were and respectively.
these datasets in our evaluation could be considered equivalent to the datasets from private repositories used to train the models locally.
we choose to split the data unevenly to simulate the likely scenario of different repositories holding different amounts of data.
the architecture of the model we used is the same as described by zafar et al.
in their paper.
we used the bert uncased model 1611exploring the under explored terrain of non open source data for software engineering ... esec fse november singapore singapore figure federated learning approach we used to train and evaluate the commit classification model.
the datasets d1 d2and d3represent the private repositories with transformer layers attention heads and a hidden layer size of .
we added an additional dense layer containing a single neuron with a sigmoid activation function.
we set the learning rate and maximum sequence lengths to 2e and tokens respectively as mentioned in the original work .
we call this model the central model depicted by m1.
we created copies of the model and trained them on the datasets d1 d2andd3.
this can be thought of as sending the model to the private repositories and training them with the private data.
the batch size of the training data was set to and the models were trained for epochs as described by zafar et al.
.
the commit messages did not have to be pre processed as bert already has a word level tokenizer.
the updated parameters of each of these models u1 u2 and u3after training was aggregated using the federated averaging algorithm.
this is equivalent to sending the updated weights from each of the repositories to the central server for aggregation.
the resultant weights from the aggregation were loaded into the central model to form the updated model m2.
the federated averaging algorithm used for aggregation was proposed in a paper by mcmahan et al.
where the method was experimentally shown to efficiently train high quality models while minimizing the number of rounds of communication.
the equation to aggregate local models using the federated averaging algorithm is shown below.
wt k k 1nk nwkt here wt 1represents the parameters of the central model wkt represents the parameters of the updated local model from the client k. the parameters of the local model at the client is scaled based on the proportion of the data points held by the client.
the fraction nk nin the equation accounts for this scaling.
results we evaluated the performance of our model trained using federated learning on a test dataset provided by zafar et al.
.
the test dataset consisted of labeled commit messages.
the authors reported the accuracy of the model as .
on their test set.
as no code for replication of their work was shared in the paper we implemented the model ourselves using the instructions provided in the paper.
considering the issues with reproducibility of deep learning models to ensure a fair comparison of the performance of the federated learning approach we also trained the model we built in a centralized approach on all of the training data and evaluated its performance.
the results of the models trained using the centralized and federated approaches are summarized in table .
the original work had achieved true positives tp true negatives tn false positives fp and false negatives fn of and respectively.
however the model we trained in the centralized mode was able to achieve tp tn fp and fn values of and respectively.
the accuracy of the centralized approach was .
with an f1 score of .
.
the difference in the accuracy values could be because of various factors such as framework hardware library versions etc.
that are known to influence the reproducibility in deep learning models.
the model trained in the federated approach was able to achieve tp tn fp and fn values of and respectively with an accuracy of .
.
the f1 score was .
.
to summarize the federated approach where the central model did not ever see the data was able to achieve a performance accuracy .
f1 score .
close to the centralized approach accuracy .
f1 score .
.
although the performance of the federated approach is slightly lower than the centralized approach we believe that the results are promising enough for further exploration as federated learning provides several advantages concerning privacy preservation.
1612esec fse november singapore singapore shriram shanbhag and sridhar chimalakonda table results of the preliminary evaluation approach true positives true negatives false positives false negatives accuracy f1 score centralized zafar et al.
.
.
centralized our model .
.
federated .
.
implications majority of the research on software engineering today is based on open source data.
one of the major impediments to the use of nonopen source data is the data privacy concerns of the organizations that own them.
it is here that we envision the use of federated learning to address this challenge in software engineering research.
few of the potential implications of this include the use of federated learning could enable different companies to collaboratively use their data to support research that could help improve software development and maintenance.
currently this is being done by a select few companies such as google microsoft meta etc.
we see a potential for similar collaborations from companies across geographies scale and domains to cater to their needs.
as an example the information technology services companies based out of india could use data from thousands of their internal projects to contribute to the research aimed at improving the software engineering practices in the context of indian it services.
the ai tools developed to solve problems in the software development domain trained using open source data could incorporate the data from non open source domains.
given the drastic variation in practices between open source and industry projects this could enhance the usefulness of the tools for the developers in the industry.
as an example a tool like github copilot2could be further improved using the data from private repositories owned by private companies.
this could enhance its utility and quality as the tool would also learn practices followed in the industry.
we envision the emergence of the area of privacy preserving software engineering research with the mechanisms established for the private entities to grant the use of their data for the research.
the mechanisms could also be designed to address many of the quirks currently associated with industry research collaborations.
replication of existing work with non open source data could lead to novel insights that may confirm or challenge existing results in software engineering research.
the global nature of open source software development makes it harder to perform organization and geographyspecific research by mining software repositories.
this could become easier to an extent as non open source data becomes available for use.
in summary we envision a new line of research that integrates and leverages open and non open source data and a wide spectrum of ai tools for software engineering designed for multiple contexts such as different geographies organizations cultures and domains.
challenges in this paper we proposed the idea of using the federated learning approach to utilize the data from industrial and non open source projects in software engineering research.
however to get to the point where researchers could use non open source data many issues questions require research.
while federated learning approach could protect the privacy of the organization s data connecting to a central server to send receive the model would require some kind of trust to be established between the researcher and the organization.
establishing connections to a third party server could pose a security issue.
even communicating the model updates could possibly reveal sensitive information .
even if federated learning could address the issue of data privacy other challenges exist in research industry collaborations.
as the data comes from multiple sources it may not always be in a format required for training the model.
a key challenge would also include establishing the mechanisms to get the data in a format required for training at the source.
the data from the clients is never sent to the server which ensures data privacy.
however the clients get permission to update the model parameters.
this leaves a loophole for malicious clients to the system to perform model poisoning attack .
malicious clients or clients with low quality data may negatively affect the model built using the federated process.
another challenge would be dealing with the statistical heterogeneity of the data.
since organizations and software development teams have varying practices influenced by many factors the data may be highly non identically distributed.
this may add additional complexity.
federated learning requires frequent transmission of the machine learning model parameters between the clients and the central server.
the use of large deep learning models with large gradient vectors resulting from training updates of each client could make communication very expensive.
conclusion driven by the need to explore non open source data we envisioned the use of federated learning to enable the use of non open source artifacts in software engineering research.
we believe that it addresses a major concern of data privacy that prevents organizations from granting access to their data for research purposes.
we performed a preliminary evaluation of the approach on an existing study of classifying bug fix commits using the federated approach achieving an f1 score of .
compared to the score of .
from the centralized approach.
furthermore we presented the possible implications of our work which include the evolution of novel ai tools based on open and non open source data and the emergence of areas such as context specific and privacy preserving software engineering research.
1613exploring the under explored terrain of non open source data for software engineering ... esec fse november singapore singapore