calibration and correctness of language models for code claudio spiess uc davis usa cvspiess ucdavis.edudavid gros uc davis usa dgros ucdavis.edukunal suresh pai uc davis usa kunpai ucdavis.edumichael pradel univ.
of stuttgart germany michael binaervarianz.demd rafiqul islam rabin univ.
of houston usa mrabin central.uh.edu amin alipour univ.
of houston usa maalipou central.uh.edususmit jha sri usa susmit.jha sri.comprem devanbu uc davis usa ptdevanbu ucdavis.edutoufique ahmed uc davis usa tfahmed ucdavis.edu abstract machine learning models are widely used but can also often be wrong.
users would benefit from a reliable indication of whether a given output from a given model should be trusted so a rational decision can be made whether to use the output or not.
for example outputs can be associated with aconfidence measure if this confidence measure is strongly associated with likelihood of correctness then the model is said to be well calibrated .
a well calibrated confidence measure can serve as a basis for rational graduated decision making on how much review and care is needed when using generated code.
calibration has so far been studied in mostly non generative e.g.
classification settings especially in software engineering.
however generated code can quite often be wrong given generated code developers must decide whether to use directly use after varying intensity of careful review or discard model generated code.
thus calibration is vital in generative settings.
we make several contributions.
we develop a framework for evaluating the calibration of code generating models.
we consider several tasks correctness criteria datasets and approaches and find that by and large generative code models we test are not well calibrated out of the box.
we then show how calibration can be improved using standard methods such as platt scaling.
since platt scaling relies on the prior availability of correctness data we evaluate the applicability and generalizability of platt scaling in software engineering discuss settings where it has good potential for practical use and settings where it does not.
our contributions will lead to better calibrated decision making in the current use of code generated by language models and offers a framework for future research to further improve calibration methods for generative models in software engineering.
index terms llms calibration confidence measure i. i ntroduction generative large language models llms are now widelyused for code completion in ides.
however llms make mistakes they can generate known buggy code or code with risky vulnerabilities .
despite these risks llm copilots are growing in popularity thus there is a growing concern that bad llm generated code could be integrated into widely used software.
given that llms might generate equal contribution.
order determined by random coin flip.buggy code how should a developer decide whether generated code is correct or not?
one possibility is to use the confidence or probability assigned to the generated code by the llm itself.
consider a developer who asks g pt .
to complete some unfinished code.
for example given the prefix def clear self tag optional none none the model generates the completion self.jobs with an average per token confidence of suggesting high confidence based on its training that the code is a likely completion for the given prefix.
however this code is known to be buggy!
in fact when we test thousands of line completions in cases where the average probability was greater than only actually passed test cases.
one can also find reverse examples where the llm has very little confidence but the generated code is actually correct.
we make two observations.
first since llms can make mistakes users would benefit from a reliable indication of confidence that the generated code is actually correct .
second such indications of confidence when well aligned with actual correctness can support well justified software quality control measures and thus improve the reliability of the ai4se ecosystem.
when an llm offered code suggestion is accompanied by a numerical confidence signal e.g.
a probability measure then this signal should be well aligned with the likelihood that the code is actually correct.
such a measure is said to be well calibrated .
calibration has been studied in other settings e.g.
classically in weather prediction and recently for software related classification tasks .
in this paper we study the calibration ofgenerative1large language models when used in practical software engineering settings such as line level code completion function synthesis and code repair.
1we note that the notion of correctness for generative tasks is quite different than for classification tasks where the output is a label rather than a sequence of tokens.arxiv .02047v4 aug 2024a well calibrated confidence measure would support rational risk management in the development process and help quality improvement processes.
for example a development team might reasonably adopt a policy that a generated code associated with high confidence could be reviewed lightly and quickly accepted b suggestions with a medium confidence value should be reviewed more carefully before acceptance and c suggestions with a low confidence value should be simply rejected or the prompt should be adjusted.
despite its importance and widespread use of llms in software engineering the correctness and calibration of codegenerating models currently is not well understood.
in particular it is currently unknown whether confidence measures provided by the llms themselves align well with actual code correctness.
this paper does an empirical study of the calibration of code generating models using several prior techniques and explores approaches for improving calibration further.
to this end we describe an evaluation framework for the calibration of code generating language models.
we instantiate the framework for different tasks e.g.
code synthesis code completion and program repair using different correctness criteria exactmatch w.r.t.
a reference solution and correctness modulotesting and by applying the framework to different models.
based on this framework we evaluate well established techniques for estimating how a model s confidence align with actual correctness then based on our findings we present improvements over existing techniques.
our work yields several findings the alignment of confidence measures provided by llms with standard notions of code correctness is poor when evaluated on realistic datasets across different tasks including completion synthesis and automated program repair.
we observe generally high ece expected calibration error described in section iii c across all settings ranging from .09to0.
suggesting intrinsic llm confidences are poor predictors of code correctness.
we evaluate several reflective approaches to improve this alignment and also confidence rescaling using known correctness labels.
while rescaling generally improves calibration reflective methods are rather inconsistent working better in some settings than others.
finally we focus on the most widely used se task for llms viz.
code completion and use the instructable gpt .
model and few shotting in a reflective setting and show that calibration improves substantially from the skill score2of0to a much higher level of .
.
our work considers the important problem of providing developers with a reliable indication of whether generated code is correct and especially for the widely used task of code completion offers an approach using bm25 aided fewshotting that has potential practical value.
2brier skill score which we explain below in iii c.a.
research agenda code llms are perhaps mostly widely used for code suggestion completion other tasks include code synthesis and program repair .
rq .
how well is the confidence of language models in their output aligned with the empirical correctness of the output specifically for common generative tasks viz.function synthesis line level code completion and program repair?
we evaluate the output for two different notions of correctness viz.
exact match with known correct code and second passing all given tests.
in general the levels of alignment between the intrinsic confidence viz.
directly provided by the llm and correctness is poor.
this indicates need for better approaches to calibration.
we then explore several engineering responses to the problem listed in the following research questions.
first we consider the standard approach of confidence rescaling using platt scaling.
rq .
can alignment between llm confidence in generated code and its correctness be improved by confidence rescaling?
while confidence rescaling can help remedy over and under confidence it does require some data to determine the parameters of the scaling function.
we also analyze and discuss some considerations in obtaining this data specifically for code generation tasks.
next we investigate the possibility that the model is able to better calibrate upon reflection .
we ask the model using a separate reflective prompt to consider its own generated code and judge its confidence in the quality.
rq .
is confidence obtained by reflection better aligned with correctness?
finally we investigate few shotting to see whether it helps calibration for the widely used task of code completion.
rq .
can we use few shot techniques to achieve better calibrated confidence for code completion using an instruction tuned model with in context learning?
ii.
b ackground a. calibration this concept originates in prediction problems like weather forecasting.
consider a weather model predicting a confidence probability of rain the next day.
if we ran this model for a while and observed rain in of the days where a forecast with confidence was made then we call it a well calibrated model.
a well calibrated model s confidence in a given output is quite close to the empirical relative frequency likelihood with which the output is actually correct.
with well calibrated rain forecasting a user has options for arational response at confidence of rain one might take a hat at higher confidences one might take an umbrella if even higher one might take the bus rather than walk etc.
from an earlier work by jiang et al.
given a model m an input xand true expected correct output y a model output m x y note that we won t always have y y and a output probability pm y x provided by the model a perfectly calibrated model satisfies the following condition p y y pm y x p p p .
in other words if we have perfectly calibrated confidence p the model s calculated probability of its prediction that the output is y then this value equals the empirical fraction pof the cases where the actual output ycorrectly matches the prediction y. usually these probabilities don t perfectly match there are various measures of the deviation including brier score and ece expected calibration error b. why calibration matters for code even powerful llms can make mistakes potentially leading users to accept incorrect code.
a well calibrated confidence signal could help developers manage this risk.
consider the confidence passociated with generated code.
a well calibrated high value of pwould indicate a high empirical probabilty pthat the code is correct and so it could be simply accepted a low value would indicate higher risk that the code is incorrect and so should be rejected.
a poorly calibrated model may lead to either unnecessary rejection of likely correct code or ill advised acceptance of likely incorrect code.
we note that good calibration allows more nuanced effective quality control q c decisions beyond simple binary decisions e.g.
carefully review each token of generated code perhaps by several people vs.just use it.
such a well calibrated quality control process has been used in medicine e.g.
for elder care and for decision making in cancer care .
given the cost consequences of properly addressing software quality and the potential benefits of llmgenerated code a well calibrated confidence signal is highly desirable.
iii.
r esearch methodology we consider three generative tasks i.e.
function synthesis line level code completion and program repair where generative llms are directly applicable and widely used e.g.
completion in copilot .
in this section we will discuss the tasks datasets models and methodology of our approach.
a. code correctness when evaluating calibration we need a notion of correctness.
for non generative models outputting labels classes true false etc.
correctness is simply an exact match with ground truth correct label.
exact match could also be viewed as a notion of correctness for code e.g.
with defect repair where there is a known incorrect buggy version and a known fixed version.
generated code is correct only if it matches exactly the fixed version given an appropriate prompt.
however this approach is overly strict the generated code might match exactly but still pass all tests.
other notions of correctness exist code review formal verification etc.we use test cases provided with the code as our preferred indication of correctness as tests are widely used and are easily automated.while test passing correctness offers the advantage of admitting different semantically identical forms test cases maybe insufficient or incorrect3 tests can also be flaky the same test on the same code might pass or fail.
b. confidence measures we usually calculate a confidence measure or probability p associated with generated output code c. we consider two categories of measures intrinsic probability which is calculated by the generative llm per se and reflective probability obtained by re invoking the model instructing it to estimate its confidence in the correctness of the code just generated see appendix figure a1 for prompts .
our measures include average token probability intrinsic pavg for an output sequence tof tokens i i .
.
.
n we collect the associated model probabilities p i and then compute the mean pavg t nnx i 1p i .
generated sequence probability intrinsic ptot the full generated sequence confidence is calculated as the product of probabilities ptot t ny i 1p i .
verbalized self ask reflective pv we instruct the model to reflect jointly on the prompt and the modelgenerated code and then output a numeric value of its confidence in the generated code.
extra logic is implemented for when the model fails to output a probability discussed further in appendix a .
question answering logit reflective pbandpnb in this case rather than prompting for a numerical score as above we ask for a true orfalse answer.
the probability associated with the true token is taken to be the confidence measure.
additionally we extend this approach using normalization the model can assign probability mass to multiple possible expressions of true orfalse or even other variations e.g true true .
thus when extending to the normalized form ask t f n we take the fraction of probability mass true assigned between only true and false .
our experiments consider the four above average token probability generated sequence probability verbalized selfevaluation and question answering logit .
in addition as a baseline we also used the length of the generated sequence.
the length baseline is calculated based on the number of characters in the generated sequence scaled such that 0is the shortest value in the dataset and 1is the longest.
for reflective measures we expected that a code generation model should perform well and provide well calibrated confidence scores for correctness first for synthesis given just 3e.g.
liu et al.
report gaps in the test sets of humaneval.a good natural language description without tests of the desired function second for completion bug fixing given the surrounding context.
in both settings we measure correctness using available hidden test cases.
we also note that including hidden or failing tests in an llm prompt is not common experimental practice for the tasks we analyze.
.
c. measures of calibration using model s confidence in its generated output and a way of determining correctness one can compute measures of calibration.
calibration measures conceptually arise from thereliability plot which plots correctness vs.confidence two reliability plots illustrating this method are shown in figure .
figure 1a is for token level code completions from c odex it shows the observed proportion of exactmatch correct tokens y axis vs.the predicted probability i.e.
confidence as per the language model based on bucketing observations into subsets s1 s2 .
.
.
s n. here there are n buckets equally spaced by confidence measure.
each bucket has an associated bar whose height indicates the proportion value of correct samples in the bucket.
the closer the bars in each siare to the diagonal line the better the calibration.
.
.
.
p estimate .
.
.
.
.
.0p correct .
codex t oken level reliability a well calibrated .
.
.
p estimate .
.
.
.
.
.0p correct .
gpt3.
verbalize reliability b poorly calibrated fig.
sample calibration plots demonstrating well vs.poorly calibrated.
we note here that c odex is a large model well trained on the task of token level completion thus it is both wellcalibrated and generally correct on the simple token level completion task.
however for notions of correctness farther from the training objective such as line level code completion and test passing c odex sintrinsic probability may not be as well calibrated.
an example figure 1b where the confidence is not well calibrated is g pt .
for line completion using verbalized confidence i.e.
asking it to write its confidence see section iii b .
we study two measures of calibration brier score and ece .
both measure the deviation from perfect calibration.
as before following we assume a model m input x actual desired output y and model prediction m x y. in our case both yand yare code rather than a single label.
calibration measures indicate the extent to which the deviations of yfrom the desired yare actually aligned with the model s confidence in its output y. from the calibration plot with the evaluation set tbucketed into subsets si i .
.
.
m s.t.s isi t i .
.
.
m .
we estimate correctness in each bucket as the fraction of predictions in that bucket which are correct as discussed in iii a confidence is the average estimated probability from the model in that bucket corr si si x xi si1 m xi conf si si x xi si pi where the model generates the code m xi with confidence probability pi and m xi indicates that the generated code is correct as per the operative experimental definition.
for a perfectly calibrated m we would have corr si conf si i .
.
.
m .
in practice we observe deviations from this ideal.
expected calibration error ece is a typical measure of calibration calculated as the weighted average of these deviations.
ece mx i si t corr si conf si ece is intuitive but can mislead as seen below a na ve predictor whose confidence is always the base rate would yield a deceptively low ece value.
an alternative measure the brier score b avoids this issue it is calculated as follows b t t x i pi m xi one can achieve an optimal brier of b 0when confidently estimating pi 1when the code is correct and estimating pi 0when the code is incorrect for each sample.
for comparison consider using the unskilled reference brier score bref attainable by a na ve unskilled model which simply assigns the base rate pras its confidence for every prediction.
here all prediction confidence values are in one bin the value pr and the empirical correctness in this single bin isthe base rate pr soece 0which is misleading thus exemplifying one of the weaknesses of ece .
the closed form brier score for this unskilled predictor is bref pr pr in a coinflip scenario assuming heads iscorrect a na ve predictor that randomly guesses correct with confidence receives bref .
.
.
.
higher base rates yield lower bref e.g.
for the mbpp dataset gpt .
generates test passing solutions for about of the programming problems here always guessing correct with confidence results in bref .
.
.
.
with well calibrated confidence scores a skilled model can achieve brier scores lower than this unskilled brefvalue if a model does worse it is indicative of poor calibration.
thus one commonly reports a skill score ss calculated thus ss bref bactual bref positive ss perfect score .
indicates improvement over baseline bref negative indicates worse calibration than the baseline.
small positive values of ss can sometimes indicate good skill.
for example the deutsche wetterdienst german weather forecasting service considers .
skill score to be a minimum threshold for a good forecast quality4.
as another data point the american data journalism site reports a skill of around .
in forecasting world cup games5 which is in the range of what we observe in our experiments for best case code generation by llms but these llm performance numbers are just a starting point and can be expected to improve in the future.
ece equation and brier score equation serve slightly different purposes the brier score is calculated for each sample and measures both the ability to correctly discriminate output categories and calibration of the output probability.
the ece measures just calibration but it can be misleadingly low as noted above for the unskilled predictor.
additionally binning must be done carefully since it can affect ece scores .
d. rescaling approaches machine learning models are not always calibrated.
guo et al.
discuss ways of rescaling probability estimates to better match observations.
a common approach is platt scaling where a logistic regression is fit to the logit values of the prediction i.e.
thelnof the measured confidence probability.
this optimizes two parameters a linear scaling multiplier and a bias i.e.
intercept that shifts the value.
to reduce the likelihood that the scaling overfits skews our results we rescale over five folds i.e.
we fit a logistic regression on a random 5of data and apply it to 5of data before sliding over and each combination of .
besides platt scaling temperature rescaling has also been used this approach applies a scalar multiplier on the logits representing each class e.g.
a multiclass image classifier.
in our binary confidence case this has similar expressivity to platt scaling without an intercept.
other approaches include histogram binning isotonic regression inter alia .
these approaches are more parameterized given the data limitations in our experimental setup e.g.
a few hundred examples in function synthesis they pose higher risk of overfitting.
as we discuss in section iv platt scaling does improve calibration with some caveats.
e. tasks dataset task dataset dataset sizecorrectness measureconfidence measurecalibration metric function synthesishumaneval test passing correctnessaverage token probability generated sequence probability verbalized self evaluation question answering logitbrier score ecembbp func line level completion dypybench test passing correctness emprogram repairdefects4j line manysstubs4j exact match em table i list of tasks with associated datasets and measures.
function synthesis this task aims to generate python functions from docstrings .6correctness is determined by functional testing.
forecasts forecast reliability.htm 5projects.fivethirtyeight.com checking our work 6docstrings are code comments that explain the code s purpose and usage.
further discussed in iii e2.we use h uman eval and mbpp datasets see appendix figure a2 for sample prompts and model output .
one caveat the samples in these datasets largely constitute artificial problems specifically assembled to test the code synthesis capacity of llms measurements both accuracy and calibration over these datasets may not generalize to real world software development.
even so these datasets provide a valuable datapoint for assessing model calibration.
we restructure all mbpp problems into a function synthesis task by placing the prompt inside the tested method as a docstring making it comparable to h uman eval.
additionally we exclude approximately problems where the reference solution fails to pass the provided test cases7.
line level code completion code completion is currently the most important and widely deployed generative task with tools like github copilot .
completion performance has been studied at both the token and line levels .
however calibration for this vital widely deployed task has so far not been evaluated in detail the current decoder only gpt models are already trained to generate the next token at low average crossentropy given all the prior tokens following the condition p token prior tokens .
unsurprisingly we found such autoregressively trained models are per se well calibrated at the token level.
in this work we will primarily focus on line level completion.
while several datasets exist for this problem we use d ypybench a new dataset consisting of popular open source python projects including test suites for these projects.
the test suites allow a testcorrectness measure in addition to the highly restrictive exactmatch viz.
the original line .
dypybench consists of complex real world projects each with hundreds of thousands of lines of python code and totaling over .
million lines of python code.
we ran all test suites for each project with coverage reporting enabled extracted all functions from the projects following and selected functions with at least lines in the body test coverage and at least one line in the docstring .
program repair program repair is a well studied problem in software engineering .
several studies report that llms are effective at this task .
however llm calibration for program repair is not well understood.
this paper focuses on small pre localized single line bugs.
we leverage the widely used defects4j dataset which includes real world examples of buggy programs with fixes and test sets.
we extract single line bugs from d efects 4j dataset.
however with only samples we may not obtain a comprehensive view of calibration.
therefore we included another dataset manysstubs4j abbr.
ss tubs which consists of single line repairs.
following the setup of the ss tubs dataset the bug might be localized to a sub7due to either buggy reference code tests or possibly missing environment networking compute time requirements.expression of the line8.
we sample uniformly at random examples from this dataset.
ss tubs does not provide test sets so the only evaluation metric available is the exactmatching of the generated text to the ground truth bug free text.
f .
the models we explore confidence calibration for three code generation models.
these include openai g pt .
openai c odex and c odegen2 16b .
we sample from the models with temperature of consistent with the reality that busy developers typically look at just the first suggestion in the completion .
for function synthesis task temperature zero is most accurate and is fairly standard practice when pass with only one solution to generate and present .
iv.
r esults we begin with a brief overview of the findings on the correctness confidence measures of llms on the various tasks and then provide detailed results on the calibrationrelated research questions.
all pass exact match codegen2 codex gpt .
codegen2 codex gpt .
sstubs .
.
.
dypybench .
.
.
.
.
.
defects4j .
.
.
.
.
.
humaneval .
.
.
mbpp .
.
.
table ii performance comparison of models on tasks.
metrics are all pass at rank all pass meaning all project test cases passed with the line completion on first and only sample at t and exact match meaning the line completion was an exact string match with the original project line.
exact match is not commonly used for function synthesis tasks since the generated output is longer and less likely to match.
sstubs dataset does not have test cases.
boldface signifies high performing model for task and metric.
a. rq how well are language models confidence in their output aligned with the empirical correctness of their output specifically for common generative tasks viz.function synthesis line level code completion and program repair?
overall correctness correctness performance rate of the various models on the various tasks and datasets are presented in table ii.
specifically we report the fraction of samples passing all test cases for a given model and dataset and the percentage of exact matches.
we found that g pt .
worked well for both function synthesis h uman eval and mbpp and line level code completion whilst c odex generally performed well on program repair.
the d ypybench benchmark reflects the most popular use of llms viz.
for code completion.
8note due to data processing errors defects4j examples have slightly mis localized bugs.
we leave these as is reasoning that model confidence should be robustly well calibrated even with slight localization noise ideally giving a lower confidence of a fix if the location is noisy.
9the gpt .
turbo instruct model models gpt turbo2 correctness test passing vs. exact match as per iii a we evaluate correctness both on test passing and exact match.
our experiment included two datasets defects 4j and d ypybench where both methods of measuring correctness were available.
since d efects 4j consists of only samples we present the results for dypybench in this case as per table ii g pt .
performed best with approximately of generated code passing all available tests and approximately matching exactly.
in this setting d ypybench gpt .
we cross tabulate performance across the two correctness measuring methods.
we note that approximately half the test passing generations didnotmatch the original code exactly furthermore .
of the cases where the code matched exactly did not pass all the test cases.
upon careful study we found that these tests were flaky depending on network conditions execution order and other variable execution environment conditions.
this aligns with the author of this dataset who observed an overall failure rate but noted that out of projects had zero failed tests.
this illustrates the relative merits demerits of each correctness evaluating approach in practical se settings.
since the correctness performance is different with these two notions of correctness the calibration is also different as we see below.
confidence measures as might be expected the two intrinsic measures pavg ptotare usually somewhat and sometimes strongly positively correlated with each other within the same model dataset and task.
calibration without rescaling table iii presents the results for line completion function synthesis and program repair for each model and the raw confidence measure without any rescaling method.
we find raw confidence measures are poorly calibrated with inconsistent exceptions.
in fact the raw baseline rate using the average fraction correct without considering the individual generation is hard to beat the best skill score is around .
.
for line completion the ptotconfidence measure is slightly worse than the baseline rate calibration error is modest ece .
.
the total probability improves on the average probability which is overconfident the average token probability exceeds the overall success rate.
for function synthesis with raw measures ptotexhibits very poor calibration for g pt .
and c odex but not for codegen2 on h uman eval while the best intrinsic measure for mbpp is pavgfor g pt .
and c odex .
the intrinsic measures are inconsistent with average probability showing indicators of better calibration for g pt .
and c odex but not for c odegen2.
for program repair intrinsic measures are consistently below the base rate for both models and are as such poorly calibrated.
there are several caveats here.
first defects 4j is a small dataset so findings may not generalize.
second c odegen2 performs poorly on defects 4j.
since c odegen2 is a smaller model without instruction tuning and relatively more limited reasoningline completion function synthesis program repair dypybench humaneval mbpp defects4j sstubs model metric b ss ece b ss ece b ss ece b ss ece b ss ece gpt .
total prob .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ask t f .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ask t f n .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
verbalize .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
length .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unskilled .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codex total prob .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ask t f .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ask t f n .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
verbalize .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
length .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unskilled .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codegen2 total prob .
.
.
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
.
.
ask t f .
.
.
.
.
.
.
.
.
ask t f n .
.
.
.
.
.
.
.
.
verbalize .
.
.
.
.
.
.
.
.
length .
.
.
.
.
.
.
.
.
unskilled .
.
.
.
.
.
.
.
.
table iii calibration measured as raw non scaled brier score b lower better skill score ss higher better and expected calibration error ece lower better with respect to all passed notion of correctness except sstubs which is exact match .
codegen2 repair values are omitted as it does not perform the task with greater than accuracy.
the unskilled row corresponds to a naive approach where the confidence is always returned as the base correctness rate with skill score ss always zero by definition.
capabilities it gets distracted by the buggy version shown in the prompt it often just repeats the buggy lines.
with very few correct outputs the estimation of the confidence measure becomes unreliable.
therefore we have removed the c odegen2 results from table iii.
a final caveat is that ss tubs uses only exact match as a correctness measure which is quite different from a test passing measure.
in general non scaled confidence measures are only well calibrated for exact match on code completion for test passing correctness they are poorly calibrated.
b. rq can alignment between llm confidence in generated code and its correctness be improved by confidence rescaling?
table iv shows the results after applying platt scaling to all measures see iii d .
figure 2a shows a reliability plot before rescaling and its equivalent after rescaling in figure 2b.
rescaling can improve calibration.
considering all values ece improves from an average of .
to .
.
for just those measures with a post scaling ssof at least .
ece improves from .
to .
.
a understanding bucket collapse platt scaling can lead to deceptively low ece .
if a confidence measure is poorly aligned with correctness platt scaling can rescale squash all the confidence values to the baseline rate this places all samples in a single confidence value bucket where probability exactly matches the baseline rate of correctness resulting in an ece near .
this indicates the problem of only considering ece .ssand brier on the other hand would reveal the poor utility of the confidence measure.
thus when applying rescaling it is important to consider skill score rather than only brier and ece .
p estimate .
.
.
.
.
.0p correct rescaling none all pass ref .
ece .
.
ss .87dypybench gpt .
reliability plot .
.
.
.
.
.
.
.
.
.
.
p estimate 05001000count a dypybench nonscaled reliability plot p estimate .
.
.
.
.
.0p correct rescaling platt all pass ref .
ece .
.
ss .08dypybench gpt .
reliability plot .
.
.
.
.
.
.
.
.
.
.
p estimate 05001000count b dypybench platt scaled reliability plot fig.
reliability plots for d ypybench line level code completion tasks with respect to all pass correctness measure and average token probability confidence measure.
g pt .
was used for both experiments.
bottom histogram represents number of samples in each bin.
bref refers to the unskilled predictor brier ece to expected calibration error bto brier score and ssto skill score.
red purple lines represent scaled non scaled quantile bins rather than evenly spaced bins with of the data at each point.
the left nonscaled plot shows over confidence as the confidence estimate is high but the actual correctness is low.
the scaled plot right improves calibration.
b results after rescaling only the intrinsic measures show skill improvement over the baseline rate for line completion.
pavgandptotare similarly calibrated.
the calibration and skill appears roughly consistent between all three models in this case.
rescaling improves calibration results for function synthesis.
the ptotmeasure reaches a ssof .
for h uman eval.
rescaling useful improvement for reflective prompts as well bringing ssandece to similar values discussed further in section iv c .
for program repair rescaling doesn t improve skill score for any measure.line completion function synthesis program repair dypybench humaneval mbpp defects4j sstubs model metric b ss ece b ss ece b ss ece b ss ece b ss ece gpt .
total prob .
.
.
.
.
.
.
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
.
.
.
.
ask t f .
.
.
.
.
.
.
.
.
.
.
.
ask t f n .
.
.
.
.
.
.
.
.
.
.
.
verbalize .
.
.
.
.
.
.
.
.
.
length .
.
.
.
.
.
.
.
.
.
unskilled .
.
.
.
.
.
.
.
.
.
codex total prob .
.
.
.
.
.
.
.
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
.
.
.
.
.
.
.
ask t f .
.
.
.
.
.
.
.
.
.
ask t f n .
.
.
.
.
.
.
.
.
.
verbalize .
.
.
.
.
.
.
.
.
.
length .
.
.
.
.
.
.
.
.
.
unskilled .
.
.
.
.
.
.
.
.
.
codegen2 total prob .
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
ask t f .
.
.
.
.
.
ask t f n .
.
.
.
.
.
verbalize .
.
.
.
.
.
length .
.
.
.
.
.
unskilled .
.
.
.
.
.
table iv calibration measured as platt scaled brier score b lower better skill score ss higher better and expected calibration error ece lower better with respect to all passed notion of correctness except sstubs which is exact match .
in cases where the ssis less than .
the ece is omitted.
this is because an estimate without any signal will become platt scaled to approximately the base rate.
this will appear as one well calibrated bin resulting in an ece near zero but does not provide information.
codegen2 repair values are omitted as it does not perform the task with greater than accuracy.
c is rescaling a panacea for calibration?
rescaling typically improves calibration it has been used in settings other than generative models of code with other notions of correctness .
however there are disadvantages.
first bucket collapse see iv b0a can mislead with deceptively low ece .
second some correctness data is needed to fit rescaling parameters.
when sweeping through various sized bootstrapped subsets of the data we find that it can take over data points for the rescaling to result in positive skill and lower ece with improvements continuing into 100s of data points see appendix figure a8 for bootstrapping analysis .
when using the full data the rescaling between tasks can vary dramatically.10ideally we want confidence measures which are reliable and allow trustworthy auditing even when applying language models to new software engineering tasks.
to study how close we are to this we fit rescaling parameters to one task and then apply it to the other tasks see appendix figure a7 .
we find it is viable to use rescaling between tasks of the same domain with similar base rates such as within the program synthesis tasks.
for example for g pt .
when fitting pnb results in next section rescaling to each of two function synthesis tasks and then applying it to the other we observe an average drop ofssfrom .
.
and average drop of ece from .
.
.
however applying the ptotal rescaling fit on dypybench to the function synthesis tasks results in an average sschange of .
.
and ece change of .
.
indicating a lack of robustness.
these reasons suggest one must be careful when analyzing and reporting calibration results based on rescaling and highlights the need 10as seen in appendix figure a6 which shows the curves learned between measure task pairs.for further work on confidence measures that might be more directly calibrated.
without rescaling total probability ptotalshows hints of calibration.
with rescaling there is a possible improvement over baseline rates and good calibration but it is inconsistent as skill is poor for c odegen2 on function synthesis.
rescaling is an effective technique for improving calibration but metric improvements bandece may be misleading by matching the base rate in a bucket collapse scenario or can lack generalization.
c. rq is confidence obtained by reflection better aligned with correctness?
the two logit based reflective measures pb pnbare usually strongly positively correlated with one another since they are calculated from similar numbers.
the reflective verbalized self ask confidence measure pv and the two logit based reflective confidence measures have no consistent relationships.
for function synthesis with raw measures pnbshows best calibration for g pt .
slightly better than unskilled for human eval.
for program repair we observe the strongest best case performance with regards to the metrics.
both g pt3.
and c odex show positive ssand low ece forpb but they are inconsistent after normalization pnb .
these metrics suggest with reflection these models confidence is calibrated regarding repair correctness however further analysis see appendix figure a3 does not indicate good calibration on this task from any confidence measure.
we find that in general the intrinsic vs.reflective measure values have no consistent relationship even for a given model dataset and task.
this lack of relationship may not necessarily be negative e.g.
perhaps the model s reflective prompted confidence maybe better calibrated as suggested by prior work .
without rescaling or few shot prompting reflective results are inconsistent.
in some cases such as g pt .
h uman eval and d efects 4j there are signs of calibration with slightly positive ssvalues and ece values less than .
.
normalizing the t f values induces some difference but there are inconsistencies vis a vis tasks and models.
for nonscaled g pt3.
results pnbimproves calibration in line completion and function synthesis by an average of .
ssand .
ece but not for program repair or for c odegen2.
rescaling generally removes any sign of a normalization trend.
for the alternative reflective approach of verbalization the probability is not well calibrated for these models on the studied se tasks.
in some cases the reflective approaches are best calibrated without rescaling see table iii and show signs of being more robust when reusing learned rescaling parameters on unseen tasks see appendix figure a7 .
reflective approaches may be best calibrated out of the box and in some settings when rescaled.
however in the tested settings they do not improve significantly over intrinsic measures.
d. rq can we use few shot techniques to achieve better calibrated confidence for code completion using an instruction tuned model with in context learning?
we investigated the impact of few shotting viz.providing a model completion and correctness as part of the pnbprompt on calibration .
to effectively perform few shotting we need a model that is instruction tuned and sufficiently large which is best matched by g pt .
.
we explore few shotting for the widely used line completion task.
we perform the experiment with shots consisting of prior completions from the same experiments presented in table iv the reflective question and the ground truth true false.
we try two variants of this experiment one where the examples are randomly selected and one where they are chosen based on the similarity to the unanswered prompt.
in both cases we exclude the ground truth result for the unanswered prompt.
we focus on line completion for this experiment as it represents widespread use and has a large number of examples available.
confidence measure b ss ece shot reflect .
.
.
shot reflect rescaled .
.
fs random .
.
.
fs random rescaled .
.
fs bm25 .
.
.
fs bm25 rescaled .
.
.
table v few shot reflective prompting using g pt .
for line completion.
fs random refers to selecting random few shot examples.
fs bm25 retrieves more relevant known completions.
ece values when rescaled values ss are close to zero are omitted to avoid confusion with bucket collapse section iv b0a for line completion the non scaled results using random examples did not result in improved calibration over the baseline pnb however using bm25 to select similar examples yielded a positive ss of .
which could beimproved further by rescaling up to .
.
this result notably exceeds any other measure for d ypybench and significantly improves over the baseline pnbssof .
while random few shotting requires limited extra data bm25 is more similar to rescaling in that it is dependent on a larger set of ground truths.
this could be actualized by logging user completions and building up ground truths on if the completion was correct based off the test case runs or acceptance of completions.
providing a few examples selected via bm25 when asking g pt .
to reflect on its own completion output significantly improves reflective calibration.
alternative and improved ways of prompting e.g.
different verbalization formats fine tuning chainof thought etc.
may alter these findings and are areas for future work.
v. d iscussion language models are now widely integrated into software engineering practice via tools like copilot and didact11.
we raise here the importance of calibration when integrating generative llms in coding practice.
we evaluate the calibration of generative llm use especially code completion with large samples of realistic data d ypybench ss tubs using widely adopted models as well as some more academic datasets h uman eval mbpp .
a using a well calibrated model beyond simple defect prediction to clarify how a well calibrated model enables more well grounded decision making concerning generated outputs as compared to as compared to a traditional process choosing a binary decision point we consider g pt .
working on code completion where correctness is determined by test passing and confidence is assigned by few shotting average token probability.
the base correctness test passing rate of completions is about .
with few shotting we get a very high skill score of .
section iv d table v .
if we didn t have a well calibrated model we might very cautiously accept only those completions that are generated at a very high confidence threshold here the fp rate could be low of course tp rate would be low as well .
while this may lower the risk of bad code it also regrettably reduces the available help from the llm.
however a well calibrated confidence measure allows a more rational graduated set of decisions.
such a well calibrated measure is visualized in figure .
in this setting for much of the confidence scale a user could look at the confidence level and get a very good idea of how likely the code is to be correct and make a well reasoned situation specific set of decisions to manage risk and allocate reviewing resources based on the model s confidence.
this provides an illustration of the greater benefit provided a well calibrated measure high skill level low ece over one that is just providing good precisionrecall trade off or roc curve the latter does not allow such graduated deployment of quality control effort.
however 11blog.research.google large sequence models for software.htmlp estimate .
.
.
.
.
.0p correct rescaling platt all pass ref .
ece .
.
ss .
3few shot reflective reliability plot .
.
.
.
.
.
.
.
.
.
.
p estimate 05001000countfig.
few shot reflective reliability plot based on fs bm25 row of table v developers would need to learn to use calibrated probabilities in decision making.
b beyond simple correctness in addition to the above uses which considered a single notion of correctness one could consider a multi class correctness prediction task where the model could indicate the confidence in correctness the absence of defects from multiple perspectives severity of possible defect the kind of defect relating to security integrity privacy fairness etc.
and defect complexity indicating the cost or schedule impact of repairs .
drawing an analogy to classical forecasting this is analogous to not just the probability it will rain but probability it will be a drizzle or be a drenching thunderstorm.
c why calibration now?
we ve always had bugs poorquality code isn t new.
our push for calibration however arises from the increasing amount of code generated by llm.
github claims that up to of code12in some systems is generated by llms.
it is also known that llms make a lot of mistakes.
a recent paper has reported that llms even when trained on properly fixed code tends to recapitulate thethe old unfixed buggy code when prompted in context.
however llms do have very high capacity and a demonstrated ability to usefully reflect on their generated text.
thus we have both a high risk of buggy code and a chance to improve productivity.
we believe that improved calibration could lead to better management of the risk benefit of llm generated code.
the studied correctness calibration is a stepping stone for more complex notions of confidence like severity and localized confidence .
additionally by studying code llms we might make progress on the general safe deployment of capable generative models .
d summarizing per token probabilities to produce a summary confidence for generated token sequences in tables iii and iv in tables iii iv we used arithmetic mean average product to summarize the per token probabilities.
12github.blog github copilot now has a better ai model andnew capabilitiesin this setting it might be more reasonable to use geometric mean as in to get a product value normalized for length indeed when we tried that we found that brier and skill scores improved marginally but consistently so future research could indicate whether these findings generalize.
vi.
t hreats to validity a sample size generalizability while three of our datasets contain more than samples each h uman eval and d efects 4j datasets consist of only and samples respectively.
results on these datasets may not generalize.
however we note that our study has a large and natural dataset for the line level code completion task which has current practical importance.
given the noise and variance we observe we recommend future work push towards larger and more natural datasets in particular for function synthesis repair.
while some of our treatments e.g.
few shotting suggest substantial improvements in skill score table v in other cases such as the different approaches to summarize per token confidence differences the differences are less clear.
in future work these differences could be judged more robustly using bootstrapped p values and effect sizes.
b artificial vs.real world data for function synthesis we used the popular h uman eval and mbpp function synthesis datasets.
these datasets contain small ish python programs that may not represent real world software development functions.
however our other datasets such as d ypybench and ss tubs are more representative of real world opensource github projects.
c model selection results might not generalize to all models especially those with greatly differing training finetuning or different architectures.
d experimental design our exploration is not exhaustive other se tasks and datasets also could benefit from calibration studies.
additionally the specific prompts we used for this paper surely played a role in our findings.
other prompts or problem phrasings such as different forms of context for line level code completion may yield different results.
regarding test flakiness the test flake rate of dypybench is not zero but is quite low and not unrealistic .
despite these caveats our study which includes three tasks and five datasets provides a good starting point for further studies.
vii.
r elated work llms for code are extensively studied .
while calibration has a long history in modeling it is not a frequently studied topic in the se community.
early work moving into modern machine learning studied the calibration of smaller neural models performing classification tasks on text and images while these early models were poorly calibrated per se their performance could be improved by simple scaling of their output probabilities.
as models became larger calibration was found to improve .
pre training was also found to improve calibration however these findings have been disputed .more recent works evaluated llm calibration on a wide variety of settings .
desai et al.
studied non code natural language tasks such as inference or paraphrasing with only intrinsic measures using oldergeneration models bert and roberta .
jiang et al.
studied calibration for natural language question answering using just intrinsic measures.
in contrast we study calibration for three coding related tasks using both artificial and natural code datasets and both intrinsic and reflective confidence measures to evaluate calibration in the se domain.
other prior work has investigated tokens that might be edited.
vasconcelos et al.
discusses code model uncertainty for function synthesis style problems and ran human evaluation of the usefullness of colored highlighting of uncertain tokens.
they found highlighting a human derived groundtruth of which tokens might be edited was helpful and more useful than raw token probabilities from the model.
johnson et al.
developed method of highlighting likely edit tokens via a utility optimization algorithm comparing different file completions.
we find exploring more on calibrated uncertainty for local areas be a interesting area for additional work.
liet al.
investigate the calibration of computer vision cv models from an operational perspective i.e.
the shift between training input and production inputs presenting it as a software quality problem that can be addressed using bayesian approaches.
minderer et al.
evaluate the calibration of at the time state of the art cv models and find improved calibration with more recent models notably those not using convolutions.
park et al.
study the effect of the mixup technique on calibration in a natural language understanding nlu setting using older generation models bert and roberta .
chen et al.
investigate the calibration of pretrained language models on various nlp tasks also using older generation models roberta and t5 .
bommasani et al.
introduce the helm benchmark which includes calibration as one of its seven metrics to evaluate language models in a natural language context.
huang et al.
explored lm uncertainty with a range of techniques and tasks including both nlp and function synthesis tasks.
they evaluated using correlation measures rather than focusing on calibration.
they explore interesting sample based and perturbation techniques which could be explored more for calibration on diverse se tasks.
other work has explored training an ml model that sees code and execution results to estimate correctness probabilities for solution reranking.
for natural language question answering tasks work has explored improving calibration by training a model to adjust token logits and training a model from llm hidden states specifically around the ece metric .
when suitably prompted kadavath et al.
found that llms can output well calibrated scores on whether their own answers are correct or not viz.
larger models know what they know .
while this work did investigate some function synthesis tasks h uman eval an unpublished python function dataset they did so using only their private models and ultimately focused on natural language tasks.
key et al.
developed an approach that given a natural language problem description produces a confidence score for a sampled candidate solution based on generated specifications allowing them to judge whether the llm can solve the problem at all.
their metrics include calibration.
recent work has also explored calibration of software topics such as root cause analysis .
viii.
c onclusion in this paper we begin with the observation that while llms are often helpful for example producing codecompletions for developers they often produce buggy code.
we argue that a well calibrated confidence score could provide a reliable indication of whether the generated code was correct and help more rational graduated quality control of of llm generated code we studied the calibration of intrinsic and reflective confidence measures in several practical settings completion and repair and a widely used competitive setting synthesis across several llms.
we find that llms are generally poorly calibrated out of the box across a variety of confidence measures both intrinsic and reflective we then found that platt scaling generally results in somewhat better calibrated confidence measures.
finally we focused in on a coding task where llms are most widely deployed viz.
code completion and b a very widely used instruction tuned model viz.gpt .
and investigated whether a reflective in context learning approach few shotting could provide better calibrated confidence measures.
in this setting we found that calibration improves substantially reaching a skill score of .
particularly with retrieval augmented few shotting.
to our knowledge our paper is the first to consider the problem of calibration in a real world code generation setting.
we do find that most models both out of the box and with simple reflection don t provide reliable confidence measures.
however our results with retrieval augmented few shotting are very encouraging and point towards a future where language models could provide developers with guidance on how to quality control the code they generate.
ix.
a cknowledgments we acknowledge partial support for this work by the intelligence advanced research projects agency iarpa under contract w911nf20c0038 the national science foundation under cise shf medium the european research council erc grant agreement and the german research foundation concsys democo and qptest projects .
devanbu was supported by a humboldt research award13.
our conclusions do not necessarily reflect the position or the policy of our sponsors and no official endorsement should be inferred.
explore the humboldt network singleview prof dr premkumar t devanbureferences k. jesse t. ahmed p. t. devanbu and e. morgan large language models and simple stupid bugs in ieee acm 20th international conference on mining software repositories msr may pp.
.
o. asare m. nagappan and n. asokan is github s copilot as bad as humans at introducing vulnerabilities in code?
empirical software engineering vol.
no.
p. sep. issn .
r. schuster c. song e. tromer and v .
shmatikov you autocomplete me poisoning vulnerabilities in neural code completion presented at the 30th usenix security symposium usenix security pp.
isbn .
d. lo.
trustworthy and synergistic artificial intelligence for software engineering vision and roadmaps.
arxiv .
.
oct. preprint.
z. zhou c. sha and x. peng on calibration of pretrained code models in ieee acm 46th international conference on software engineering icse ieee computer society pp.
.
n. nashid m. sintaha and a. mesbah retrievalbased prompt selection for code related few shot learning in ieee acm 45th international conference on software engineering icse pp.
.
.
icse48619.
.
.
z. jiang j. araki h. ding and g. neubig how can we know when language models know?
on the calibration of language models for question answering transactions of the association for computational linguistics vol.
pp.
sep. issn 387x.
g. w. brier verification of forecasts expressed in terms of probability monthly weather review vol.
no.
pp.
jan. issn .
m. p. naeini g. cooper and m. hauskrecht obtaining well calibrated probabilities using bayesian binning proceedings of the aaai conference on artificial intelligence vol.
no.
feb. issn .
d. gros p. devanbu and z. yu.
ai safety subproblems for software engineering researchers.
arxiv .
.
aug. preprint.
d. m. nierman c. b. schechter l. m. cannon and d. e. meier outcome prediction model for very elderly critically ill patients critical care medicine vol.
no.
p. oct. issn .
j. schwarz and d. heider guess projecting machine learning scores to well calibrated probability estimates for clinical decision making bioinformatics vol.
no.
pp.
jul.
issn .
j. liu c. s. xia y .
wang and l. zhang is your code generated by chatgpt really correct?
rigorous evaluation of large language models for code generation advances in neural information processing systems vol.
pp.
dec. .
q. luo f. hariri l. eloussi and d. marinov an empirical analysis of flaky tests in proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering ser.
fse new york ny usa association for computing machinery nov. pp.
isbn .
s. lin j. hilton and o. evans teaching models to express their uncertainty in words transactions on machine learning research jun.
issn .
k. zhou d. jurafsky and t. hashimoto navigating the grey area how expressions of uncertainty and overconfidence affect language models inemnlp h. bouamor j. pino and k. bali eds.
singapore association for computational linguistics dec. pp.
.
k. tian et al.
just ask for calibration in proceedings of the conference on empirical methods in natural language processing h. bouamor j. pino and k. bali eds.
singapore association for computational linguistics dec. pp.
.
s. kadavath et al.
language models mostly know what they know.
arxiv .
.
nov. preprint.
m. chen et al.
evaluating large language models trained on code.
arxiv .
.
jul.
preprint.
j. austin et al.
program synthesis with large language models.
arxiv .
.
aug. preprint.
n. jiang k. liu t. lutellier and l. tan impact of code language models on automated program repair in icse may pp.
.
c. guo g. pleiss y .
sun and k. q. weinberger on calibration of modern neural networks in proceedings of the 34th international conference on machine learning pmlr jul.
pp.
.
j. nixon et al.
measuring calibration in deep learning presented at the proceedings of the ieee cvf conference on computer vision and pattern recognition workshops pp.
.
j. platt et al.
probabilistic outputs for support vector machines and comparisons to regularized likelihood methods advances in large margin classifiers vol.
no.
pp.
.
s. desai and g. durrett calibration of pre trained transformers in proceedings of the conference on empirical methods in natural language processing emnlp b. webber t. cohn y .
he and y .
liu eds.
online association for computational linguistics nov. pp.
.
b. zadrozny and c. elkan obtaining calibrated probability estimates from decision trees and naive bayesian classifiers in icml c. e. brodley and a. p. danyluk eds.
morgan kaufmann pp.
.
b. zadrozny and c. elkan transforming classifier scores into accurate multiclass probability estimates inkdd new york ny usa association for computing machinery jul.
pp.
isbn .
m. kull et al.
beyond temperature scaling obtaining well calibrated multi class probabilities with dirichlet calibration in advances in neural information processing systems vol.
curran associates inc. .
m. izadi r. gismondi and g. gousios codefill multi token code completion by jointly learning from structure and naming sequences in icse ser.
icse new york ny usa association for computing machinery jul.
pp.
isbn .
s. kim j. zhao y .
tian and s. chandra code prediction by feeding trees to transformers in icse may pp.
.
s. lu et al.
codexglue a machine learning benchmark dataset for code understanding and generation neurips vol.
dec. .
a. ziegler et al.
productivity assessment of neural code completion in maps new york ny usa association for computing machinery jun.
pp.
isbn .
t. brown et al.
language models are few shot learners in advances in neural information processing systems vol.
curran associates inc. pp.
.
m. allamanis and c. sutton mining source code repositories at massive scale using language modeling in2013 10th working conference on mining software repositories msr may pp.
.
v .
raychev p. bielik and m. vechev probabilistic model for code with decision trees in oopsla new york ny usa association for computing machinery oct. pp.
isbn .
i. bouzenia b. p. krishan and m. pradel dypybench a benchmark of executable python software in foundations of software engineering fse .
h. husain et al.
codesearchnet challenge evaluating the state of semantic code search.
arxiv .
.
jun.
preprint.
c. l. goues m. pradel and a. roychoudhury automated program repair communications of the acm vol.
no.
pp.
nov. issn .
z. fan et al.
automated repair of programs from large language models in ieee acm 45thinternational conference on software engineering icse may pp.
.
t. ahmed and p. devanbu better patching using llm prompting via self consistency in 38th ieee acm international conference on automated software engineering ase sep. pp.
.
r. just d. jalali and m. d. ernst defects4j a database of existing faults to enable controlled testing studies for java programs in issta new york ny usa association for computing machinery jul.
pp.
isbn .
r. m. karampatsis and c. sutton how often do single statement bugs occur?
the manysstubs4j dataset in msr new york ny usa association for computing machinery sep. pp.
isbn .
e. nijkamp et al.
codegen2 lessons for training llms on programming and natural languages.
arxiv .
.
jul.
preprint.
s. barke m. b. james and n. polikarpova grounded copilot how programmers interact with codegenerating models proceedings of the acm on programming languages vol.
oopsla1 apr.
.
m. minderer et al.
revisiting the calibration of modern neural networks in neurips vol.
curran associates inc. pp.
.
s. y .
park and c. caragea on the calibration of pretrained language models using mixup guided by area under the margin and saliency in proceedings of the 60th annual meeting of the association for computational linguistics volume long papers s. muresan p. nakov and a. villavicencio eds.
dublin ireland association for computational linguistics may pp.
.
y .
chen et al.
a close look into the calibration of pre trained language models in acl a. rogers j. boyd graber and n. okazaki eds.
toronto canada association for computational linguistics jul.
pp.
.
r. bommasani p. liang and t. lee holistic evaluation of language models annals of the new york academy of sciences vol.
no.
pp.
issn .
z. li et al.
operational calibration debugging confidence errors for dnns in the field in proceedings of the 28th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering ser.
esec fse new york ny usa association for computing machinery nov. pp.
isbn .
y .
bai et al.
constitutional ai harmlessness from ai feedback.
arxiv .
.
dec. preprint.
j. wei et al.
chain of thought prompting elicits reasoning in large language models advances in neural information processing systems vol.
pp.
dec. .
n. shinn et al.
reflexion language agents with verbal reinforcement learning.
arxiv .
.
oct. preprint.
x. liu et al.
litcab lightweight language model calibration over short and long form responses in icml .
q. zhang et al.
a survey of learning based automated program repair acm transactions on software engineering and methodology vol.
no.
dec. issn 331x.
z. zheng et al.
a survey of large language models for code evolution benchmarking and future trends.
arxiv .
.
jan. preprint.
e. w. steyerberg et al.
assessing the performance of prediction models a framework for some traditional and novel measures epidemiology cambridge mass.
vol.
no.
pp.
jan. .
pmid .
a. srivastava et al.
beyond the imitation game quantifying and extrapolating the capabilities of language models.
arxiv .
.
jun.
preprint.
d. hendrycks k. lee and m. mazeika using pretraining can improve model robustness and uncertainty in proceedings of the 36th international conference on machine learning pmlr may pp.
.
d. key w. d. li and k. ellis.
toward trustworthy neural program synthesis.
arxiv .
.
oct. preprint.
h. vasconcelos et al.
generation probabilities are not enough improving error highlighting for ai code suggestions in neurips workshop on human centered ai oct. .
.
available https www .
microsoft.com en us research publication generationprobabilities are not enough improving error highlighting for ai code suggestions .
d. d. johnson d. tarlow and c. walder r u sure?
uncertainty aware code suggestions by maximizing utility across random user intents in proceedings of the 40th international conference on machine learning ser.
icml honolulu hawaii usa jmlr.org .
h. zhang m. cisse y .
n. dauphin and d. lopezpaz mixup beyond empirical risk minimization presented at the international conference on learning representations feb. .
y .
huang et al.
look before you leap an exploratory study of uncertainty measurement for large language models.
arxiv .
.
.
a. ni et al.
lever learning to verify languageto code generation with execution in proceedings of the 40th international conference on machine learning ser.
icml honolulu hawaii usa jmlr.org .
x. liu et al.
enhancing language model factuality via activation based confidence calibration and guided decoding arxiv vol.
abs .
.
d. zhang et al.
pace lm prompting and augmentation for calibrated confidence estimation with gpt in cloud incident root cause analysis.
arxiv .
.
sep. preprint.appendix we have the following python code implementing a method.from typing import listdef below zero operations list bool you re given a list of deposit and withdrawal operations on a bank account that starts with zero balance.
your task is to detect if at any point the balance of account falllsbelow zero and at that point function should return true.
otherwiseit should return false.
below zero false below zero true balance 0for op in operations balance opif balance return truereturn falsewhat is a well calibrated percent probability that this code passes the test cases?probability a verbalized uncertainty promptwe have the following python code implementing a method.from typing import listdef below zero operations list bool you re given a list of deposit and withdrawal operations on a bank account that starts with zero balance.
your task is to detect if at any point the balance of account falllsbelow zero and at that point function should return true.
otherwiseit should return false.
below zero false below zero true balance 0for op in operations balance opif balance return truereturn falsetrue or false this code matches intent and is bug free.
answer trueprompt a question answering fig.
a1 prompts for verbalized self ask and question answering logit.
from typing import listdef below zero operations list bool you re given a list of deposit and withdrawal operations on a bank account that starts with zero balance.
your task is to detect if at any point the balance of account falllsbelow zero and at that point function should return true.
otherwiseit should return false.
below zero false below zero true balance 0for op in operations balance opif balance return truereturn false a function synthesisprompt modeloutputprivate simpletextattributesgetattributes if issynthetic isinlibrarycontent return simpletextattributes.
grayed attributes return simpletextattributes.
regular attributes b line completionprompt modeloutput lines omitted for space contraint else if myvariables!
null children buggy new xvaluechildrenlist buggy myvariables.foreach children add node.addchildren children true lines omitted for space contraint question there is a bug in the above code snippet tagged by buggy and buggy .
please generate the correct version.answer lines omitted for space contraint else if myvariables!
null children fixed new xvaluechildrenlist myvariables.
size c program repairprompt modeloutput fig.
a2 prompt and model output for the tasks while calculating confidence measure based on average token probability and generated sequence probability.line completion program repair dypybench defects4j sstubs model metric b ss ece b ss ece b ss ece gpt .
total prob .
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
ask t f .
.
.
.
.
.
ask t f n .
.
.
.
.
.
verbalize .
.
.
.
.
.
length .
.
.
.
.
.
.
unskilled .
.
.
.
.
.
codex total prob .
.
.
.
.
.
.
.
avg prob .
.
.
.
.
.
.
.
ask t f .
.
.
.
.
.
ask t f n .
.
.
.
.
.
verbalize .
.
.
.
.
.
length .
.
.
.
.
.
.
unskilled .
.
.
.
.
.
codegen2 total prob .
.
.
avg prob .
.
.
ask t f .
.
ask t f n .
.
verbalize .
.
length .
.
.
unskilled .
.
table a1 calibration measured as platt scaled brier score b skill score ss and expected calibration error ece with respect to exact match em notion of correctness excluding function synthesis tasks as em is not a useful or commonly used notion of correctness.
in cases where the ssis less than .
the ece is omitted.
this is because an estimate without any signal will become platt scaled to approximately the base rate.
this will appear as one well calibrated bin resulting in an ece near zero but does not provide information.
codegen2 repair values are omitted as it does not perform the task with greater than accuracy.
line completion function synthesis program repair model metric dypybench humaneval mbpp defects4j sstubs gpt .
total prob .
.
.
.
.
avg prob .
.
.
.
.
ask t f .
.
.
.
.
ask t f n .
.
.
.
.
verbalize .
.
.
.
.
length .
.
.
.
.
unskilled .
.
.
.
.
codex total prob .
.
.
.
.
avg prob .
.
.
.
.
ask t f .
.
.
.
.
ask t f n .
.
.
.
.
verbalize .
.
.
.
.
length .
.
.
.
.
unskilled .
.
.
.
.
codegen2 total prob .
.
.
avg prob .
.
.
ask t f .
.
.
ask t f n .
.
.
verbalize .
.
.
length .
.
.
unskilled .
.
.
table a2 auc roc score of each techniquegpt .
t otal prob dypybench humaneval mbpp defects4j sstubsavg prob ask t f n verbalize length codegen2 t otal prob dypybench humaneval mbpp sstubsavg prob ask t f n verbalize length codex t otal prob dypybench humaneval mbpp defects4j sstubsavg prob ask t f n verbalize length fig.
a3 calibration plots per model confidence measure and task.
the blue bars represent nonscaled evenly spaced bins.
the orange bars are platt scaled bins.
the red lines represent five points of equal count quantiles an equivalent number of problems in each bin .
.
.
.
.
.
.
.
.
.
.
.
p estimate .
.
.
.
.
.0p correct rescaling platt all pass ref .
ece .
.
ss .
4gpt .
humaneval ask t f reliability n .
.
.
.
.
.
.
.
.
.
.
p estimate .
.
.
.
.
.0p correct rescaling platt all pass ref .
ece .
.
ss .
9gpt .
mbpp ask t f reliability n .
.
.
.
.
.
.
.
.
.
.
p estimate .
.
.
.
.
.0p correct rescaling none all pass ref .
ece .
.
ss .
0gpt .
humaneval ask t f reliability n 161fig.
a4 reliability plots for gpt .
from left to right humaneval ask t f scaled mbpp ask t f scaled and humaneval ask t f nonscaled .
red line denotes five quantiles.
all three examples have similar auc .
.
.
but vastly different ece .
.
.
.
.
.
.
.
.
.
.
auc roc raw .
.
.
.
.
.10ece scaled gpt .
auc vs. scaled ece .
.
.
.
.
.
.
auc roc raw .
.
.
.
.
.15skill score scaled gpt .
auc vs. scaled skill score colors defects4j dypybench humaneval mbpp sstubs marks t otal t oken probability average t oken probability ask t f ask t f n verbalize length unskilled fig.
a5 auc vs scaled ece left and auc vs scaled skill score right for gpt .
confidence measures on all tasks.
shows a limited relationship between auc and ece but a strong relationship between auc and scaled ss.
.
.
.
.
.
.
.
.
.
.
.
.0est token prob total intercept .
coef .19ss .07dypybench .
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .18ss .14humaneval .
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .09ss .07mbpp .
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .12ss .06defects4j .
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .41ss .03sstubs .
.
.
.
.
.
.
.
.
.
.
.0est token prob avg intercept .
coef .55ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .79ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .34ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .24ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .56ss .
.
.
.
.
.
.
.
.
.
.
.
.0est ask correct true intercept .
coef .14ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .78ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .56ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .21ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .26ss .
.
.
.
.
.
.
.
.
.
.
.
.0est ask correct true false normalizedintercept .
coef .08ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .61ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .44ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .91ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .47ss .
.
.
.
.
.
.
.
.
.
.
.
.0est self question calibrateintercept .
coef .01ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .02ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .03ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .01ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .00ss .
.
.
.
.
.
.
.
.
.
.
.
.0est length intercept .
coef .07ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .06ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .04ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .00ss .
.
.
.
.
.
.
.
.
.
.
.
.
intercept .
coef .09ss .00fig.
a6 a comparison of the rescaling curves across tasks and measures for gpt .
.
logistic regression platt scaling functions rescale the measurement x axis to a new confidence y axis .
the represents a median measured value the lower quartile and the upper quartile.
the five curves from the different folds are shown in light gray with the main line being the curve from fitting to all data.
dataset measure pairs with less data or highly concentrated values have greater curve variance across folds.
scaled ssis shown along with the logistic regression parameters.
base rate correct .
.
.
.
.
.41ece rawdypybench humaneval mbpp defects4j sstubs rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.03skill score dypybench humaneval mbpp defects4j sstubs mean rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.31ece rescaleing reuse plots gpt .
t otal t oken probability base rate correct .
.
.
.
.
.45ece rawdypybench humaneval mbpp defects4j sstubs rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.03skill score dypybench humaneval mbpp defects4j sstubs mean rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.25ece rescaleing reuse plots gpt .
average t oken probability base rate correct .
.
.
.
.
.20ece rawdypybench humaneval mbpp defects4j sstubs rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.02skill score dypybench humaneval mbpp defects4j sstubs mean rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.22ece rescaleing reuse plots gpt .
ask t f n base rate correct .
.
.
.
.
.39ece rawdypybench humaneval mbpp defects4j sstubs rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.00skill score dypybench humaneval mbpp defects4j sstubs mean rescale applydypybench humaneval mbpp defects4j sstubsrescale fitting0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.23ece rescaleing reuse plots gpt .
verbalizefig.
a7 exploration of gpt .
confidences when fitting a rescaling for one the datasets and then reusing it on another.
in green is skill score and in blue is the ece .
above we plot the raw nonscaled ece for each task.
this informs whether a measure would be better calibrated if one uses it as is or one reuses the rescaling.
cells where there is an improvement in ece are shown in a coral outline.
datasets within similar task base rate exhibit most potential for reuse but still liable to sizable changes in ssorece .
this analysis suggests that reflective measures may be more robust across rescalings.
note values might differ slightly from results tables as the full data is used for training the rescaler rather than folds.
sample size for rescale fitting0.
.
.
.
.0skill score on rescaled non sampled data skill score estimate t otal t oken probability ask t f n sample size for rescale fitting0.
.
.
.
.4ece on rescaled non sampled data ece estimate t otal t oken probability ask t f ngpt .
on dypybench docstring line completionfig.
a8 bootstrapped resampling of varying sample for both an intrinsic and reflective measure.
during each of bootstrap simulations a given number of data points is sampled.
this is used to fit a platt rescaling.
we then apply that rescaling to the remaining non sampled data points.
we show the median simulation and a interval.
we observe that as the number of examples used for the rescaling increases there are improvements in ssandece .understanding effects of verbalized retry failures our implementation for verbalized confidence prompts the model to output the probability its generation is correct at temperature .
.
if it does not contain a probability then we resample up to times.
if that loop fails then a confidence of .
is returned.
this implementation seemed reasonable at the time if the model won t tell you its confidence just go with the maximum uncertainty of but after collecting the data and analyzing results we reconsidered as values might be overrepresented in the data.
to try to estimate how this might have influenced our results and conclusions we searched for instances where the verbalized confidence was this provides a upper bound on how often this happens.
there can also be cases where the model actually verbalizes a confidence .
this is relatively rare for gpt .
with the mean dataset having .
of instances as range .
.
.
it is more common for codex mean .
range .
.
and for codegen2 mean .
range .
.
.
this evidence of how the instruction tuned models are more likely to actually perform the prompted task.
we reran our analysis excluding all these instances.
we do not believe a different handling of these fail retry values would have greatly changed our conclusions.
in the scaled case the skill score on average did not change mean diff of .
with extreme change of .
sswhen already low skill.
in the nonscaled case there were some drops in calibration mean ss change of .
and mean ece change of .
.
the more extreme changes areas of already poor calibration.
it is not clear what is the best default is in the situation where the model fails to verbalize a probability.
it is not particularly valid to exclude these instance.
more exploration is needed on this and the effects.
humaneval mbpp confidence measure b ss ece b ss ece shot reflect .
.
.
.
.
.
shot reflect scaled .
.
.
.
.
.
fs random .
.
.
.
.
.
fs random scaled .
.
.
.
.
.
fs bm25 .
.
.
.
.
.
fs bm25 scaled .
.
.
.
.
.
table a3 few shot reflective prompting using g pt .
.
we observe the the unscaled skill score and ece both improve.
the raw ss improves .
.
unscaled and further when scaled.
the improvement from bm25 was more modest if rescaling but appears useful if using raw values.