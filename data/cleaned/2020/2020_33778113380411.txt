seenomaly vision based linting of gui animation effects against design don t guidelines dehai zhao zhenchang xing dehai.zhao anu.edu.au zhenchang.xing anu.edu.au research school of computer science australian national universitychunyang chen chunyang.chen monash.edu faculty of information technology monash university australiaxiwei xu liming zhu xiwei.xu data61.csiro.au liming.zhu data61.csiro.au data61 csiro australia guoqiang li li.g sjtu.edu.cn school of software shanghai jiao tong university shanghai chinajinshui wang jinshui.wang anu.edu.au research school of computer science australian national university abstract gui animations such as card movement menu slide in out snackbar display provide appealing user experience and enhance the usability of mobile applications.
these gui animations should not violate the platform s ui design guidelines referred to as designdon t guideline in this work regarding component motion and interaction content appearing and disappearing and elevation and shadow changes.
however none of existing static code analysis functional gui testing and gui image comparison techniques can see the gui animations on the scree and thus they cannot support the linting of gui animations against design don t guidelines.
in this work we formulate this gui animation linting problem as a multi class screencast classification task but we do not have sufficient labeled gui animations to train the classifier.
instead we propose an unsupervised computer vision based adversarial autoencoder to solve this linting problem.
our autoencoder learns to group similar gui animations by seeing lots of unlabeled realapplication gui animations and learning to generate them.
as the first work of its kind we build the datasets of synthetic and realworld gui animations.
through experiments on these datasets we systematically investigate the learning capability of our model and its effectiveness and practicality for linting gui animations and identify the challenges in this linting problem for future work.
also with data61 csiro.
also with university of new south wales.
corresponding author corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
concepts software and its engineering software testing and debugging software usability .
keywords gui animation design guidelines lint unsupervised learning acm reference format dehai zhao zhenchang xing chunyang chen xiwei xu liming zhu guoqiang li and jinshui wang.
.
seenomaly vision based linting of gui animation effects against design don t guidelines.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
.
introduction graphical user interface gui is an ubiquitous feature of mobile applications.
mobile platforms provide gui design guidelines to which the mobile applications run on the platforms are expected to adhere for example android material design andios design themes.
in addition to the guidelines about static gui visual effects like color system and typography many design guidelines are about gui animations such as sliding in out menu or sheet expanding collapsing cards.
figure shows two examples of gui animation1guidelines in the android material design a regarding the use of a visible scrim with modal bottom sheets b regarding how to reveal card information.
gui animations if done properly according to such design guidelines make a gui more appealing and more usable.
but a design guideline documentation has so many rules regarding many aspects of gui design such as layout color typography shape motion which make it nearly impossible for a developer to remember all these rules and properly adopt them in actual app development.
for example figure shows some real application guis that violate some design don t guidelines in figure and table .
although 1we cannot show animated images in the paper.
all gui animation examples in this paper are available at ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea dehai zhao zhenchang xing chunyang chen xiwei xu liming zhu guoqiang li and jinshui wang figure examples of gui animation dos versus don ts such gui design violations do not affect the functionalities of an application they may result in poor user experience.
it is desirable to automatically validate the gui animation of an application against design don t guidelines to flag potential gui design violations.
this type of software tool is called lint or linter.
almost all major programming languages and platforms have corresponding lint tools to name a few the origin unix lint for c the famous findbugs and checkstyle for java and stylelint for cascading style sheet.
these lint tools rely on human engineered rules to detect bugs and stylistic errors.
unfortunately it is often not straightforward to convert gui design don t guidelines into programming rules especially when they involve gui animation effects.
this is why many gui animation guidelines have to be accompanied by illustrative do and don t examples.
furthermore modern mobile platforms have very complex gui style and theme systems see for example style and theme of android system .
developers can specify the desired style either declaratively in manifest file or programmatically in source code.
they can also extend and customize existing styles and themes.
all these complexities make it difficult if not impossible to precisely determine the actual gui visual effects by static program analysis.
gui testing can dynamically explores gui behaviors of an application.
however gui testing techniques including the recently proposed deep learning based techniques focus on simulating the user interaction with the guis to trigger the app functionalities.
their effectiveness is evaluated with the code and gui coverage.
they cannot validate the visual effects of gui designs.
moran et al.
recently proposes a computervision based method to detect the violations of an implemented gui against the up front ui design.
their technique examine only static gui visual effects.
however gui animations involve component movement appearing and or disappearing and thus cannot be reliably detected by analyzing static gui images see examples in figure and the discussion in section .
.
furthermore their technique assumes that the two compared guis are largely similar but have minor differences which indicates the design violations.
this assumption obviously does not hold for the actual guis e.g.
those in figure that violate certain design guidelines and the gui animation examples illustrating the guidelines e.g.
those in fig.
.
in fact the do and don t gui animations illustrating a design guideline represent a class of conforming and violating guis.
in this sense we can regard the validation of a gui animation against several design don t guidelines as a multi class classification problem given a gui animation predict the design don t guideline that this gui animation likely conforms or violates .
multi class video classification has been well studied in computer visiondomain and the recent advances of deep learning based methods have led to many breakthroughs.
unfortunately we cannot adopt these multi class video classification techniques to our gui animation linting problem because they are supervised learning methods which demand large amount of labeled video data.
however we have only a very small number of illustrative gui animation examples for each design guideline.
in machine learning terminology our problem is a few shot learning problem .
manually collecting a large number of conforming and violating gui animations for each design guideline would demand significant time and effort.
in this paper we propose an unsupervised deep learning method to solve the gui animation linting problem in the context of few shot learning.
we first use automatic gui exploration method to build a large dataset of unlabeled gui animations.
using this dataset we train a vision based gui animation feature extractor to learn to extract important temporal spatial features of gui animations in an unsupervised way.
next we use the trained feature extractor to map a few typical gui animations that violate a designdon t guideline into a dense vector space.
a gui animation to be linted is also mapped into this vector space.
we can then determine the design don t guideline that this linted gui animation most likely violates by a simple k nearest neighbor knn search of the most similar violating gui animations.
as knn is a nonparametric method which does not require model training our approach elegantly solves the challenge of few shot learning.
we implement our approach and use the tool to lint the gui animations of android mobile applications against nine android material design guidelines that cover diverse animation aspects screen positions and ui components.
we build a large scale unlabeled real application gui animations from the rico dataset for training our adversarial autoencoder.
through experiments on a dataset of labeled synthetic gui animations we confirm the practical settings for applying our approach and the effectiveness of our model design over other model variants.
through experiments on a dataset of labeled distinctive real world gui animations we show that our approach can accurately lint realworld gui animations against a small number of gui animation examples of design don t guidelines.
as the first work of its kind our experiments also identify several challenging gui animations instantaneous gui animations simultaneous multiple component animations which deserve further research.
we make the following contributions in this paper to the best of our knowledge our work is the first to lint gui animation effects against design don t guidelines.
we propose the first deep learning based computer vision technique for the gui animation linting problem.
the proposed model is trained by unsupervised learning and the unlabeled training data is easy to collect.
our experiments confirm the effectiveness of our approach on both synthetic and real world gui animations.
approach our approach aims to lint the gui animations of an application in response to the user actions referred to as the linted gui animation against a list of the design don t guidelines in a design guideline documentation e.g.
google material design .
as illustrated in 1287seenomaly vision based linting of gui animation effects against design don t guidelines icse may seoul republic of korea figure approach overview fig.
we formulate this linting task as a multi class gui animation classification problem and solve the problem by the k nearest neighbor knn search of the design don t guideline that has the most similar gui animation examples to the linted gui animation.
we design an adversarial autoencoder based feature extractor which learns to extract abstract temporal and spatial feature representations of unlabeled real application gui animations in an unsupervised way in the absence of a large number of the labeled gui animations.
.
input screencasts of gui animations gui animations can be triggered by users e.g.
slide in out menu or by applications e.g.
.
display snackbars .
the gui animation effect can be recorded in a gui screencast i.e.
a sequence of gui screenshots taken at a fixed time interval.
a gui screenshot is a dimensional height and width image of the application s gui.
.
.
input types.
as shown in figure our approach has three types of input gui animations.
the first type is the linted gui animation screencast that the developer wants to validate against a list of design don t guidelines.
the second type is a small number of the gui animation screencasts that demonstrate the design don t guideline see examples in figure against which the linted gui animation is compared.
the second type of gui animations is the labeled data for a design don t guideline and can be collected from the design guideline documentation online blogs discussing relevant design guidelines and real applications.
the third type is a large number of gui animation screencasts that are automatically collected from real applications .
these screencasts are unlabeled as we do not know whether and which design guideline they violate.
we use this large amount of unlabeled data to train the unsupervised gui animation feature extractor see section .
.
.
.
gui animation capturing.
we record a gui animation screencast at screenshot frames per second fps .
this recording rate can record significant screen changes during gui animation without recording many static frames in between the screen changes.
the gui animation screencasts are recorded in full rgb color.
we record a gui animation screencast for each individual user action rather than a continuous screencast that may involve multiple user actions and also no action periods.
the recording starts after the user interacts with the application and the significant screen changes are detected and it stops when the next user action comes or the screen remains static over second.
we set seconds as the figure model architecture maximum recording duration because we observe that an individual gui animation is rarely longer than seconds.
based on android material design touch targets should be at least x dp density independent pixel .
therefore we consider the screen as static if the two adjacent frames have pixel differences less than x dp may translate into different numbers of physical pixels depending on the screen size and resolution.
see pixel density .
this threshold filters out minor gui changes like the time changes or notification changes in the system bar.
note that we do not stop recording immediately after the first two static frames are detected because some static frames may be recorded between the two screen changes during the gui animation.
.
gui animation feature extractor as the examples in figure and figure show the gui animations that violate the same design don t guideline can be very different in the detailed gui designs e.g.
contents and visual effects .
this makes it impossible to directly measure the similarity of gui animations with respect to a design don t guideline in the highdimensional screencast space.
instead low dimensional abstract temporal spatial features of similar gui animations with respect to a design don t guideline must be extracted from the screencasts.
unfortunately the amount of the examples that demonstrates the design don t guideline i.e.
the labeled data is not enough to effectively train a gui animation feature extractor in a supervised way.
however the gui animations that conform or violate certain design guidelines are present albeit implicit in the real applications we design an unsupervised gui animation feature extractor which learns to extract the latent temporal spatial feature representations from such unlabeled real application gui animations.
.
.
model architecture.
our feature extractor is based on deep generative models variational autoencoder vae and generative adversarial network gan .
the core idea to infer the latent temporalspatial feature representations by learning to generate gui animation screencasts and discriminate the real gui animation screencasts from the generated ones.
as shown in figure our model consists of three components a vae that learns to reconstruct the input gui animation screencast a gan that enhance the generation capability of the vae by adversarial training and an additional feature encoder network that learns more abstract feature representation from the reconstructed screencast.
as the input screencasts are temporal spatial data the vae encoder the discriminator and 1288icse may seoul republic of korea dehai zhao zhenchang xing chunyang chen xiwei xu liming zhu guoqiang li and jinshui wang the feature encoder use 3d convolutional neural network 3dcnn to extract temporal spatial features from the screencasts.
a 3d convolutional kernel with k kspatial size and dtemporal depth slides over a sequence of frames on three directions height width and depth .
3d cnn has been successfully used for the action recognition tasks which is similar to our linting task.
the vae gan and feature encoder networks are jointly trained using unlabeled gui animation screencasts and the training is guided by three loss functions in different networks that aim to minimize the distance between the input screencasts and the generated screencasts as well as between the latent feature vectors of these screencasts.
these loss functions are the reconstruction losslrecfor the vae network the adversarial loss ladvfor the gan and the encoder loss lencfor the additional feature encoder.
these three loss functions are combined together as the model loss l lrec ladv lencto optimize the networks jointly.
at the inference time the vae encoder and the additional feature encoder are used to obtain zand zfor the input and the reconstructed gui animation screencast respectively which are then concatenated to map the input screencast into the latent feature space.
in this space the similarity of the gui animations can be determined by the l2distance of their feature vectors.
.
.
variational autoencoder network.
we adopt the vae network because of its capability of dimensionality reduction which meets our goal to extract low dimensional temporal spatial features from high dimensional gui animation screencasts.
the vae network consists of an encoder subnetwork vae e and a decoder subnetwork vae d. in our model the input xis a gui animation screencast as descried in section .
.
the encoder subnetwork is a 3d cnn with batch normalization and relu activation relu a max a .
it encodes the input gui animation screencast xinto a low dimensional latent vector z. the decoder subnetwork vae dworks in the same way as the generator in gan but we use 3d instead of 2d convolutional transpose layers .
it generates a gui animation screencast xfrom the latent vector z. xis referred to as the reconstructed screencast.
we also use batch normalization and relu activation in vae d. through this encode reconstruct process the vae can be trained with unlabeled input data which fits well with our data setting.
the learning objective of the vae network is to minimize the difference between the input xand the reconstructed input x. in this work we compute l1distance between xand xas the loss function of the vae network referred to as reconstruction loss lrec x x n i fi fi where fiand fiare the i th screenshot in xand xrespectively.
by minimizing the reconstruction loss the latent vector zcan capture the most important temporal spatial features of the input xsuch that the reconstructed input xhas the least information loss.
.
.
adversarial training by gan.
although the vae is generally effective in dimensionality reduction the latent feature representation zlearned by the vae alone is often not good enough to reconstruct the input screencast .
inspired by adversarial autoencoder in we adopt the adversarial training to enhance the reconstruction of the input gui animation screencast by the vae.
adversarial training was introduced in gan which isa leading technique for unsupervised representation learning.
a gan consists of a pair of networks a generator gtries to generate samples from a data distribution and a discriminator dtries to distinguish real samples and generated fake samples.
the network is trained in a min max game where the generator seeks to maximally fool the discriminator while simultaneously the discriminator seeks to determine the sample validity i.e.
real versus fake and the two networks finally reach nash equilibrium .
in our model the vae decoder plays the role of the generator g. the discriminator dis similar with the discriminator network in deep convolutional generative adversarial network dcgan but our model uses 3d instead of 2d convolutional layers.
this discriminator is a binary classifier to predict real or fake input.
in our work the real input is xand the fake input is x. in the adversarial training setting the generator is updated based on the classification output of the discriminator.
the loss of the discriminator referred to as adversarial loss is the cross entropy loss of the binary classification output ladv logd x log d x .
.
.
feature encoder network.
in addition to the vae encoder that learns the latent feature representation zfrom the input screencast x we add an additional feature encoder that learns an additional latent feature representation zfrom the reconstructed screencast x. the underlying intuition is that zwould be a more abstract feature representation than z because xis reconstructed from the most important features of xonly.
the feature encoder network ehas the same network structure as the vae encoder vae e but the two networks do not share weights so that different feature representations can be extracted.
for the training of e we minimize the l2distance between zand zso that ecan learn how to extract useful features from the reconstructed screencast.
that is the encoder loss function of eislenc z z .
.
gui animation linting by knn search given a gui animation screencast we want to determine if it violates some design don t guidelines and if so which guideline.
our approach regards a design don t guideline as a class of gui design violation see examples in figure .
as our gui animation screencasts are recorded for each individual user action they generally involve only one primary animation effect and one design violation if any .
therefore we solve the gui animation linting problem by a knn search based multi class classification.
specifically we use the vae encoder and the additional feature encoder to map both the linted gui animation and the gui animation examples of design don t guidelines into a low dimensional latent feature space.
then we find the top kgui animation examples that are the closet to the linted gui animation in the feature space.
the distance of the two gui animations is computed by the l2distance their feature vectors v1andv2 i.e.
v1 v2 .
we use the majority vote by the k nearest gui animation examples to determine the guideline that the linted gui animation violates.
if the majority vote has a tie we compare the average distance of the gui animation examples of the tie guidelines and consider the guideline with the shortest average distance as the guideline that the linted gui animation violates.
1289seenomaly vision based linting of gui animation effects against design don t guidelines icse may seoul republic of korea figure design violations look normal in static images tool implementation our current tool2takes gui animation screencasts as the input.
each sample contains frames and has the height and width and respectively.
all input gui animation screencasts are stored in the same format animated gifs .
we use the setting of screenshot frames because this setting is long enough to cover the duration of most gui animations and contains the least duplicate frames.
the height and width setting are the same as that of the gui animation screencasts in the rico dataset .
in this way we do not need to scale the size of the large amount of unlabeled gui animation screencasts derived from the screencasts in the rico dataset see section .
.
.
for the screencasts that do not meet these requirements our input processor normalizes them accordingly by sampling frames as many distinct ones as possible and by scaling up down the screencasts to .
experiment design we now describe the choice of design don t guidelines used in our study the datasets for training and evaluating our model and the evaluation metrics used in our experiments.
.
the design don t guidelines table summarizes the nine design don t guidelines in android material design we use to test the effectiveness and practicality of our approach.
the nine guidelines cover common gui animation aspects in android material design including elevation light and shadow content display component motion and component interaction guidelines.
they involve common android ui components for user interaction including dialog snackbar banner app bar sheet menu popup window card and these components may appear at the top bottom side middle or anywhere on the screen.
the guideline c4 may involve any ui components.
the guideline c10 is not a specific design don t guideline.
instead it represents the normal gui animations that do not violate any design don t guidelines to the best of our android gui design knowledge.
furthermore we select these nine guidelines because they all involve component motion appearing and or disappearing.
as such they cannot be reliably detected by analyzing static gui images.
figure shows three static gui images after relevant gui animations end a a window pops up over a non scrimmed background 2the code is available on b a menu slides out without boundary shadow c3 and c two banners are stacked at the top of the screen c9 .
without looking at the gui animations that lead to these gui images these gui images alone looks like normal gui designs.
but they actually violate the design don t guidelines for gui animations.
we carefully select the design don t guidelines that have similar or related gui animation effects to test the capability of our approach to distinguish them.
for example c1 c2 and c3 are all related to elevation.
but c1 and c2 express elevation difference by scrimmed backgrounds while c3 does so by shadows.
c1 c2 and c3 are also different in terms of ui components involved and screen position affected c1 involves dialogs which usually appear in the middle of the screen c2 involves sheets appearing at the bottom of the screen and c3 involves menu appearing at the side or popup window which may appear anywhere.
c4 and c5 are also related.
but c4 is generic about any material surface passing through another material surface while c5 is specific about moving one card behind other card s .
both c5 and c6 are card animation.
but c5 is about card movement while c6 is about card flipping.
both c2 and c7 involves something appearing from the bottom of the screen but sheets in c2 is usually much larger than snackbars in c7.
both c7 and c8 are snackbar animation.
but c7 is about snackbar blocking app bar while c8 is about multiple snackbars.
however we avoid to choosing repeating guidelines which may have little added value to the already chosen guidelines in the experiments.
for example snackbars and banners are similar in terms of content display but they appear in different places on the screen and also behave differently.
as we already have two guidelines cover similar gui animation effects of snackbars and banners i.e.
c8 and c9 stack multiple snackbars or banners but one changes the screen bottom and the other changes the screen top we do not include the guideline place a banner in front of a top app bar which essentially repeats another similar gui animation effects of snackbars and banners compared with c7.
.
datasets we build two labeled datasets one with real world gui animations and the other with synthetic gui animations for evaluating our model and a dataset of unlabeled real application gui animations for unsupervised training of gui animation feature extractor.
.
.
labeled datasets of real world gui animations.
we build a dataset of real world gui animations of the design don t guidelines in table from three sources.
first we collect the illustrative examples of the selected design don t guidelines in android material design website e.g.
the don ts in figure .
second we search the web using the guideline descriptions to find more illustrative examples.
for example we find the online blog alexzh dev which contains some examples of the c1 c3 and c9 guideline in table .
third we collect the violations of the design don t guidelines from the android apps developed by the students in a master level android development course in our school.
finally we collect realworld gui animation screencasts for each design don t guideline in table .
for the guideline c10 we randomly sample normal gui animations from the unlabeled dataset of real application gui animations described in section .
.
.
we use this real world dataset to evaluate our model s performance in practice rq3 .
in order to 1290icse may seoul republic of korea dehai zhao zhenchang xing chunyang chen xiwei xu liming zhu guoqiang li and jinshui wang table design don t guidelines used in our study design don t guideline animation aspect screen position ui component c1 lack of scrimmed background elevation middle dialog c2 invisible scrim of modal bottom sheet elevation bottom sheet c3 lack of shadow elevation light and shadowside anywheremenu popupwindow c4 pass through other material elevation component motion anywhere any component c5 move one card behind other card s elevation component motion anywhere cards c6 flip card to reveal information content display component motion anywhere card c7 snackbar blocks bottom app bar content display component interaction bottom snackbar app bar c8 stack multiple snackbars content display component interaction bottom snackbars c9 stack multiple banners content display component interaction top banners c10 normal gui animation anything anywhere any component test the capability boundary of our approach we try our best to maximize the distinction between the selected gui animations in terms of gui content and visual effects see examples in figure rather than having many similar ones to inflate the performance.
.
.
labeled datasets of synthetic gui animations.
we semi automatically build a dataset of synthetic gui design violations based on the collected real world design violations and the real application gui screenshots.
first we use the screen change detection method described in section .
.
to detect the screen changes in each of the real world gui animation screencasts including those of c10 in the real world dataset and then crop the changing gui regions during the animation as a screencast.
then we randomly select a real application gui screenshot from the rico dataset and replay the screencast of the cropped changing gui regions on top of this screenshot.
in this way we obtain a synthetic gui animation for the design don t guideline of the original real world gui animation.
when synthesizing gui animations we randomly apply a combination of scaling brightness contrast saturation hue jitter lighting augmentation color normalization techniques to augment the foreground gui animations and or the background gui screenshots.
the goal is to increase the diversity of the synthetic gui animations and the difference between the synthetic gui animations and the original real world gui animations.
the first author reviews the synthetic gui animations and discard non violation ones such as those shown in figure .
in figure a a dialog is synthesized on top of a gui screenshot that already shows a dialog which is unrealistic.
furthermore that gui already shows the dialog over the dark background so the synthetic gui is not a violation of c1 lack of scrimmed background .
similarly the gui animations in figure b and figure c do not violate c2 invisible scrim of modal bottom sheet and c3 lack of shadow .
in figure d a snackbar appears over a text window without a bottom app bar so it is not a violation of c7 snackbar block app bar .
note that the other guidelines c4 c5 c6 c8 c9 involve explicit component motion or interaction which will always be a violation when that kind of component motion or interaction occurs.
finally we obtain synthetic gui animation screencasts for each design don t guideline in table .
we use this synthetic dataset to investigate the impact of knn settings rq1 and the ablation of model components rq2 .
figure non violation synthetic gui animations note that due to the manual examination effort required it is impossible to build a large enough synthetic dataset for supervised training of gui animation feature extractor.
furthermore as our rq4 shows the synthetic gui animations have latent difference from the real word ones and thus the feature spaces of synthetic and real world gui animation do not match.
this suggest that even we train a feature extractor using labeled synthetic gui animations it will not work effectively for encoding real world gui animations.
.
.
unlabeled dataset of real application gui animations.
we build a large scale unlabeled dataset of real application gui animation screencasts from the gui animation screencasts in the rico dataset which are collected from real human app interactions and automatic gui exploration.
however the duration of these screencasts spans from .
second to seconds.
many screencasts contain multiple user actions and no action periods.
we statistically analyze these screencasts to determine the appropriate maximum recording seconds duration and the stop recording threshold second for our gui animation capturing method describe in section .
.
.
we use this animation capturing method to playback each gui animation screencast in the rico dataset and re record it into the gui animation screencasts for individual user actions.
finally we obtain gui animation screencasts for training our gui animation feature extractor.
.
evaluation metrics we evaluate and compare the model performance for gui animation linting by four metrics accuracy precision recall and f1 score.
1291seenomaly vision based linting of gui animation effects against design don t guidelines icse may seoul republic of korea the test data is a set of linted gui animations that have no overlap with the gui animations in the search space.
for one gui guideline class c precision is proportion of gui animations that are correctly predicted as camong all gui animations predicted as c recall is the proportion of gui animations that are correctly predicted ascamong all ground truth gui animations labeled as c and f1score is computed as precision c recall c precison c recall c .
accuracy is an overall performance of the guidelines which is computed by the number of gui animations correctly predicted by the model over all testing gui animations.
experiment results and findings we conduct extensive experiments to investigate the following four research questions rq1.
how do different knn settings the number of gui animation examples in the search space and the number of k nearest neighbors searched affect the classification performance?
rq2.
how do different model components i.e.
vae gan and additional feature encoder in our gui animation feature extractor affect the classification performance?
rq3.
how well does our approach perform on linting real world gui animations against design don t guidelines?
rq4.
are the latent feature space of synthetic and real world gui animations different?
how does the difference if exists affect the cross dataset design linting?
.
impact of knn settings rq1 .
.
motivation.
our knn search based gui animation linting method has two important parameters that affect the classification accuracy and speed the number tof gui animation examples of a design don t guideline in the search space.
the number k of the nearest neighbors that vote for the design don t guideline violated by the linited gui animation.
the rq1 aims to study the impact of these two parameters on the classification performance.
.
.
method.
we use the synthetic dataset in this experiment because it allows the larger scale experiments than the real world dataset.
we experiment t and k i.e.
t kcombinations .
for each guideline ci i we puttrandomly selected gui animations of ciin the search space and use the rest gui animations as the linted gui animations.
we incrementally add more randomly sampled gui animations to the search space which simulates the practical situation in which more and more guideline violating gui animations are found and added to the search space over time.
we use t as a stress testing to investigate whether too many gui animations in the search space will disturb the performance.
.
.
results.
table 23shows the accuracy and f1 score for the 28t kcombinations.
we can see that the t and 10nn setting has the worst accuracy .
and f1 score .
much lower than all other settings.
this is not surprising because only gui animation examples for each guideline are in the search space and considering the nearest neighbors means that there are at least examples 3due to space limitation we show the guideline level performance results for all t kcombinations in impact of knn settings on synthetic data accuracy f1 score t 1nn 3nn 5nn 10nn 1nn 3nn 5nn 10nn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
whose design don t guidelines are irrelevant to the linted gui animation.
these noisy samples highly likely mislead the classification decision resulting in the performance drop.
however when considering less nearest neighbors e.g.
k for t or increasing the number of gui animation examples t in the search space the accuracy becomes very close across different ksettings for a specific t with the slight .
.
degradation when kincreases.
f1 score has the similar trend.
increasing tcontinuously improve the classification performance from .
accuracy and .
f1 score for t to .
accuracy and .
f1 score for t .
having gui animation examples per guideline in the search space does not disturb the classification decisions.
instead it leads to a significantly performance boost to accuracy .
.
and f1 score .
.
for different k values.
although t 50leads to better performance it may not be practical to collect or more examples per guideline in practice.
we see a trend of performance gap narrowing across kwhen tincreases from to accuracy difference between 1nn and 10nn down from .
to .
and f1 score difference between 1nn and 10nn down from .
to .
.
however the performance gap becomes a bit wider again when t .
this indicates that the 10nearest neighbors still remain highly accurate but become slightly more noisy when the search space has examples per guideline compared with less examples per guideline in the search space.
however when tincreases from to the average search time increases from on average .
second per linted gui animation to .
seconds.
the average search time for t or is .
and .
seconds respectively.
the more gui animation examples in the search space the more comprehensive the search space becomes and the better the classification performance is.
when the search space have at least examples considering different numbers of nearest neighbors does not significantly affect the classification performance.
the setting of to examples per guideline in the search space and searching for to nearest neighbors can achieve a good balance between the classification accuracy the search time and the effort to collect gui animation examples.
.
model component ablation study rq2 .
.
motivation.
our model see figure consists of three components a vae network a gan architecture and an additional 1292icse may seoul republic of korea dehai zhao zhenchang xing chunyang chen xiwei xu liming zhu guoqiang li and jinshui wang feature encoder.
this rq aims to investigate the impact and synergy of these components on the classification performance.
.
.
method.
we develop three variants of our model the vaeonly the vae with the gan architecture vae gan the vae plus the additional feature encoder vae fe .
these variants adopt the same model configuration as our tool.
we train the three variants using the same unlabeled dataset for training our tool.
the vae only and the vae gan use only zas the feature representation of gui animations but the vae fe uses both zand zas our method does.
we run these three variants on the synthetic dataset in the same way as described in section .
.
.
that is each variant has experiments with different t kcombinations.
due to the space limitation we report only the performance results for t 100andk including the overall performance and the guideline level performance.
readers are referred to for the performance results of other t ksettings which support the same findings as those shown in table .
.
.
result.
table shows the precision recall and f1 score for each guideline as well as the average precision recall f1 score and the overall accuracy over all guidelines.
the vae network as the core component of our unsupervised gui animation feature extractor already sets a very good performance baseline with average precision .
recall .
f1 score .
and overall accuracy .
.
just adding adversarial training through the gan architecture or adding the additional feature encoder for extracting more abstract features do not affect the overall performance but we see some variations in the guideline level performance across vaeonly vae gan and vae fe.
vae gan and vae fe have a more balanced precision and recall than vae only.
when combining the vae with both the gan architecture and the additional feature encoder i.e.
our model in figure we obtain better average f1score and overall accuracy than vae only with a more balanced average precision and recall than vae only.
at guideline level our model achieve better f1 score .
.
for guidelines and worse f1 score .
.
for guidelines than vae only.
looking into the guideline level performance our model performs the best for the guideline c4 pass through other material f1 score .
and c6 flip card to reveal information f1 score .
.
gui animations related to these two guidelines involve salient component motion features which are well learned by our model to classify these two types of gui animations.
however our model does not perform so well for c5 move one card behind other card s f1 score .
which is also a component motion guideline.
moving one card among many others usually involve much more complex and diverse simultaneous multiple card animations compared with the single component animation for c4 and c6.
our model also perform very well for classifying normal gui animations c10 f1 score .
.
this indicates that although our model is trained in an unsupervised way it can very well capture the data distribution differences between normal gui animations and those with design violations.
as a result normal gui animations are well separated from abnormal ones in the search space.
our model performs much better for c2 invisible scrim for modal bottom sheet f1 score .
than c1 lack of scrimmed background f1 score .
.
although both c2 and c4 are about figure incorrectly classified gui animations green labeled guideline red predicted guideline scrimmed background the modal bottom sheet in c2 appears gradually from the bottom of the screen from which our model can learn much richer temporal spatial features to recognize the animation of modal bottom sheet than the instantaneous dialog popup in c1.
our model performs reasonably well for c9 stack multiple banners f1 score .
but it performs not so well for another similar gui animation c8 stack multiple snackbars .
for c8 the precision is still good .
but the model misses many cases recall only .
.
our analysis reveals that the appearing disappearing of multiple snackbars or banners usually involves longer animation.
our animation capturing method see section .
may drop some important animation frames of snackbar or banner appearing disappearing.
as a result our model does not learn very well the complete features of the appearing disappearing of multiple snackbars or banners.
however as banners are larger and contain more information and gui components than snackbars the model learns richer features to better classifying c9 related gui animation than c8 related gui animations.
we will refine our gui animation capturing method to handle long gui animations more properly.
our model perform poorly for another snackbar related gui animation c7 snackbar blocks app bar f1 score only .
as well as for c3 lack of shadow f1 score only .
.
figure shows typical examples that make the classification of these two guidelines very challenging.
for c7 the classification becomes challenging when snackbars and app bar or background guis have very similar visual features e.g.
figure a b .
in such cases a c7 violating gui animation may be classified as normal c10 .
another challenge in classifying c7 is that the model may erroneously recognize nonapp bar components as app bars.
for example a snackbar appears in front of a button in figure c and an image in figure d which are a normal animation c10 but our model misclassifies them as c7.
for c3 the key challenge lies in the fact that the shadow animation has only very minor screen changes.
as such the model may misclassify a normal menu with shadow as c3 e.g.
1293seenomaly vision based linting of gui animation effects against design don t guidelines icse may seoul republic of korea table overall and guideline level performance of the three variants and our method for t and k vae only vae gan vae fe our method guideline prec recall f1 prec recall f1 prec recall f1 prec recall f1 c1 .
.
.
.
.
.
.
.
.
.
.
.
c2 .
.
.
.
.
.
.
.
.
.
.
.
c3 .
.
.
.
.
.
.
.
.
.
.
.
c4 .
.
.
.
.
.
.
.
.
.
.
.
c5 .
.
.
.
.
.
.
.
.
.
.
.
c6 .
.
.
.
.
.
.
.
.
.
.
.
c7 .
.
.
.
.
.
.
.
.
.
.
.
c8 .
.
.
.
.
.
.
.
.
.
.
.
c9 .
.
.
.
.
.
.
.
.
.
.
.
c10 .
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
accuracy .
.
.
.
table impact of knn setting on real world data accuracy f1 score t 1nn 3nn 5nn 10nn 1nn 3nn 5nn 10nn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure e or misclassify a menu lack of shadow as c10 e.g.
figure f g h .
these issues could be addressed by separating foreground animations from background gui screenshots and then encoding them separately rather than encoding them as a whole in our current model.
we leave this as our future work.
our model can learn distinctive temporal spatial features from real application gui animations in an unsupervised manner based on which it can accurately lint gui animations against a wide range of design don t guideline involving very diverse animation effects screen positions and gui components.
our model currently has limitations on simultaneous multiple component animations and instantaneous component animations but gui animations with small screen changes or overlapping components with similar visual features is the most challenging gui animations to classify.
.
performance of real gui linting rq3 .
.
motivation.
although synthetic data supports large scale experiments in rq1 and rq2 it may have latent difference from real world gui animations which may not be visually observable.
therefore even though our model performs very well on synthetic data we still need to test our model s capability of linting real world gui animations against design don t guidelines.
.
.
method.
we use the same experiment method described in section .
.
for the experiments on the real world dataset.
we still experiment k but as the real world dataset has figure correctly classified gui animations 1294icse may seoul republic of korea dehai zhao zhenchang xing chunyang chen xiwei xu liming zhu guoqiang li and jinshui wang table performance in the real synthetic context accuracy f1 score t 1nn 3nn 5nn 10nn 1nn 3nn 5nn 10nn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
only gui animations for each guideline we experiment only t .
so we have in total t kcombinations in rq3.
we put at most gui animations per guideline in the search space and this will leave us at least gui animations as the linted gui animations.
in addition to computing various evaluation metrics we also manually examine the linting results which is feasible as the real world dataset is small to understand the learning capability of our model qualitatively.
.
.
result.
table shows the accuracy and f1 score of for the t kcombinations on the real world dataset.
we can make similar observations as those discussed in section .
.
in terms of the impact of different knn settings on the model performance.
the performance of our model on real world data is generally better than that on synthetic data in the same t ksetting.
this could be because the number of linted gui animations is much smaller.
however as we try to maximize the distinction of the gui animations see examples in figure when building the real world dataset this performance results should not be because the linted gui animations are too similar.
compared with the results on synthetic data kvalue has larger impact on the performance for real world data especially when tis small and kis large.
figure shows some examples that our model classifies correctly.
first we can see that both the animated components and the background guis are very diverse in their content and visual spatial features.
in face of such data diversity our model can still correctly classify them.
second the linted gui animations do not have to be similar to the gui animation examples in the search space at the fine grained design level compare the examples a b with the flipping card example and compare the examples e h with the model bottom sheet example in figure third our model has certain level of generalizability.
for example the example n shows a modal side sheet over a scrimmed background.
although it looks like a side menu without shadow c3 our model correctly classifies it as normal.
the examples o and p involve moving map and scrolling text respectively.
our model also correctly classifies them as normal which means that our model can place them at very different locations from those guideline violating gui animations.
our approach can accurately lint real world gui animations against a set of diverse design don t guidelines because it can learn abstract temporal spatial features from gui animations and ignore fine grained gui design differences.table performance in the synthetic real context accuracy f1 score t 1nn 3nn 5nn 10nn 1nn 3nn 5nn 10nn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cross dataset design linting rq4 .
.
motivation.
we observe the performance difference between linting the real world gui animations and linting the synthetic gui animations .
we hypothesize that the latent feature space of real world gui animations and synthetic gui animations has significant difference.
we conduct cross dataset design linting experiments to validate this hypothesis.
.
.
method.
we use the same experiment method described in section .
.
.
that is we experiment t kcombinations t and k .
in rq4 we take gui animations in one dataset as the examples in the search space and those in the other dataset as the linted gui animations.
we refer to this context as cross dataset design linting as opposed to within dataset design linting in rq1 rq2 and rq3.
we have two cross dataset contexts denoted as real synthetic and synthetic real.
.
.
results.
table and table show the accuracy and f1 score for the t kcombinations in the real synthetic and synthetic real contexts.
first we can see that the performance in both cross dataset design linting contexts is significantly worse than that in within dataset design linting see table and table .
second we do not see the continuous performance improvement when tincreases as in the two within dataset contexts.
the impact of increasing the number of gui animation examples in the search space on the performance seems rather random.
this indicates that the similar gui animation examples are not consistently mapped to the close locations in the search space.
third in the context of within dataset linting increasing kgenerally leads to performance degradation which indicates that the most similar gui animation examples appear at the top of k nearest neighbors.
so a larger k tends to include more irrelevant examples.
in contrast we observe the opposite results in cross data contexts.
in fact the t and k setting in the two cross dataset contexts has the best performance among all the t kcombinations which is the exact opposite to the results in the within dataset contexts.
this indicates that the actually relevant gui animation examples are not the closest ones to the linted gui animations in the search space.
they can only be included when considering more k nearest neighbors.
although synthetic gui animations are constructed using the cropped real world gui animations and real application gui screenshots the combination of both in synthetic gui animations may have latent unrealistic features which may not be visually observable by human inspector for example incompatible color systems typography or shape.
to our feature extractor which is trained from the real application gui animations such latent unrealistic features would make synthetic gui animations look like anomalies to the 1295seenomaly vision based linting of gui animation effects against design don t guidelines icse may seoul republic of korea real world gui animations.
the reasonably good performance in the within synthetic context see section .
.
shows that our feature extractor can consistently map those anomalies in the search space so that relevant anomalies are still close to one another.
however as anomalies to the feature extractor synthetic gui animations would be mapped into very different locations in the feature space from the relevant real world gui animations which results in very poor cross dataset linting performance.
the feature space of synthetic gui animations is very different from that of real world gui animations.
therefore it is unrealistic to perform cross dataset design linting.
a feature extractor trained using synthetic gui animations would not be effective in encoding real world gui animations.
related work our work proposes a novel software linting problem.
lint or a linter is a static analysis tool that flags programming errors bugs stylistic errors and suspicious constructs.
the term originates from a unix utility that examines c language source code.
nowadays all major programming languages and platforms have corresponding lint tools.
for example android lint reports over different types of android bugs including correctness performance security usability and accessibility.
stylelint helps developers avoid errors and enforce conventions in styles.
all such lint tools rely on human engineered rules.
although such rules could be defined for some static gui visual effects e.g.
too small font too large gap too dark background lack of accessibility information it is not straightforward to define rules for gui animations involving complex component motion interaction appearing or disappearing.
furthermore the complexity of style and theme systems in modern gui frameworks makes it very difficult to precisely determine the actual gui effects by static programming analysis.
different from static linting automatic gui testing dynamically explore guis of an application.
several surveys compare different tools for gui testing for android applications.
unlike traditional gui testing which explores the guis by dynamic program analysis these two techniques use computer vision techniques to detect gui components on the screen to determine next actions.
these gui testing techniques focus on functional testing and they are evaluated against code or gui coverage.
in contrast our work lints gui animations obtained at runtime against designdon t guidelines.
recently deep learning based techniques have been proposed for automatic gui testing.
in addition to gui testing deep learning has also been used for ui design to code transformation phishing app generation app localization and app accessibility .
zhao et al.
proposes a deep learning based method for extracting programming actions from screencasts.
the goal of our work is completely different from these works.
furthermore they all use supervised deep learning techniques which requires large amount of labeled data for model training.
in contrast our work deals with a novel problem with only smallscale labeled data and our work is the first to use unsupervised representation learning for gui related linting problem.our approach is inspired by deep learning based action recognition .
in particular we adopt the 3d cnn as our basic feature extractor.
these methods performs well on natural scene videos e.g.
human action recognition which have very different environment from our work screencasts of gui animations .
in addition these models are trained via supervised learning.
as the first work of its kind our work is fundamentally limited by labeled data we motivate us to adopt unsupervised learning method to solve our linting problem.
our model is also inspired by anomaly detection with unsupervised learning .
as reviewed in adversarial learning is suitable for detecting anomaly data from videos which is similar to our goal.
the difference is that anomaly detection is a binary classification task normal or abnormal while our task is a multi class classification which is a much harder task for unsupervised representation learning.
we connect an additional feature encoder with the gan to satisfy the need of extracting abstract animation features while ignoring fine grained gui details for our specific gui linting problem.
conclusion this paper investigates a novel software linting problem lint gui animations against design don t guidelines.
we innovatively solve the problem using multi class gui animation classification.
in absence of sufficient labeled data for training the classifier we design a novel adversarial autoencoder for unsupervised representation learning of gui animations.
from a large amount of unlabeled gui animations that are automatically collected from real applications our model learns to extract abstract temporal spatial feature representations from gui animations with different fine grained gui content and visual effects based on which a simple k nearest neighbor search can accurately see the anomaly of an unseen gui animations against a small number of gui animation examples of design don t guidelines for a wide range of animation effects screen positions and gui components.
as the first work of its kind we also contribute two labeled and one unlabeled datasets for future research and unveil the challenges that deserve further research in this novel software linting problem.
acknowledgment we gratefully acknowledge the support of nvidia corporation with the donation of the titan xp gpu used for this research.