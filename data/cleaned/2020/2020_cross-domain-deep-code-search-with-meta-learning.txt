cross domain deep code search with meta learning yitian chai1 hongyu zhang2 beijun shen1 xiaodong gu1 1school of software shanghai jiao tong university china 2the university of newcastle australia sjtu chaiyt bjshen xiaodong.gu sjtu.edu.cn hongyu.zhang newcastle.edu.au abstract recently pre trained programming language models such as codebert have demonstrated substantial gains in code search.
despite their success they rely on the availability of large amounts of parallel data to fine tune the semantic mappings between queries and code.
this restricts their practicality in domain specific languages withrelativelyscarceandexpensivedata.inthispaper wepropose cdcs a novel approach for domain specific code search.
cdcs employsatransferlearningframeworkwhereaninitialprogram representationmodelispre trainedonalargecorpusofcommon programming languages such as java and python and is further adaptedtodomain specificlanguagessuchas solidityandsql.unlikecross language codebert whichis directlyfine tunedin the target language cdcs adapts a few shot meta learning algorithm called maml to learn the good initialization of model parameters whichcanbebestreusedinadomain specificlanguage.weevaluate the proposed approach on two domain specific languages namely solidity and sql with model transferred from two widely used languages pythonandjava .experimentalresultsshowthatcdcs significantly outperforms conventional pre trained code models that are directly fine tuned in domain specific languages and it is particularly effective for scarce data.
ccs concepts softwareanditsengineering reusability automaticprogramming.
keywords codesearch pre trainedcodemodels metalearning few shot learning deep learning acm reference format yitian chai1 hongyu zhang2 beijun shen1 xiaodong gu1.
.
crossdomaindeepcodesearchwithmetalearning.in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm newyork ny usa 12pages.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction recently deepneuralnetworks dnn havebeenwidelyutilizedfor code search .
unlike traditional keyword matching methods deep code search models employ deep neural networks to learn the representations of both queries and code and measure their similarities through vector distances.
the application of dnns significantly improves the understanding ofcodesemantics therebyachievingsuperbperformanceincode search tasks .
amajorchallengefordeepcodesearchistheadaptationofdeep learningmodelstodomain specificlanguages.state of the artcode searchmethodsaremainlydesignedforcommonlanguagessuch asjavaandpython.theyrelyheavilyontheavailabilityoflarge parallel data to learn the semantic mappings between code andnatural language .
on the other hand there is an emerging trend of domain specific languages such as solidity for smart contracts where code search is also needed.
there is often insufficient training data in specific domains causing poor fit of deeplearningmodels.furthermore foreachspecificdomain the costs of data collection cleaning and model training for constructing an accurate model are all non neglectable.
one potential route towards addressing this issue is the pretrained code models which pre train a common representation model on a large multilingual code corpus and then fine tune the model on task specific data .
this enables code search models to transfer prior knowledge from the data rich languages to the low resourcelanguage.forexample codebert thestate ofthe artcoderepresentationmodel canbepre trainedonmultiple commonlanguagesandthenfine tunedinthecodesearchtaskforatargetlanguage .however itischallengingtoreuseknowledge from a mix of source languages for code search in the target language.differentlanguageshavetheiruniquecharacteristics and correspond to different representations.
parameters learnt fromeach language can distract each other resulting in a conflict inthe shared representations.
this is even more challenging in the domain specificcodesearch wherethetargetlanguageusuallyhas scarce training samples.
inthispaper wepresentcdcs cross domaindeepcodesearch a cross domain code search technique based on few shot meta learning.cdcsextendsthe pretraining finetuning paradigmof codebert with a meta learning phase that explicitly adapts themodel parameters learnt from multiple source languages to the targetlanguage.cdcsbeginsbypre trainingcodebertonalarge corpus of multiple common languages such as java and python.
then ametalearningalgorithmnamedmaml model agnostic meta learning isemployedin ordertopreventthemodelparametersfromfallingintothelocaloptimizationofsourcelanguages.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chai et al.
thegoalofthisalgorithmistofindtheinitializationofmodelparameters that enables fast adaptation to a new task with a small amount of training examples.
to evaluate the effectiveness of cdcs we pre train cdcs on a large corpus of common languages such as python and java.
then we perform code search on two domain specific datasets written in solidity and sql.
we compare our approach with three baseline models namely aneural codesearch modelwithout pretraining a within domain pre training model codebert and a cross languagecodebert thatdirectlyfine tunesthetarget languageonapre trainedmodel.experimentalresultsshowthat cdcs significant outperforms within domain counterparts.
in particular our approach shows more strength when the data is scarce indicatingthesuperbeffectivenessofourapproachincross domain code search.
the contributions of this work can be summarized as weproposecdcs anovelcross domaincodesearchmethod using few shot meta learning.
weextensivelyevaluatecdcsonavarietyofcross language code search tasks.
experimental results have shown thatcdcs outperforms the pre training and fine tuning counterparts by a large margin.
background .
code search based on deep learning the past few years have witnessed a rapid development of deep learningforsoftwareengineering inwhichcodesearchhasbeen one of the most successful applications.
compared with traditional textretrievalmethods deeplearningbasedcodesearchlearnsrepresentationsofcodeandnaturallanguageusingdeepneuralnetworks and thus has achieved superb performance .
figure deep learning based code search.
figure shows the overall framework of deep learning based codesearch.inthetrainingphase abi modaldeepneuralnetworkistrainedbasedonalargeparallelcorpusofcodeandnaturallanguage tolearnthesemanticrepresentations high dimensionalvectors of both queries and code snippets.
then a similarity function is employed to numerically compute the similarity between code and query vectors.
the model is usually trained by minimizing the triplet ranking loss namely l c d d max cos c d cos c d figure architecture of codebert with masked languagemodel.
where c d and d represent the vector representations for the code the correct description and the distracting description respectively.
cosdenotesthecosinesimilaritybetweentwovectors.
is a margin which ensures that d is at least closer to cthan d .
in the search phase the search engine is given a query from the user.
it computes the vectors for both the query and code snippets in the codebase using the trained model.
then it goes through thecodebase and matches the query with each code snippet according to their vector distances.
snippets that have the best matching scores are returned as the search results.
.
pre trained models for code search recently pre trained models such as bert and gpt have achieved remarkable success in the field of nlp .
as such researchers start to investigate the adaptation of pre trained models to software programs .
code search is one of the mostsuccessfulapplicationsofpre trainedmodelsforprogramming languages.
one of the most successful pre trained models for code is the codebert .codebertisbuiltontopofbert androberta twopopularpre trainedmodelsfornaturallanguage.unlikepretrainedmodelsinnlp codebertisdesignedtorepresentbi modal data namely programming and natural languages.
figure shows the architecture of codebert.
in general the model is built uponatransformer encoder.thetraininginvolvestwopre training tasksinsixprogramminglanguages.oneisthemaskedlanguage modeling mlm whichtrainsthemodeltofillthemaskedtokenin the input sequences.
the other task is the replaced token detection rtd which trains the model to detect the replaced tokens in the inputsequences.thesetwopre trainingtasksendowcodebert with generalization ability so that it can be fine tuned to adapt to downstream tasks such as code search and code summarization.
asacoderepresentationmodel codeberthasbeensuccessfully employed for code search .
specifically a binary classifier is employed which takes as input the representation of the tokenandpredictswhether agiven nl pl pairissemantically related.thisclassifieristhenfine tunedonacodesearchdatasetbyminimizingthecross entropyloss.inthesearchphase theclassifier authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cross domain deep code search with meta learning icse may pittsburgh pa usa predictsthematchingscorebetweenannlqueryandeachcode snippet in thecodebase.
the search engine returnsthe top kcode snippets that have the highest matching scores.
due to the superb performance researchers have also applied codebertforcross languagecodesearch .theypre trained codebert with multiple languages such as python java php javascript and go and then fine tuned a code search model on anunseenlanguagesuchasruby.resultshaveshownthatcrosslanguagecodesearchachievesbetterperformancethantrainingina singlelanguagefromscratch.thisfurthersupportstheeffectiveness of transfer learning for code search .
.
meta learning and few shot learning few shotlearningisamachinelearningtechnologythataimsto quickly adapt a trained model to new tasks with less example .
despite the superb performance deep learning models are often data hungry .theyrely ontheavailabilityoflarge scaledata for training.
that means the performance can be limited due to thescarcityofdatainspecificdomains .bycontrast humans canlearnknowledgefromafewexamples.forexample achildcanlearntodistinguishbetweenlionsandtigerswhenprovidedwitha few photos probably because human beings have prior knowledge beforelearningnewdataorbecausehumanbrainshaveaspecial waytoprocessknowledge.basedonthisintuition researchershave proposed few shot learning.
few shot learning methods can be roughly classified into the following two categories metric based methods which learn a distance function betweendatapointssothatnewtestsamplescanbeclassifiedthroughcomparisonwiththe klabeledexamples .thereareafewtypical algorithms for metric based few shot learning such as siamese network prototypical network and relation network .
metalearning alsoknownas learning to learn whichtrains amodelonavarietyoflearningtasks suchthatitcansolvenew learning tasksusing only asmall number oftraining samples .
unlike the conventional machine learning prototype that a model is optimized in the training set to minimize the training loss meta learningupdatesmodelparametersusingthevalidationlossinorder to enhance the generalization to different tasks.
there are some typical algorithms for few shot meta learning such as maml andreptile .
maml model agnosticmeta learning isafew shotmetalearningalgorithmwhichaimsatlearningagoodinitializationofmodelparameterssothatthemodelcanquicklyreachtheoptimalpointin a new task with a small number of data samples .
the algorithmassumesthatthedatausedfortrainingfollowsadistribution p t overktasks t1 ... t k wheretistandsforaspecificmachine learningtaskonthedata.theintuitionisthatsomedatafeatures aremoretransferrablethanothers.inotherwords theyarebroadly applicabletoalltasksin p t ratherthanasingleindividualtask ti.tofindsuchgeneral purposerepresentations mamlupdates modelparametersthataresensitivetochangesinthetask suchthat small changes in the parameters will produce large improvements on the loss function of any task drawn from p t .
motivated by this maml separates data into individual tasks.
a meta learneris employed to update parameters using gradients on each local figure architecture of cdcs.
taskti .
a more detailed description of the algorithm and how it is adapted to code search will be presented in section .
.
approach .
overview figure3showsthearchitectureofcdcs.ingeneral cdcstakes codebert asthebackbone andextendsitwithametalearning phase.
the core component of cdcs is roberta which is built upon a multi layer bidirectional transformer encoder.
thepipelineofcdcsinvolvesfourphases.similartocodebert we start by pre training cdcs to learn code representations in a large corpus of multiple source languages.
next we perform metalearningtoexplicitlytransfertherepresentationsofsource languagesintothetargetlanguage.afterthedomainadaptation we fine tune it on the code search data of the target language in order to train the semantic mapping between code and natural language.
wefinallyperformcodesearchusingthefine tunedmodel.wewill describethedetaileddesignofeachphaseinthefollowingsections.
.
pre training the pre training phase aims to learn code and nl representations from a large corpus of multiple common languages such as javaand python.
similar to codebert we use the pre training taskof masked language modeling mlm .
we did not use the rtd replaced token detection pre training task of codebert because the effect of this task has been shown to be marginal .
in the pre training phase the model takes as input an angbracketleftnl pl angbracketright pair which is formatted into a sequence of w1 w2 ...w n c1 c2 ... c m wherew1 w2 ... w ndenotesasequenceof nwordsinthenatural languagetext while c1 c2 ... c mrepresentsasequenceof mtokens in the code snippet.
the special token at the beginning is authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chai et al.
aplaceholderfortherepresentationoftheentireinputsequence.
the tokenindicatestheborderofthecodesnippetandthe natural language text.
the token indicates the end of the sequence.
duringthepre trainingprocess werandomlyreplace15 ofthe tokens in the input sequence with a special token and let the model predict the original token.
the task can be optimized by minimizingthecross entropylossbetweenthepredictedandthe original tokens.
the pre trained model can be used to produce the contextual vectorrepresentationofeachtokenforbothnaturallanguagedescriptions and code snippets.
in particular the representation of the tokenstandsfortheaggregatedsequencerepresentation which can be used for classifying the entire input sequence.
.
meta learning wenextperformmetalearningtoadaptthepre trainedcodemodel tothetargetdomain.weemployameta learningalgorithmnamed maml model agnostic meta learning which is a typical algorithmforfew shotlearning .thekeyideaofmaml is to use a set of source tasks t1 ... tk to find the initialization of parameters 0from which learning a target task t0would require onlyasmallnumberoftrainingsamples .inthecontextofcode search thisamountstousinglargedataofcommonlanguagesto find good initial parameters and training a new code search model onasmall domain specificlanguagestartingfromthefoundinitial parameters.weformulatecodesearchasabinaryclassificationtask twhichpredictswhetheragiven angbracketleftnl pl angbracketrightpairmatches match irrelevant .
unlike codebert which directly fine tunes on the code search task t the maml algorithm assumes that the dataset usedfortrainingfollowsadistribution p t overktasks t1 ... t k .
hence it splits tinto a set of ktasks t1 ... t k .
each task tiaims at training a code search model with small sized data therefore simulatesthe low resourcelearning.basedon thisidea each tiis assignedtotrainthecodesearchmodelinaprivatetrainingand validation set denoted as ti dtrain dvalid .
let denotetheglobalparametersfortheentiremodeland i denotethelocalparametersfortask ti.ametalearneristrained to update model parameters iusing one or more gradient descent updatesontask ti.forexample whenusingonegradientupdate the training step can be formulated as i lti f i ... k wheref denotes the deep learning model for specific task with parameters ltirepresentsthelossfunctionfortask ti denotes the step size for each task and is fixed as a hyperparameter for the meta learner.
in our approach the training set dtrainfor the original code search task tis randomly segmented into kbatches d1 ... d k equally.
each diis used as the data set for the local task ti.t o performthelocaltask diisfurthersplitintoatrainingandvalidationset dtrain i dvalid i withthesamedatasize.each tiisthen performedon dtrain i dvalid i toobtainthelocalgradient lti f .
these local gradients are aggregated by the meta learner every m steps in order to update the global parameter .
the procedure of our algorithm is summarized in algorithm .
figure an overview of the maml algorithm.
algorithm meta learning for code search require step size m meta update steps pre train the global model and obtain the initial parameters createkcopies of with each ibeing the local parameters forti.
whilenot done do dividethedataset dtrainintokbatcheswiththe i thbatchdi for taskti for each di dtraindo splitdiinto dtrain i dvalid i runtion dtrain i dvalid i and evaluate local gradients lti f using the cross entropy loss lti update local parameters iwith gradient descent i lti f ifimodm 0then evaluategradients lti f i usingthe cross entropy lossltiindvalid i update the global parameters using the gradients on the validation set lti f i end if end for end while .
fine tuning in the fine tuning phase we adapt cdcs to the code search task inthetargetlanguage.wefine tunethemodelonthecodesearchtask whichcanbeformulatedasabinaryclassificationproblem.fora corpusof angbracketleftnl pl angbracketrightpairs wecreatethesamenumberofnegative samplesbyrandomlyreplacingnlorplintheoriginalpairs.weas signalabeltoeachpairtoindicatewhetherthenliscorresponding to the pl in the pair relevant irrelevant .
for each training instance we build an input sequence with the sameformatasinthepre trainingphase.wetakethehiddenstatein the position of codebert as the aggregated representation of the input sequence.
the representation is further taken as input toafullyconnectedneuralclassifiertopredictwhetherthegiven angbracketleftnl pl angbracketrightpair is relevant.
we fine tune the model by minimizing the binary cross entropy loss between predictions and labels.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cross domain deep code search with meta learning icse may pittsburgh pa usa .
domain specific code search finally weperformcodesearchbasedonthefine tunedmodelina domain specific codebase.
the code search engine works with the following steps anaturallanguagequery qisprovidedtothecodesearch system.
spliceqseparatelywitheachcodesnippet ciinthecodebase to obtain a series of input sequences q c1 ... q cn input these sequences into the trained model and obtain their matching scores.
sort code snippets according to their matching scores.
return the top k code snippets as the results.
experimental setup we evaluate the performance of cdcs in domain specific code search tasks and explore the effect of training data size on theperformance.
finally we extend our method to other backbone pre trainedmodelssuchasgpt .insummary weevaluate cdcs by addressing the following research questions rq1 howeffectiveiscdcsincross domaincodesearch?
to verify the effectiveness of cdcs in cross domain code search tasks we take python and java as the source languages and adapt the learned model to two domain specific languages namely solidity and sql.
we compare the accuracy of code search by various approaches in the two target languages.
rq2 what is the impact of data size on the perfor mance of cross domain code search?
asmentioned oneofthe challengesfor cross domaincode search is the scarcity of data in the domain specific language.
in rq2 we aim to study the effect of data size on the performance.wevarythesizeofdatasetandcomparethe performance under different data sizes.
rq3 howeffectiveiscdcsappliedtootherpre trained programming language models?besides codebert there are other pre trained models that also achieve outstanding results in software engineering tasks .wewonderwhetherotherpre trainedmodels can have the same effectiveness on code search when equipped with meta learning.
we replace the backbone pretrained model with gpt which is also a popular pre trained language model based on transformer.
gpt 2differs from bert in that it is an autoregressive language model built on top of the transformer decoder.
we evaluate theeffectivenessof cdcsgpt 2andcompareitwiththose of baseline models.
rq4 howdodifferenthyperparametersaffecttheper formance of cdcs?
in order to study the effect of hyperparameters to the performance of cdcs we assign different hyperparameters to cdcs and examine their impact to the performance of code search.table1 statisticsofdatasetsforpre trainingandmetalearning.
phase python java pre train functions comments meta learning functions comments table number of functions on the dataset of target languages.
language train finetune valid test solidity sql .
implementation details we build our models on top of the roberta using the same configurationasroberta base h a l .therateof maskedtokensissetto15 .weusethedefaultcodeberttokenizer namely microsoft codebert base mlm with the same vocabulary size .
we set the maximum sequence length to to fit ourmaximumcomputationalresources.thedefaultbatchsizeis set to .
the three hyperparameters min algorithm are empirically set to e 1e and respectively.
our experimentalimplementationisbasedonthetoolprovidedbyhuggingface transformers1and the higher library provided by facebook research2.
all models are trained on a gpu machine with nvidia tesla v10032gusingtheadam algorithm.weusealearningrate of 5e in the pre training phase which warms up in the first 000stepsandlinearlydecays.wemeasuretheperformanceonthe validationsetduringthetrainingprocess andselectthecheckpoint of the model which has the best accuracy on the validation set for testing.
.
datasets .
.
data used for pre training and meta learning.
wepre train and perform meta learning using the training data for the codesearch task provided by codebert .
we select two popular languages namely pythonandjavaasthesourcelanguages.the statistics of the dataset are shown in table .
for each language the dataset contains parallel data of angbracketleftnl pl angbracketrightpairs including both positiveandnegativesamples.inordertopreventthetrainingfrom fallingintoalocaloptimumofonesourcelanguage weuseonly positive samples for pre training and use the entire set of pairs for meta learning.
.
.
data used for fine tuning and code search.
we fine tune and test the code search task using two domain specific languages namely solidity and sql .
the statistics about the datasets are shown in table .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chai et al.
solidity is an object oriented language that is specifically designed for smart contracts .
the dataset of solidity used in our experimentsisprovidedby forsmartcontractcodesummarization.wepreprocessthedatasetbyremovingallinlinecomments fromfunctions.weremoveduplicatepairs namely two angbracketleftnl pl angbracketright pairs that have the same comment but differ only in the number of positioninthedatasetandafewvariablenamesincode.wealso balance positive and negative samples where the negative samples aregeneratedbyrandomlyreplacingnl i.e.
c d andpl i.e.
c d of positive samples.
sqlisawell knownlanguagethatisspecificallydesignedfor manipulatingdatabasesystems.thedatasetweusedforfine tuning and testing sql is provided by for cross domain semantic parsingandsqlcodegeneration text to sql .theoriginaldata is in a json format and contains the following fields question the natural language question.
question toks the natural language question tokens.
db id the database id to which this question is addressed.
query the sql query corresponding to the question.
query toks thesqlquerytokenscorrespondingtothequestion.
sql parsed results of this sql query.
wepreprocessthesqldatasetbyselectingthe question and query fieldsfromthe .jsondataasour nlandpl respectively.we removeduplicatedatathathasthesamecodefromtheoriginaltest set.
we also balance positive and negative samples where the negativesamplesaregeneratedbyrandomlydisruptingdescriptions and code based on positive samples.
.
evaluation metrics we measure the performance of code search using two popular quantitative criteria on the test set including mrr mean reciprocalrank andthetop kaccuracy.theyarecommonlyusedfor evaluating code search engines .
mrr aimstoletasearchalgorithmscoresearchresults inturnaccordingtothesearchcontent andthenarrangetheresults accordingtothescoresinadescendorder.for ntestqueries the mrr can be computed as mrr nn summationdisplay.
i rank i whererank i representsthepositionofthecorrectcodesnippet in the returned results for query i. the greater the mrr score the better the performance on the code search task.
top k accuracy measures how many answers in the first k results hit the query.
this metric is close to the real world scenario of search tasks that is users want the most matching results to be placed at the top of the results.
in our experiments we compute the top kaccuracy with k and respectively.
weusethetrainedmodeltopredictthematchingscoresof1 angbracketleftnl pl angbracketrightpairs in the test set.
for each pair the model computes the similarities between the text description and all code snippets.
the top k similar snippets are selected for calculating the evaluation metrics.
we report the average score of all the pairs in the test set.table performance of each method in the sql dataset.
model acc acc acc mrr no pretraining .
.
.
.
codebert nl based .
.
.
.
codebert within domain .
.
.
.7351codebert cross language .
.
.
.
cdcs .
.
.
.
table4 performanceofeachmethodinthesoliditydataset.
model acc acc acc mrr no pretraing .
.
.
.
codebert nl based .
.
.
.5801codebert within domain .
.
.
.6383codebert cross language .
.
.
.
cdcs .
.
.
.
.
comparison methods we compare our approach with four baseline methods.
codesearchwithoutpre training whichtrainsthecode search model using only domain specific data in table withoutpre trainingandmetalearning.throughcomparing to this baseline model we aim to verify the effectiveness of pre training and meta learning in our approach.
codesearchbasedonpre trainedmodelwithnatural language whichfine tunesthecodesearchmodelonthe domain specific data in table based on the pre trained modelthatisinitializedbythenaturallanguagepre training models namely roberta and gpt .
within domaincodesearchwithcodebert which pre trainsandfine tunesonlywiththedomain specificdata in table without prior knowledge of common languages.
cross languagecodesearchwithcodebert which directly fine tunes the code search model on the domain specific data on a model that is pre trained on thedataofmultiplecommonlanguages .through comparing to this baseline model we aim to validate the usefulness of meta learning in our approach.
weimplementallbaselinemodelsbasedontheopensourcecode ofcodebert3usingthesamehyperparametersasinthecodebert paper .
experimental results .
effectiveness in cross domain deep code search rq1 table and show the performance of different approaches in the cross domain code search task.
we take python and java as the source languages and test the performance on two domain specific languages namely sql and solidity.
overall cdcs achieves the best performance among all the methods.
from the results on the sql dataset we can see that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cross domain deep code search with meta learning icse may pittsburgh pa usa a mrr b top accuracy c top accuracy d top accuracy figure performance of cdcs under different training data sizes on the sql dataset.
a mrr b top accuracy c top accuracy d top accuracy figure performance of cdcs under different training data sizes on the solidity dataset.
cdcs outperforms the baseline models in terms of all metrics especiallythetop 1accuracyandmrr whichareabout11 and greater than the strong baselines respectively.
the improvement is more significant on the solidity dataset .
we can see that cdcs substantially outperforms strong baselinesespeciallyinthetop 1accuracyandmrr whichareabout and stronger respectively.
there is a large margin between codebert nl based and codebert within domain .wehypothesizethatthisisbecausethe sql corpus is too scarce so that the pre training may not provide sufficientpriorknowledgetothecode searchmodel.cdcsobtains more significant improvement against codebert nl based in sqlthanthatinthesoliditydataset probablybecausesqlismuch closer to natural language than solidity.
the results demonstrate that cdcs is remarkably effective in domain specific code search tasks.
.
effect of data size rq2 figure5and6showtheperformanceofcdcsunderdifferentdata sizescomparedwiththecross languagecodebert .wevary the size of training data from to full data.
astheresultshows cdcsoutperformsthebaselinemodelunder all data sizes which supports the significance of the improvement achievedbycdcs.inparticular wenotethatwhenthedatasize gets smaller e.g.
the improvement of cdcs against the baselinemodel becomesmore significant.that meansthatcdcs isparticularlyeffectiveinscarcedata indicatingtheoutstanding ability of cdcs on domain specific languages.
by contrast the baseline model without meta learning can not adapt to the task well due to the insufficiency of data.
.
performance on other pre trained models rq3 table performance of each method based on gpt .
language model acc acc acc mrr sqlno pretraining .
.
.
.
gpt2 nl based .
.
.
.6204gpt2 within domain .
.
.
.6088gpt2 cross language .
.
.
.5899cdcs gpt .
.
.
.
solidityno pretraining .
.
.
.0101gpt2 nl based .
.
.
.6079gpt2 within domain .
.
.
.6073gpt2 cross language .
.
.
.6057cdcs gpt .
.
.
.
weevaluatetheperformanceof cdcsgpt 2andcompareitwith baseline models that are also based on gpt .
we experiment with python java asthesourcelanguagesandtesttheperformancein solidityandsql.thetrainingdiffersalittlebitinthemetalearning phase we formulate the input for code search as w1 ... w n c1 ... c m where and representthe beginning and ending of the sequence respectively.
the representation of the token standsforthe aggregatedsequencerepresentationand isusedfor classification.weimplement cdcsgptbased onthe huggineface repository1.
the hyperparameters are set as follows we set the batch size to learning rate to .
e which warms up in the first steps and decays according to a cosine curve.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chai et al.
a batch sizes sql b batch sizes solidity c learning rates sql d learning rates solidity figure performance of cdcs under different batch sizes a b and learning rates c d .
table5showstheperformanceof cdcsgpt 2comparedagainst baselinemodels.clearly cdcsgpt 2worksbetterthanallthebaseline models.
the mrr scores of cdcsgpt 2are about and greaterthanthoseofthebaselinemodelinthesqlandsoliditylanguages respectively.
this affirms the effectiveness of cdcsgpt when equipped with meta learning.
wenoticethatthegpt 2pre trainedinnaturallanguagecorpus shows a comparable performance to ours in the sql language.
we conjecture that sql is simple and similar to natural languages hencepre trainingonmassivetextcorpusiseffectiveforthetarget taskwithoutheavyadaptation.anothernotablepointweobserveis that the results of cdcsgpt 2are lower than thoseof cdcsbert presumably because gpt is a unidirectional language model which dynamically estimates the probability of text sequences and canbemoresuitableforgenerationthansearchtasks.gpt 2processeseachinputtextfromlefttorightsequentially thuscanbelimited in representing context sensitive features.
by contrast bertstylemodelsaretrainedwithde noisingstrategies e.g.
themlm task which enable them to obtain bidirectional context sensitive features.
.
impactofdifferenthyperparameters rq4 figure7 a and7 b showtheperformanceofcdcsunderdifferent batchsizesonthesqlandsoliditydatasets.wevarybatchsizes to and respectively.
the results show that larger batch sizeshaveslightimpactontheperformance whilesmallerbatch sizes have evident effect on the performance.
figure7 c and7 d showtheperformanceofcdcsunderdifferent learning rates on the sql and solidity datasets.
we vary the learning rate to e 1e and 5e respectively.
as we can see the performance is insensitive to learning rates lower than e .
however learningratesthatarelargerthan1 e 5havesignificant impacts on performance.tosumup theimpactofhyperparametersoncdcsislimited to a certain range.
the performance is sensitive to the hyperparameters when the batch size is less than or the learning rate is greater than1 e .
inaddition our model ismore sensitive toboth batch size and learning rate on the solidity dataset than sql.
.
case study we now provide specific search examples to demonstrate the effectiveness of cdcs in domain specific code search.
listing and compare the top results for the query what is the smallest city in the usa returned by cdcs and the cross language codebert respectively.
the query involves complexsemantics such as the word smallest .
a code search system is expectedtoassociate small withthecorrespondingsqlkeyword min.
they are different but are semantically relevant.
listing shows that cdcs can successfully understand the semantics of smallest while the cross language codebert cannot.
the example suggests that cdcs is better than the cross language codebert in terms of semantic understanding.
listing3and4showtheresultsreturnedbycdcsandthecrosslanguage codebert for the query reset all the balances to and the state to false in the solidity language.
the keywords in thequery are balances state and false.
it can be seen that both approaches return code snippets that hit some of the keywords.however thesnippetreturnedbycdcsisclearlymorerelevant than that returned by the cross language codebert.
for example it explicitly states benificiary.balance andfilled false in the source code.
on the other hand the snippet provided by the cross languagecodebertisvagueinsemantics.cross language codebert may pay more attention to similar words and is limited in understanding semantics.
theseexamplesdemonstratethesuperiorityofcdcsincrossdomain code search affirming the strong ability of learning representations at both token and semantic levels.
listing the first result of query what is the smallest city in the usa returned by cdcs.
select city name from city where population select min population from city listing the first result of query what is the smallest city in the usa returned by the cross language codebert.
select population from city where population select max population from city listing the first result of query reset all the balances to and the state to false.
returned by cdcs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cross domain deep code search with meta learning icse may pittsburgh pa usa contract c8239 function clean public onlyowner for uint256 i i addresses.length i beneficiary storage beneficiary beneficiaries beneficiary.balance beneficiary.airdrop filled false airdropped false tovault emit cleaned addresses.length listing the first result of query reset all the balances to and the state to false.
returned by the cross languagecodebert.
contract c281 function settransferagent address addr bool state external onlyowner inreleasestate false transferagents state .
summary across all the experiments the performance of the experimental group using pre training is better than those without pre training andtheevaluationresultsofthecdcsexperimentalgroupcombinedwithmetalearningarebetterthanthoseonlytrainedwith pre training and fine tuning.
these results suggest that both transfer learning pre training fine tuning and meta learning have significant efficacy in deep code search.
theadvantagesofmetalearningcanbeparticularlyseenfrom the experimental results of rq2.
the accuracy gap between cdcs andthebaselinemodelsisbecomingmoresignificantasthedata size decreases which means that the size of training data has little effectoncdcs.furthermore theresultsofrq3suggestthatour approachcanbegeneralizedtootherpre trainedmodelssuchas gpt .
overall theexperimentalresultssuggestthatcdcshasremarkable effectiveness in cross domain code search especially when the training data is scarce.
discussion .
why does cdcs work better than the cross language codebert?
we believe that the advantage of cdcs mainly comes from the differencebetweenmetalearningandsimplypre training fine tuning.asfigure8illustrates thetraditional pre training finetuningparadigm tries to learn the common features of multiple source languages in the pre training phase and directly reuses thepre trainedparameterstospecifictasksthroughfine tuning.the featuresofdifferentsourcelanguagesdistracteachother leading toanill posedrepresentationtobereusedbythe targetlanguage.
by contrast meta learning employed by cdcs tries to adapt the pre trainedparameterstonewtasksduringthelearningprocess resultinginrepresentationsthattakeintoaccountallsourcelanguages.
inaviewofmachinelearning boththe pre training fine tuning paradigm and meta learning aim to enhance the generalization abilityofdeepneuralnetworksinmultipletasks.however inthe pre training fine tuning paradigm the model will not obtain task informationbeforefine tuningonspecificdownstreamtasks while meta learning focuses on learning information in specific tasks and can enhance the generalization ability of the model.
cdcs successfully combines the two methods.
.
limitations althougheffective werecognizethattheadaptationofmeta learning tocodesearchmightnotbeaperfectfit.meta learningisusually used for classification tasks on scarce data whereas we adaptittothecontextofcodesearch.thesetwoconcepts i.e.
classification vs. ranking are not a natural fit.
hence meta learningmightnotperfectlysolvetherootproblemofcross domaincode search.
more adaptations are demanded to fit the two concepts.
in order to efficiently adapt code search tasks to scarce data scenarios wefollowthemamlpaper anddividethedatainto machine learning tasks with each task aiming at training a code search model with small sized data.
such an approach has a fewbenefits.forexample itiseasyfortaskadaptationssinceitdoes notintroduceanylearnedparameters.furthermore adaptationcan beperformedwithanyamountofdatasinceitaimsatproducing an optimal weight initialization .
the limitation is that the divisionofthedatainto tasks israndomandthereneedsaconcrete explanationonhowsplittasksarerelatedtocross languagecode search.
it remains to investigate how such divisions turn out to be effective in scarce data.
another downside of cdcsis that the maml algorithm it employscanbringmoretimeandcomputationalcostinthelarge scaledataset.differentfromtheconventionalgradientdescentmethods maml needs to compute a meta gradient based on multiple losses computedfromsub tasks.thiscostsextratimeforsavingmodel parametersandgatheringmetagradients.forexample inourexperiments it requires around extra hours for meta learning comparedto thebaseline models.we leavemore efficienttransfer learning techniques for future directions.
.
threats to validity we have identified the following threats to our approach thenumberofsourcelanguages.
duetotherestrictionofcomputational resources we only selected two source languages and two domain specific target languages.
meta learning with more source languages could have different results.
in our future work we will evaluatetheeffectivenessofourapproachwithmoresourceand target languages.
theselectionofpre trainingtasks.
theoriginalcodebertuses twopre trainingtasks namely maskedlanguagemodel mlm and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chai et al.
java pythonc sqljava pythonc sql figure an illustration of the difference between meta learning and simply pre training fine tuning.
replacedtokendetection rtd .however inourexperiments we only use the mlm as the pre training task.
combining mlm with rtd may have effects on the results.
however we believe that the results of the mlm task can stand for the performance ofpre trainingbecausetheobjectiveofrtdissimilartomlmin that both are based on the idea of de noising.
more importantly the rtd task requires too much cost of time and computational resources whiletheimprovementitbringsismarginalaccording to the ablation experiments in the codebert paper .
moreover compared with rtd the mlm task is more widely used i n domains other than programming languages.
generalization to other pre trained models.
we have built and evaluated our approach on top of two pre trainedmodels namely bert and gpt .
thus it remains to be verified whether or notthe proposed approach is applicable to other pre trained models such as bart and t5 .
related work .
deep learning based code search with the development of deep learning there is a growing interestinadaptingdeeplearningtocodesearch .themain idea of deep learning based code search is to map natural and programminglanguagesintohigh dimensionalvectorsusingbi modal deep neural networks and train the model to match code and naturallanguageaccordingtotheirvectorsimilarities.ncs neural code search proposed by facebook learns the embeddings of code using unsupervised neural networks.
gu et al.
proposed codenn code descriptionembeddingneuralnetwork which learns the joint embedding of both code and natural language.
codenn learns code representations by encoding three individual channels of source code namely method names api sequences and code tokens.
unif developed by facebook can be regarded as a supervised version of ncs.
similar to codenn unif designs two embedding networks to encode natural and programming languages respectively.semanticcodesearch scs firsttrains natural language embedding network and programming language embeddingnetworkrespectivelyandthentrainsthecodesearch task by integrating the two embedding network with similarityfunction.
codematcher which is inspired by deepcs combinesquerykeywordswiththeoriginalorderandperformsa fuzzysearchonmethodnamesandbodies.zhuetal.
proposed ocor a code retriever that handles the overlaps between different names used by different developers e.g.
message and msg .wangetal.
proposedtoenrichquerysemanticsforcodesearch with reinforcement learning.
whilethesemethodsaremainlydesignedforcommonlanguages cdcs focuses on domain specific code search where training data isoftenscarceandcostly.cdcsextendspre trainedmodelswith metalearningtoextractpriorknowledgefrompopularcommonprogramming language for searching code written in domain specific languages.
.
pre trained language models for code in recent years pre trained language models for source code have received much attention .
codebert built on top of the popular model of bert is one of the earliest attemptsthat adaptpre trainedmodels forprogramminglanguages.
codebert is trained with six common programming languages python java javascript php ruby and go .
besides they creatively proposed the replaced token detection rtd task for the pre trainingofprogramminglanguage.cotext isapre trained transformer model for both natural language and programming languages.
it follows the encoder decoder architecture proposedby .
plbart learns multilingual representations of programmingandnaturallanguagejointly.itextendsthescopeofpretraining to denoising pre training which involves token masking deletion andinfilling.mastropaoloetal.
empiricallyinvestigatedhowt5 text to texttransfertransformer oneofthestateof the art plms in nlp can be adapted to support code related tasks.theauthorspre trainedt5usingadatasetcomposedofenglishtextsandsourcecode andthenfine tunedthemodelinfour code related tasks such as bug fix and code comment generation.
although these pre trained models for source code can be used forcross languagecodesearch throughpre traininginmultiple languages and fine tuning in the domain specific language they donottakeintoaccount thedifferencebetweensourceandtarget languages and are limited in performing domain specific codesearch.
by contrast cdcs explicitly transfers representations ofmultiple source languages to the target language through meta learning.
.
transfer learning for code search toourknowledge thereisonlyonepreviousworkthatisclosely related to ours.
salza et al.
investigated the effectiveness of transferlearningforcodesearch.theybuiltabert basedmodel which we refer to as cross language codebert to examine how bert pre trained on source code of multiple languages can be authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cross domain deep code search with meta learning icse may pittsburgh pa usa transferred to code search tasks of another language.
their results showthatthepre trainedmodelperformsbetterthanthosewithout pre training and transfer learning is particularly effective in cases where a large amount of data is available for pre training while data for fine tuning is insufficient .
cdcs differs significantly from theirs.
we employ a meta learningalgorithmtoexplicitlyadapttheparametersfromsourcelanguagestothetargetdomain whiletheirworkdirectlyfine tunes the pre trained model in the target language.
conclusion in this paper we present cdcs a cross domain code search approach that reuses prior knowledge from large corpus of common languages to domain specific languages such as sql and solidity.
cdcs extends pre trained models such as codebert with metalearning.itemploysameta learningalgorithmnamedmaml which learns a good initialization of model parameters so that the model can quickly reach the optimal point in a new task with a fewdatasamples.experimentalresultsshowthatcdcsachieves significant improvement in domain specific code search compared to pre training fine tuning counterparts.
in the future we will investigateourmethodinmorelanguagesandothersoftwareengineering tasks.
source code and datasets to reproduce our work are available at acknowledge this work was sponsored by the national natural science foundation of china under and the ccf baidu open fund no.2021pp15002000.
xiaodong gu is the corresponding author.