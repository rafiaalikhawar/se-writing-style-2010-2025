structure invariant testing for machine translation pinjia he departmentof computer science eth zurich switzerland pinjia.he inf.ethz.chclara meister departmentof computer science eth zurich switzerland clara.meister inf.ethz.chzhendong su departmentof computer science eth zurich switzerland zhendong.su inf.ethz.ch abstract inrecentyears machinetranslationsoftwarehasincreasinglybeen integrated into our daily lives.
people routinely use machine translationforvariousapplications suchasdescribingsymptomstoa foreign doctor and reading political news in a foreign language.
however thecomplexityandintractabilityofneuralmachinetranslation nmt modelsthatpowermodernmachinetranslationmake therobustnessofthesesystemsdifficulttoevenassess muchless guarantee.
machine translation systems can return inferior results that lead to misunderstanding medical misdiagnoses threats to personal safety or political conflicts.
despite its apparent importance validating the robustness of machine translation systems is very difficult and has therefore been much under explored.
to tackle this challenge we introduce structure invariant testing sit anovelmetamorphictestingapproachforvalidatingmachine translationsoftware.ourkeyinsightisthatthetranslationresultsof similar source sentences should typically exhibit similar sentence structures.specifically sit generatessimilarsourcesentencesby substitutingonewordinagivensentencewithsemanticallysimilar syntactically equivalent words represents sentence structureby syntax parse trees obtained via constituency or dependency parsing reports sentence pairs whose structures differ quantitatively by more than some threshold.
to evaluate sit we use itto test google translate and bing microsoft translator with sourcesentencesasinput whichledto64and70buggyissueswith .
and70 top 1accuracy respectively.thetranslationerrors arediverse includingunder translation over translation incorrect modification word phrase mistranslation and unclear logic.
ccsconcepts softwareverificationandvalidation machinetranslation keywords metamorphictesting machinetranslation structuralinvariance acm reference format pinjiahe clarameister andzhendong su.
.structure invariant testingformachinetranslation.in 42ndinternationalconferenceonsoftware engineering icse may23 seoul republicofkorea.
acm new york ny usa pages.
permissionto make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may23 seoul republic of korea 2020association for computing machinery.
acm isbn ... .
introduction machine translation software has seen rapid growth in the last decade usersnow rely onmachine translation fora variety ofapplications suchassigningleaseagreementswhenstudyingabroad describingsymptomstoaforeigndoctor andreadingpoliticalnews inaforeignlanguage.in2016 googletranslate themostwidelyused online translation service attracted more than million users and translated more than billion words per day .
on topofthis machinetranslationservicesarealsoembeddedintovarious software applications such as facebook and twitter .
the advances in machine translation that are responsible for such growth can largely be attributed to neural machine transla tion nmt models which have become the core component of manymachine translationsystems.
asreportedby researchfrom google andmicrosoft state of the artnmtmodelsare approaching human level performance in terms of accuracy i.e.
bleu .theserecentbreakthroughshaveleduserstostartrelying on machine translation software e.g.
google translate and bing microsofttranslator in their daily lives.
however nmt models are not as reliable as many may believe.
recently sub optimal and incorrect outputs have been found in varioussoftwaresystemswithneuralnetworksastheircorecomponents.
typical examples include autonomous cars sentiment analysis tools and speech recognition services .
these recent research efforts show that neural networks can easily return inferior results e.g.
wrong class labels givenspecially craftedinputs i.e.
adversarialexamples .nmtmod elsarenoexception theycanbefooledbyadversarialexamples ornaturalnoise e.g.
typosininput sentences .theseinferior results i.e.
sub optimalorincorrecttranslations canleadtomisunderstanding embarrassment financialloss medicalmisdiagnoses threatstopersonalsafety orpoliticalconflicts .
thus assuringtherobustnessofmachinetranslationsoftwareis an important endeavor.
yet testing machine translationsoftwareis extremely challenging.first differentfromtraditionalsoftwarewhoselogicisencoded in source code machine translation software is based on complex neural networks with millions of parameters.
therefore testing techniquesfortraditional software whichare mostlycode based are ineffective.
second the line of recent research on testing artificialintelligence ai software focuses on tasks withmuch simpleroutput formats forexample testing image classifiers which output class labels given an image.
however asoneofthemostdifficultnaturallanguageprocessing nlp tasks theoutputofmachinetranslationsystems i.e.
translatedsentences issignificantlymorecomplex.becausetheyarenotstructuredto handlesuchcomplexoutputs whenappliedtonmtmodels typical .
oe oufsobujpobm pogfsfodf po 4pguxbsf ohjoffsjoh icse may seoul republic of korea pinjia he clara meister and zhendong su figure examples of similar source sentences and google translate results.
aitestingapproachesalmostsolelyfind illegal inputs suchassentences with syntax errors or obvious misspellings that are unlikely givenasinput.yettheseerrorsarenottheproblematiconesinpractice as reported by wechat a messenger app with over one billionmonthlyactive users itsembedded nmt model can return inferiorresultsevenwhentheinputsentencesaresyntacticallycorrect .
due to the difficultyof building an effective automated approach toevaluatethecorrectnessoftranslation currentapproachesfor testingmachinetranslationsoftware have many shortcomings.
approaches that try to address these aforementioned problems still have their own deficiencies namely the inability to detect grammaticalerrorsandthelackofreal worldtestcases.current testing procedures for machine translation software typically involvethreesteps collectingbilingualsentencepairs1and splittingthemintotraining validation andtestingdata calculating translation quality scores e.g.
bleu and rouge ofthetrainednmtmodelonthetestingdata and comparing the scores with predefined thresholds to determine whether thetest cases pass.
however testing based on a threshold score like bleu which is a measurement of the overlap between n grams of the target and reference can easily overlook grammatical errors.
additionally the calculation of translation quality scores e.g.
bleu requires bilingual sentence pairs as input which need to bemanuallyconstructedbeforehand.totestwithreal worlduser input outside of the training set extensive manual effort is needed forground truthtranslations.
thus an effectiveand efficient testing methodology that can automatically detect errors2in machine translationsoftware is in high demand.
toaddresstheaboveproblems weintroducestructure invariant testing sit anovel widely applicablemethodologyforvalidatingmachinetranslationsoftware.thekeyinsightisthatsimilarsource sentences e.g.
sentences that differ by a single word typically havetranslationresultsofsimilarsentencestructures.forexample fig.
shows three similar source sentences in english and their targetsentencesinchinese.thefirsttwotranslationsarecorrect whilethethirdisnot.wecanobservethatthestructureofthethird sentence in chinese significantly differs from those of the other two.
for each source sentence sit generates a list of its similar sentences by modifying a single word in the source sentence via nlptechniques i.e.
bert feedsallthesentencestothe software under test to obtain their translations uses specialized data structures i.e.
constituency parse tree and dependency parse tree torepresentthesyntaxstructureofeachofthetranslatedsentences and compares the structures of the translated sentences.
1byasentencepair werefertoasourcesentenceanditscorrespondingtargetsentence.
2by atranslation error we referto mistranslationof someparts ofa source sentence.
thetranslatedsentence i.e.
targetsentence containingtranslationerror s isregarded asabuggysentence.weuse errorinthetargetsentence and errorinthesentence pair interchangeablyin this paper.if a large difference exists between the structures of the translated original and any of the translated modified sentences we report the modified sentence pair along with the original sentence pair as potential errors.
weapplysittotestgoogletranslateandbingmicrosofttranslator with source sentences crawled from the web as input.sit successfully found buggy issues defined in section in google translate and buggy issues in bing microsoft translator withhighaccuracy i.e.
.
and70 top 1accuracyrespectively .
the reported errors3are diverse including under translation overtranslation incorrect modification word phrase mistranslation and unclearlogic noneof whichcould bedetected bythe widelyusedmetricsbleuandrouge.examplesofdifferenttranslation errors are illustrated in fig.
.
the source code and datasets arealso released for reuse.
note that our results were w.r.t.the snapshots of google translate and bing microsoft translator when we performed our testing.
after releasing our results dataset in july wenoticethatsomeofthereportedtranslationerrorshave recently been addressed.
thispaper makes the following main contributions itintroducesstructure invarianttesting sit anovel widely applicablemethodologyforvalidatingmachinetranslation software itdescribesapracticalimplementationofsitbyadapting bert to generate similar sentences and leveraging syntax parsers to represent sentence structures it presentsthe evaluationof situsing only200 sourcesentencescrawledfromthewebtosuccessfullyfind64buggy issuesingoogletranslateand70buggyissuesinbingmicrosoft translator with high accuracy and it discusses the diverse error categories found by sit of which nonecould be found by state of the art metrics.
a real world example tom planned to take his son david who is years old to the zurichzoo.
before their zoo visit he checked the zoo s website4 aboutpurchasing ticketsandsawthe followinggermansentence tom is from the united states and he does not understand german.
to figure out its meaning tom used google translate a populartranslationservicepoweredbynmtmodels .google translate returned the following english sentence structure invariant testing for machine translation icse may seoul republic of korea source sentence target sentence target sentence meaning i am very willing to share my point of view.
i had a joke to tell and i wanted to finish it draper says.
by bing but even so they remain prisoners of privilege .under translation incorrect modification word phrase mistranslation !
?
?
by google i am very willing to agree with my point of view.
i joked that i want to finish it draper says.unclear logicit is believed in the field that amazon employs more phd economists than any other tech company.
by google amazon employs more phd economists than any other tech company.
entering talks brazil hoped to see itself elevated to major non nato ally status by the trump administration a big step that would help it purchase military equipment.over translation ?
!
.
by bing entering talks brazil hoped to see itself elevated to major non nato ally status by the trump administration one a big step that would help it purchase military equipment.
?
by google but even so they remain prisoners priviliege .error type figure examples of translation errors english to chinese detected by sit.
!
figure overview of sit.
however davidwasdeniedfreeentrybythezoostaffevenwith avalidid.theyfoundoutthattheyhadmisunderstoodthezoo s regulation because of the incorrect translation returned by google translate.
the correct translation should be thisisarealtranslationerrorthatledtoaconfusing unpleasant experience.translationerrorscouldalsocauseextremelyserious consequences .forexample apalestinian manwas arrestedbyisraelipoliceforapostsaying goodmorning which facebook s machinetranslation serviceerroneously translated as attackthem in hebrew and hurt them in english .
this demonstrates both the widespread reliance on machine translation software and the potential negative effects when it fails.
to enhance the reliability of machine translation software this paper introduces a general validation approach called structure invarianttesting which automatically and accurately detects translation errors without oracles.
approach and implementation this section introduces structure invariant testing sit and describes our implementation.
the input of sit is a list of unlabeled monolingualsentences whileitsoutputisalistofsuspicious issues.
for each original sentence sit reports either i.e.
no buggy sentenceisfound or1 issue i.e.
atleast1buggysentenceisfound .
eachissuecontains the original source sentence and its translation and top kfarthest5generatedsourcesentencesandtheir translations.theoriginalsentencepairisreportedforthefollowing reasons seeing how the original sentence was modified may helptheuserunderstandwhythetranslationsystemmadeamistake the error may actually lie in the translation of the original sentence.
fig.3illustratestheoverviewofsit.inthisfigure weuseone source sentence as input for simplicity and clarity.
the key insight 5thedistancemetrichereisbetweenthestructuresoftheoriginalsentencetranslation andthemodified sentence translations icse may seoul republic of korea pinjia he clara meister and zhendong su figure similar sentence generation process.
of sitis thatsimilarsource sentencesoften havetarget sentences ofsimilarsyntacticstructure.derivedfromthisinsight sitcarries out the following four steps generatingsimilar sentences.
foreach sourcesentence we generate a list of its similar sentences by modifying a single word in the sentence.
collectingtargetsentences.
wefeedtheoriginalandthegeneratedsimilarsentencestothemachinetranslationsystem undertestand collect their target sentences.
representing target sentence structures.
all the target sentences are encoded as data structures specialized for natural language processing.
detecting translation errors.
the structures of the translated modified sentences are compared to the structure of the translated original sentence.
if there is a large difference between the structures sit reports a potential error.
.
generatingsimilarsentences inordertotestforstructuralinvariance wemustcomparetwosentencesthathavethesamesyntacticstructurebutdifferinatleast one token.
we have found that given an input sentence chang ing one word in the sentence at a time under certain constraints effectively produces a set of structurally identical and semantically similarsentences.
explicitly the approach we take modifies a single token in an input sentence replacing it with another token of the same partof speech pos 6to produce an alternate sentence.
for example we will mask hairy in the source sentence in fig.
and replace it with the top k most similar tokens to generate k similar sentences.wedothisforeverycandidatetokeninthesentence forthe sake of simplicity and to avoid grammatically strange or incorrect sentences we only use nouns and adjectives as candidate tokens.
now we discuss the problem of selecting replacement tokens.
perhapsthesimplestalgorithmforselectingasetofreplacementto kenswouldinvolveusingwordembeddings .onecouldchoose words that have high vector similarity with and identical pos tags to a given token in the original sentence as replacements in themodified sentences.
however since word embeddings have the samevalueregardlessofcontext thisapproachoftenproducessen tencesthatwouldnotoccurincommonlanguage.forexample the word fork mighthavehighvectorsimilaritywithandthesame postagastheword plate.
however whilethesentence hecame toaforkintheroad makessense thesentence hecametoaplate in the road does not.
edia.org wiki part of speechrather we wanta model that considersthe surrounding words andcomesupwithasetofreplacementsthat wheninserted create realisticsentences.amodelthatdoesjustthisisthemaskedlanguagemodel mlm inspiredbytheclozetask .theinput toanmlmisapieceoftextwithasingletokenmasked i.e.
deletedfromthesentenceandreplacedwithaspecialindicatortoken .the job of the model is then to predict the token in that position given thecontext.thismethodforcesthemodeltolearnthedependenciesbetweendifferenttokens.sincethereareanumberofdifferentcontextsasinglewordcanfitin thismodel inasense allowsfora singletokentohavemultiplerepresentations.wethereforegeta set of replacement tokens that are context dependent.
while the predictedtokensarenotguaranteedtohavethesamemeaningas theoriginaltoken ifthemlmiswelltrained itishighlylikelythat the sentence with the new predicted token is both syntactically correct and meaningful.
an example of the sentence generation process is demonstrated infig.
.forourimplementation weusebert whichisastateof the art language representation model recently proposed by google.theout of boxbertmodelprovidespre trainedlanguage representations that can be fine tuned by adding an additional lightweight softmax classification layer to create models for a widerangeoflanguage relatedtasks suchasmaskedlanguagemodelling.
bertwastrainedonahugeamountofdata aconcatenationof bookscorpus 800mwords andenglishwikipedia 500mwords withthemaskedlanguagetaskbeingoneoftwomaintasksusedfor training.thus webelievethatbertfitsthisaspectofourapproach well.
.
collecting target sentences oncewehavegeneratedalistofsimilarsentencesfromouroriginalsentence thenextstepistoinputallthesourcesentencestothema chinetranslationsoftwareundertestandcollectthecorresponding translation results i.e.
target sentences .
we subsequently analyze the results to find errors.
we use google s and bing s machine translationsystemsastestsystemsforourexperiment.toobtain translation results we invoke the apis provided by google translateandbingmicrosofttranslator whichreturnidenticalresults as their web interfaces .
.
representationsof the target sentences next we must model the target sentences obtained from the translation system under test as this allows us to compare structuresto detect errors.
choosing the structure with which to represent oursentenceswillaffectourabilitytoperformmeaningfulcomparisons.weultimatelywantarepresentationthatpreciselymodels the structure of a sentence while offering fast comparison between two values.
the simplest and fastest approach is to compare sentences in theirraw form as strings.
indeed we test this method and performance is reasonable.
however there are many scenarios in which thismethodfallsshort.forexample theprepositionalphrase on friday inthesentence onfriday wewenttothemovies canalso be placed on the end of the sentence as follows we went to the movies on friday.
the sentences are interchangeable but a metric such as character edit distance would indicate a large difference structure invariant testing for machine translation icse may seoul republic of korea !
figure representing sentence structures both dependency constituency relations can be displayed as trees.
between the strings.
syntax parsing overcomes the above issue.
with a syntax parser we can model the syntactic structure of a string and the relationship between words or groups of words.
for example if parsing is done correctly our two sample sentencesabove should have identical representations in terms of relation valuesand parsestructure.
.
.
raw target sentence.
forthismethod weleaveourtarget sentence in its original format i.e.
as a string.
in most cases wemay expect that editing a single token in a sentence in one language would lead to the change of a single token in the translated sentence.
giventhe syntacticrole of thereplacement tokenis the same this would ideally happen in all machine translation sys tems.
however this is not always the case in practice as prepo sitional phrases modifiers and other constituents can often beplaced in different locations by the translation system and produce a semantically equivalent grammatically correct sentence.
nonetheless this method serves as a good baseline.
.
.
constituency parse tree.
constituencyparsingisonemethod for deriving the syntactic structure of a string.
it generates a set of constituency relations which show how a word or group of wordsformdifferentunitswithinasentence.thissetofrelationsis particularlyusefulforsitbecauseitwillreflectchangestothetype of phrases in a sentence.
for example while a prepositional phrase can be placed in multiple locations to produce a sentence withthe same meaning the set of constituency relations will remain unchanged.constituencyrelationscanbevisualizedasatree as shown in fig.
.
a constituency parse tree is an ordered rootedtree where non terminal nodes are the constituent relations andterminal nodes are the words.
formally in constituency parsing a sentence is broken down into its constituent parts according tothephrasestructurerules outlinedbyagivencontext freegrammar.forourexperiments weusetheshift reduceconstituency parserbyzhu etal.
andimplementedinstanford scorenlp library .
it can parse about sentences per second.
.
.
dependency parse tree.
dependency parsing likewise derivesthesyntacticstructureofastring.however thesetofrelations produceddescribethedirectrelationshipsbetweenwordsrather thanhowwordsconstituteasentence.thissetofrelationsgives us different insights about structure and is intuitively useful forsit because it will reflect changes between how words interact.
muchprogresshasbeenmadeoverthepast15yearsondependency parsing.
speed and accuracy increased dramatically with the introduction of neural network based parsers .
as with shift reduce constituencyparsers neuralnetworkbaseddependencyparsersuse a stack likesystem wheretransitions arechosen using aclassifier.
theclassifierinthiscaseisaneuralnetwork likewisetrainedon annotated tree banks.
for our implementation we use the mostrecent neural network based parsers made available by stanford corenlp which can parse about sentences per second.
we use theuniversaldependenciesasourannotationscheme whichhas evolved based off the stanford dependencies .
.
translation error detection via structure comparison finally in order to find translation errors we search for structural variation by comparing sentence representations.
whether sentences are modelled as raw strings word embeddings or parsetrees there are a number of different metrics for calculating the distancebetweentwovalues.thesemetricstendtobequitedomainspecificandmighthavelowcorrelationwitheachother makingthechoiceofmetricincrediblyimportant.forexample ametricsuchas word mover s distance would give us a distance of between thetwosentences hewenttothestore and storehethewentto while character edit distance would give a distance of .
we explore several different metrics for evaluating the distance between sentences character levenshtein editdistance constituencyset difference and dependency set difference.
.
.
levenshtein distance between raw sentences.
the levenshteindistance sometimesmoregenerallyreferredtoasthe editdistance comparestwostringsanddetermineshowclosely they match each other by calculating the minimum number of characteredits deletions insertions andsubstitutions neededto transform one string into the other.
while the method may not demonstratesyntacticsimilaritybetweensentenceswell itexploits the expectation that editing a single token in a sentence in one language will often lead to the change of only a single token in the translatedsentence.
therefore thelevenshteindistance servesas a good baseline metric.
.
.
relation distance between constituency parse trees.
to evaluate the distance between two sets of constituency relations we calculate the distance between two lists of constituency grammars as the sum of absolute difference in the count of each phrasal type whichgivesusabasicunderstandingofhowasentencehaschanged after modification.
the motivation behind this heuristic is that the constituentsofasentenceshouldstaythesamebetweentwosentences where only a single token of the same part of speech differs.
icse may seoul republic of korea pinjia he clara meister and zhendong su in a robust machine translation system this should be reflected in the targetsentences as well.
.
.
relation distance between dependency parse trees.
similarly for calculating the distance between two lists of dependencies we sumtheabsolutedifferenceinthenumberofeachtypeofdependency relations.
again the motivation is that the relationships between words will ideally remain unchanged when a single token is replaced.
therefore a change in the set is reasonable indication that structural invariance has been violated and presumably there is a translation error.
.
.
distance thresholding.
usingoneoftheabovemetrics we calculate the distance between the original target sentence and the generated target sentences.
we must then decide whether a modified target sentence is far enough from the its corresponding original target sentence to indicate the presence of a translation error.
to do this we first filter based on a distance threshold only keeping sentences that are farther from the original sentence than the chosen threshold.
then for a given original target sentence we report the top k kalso being a chosen parameter farthest modified target sentences.
we leave the distance threshold as a manualparametersincetheusermayprioritizeminimizingfalse positive reports or minimizing false negative reports depending on their goal.
in section .
we show tradeoffs for different threshold values.
for each original sentence an issue will be reported if at least one translated generated sentence is considered buggy.
evaluation in this section we evaluate our approach by applying it to google translate and bing microsoft translator with real world unlabeled sentences crawled from the web.
our main research questions are rq1 howeffectiveistheapproachatfindingbuggytranslationsin machine translation software?
rq2 whatkindsoftranslationerrorscanourapproachfind?
rq3 how efficient is the approach?
rq4 how do we select the distance threshold in practice?
.
experimentalsetup toverifytheresultsofsit wemanuallyinspecteachissuereported andcollectivelydecide whethertheissuecontainsbuggysentences and if yes what kind of translation errors it contains.
allexperimentsarerunonalinuxworkstationwith6coreintel corei7 .2ghzprocessor 16gbddr42666mhzmemory and geforcegtx 1070gpu.the linuxworkstationis running64 bit ubuntu18.
.02with linuxkernel .
.
.
.
dataset typically to test a machine translation system developers can adopt sit with any source sentence as input.
thus to evaluate the effectiveness of our approach we collect real world source sen tences from the web.
specifically input sentences are extracted from cnn7 cable news network articles in two categories politics and business.
the datasets are collected from two categories articles because we intend to evaluate whether sit consistently performs well on sentences of different semantic context.
for each category we crawled the latest articles extracted their main text contents and split them into a list of sentences.
then we randomly select sentences from each sentence list as theexperimentaldatasets 200intotal .inthisprocess sentences that contain more than words are filtered because we intend to demonstrate that machine translation software can return inferior results even for relatively short simple sentences.
the details of the collected datasets are illustrated in table .
table1 statisticsofinputsentencesforevaluation.eachcorpus contains100 sentences.
of words average of corpus sentence words sentence total distinct politics .
business .
of words .
theeffectiveness of sit ourapproachaimstoautomaticallyfindtranslationerrorsusing unlabeled sentences and report them to developers.
thus the effectiveness of the approach lies in two aspects how accurateare the reported results and how many buggy sentences can sit find?
in this section we evaluate both aspects by applying sit totestgoogletranslateandbingmicrosofttranslatorusingthe datasetsillustrated in table .
.
.
evaluation metric.
theoutputofsitis alistof issues each containing an original source sentence and its translation thetop kreportedgeneratedsentencesandtheirtranslations i.e.
thekfarthest translations from the source sentence translation .
here we define top kaccuracy asthe percentage ofreported issues whereatleastoneofthetop kreportedsentencesortheoriginal sentence contains an error.
we use this as our accuracy metric for sit.explicitly ifthereisabuggysentenceinthetop kgenerated sentences of issue i we consider the issue to be accurate and set bu y i k to true else we set bu y i k to false.
if the original sentence is buggy and was reported as an issue then we also set bu y i k to true.
given a list of issues i its top kaccuracy is calculated as accuracyk summationtext.
i i1 bu y i k i where i is the number of the issues returned by sit.
.
.
results.
top kaccuracy.
theresultsaresummarizedintable .
sit raw sit constituency and sit dependency are sit implementations with raw sentence constituency structure and dependency structure as sentence structure representation respectively.eachiteminthetablepresentsthetop kaccuracyalongwith thenumberofbuggyissuesfound.insubsequentdiscussions for brevity werefersit constituency andsit dependency assit con and sit dep respectively.
weobservethatsit con andsit dep consistentlyperform betterthansit raw whichdemonstratestheimportanceofthe structure invariant testing for machine translation icse may seoul republic of korea table top k accuracy of sit.
top top top buggy issues buggy issues buggy issues sit raw .
.
.
sit constituency .
.
.
sit dependency .
.
.
top top top buggy issues buggy issues buggy issues sit raw .
.
.
sit constituency .
.
.
sit dependency .
.
.
google translate bing microsoft translator structurerepresentationofsentences.themetricusedinsit raw which is based only on the characters in the sentences is brittle andsubjecttooverandunderreporterrors.forexample sit raw may report sentences that are different in word level but similar in sentence structure leading to false positives.
sit con and sit dep achievecomparableperformanceintermsofbothtop kaccuracy and the number of reported buggy issues.
in particular when testing bing microsoft translator sit dep reports suspicious issues.amongtheseissues 70ofthemcontaintranslationerrors in the first reported sentence or the original sentence achieving top 1accuracy.sit dep hasthebestperformanceontop accuracy for both google translate and bing microsoft translator.
itsuccessfullyfinds64and70buggyissueswith69.
and71 top1 accuracy respectively.
sit dep also achieves the highest top accuracy .
and78 .notethatsourcesentencesinthesame issueonlydifferbyoneword.thus inspectingtop 3sentenceswill not cause more effort compared with inspecting top sentences.
in addition we study whether sit can trigger new errors in thegeneratedsentences.asillustratedintable3 inthereported issues 55and60uniqueerrorsarefoundinthetranslationoforiginalsentencesbygoogletranslateandbingmicrosofttranslator respectively.
besides these errors sit finds and extra unique errors that are revealed only in the generated sentence pairs but not in the original.
thus given its high top k accuracy and lotsof extra unique errors reported we believe sit is very useful in practice.
we did not compared sit s accuracy with and because of the following reasons.
sit targets general mistranslation errors while focuses on under over translations.
thus we did not empirically compare with it.
in terms of error type and quantity canonlyfindsomeunder over translationerrorsinoriginal sentencetranslations whilesitfindsgeneralerrorsintranslations of both original sentences and their derived similar sentences.
requires input sentences with specialized structures and thus it cannotdetect any errors using our datasets.
.
translation error reported by sit sit is capable of finding translation errors of diverse kinds.
in our experiments with google translate and bing microsoft translator wemainlyfind5kindsoftranslationerrors under translation overtranslation incorrectmodification word phrasemistranslation and unclear logic.
the error types are derived from error classificationtable number of unique errors.
top k unique errors by sit are errors only in generated sentences output by sit dep .
google bing top unique errors by sitoriginal sentences top unique errors by sit top unique errors by sit table number of sentences that have specific errors in each category sit dep .
top top top 59unclear logicword phrase mistranslationgoogle bing under translationover translationincorrect modification methods for machine translation.
each of the five is a subset of lexical syntactic or semantic errors .
we rename them in a more intuitive manner to aid the readers.
to provide a glimpseof the diversity of the uncovered errors this section highlights examplesforallthe5kindsoferrors.table4presentsthestatisticsofthetranslationerrorssitfound.under translation word phrase mistranslation andunclearlogicaccountformostofthetranslation errors foundby sit.
.
.
under translation.
if some words are mistakenly untranslated i.e.donotappearinthetranslation itisanunder translation error.fig.6presentsasentencepairthatcontainsunder translation error.
in this example to congress is mistakenly untranslated which leads to target sentences of different semantic meaning.
specifically lying to congress is illegal while lying is just an inappropriate behavior.
likewise the real world example introduced in section is caused by an under translation error.
after pleading guilty in the manhattan probe cohen also later pleaded guilty to lying in a case brought by mueller s website.target meaningsource targetafter pleading guilty in the manhattan probe cohen also later pleaded guilty to lying to congress in a case brought by mueller s website.
by bing figure example of under translation errors detected.
.
.
over translation.
ifsomewordsareunnecessarilytranslated multiple times or some words in the target sentence are not translatedfromanywordsinthesourcesentence itisanover translation error.
in fig.
thought in the target sentence is not translated from any words in the source sentence so it is an over translation error.interestingly wefoundthatanover translationerroroften happensalongwithsomeotherkindsoferrors.theexamplealso contains an under translation error because were right in the icse may seoul republic of korea pinjia he clara meister and zhendong su source sentence is mistakenly untranslated.
in the second exampleinfig.
theword a isunnecessarilytranslatedtwice which makesit an over translation error.
the investigators thought that the airplane itself was safe.target meaningsource targetthe investigators were right that the airplane itself was safe.
by google figure example of over translation errors detected.
.
.
incorrect modification.
if some modifiers modify the wrong element in the sentence it is an incorrect modification error.
in fig.
the modifier new modifies auto manufacturing in thesource sentence.
however google translate thinks that new should modify hub.
in fig.
the third example also shows an interestingincorrectmodificationerror.inthisexample prisoners of privilege privilege modifies prisoners in the source sen tence while google translate thinks prisoners should modify privilege.
we think that in the training data of the nmt model there are some phrases with the similar pattern a ofb where a modifies b whichleadstoanincorrectmodificationerrorinthis scenario.
interestingly the original source sentence that triggers thiserroris butevenso theyremain bastionsofprivilege.
inthe originalsentence bastions modifies privilege whichfitsthesupposed archetype.
as we might expect this sentence is correctly translated by google translate.
the south has emerged as a new hub of auto manufacturing by foreign makers thanks to the reducing manufacturing costs and less powerful businesses.target meaningsource targetthe south has emerged as a hub of new auto manufacturing by foreign makers thanks to lower manufacturing costs and less powerful businesses.
by google figure8 exampleofincorrectmodificationerrorsdetected.
.
.
word phrase mistranslation.
ifsometokensorphrasesare incorrectly translated in the target sentence it is a word phrase mistranslationerror.fig.9presentstwomainsub categoriesofthis kind of error ambiguity of polysemy and wrong translation.
ambiguityofpolysemy.
eachtoken phrasemayhavemultiple correcttranslations.forexample admitmeans allowsomebody to join an organization or agree with something unwillingly.
however usually in a specific semantic context e.g.
a sentence a token phrase only has one correct translation.
modern translation softwaredoesnotperformwellonpolysemy.inthefirstexample in fig.
google translate thinks the admit in the source sentence refers to agree with something unwillingly leading to a token phrasemistranslationerror.
wrong translation.
a token phrase could also be incorrectly translatedtoanothermeaningthatseemssemanticallyunrelated.forexample inthesecondexampleinfig.
bingmicrosofttransla torthinks south refersto southkorea leadingtoaword phrase mistranslationerror.
the most elite public universities agree unwillingly that considerably larger percentage of students from lower income backgrounds than do the elite private schools.target meaningsource targetthe most elite public universities admit a considerably larger percentage of students from lower income backgrounds than do the elite private schools.
!
!
by google sourcethe south has emerged as a hub of new auto manufacturing by foreign makers thanks to lower manufacturing costs and less powerful unions.
target !
by bing target meaningthe south korea has emerged as a hub of new auto manufacturing by foreign makers thanks to lower manufacturing costs and less powerful unions.
figure9 examplesofword phrasemistranslationerrorsdetected.
.
.
unclear logic.
ifallthetokens phrasesarecorrectlytranslated but the sentence logic is incorrect it is an unclear logic error.
in fig.
google translate correctly translates serving in the elected office and country.
however google translate generates servingintheelectedofficeasacountry insteadof servingthe country in elected office because google translate does not un derstand the logical relation between them.
unclear logic errors existwidelyintranslationsgivenbynmtmodels whichistosomeextentasignofwhetheramodeltrulyunderstandscertainsemantic meanings.
and attacking a dead man who spent five years as a prisoner of war and another three decades serving in elected office as a country is simply wrong.target meaningsource targetand attacking a dead man who spent five years as a prisoner of war and another three decades serving the country in elected office is simply wrong.
by google figure example of unclear logic errors detected.
.
.
sentences with multiple translation errors.
a certain percentage of reported sentence pairs contain multiple translation errors.
fig.
presents a sentence pair that contains three kindsof errors.
specifically covering means reporting news in the sourcesentence.however itistranslatedto holding leadingto aword phrasemistranslationerror.additionally church inthe target sentence is not the translation of any words from the source sentence soitis anover translationerror.bingmicrosofttranslator also wrongly thinks the subject is attending a funeral train.
butthesourcesentenceactuallymeansthesubjectis coveringa funeral train so it is an unclear logic error.
structure invariant testing for machine translation icse may seoul republic of korea word phrase logic over sourcecovering a memorial service in the nation s capital and then traveling to texas for another service as well as a funeral train was an honor he says.
target !
by bing target meaningholding a memorial service in the nation s capital and then traveling to texas for attending another church service and a funeral train was an honor he says.errors figure11 exampleofsentencewithmultipletranslationerrors detected.
.
the runningtimeof sit in this section we evaluate the running time of sit on the two datasets.weapplysitwith3differentsentencestructurerepresentations to test google translate and bing microsoft translator.
we runeachexperimentsetting10timesandreporttheiraverageas the results.the overallrunning timeof sitis illustrated in table5 and the running time of each step of sit on google translate is presentedinfig.
bing sresultissimilar .wecanobservethat situsingrawsentencesasstructurerepresentationisthefastest.
this is because sit raw does not require any structure representation generation time.
sit using a dependency parser achieves comparablerunningtimetosit raw .inparticular sit dep uses seconds to parse sentences as opposed to seconds by sit raw which we think is efficient and reasonable.
table average running time of sit on politics and business datasets.
google bingrunning time sec translation time sec sentence translatedtime of other sit steps sec sit raw sit constituency sit dependency in these experiments we ran the translation step once per translationsystemandreusedthetranslationresultsinallexperiment settings since the other settings had no impact on translation time.
thus in table the translation time valuesare the same for different sit implementations.
we can observe that sit spends most of thetime collectingtranslation results.in this step for eachsentence we invoked the apis provided by google and bing to collect the translated sentence.
in practice if users want to test their own machine translation software with sit the running time of this step will be much less.
as indicated in a recent study current nmt model can translate around sentences per second using a single nvidia geforce gtx gpu.
with more powerful computingresource e.g tpu modernnmtmodelscanachieve thespeedofhundredsof sentencestranslationpersecond which would be about magnitudes faster than in our experiments.
the other steps of sit are quite efficient as indicated in table andfig.
.bothsit raw andsit dep tookaround1minand sit con tookaround2mins.comparedwithsit dep sit con is slower because models for constituency parsing are slower than those for dependency parsing.
we conclude that as a tool working !
!
!
!
!
!
figure running time details of sit excluding translation time in testing google translate.
in an offline manner sit is efficient in practice for testing machine translationsoftware.
.
theimpactof distance threshold sitreportsthetop ksentencepairsinanissueifthedistancebetween the translated generated sentence and the original targetsentence is larger than a distance threshold.
thus this distancethreshold controls the number of buggy issues reported and the top k accuracy of sit.
intuitively if we lower the threshold more buggy issues will be reported while the accuracy will decrease.
fig.
demonstrates the impact of the distance threshold on these two factors.
in this figure sit dep was applied to test thebingmicrosofttranslatoronourpoliticsandbusinessdatasets with different distance thresholds.
we can observe that both the numberofbuggyissuesandtop 1accuracyremainstablewhenthe thresholdiseithersmallorlargewhilethevaluesfluctuateinthe middle.
the impact of changing the distance threshold is similar whentestinggoogle translate.
basedonthese results wepresentsomeguidance onusingsit in practice.
first if we intend to uncover as many translation errors as possible we should use a small distance threshold.
a small threshold e.g.
for dependency sets works well on all our experiment settings.
in particular with a small threshold sit reports themostissueswithdecentaccuracy e.g.
top 1accuracy .we adopt this strategy in our accuracy experiments in section .
.
.
developers could use sit with small distance threshold when they want to intensively test software before a release.
second if we intend to make sit as accurate as possible we could use a large threshold e.g.
.withalargethreshold sitreportsfewerissues withveryhighaccuracy e.g.
top 1accuracy .giventhatthe number of source sentences are unlimited on the web we could keeprunningsitwith alargedistancethresholdandperiodically report issues.
thus we think sit is effective and easy to use in practice.
.
fine tuning with errors reported by sit inthissection westudywhetherthereportedbuggysentencescan act as a fine tuning set to improve the robustness of nmt models.
fine tuningisacommonpracticeinnmt wheretrainingdataandtargetdatacanoftenoccupydifferentdomains .specifically wetrainanencoder decodermodelwithglobalattention a standard architecture for nmt models on a subset of the cwmt corpus with 2m bilingual sentence pairs .
the encoder and icse may seoul republic of korea pinjia he clara meister and zhendong su figure impact of distance threshold when testing bing microsofttranslator.
decoder are unidirectional single layer lstms.
we train the model using the adam optimizer calculating the bleu score on aheldoutvalidationsetaftereachepoch.weusethemodelwith parameters from the epoch with the best validation bleu score.
note that we did not use google or bing s translation models here becausetheyarenotopen source however theencoder decoder model with attention is a very representative nmt model.
totestthenmtmodel sitisrunon40englishsentences which areselectedfromthevalidationsetofwmt byremoving long sentences i.e.
longer than words and ensuring that all words are in the nmt model s vocabulary.
note that since the modelwasnottrainedorvalidatedondatafromthisdomain we simulatedthepracticalscenariowherereal worldinputsdifferfrom model trainingdata.
basedon theseinputs sitsuccessfully finds 105buggy sentences.we label them with correct translations and fine tune the nmt model on these sentences for epochs withadecreasinglearningrate.afterthisfine tuning allthe105 sentencescan be correctly translated.
meanwhile the bleu score on the original validation set used during training increases by .
which to some degree shows that the translation of othersentences has also been improved.
this demonstrates the ability to fix errors reported by sit in an efficient and easy manner.
sit s utility on building robust machine translation software will be furtherelaborated in section .
.
discussions .
false positives while sit can accurately detect translation errors its precision can be further improved.
in particular the false positives of sit come from three main sources.
first the generated sentences may have strange semanticmeanings leading to changes in thetarget sentence structure.
for example based on the phrase on the way the current implementation of sit could generate the sentence on thefact whichnaturallyhasaverydifferenttranslationinchinese.
using bert which at the time of our experiments provided the state of the art masked language model helped alleviate this issue.
second althoughtheexistingsyntaxparsersarehighlyaccurate theymayproducewrongconstituencyordependencystructures whichcanleadtoerroneousreportederrors.third asourcesentence could have multiple correct translations of different sentence structures.forexample targetsentence 10yearsfromnow and after10years canbeusedinterchangeablywhiletheirsentence structuresaredifferent.tolowertheimpactofthesefactors sitreturnsthetop ksuspicioussentencepairsrankedbydistanceto the originaltargetsentence.
.
building robust translation software the ultimate goal of testing machine translation similar to testing traditional software is to build robust software.
toward this end sit sutilityisasfollows.first thereportedmistranslations typically act as early alarms and thus developers can hard code translationmappingsin advance which isthe quickest bug fixing solution adopted in industry.
second the reported sentences could beusedasafine tuningset whichhasbeendiscussedinsection4.
.
third developersmayfindthereportedbuggysentencepairsuseful forfurtheranalysis debuggingsincethesentencesineachpaironly differ by one word.
this resembles debugging traditional software via input minimization localization.
additionally the structural invariance concept could be utilized as inductive bias to design robust nmt models similar to how shen et al.
introduce bias to standard lstms.
compared with traditional software the debugging and bug fixing process of machine translation software is more difficult because the logic of an nmt model mainly lies in its modelstructureandparameters.whilethisisnotthemainfocus of our work we believe it is an important research direction for future work.
related work .
robustness of ai software thesuccess ofdeeplearningmodelshas ledtothewide adoption of artificial intelligence ai software in our daily lives.
despite theirhighaccuracies deeplearningmodelscangenerateinferior results someofwhichhaveevenleadtofatalaccidents .
recently researchers have designed a variety of approaches to attackdeeplearning dl systems .toprotect dl systems against these attacks excellent research has been conductedtotestdlsystems assistthedebuggingprocess detectadversarialexamplesonline ortrainnetworksinarobustway .
comparedwiththeseapproaches ourpaperfocusesonmachine translation systems which these works do not explore.
in addition mostoftheseapproachesrequireknowledgeofgradientsoractivation valuesin the neuralnetwork under test white box while our approach does not require any internal details of the model black box .
.
robustness of nlp algorithms deep neural networks have boosted the performance of many nlp tasks such as reading comprehension code analysis and machine translation .
however in recent years inspiredbytheworkonadversarialexamplesinthecomputer visionfield researcherssuccessfullyfoundbugsproducedbythe neuralnetworksusedforvariousnlpsystems .
compared with our approach these works focus on simplertaskssuchas text classification.
zhengetal.
introducedtwoalgorithmstodetecttwospecific translationerrors under translationandover translation respectively.comparatively ourproposedapproachismoresystematic andnotlimitedtospecificerrors.basedontheexperimentalresults structure invariant testing for machine translation icse may seoul republic of korea wecanfindthefollowingerrors under translation over translation incorrectmodification ambiguityofpolysemy andunclearlogic.
zhou and sun proposed a metamorphic testing approach i.e.
mt4mt for machinetranslation theyfollowed aconcept similar tostructuralinvariance.however mt4mtcanonlybeusedwith simple sentences in a subject verb object pattern e.g.
tom likes nike .inparticular theychangeapersonnameorabrandname in a sentence and check whether the translation differs by more thanonetoken.thus mt4mtcannotreporterrorsfrommostrealworld sentences such asthe data set used in our paper.
in addition mt4mt doesnot proposegeneral techniquesto realizetheir idea.
our work introduces an effective realization via nontrivial techniques e.g.
adaptingbertforwordsubstitutionandleveraging language parsers for generating sentence structures and conducts an extensive evaluation.
.
machinetranslation thepastfewyearshavewitnessedrapidgrowthforneuralmachine translation nmt architectures .
typically an nmt model usesanencoder decoderframeworkwithattention .underthis framework researchers have designed various advanced neural network architectures ranging from recurrent neural networks rnn convolutional neural networks cnn to full attention networks without recurrence or convolution .
these existing papers aim at improving the capability of nmt models.
different from them this paper focuses on the robustness of nmt models.
we believe robustness is as important as accuracy formachine translationinpractice.
thus ourproposedapproach can complementexisting machine translation research.
.
metamorphictesting metamorphic testing is a way of generating test cases based onexisting ones .
the key idea is to detect violations of domain specific metamorphic relations across outputs from multiple runs of the program with different inputs.
metamorphic testinghasbeenappliedfortestingvarioustraditionalsoftware such as compilers scientific libraries and database systems .
due to its effectiveness on testing non testable programs researchers have also used it to test ai software such as statistical classifiers search engines and autonomous cars .
in this paper we introduce structure invariant testing anovel widelyapplicablemetamorphictestingapproach for machinetranslationsoftware.
conclusion wehavepresentedstructure invarianttesting sit anew effective approach for testing machine translation software.
the distinctbenefits of sit are its simplicity and generality and thus wideapplicability.
sit has been applied to test google translate and bingmicrosofttranslators andsuccessfullyfound64and70buggyissueswith69.
and70 top 1accuracy respectively.moreover asageneralmethodology sitcanuncoverdiversekindsoftranslation errors that cannot be found by state of the art approaches.
we believethatthisworkistheimportant firststeptowardsystematic testingofmachinetranslationsoftware.forfuturework wewill continue refining the general approach and extend it to other aisoftware e.g.
figurecaptioningtoolsandfacerecognitionsystems .
we will also launch an extensive effort to help continuously test and improve widely used translation systems.