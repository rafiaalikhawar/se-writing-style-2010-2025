boosting static analysisaccuracy with instrumentedtest exe cutions tianyichen tianyichen alumni.usc.edu universityof southerncalifornia usakihongheo kihong.heo kaist.ac.kr kaist koreamukundraghothaman raghotha usc.edu universityof southerncalifornia usa abstract the two broad approaches to discover properties of programs staticanddynamicanalyses havecomplementarystrengths static techniquesperformexhaustiveexplorationandproveupperbounds onprogrambehaviors whilethedynamicanalysisoftestcasesprovidesconcreteevidenceofthesebehaviorsandpromiselowfalse alarm rates.
in this paper we present dynaboost a system which uses information obtained from test executions to prioritize the alarms of a static analyzer.
we instrument the program to dynamicallylookfordataflowbehaviorspredictedbythestaticanalyzer and use these results to bootstrap a probabilistic alarm ranking system where the user repeatedly inspects the alarm judged most likelytobearealbug andwherethesystemre rankstheremaining alarms in response to user feedback.
the combined systemis able to exploit information that cannot be easily provided by users and provides significant improvements in the human alarm inspection burden by35 comparedtothebaselinerankingsystem andby comparedto an unaidedprogrammer triaging alarm reports.
ccs concepts softwareanditsengineering automatedstaticanalysis dynamicanalysis mathematicsofcomputing bayesian networks information systems probabilistic retrieval models.
keywords staticanalysis dynamicanalysis beliefnetworks bayesianinference alarm ranking acmreference format tianyichen kihongheo andmukundraghothaman.
.boostingstatic analysisaccuracywithinstrumentedtestexecutions.in proceedingsofthe 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm newyork ny usa 12pages.
.
.
introduction bothstaticanddynamicanalysistechniqueshaveestablishedthemselves asimportant andcomplementaryapproaches to determine esec fse august athens greece copyright held by the owner author s .
acm isbn .
about programs.
on the one hand static analyses provide exhaustivevalidation butemitmanyfalsewarnings especiallywhen analyzinglargepiecesofcode.ontheotherhand dynamicanalysis tools have much higher precision instrumentation frameworks suchasvalgrind memorysafetycheckerssuchasaddresssanitizer andmemorysanitizer anddataracedetectorssuchas threadsanitizer and roadrunner have found hundreds of bugsandsecurityvulnerabilitiesinlargeopensourceprojects but routinely miss bugs because of the low coverage induced by test suites.assuch whilestaticanalysesprovideanupperboundonthe spaceofprogram behaviors dynamicanalysis providesconcrete evidence for their existence thus establishing a lower bound.
this naturally raises the question can we use empirical data gathered fromwitnessingprogramexecutionstoimprovetheeffectiveaccuracy ofstatic analysistools?
in a parallel thread researchershave recently developed probabilistic techniques to incorporate feedback from human users into theoutputofstaticanalyzers .theseapproachesbuildon the observation that analyzers reuse portions of their reasoning to derivemultiplealarms asaresult aninaccuratefunctionsummary dataflow fact or may happen in parallel assertion can lead to multiple false warnings.
the idea then is to recover these reasoning traces andusethemtoconstructabayesiannetworkthatcaptures correlations between theground truthsofeach ofthealarms.
the userthen repeatedly inspects alarmsand indicateswhetheror not they represent real bugs.
in response the analyzer computes the conditional probabilities of the remaining alarms in light of this newinformation andreprioritizesthemindecreasingorderofconfidence.asaresult thesesystemsareabletorapidlydeprioritize false warnings anduncover the real bugsinthe program.
despite its experimental success bingo which forms our conceptualstartingpoint suffers from a few important limitations first apurelystaticinitializationofthebayesiannetworkhas limited information and requires more human guidance.
this is evident in cases of false generalization since the abstraction necessarily over approximates program behaviors a probabilistic model derivedfromtheabstractbehavioroftheprogramsometimescauses negative feedback given by the user to erroneously propagate to the true bugs thereby suppressing them.
furthermore users are typicallyonlyabletoanswerquestionsaboutthecorrectnessofthe final warnings and not about intermediate assertions and dataflow facts derived by the analysis and finally erroneous human feedback hasthe potentialto greatly degrade the quality of ranking in subsequent iterations.
the central insight of our paper is that such interactive alarm ranking systems can incorporate feedback not just from human users but also from diverse sources of knowledge including by dynamicinstrumentationoftestcases.webeginwiththewarnings 1154this work is licensed under a creative commons attribution international .
license.
esec fse august athens greece tianyi chen kihong heo andmukund raghothaman and dataflow facts emitted by the sparrow static analyzer which checks cprograms for a rangeof memory safety errors and use these to selectively instrument the program for analysis by the dfsan dynamic data flow tracking framework .
we then run the instrumentedprogram onitstest inputs andcollectempirical evidence for each theoretically predicted data flow fact.
we use the resultinginformationasa firstroundoffeedbacktothealarmprioritization system thus greatly improving the quality of the ranking even before human intervention.
we present the architecture of this system whichwe calld ynaboost infigure .
we emphasize that in non pathological cases where test inputs donotthemselvesexposeerroneous programbehaviors thefeedbackprovidedbydfsanislimitedtointermediatedata flowfacts rather than the final alarms raised by the static analysis.
in this situation providingfeedback from dynamicanalysiscorresponds toprovidingevidenceforinternalnodesinthebayesiannetwork the conditional independencies thus created are responsible for limiting the impact of false generalization.
conversely because of the incompleteness of static analysis the presence of an abstract dataflow path from a tainted source to a sensitive sink does not by itself imply the possibility of tainted data causing program errors as a result transferring feedback from dfsan to the probabilistic ranking subsystem requires some care while engineering the bayesian network.
weimplementedtheseideasusingbingo sparrow and dfsan as building blocks.
we evaluated our algorithms on a suite of unix command line programs ranging in size from kloc to kloc and which contain a set of known historicalbugs.
onaverage acrossall thesebenchmarks sparrow emits566warningsperprogram.usingoursystem d ynaboost aprogrammerisabletodiscoverallthesebugsaftertriagingjust .
warnings on average per program.
notably this is an reduction compared to an unaided user and a improvement over bingo which requires .
rounds of feedback on average.
muchofthisrankingimprovementisduetoadramaticreductionin the frequency and severity of false generalization events compared to bingo see figure on average dynaboost has fewer false generalizationevents eachofwhichisitselfonly11 ofthesizeof the averageeventoccurringwithin bingo.
contributions.
tosummarize wemakethefollowingcontributionsinthis paper wedevelopabayesianframework d ynaboost tocombineinformation extracted from static and dynamic program analysis.
we implement a system to perform targeted dynamic instrumentation basedontheresultsofan over approximate static analysis.
wepresentanexperimentalevaluationacrossasuiteofunix utilities and demonstrate an average drop of in human alarm annotation e ffort.
motivating example in this section we provide an overview of our approach by considering an example bugfrom thelinux command line program sort.
we will discuss how d ynaboostcoordinates the static analyzer sparrow the dynamic analyzer dfsan and the bayesian alarm prioritization processto accelerate bugdiscovery.
.
postmortem ofacoreutils bug infigure we show a snippet of code adapted from version .
of thegnucoreutilsprogram sortwhichcontainsabu fferoverrun bug .
the program has a feature to merge a set of previously sortedfiles and this functionality is implemented in the merge function shown in the figure.
this function in turn calls another functionnamed avoid trashing input toaccountforcaseswhere one ofthe input files isreusedas an output file.
theavoid trashing input function iterates over the input files andcheckswhethertheyarethesameastheoutput file.ifso thenit attempts to merge the remaining files into a new temporary file on line34.itmightbeunabletocompletethismergeduetothelimited availabilityof filehandlesfromtheoperatingsystem soitpacks thefilesarraybymovingtheremainingunmerged filesonline .
unfortunately instead of moving nfiles i num merged files thethirdargumentrequeststhat num merged filesbemoved possibly resultinginan out of bounds memory access.
to statically find this bug we use the s parrowprogram analyzer .
it uses a def use graph computed using the sparse analysisframework todeterminealldatadependenciesleading up to sensitive memory accesses and subsequently performs an intervalanalysistoprovememorysafety.asonewouldexpect it isunabletoprovethesafetyofthecallto memmoveonline38 and raisesan alarm at this line.
notably however theunderlyingintervalanalysisisnon relational and since the filesarray is dynamically allocated it is also unable to show that the accesses to files on lines36and37 are safe.
it therefore raises two additional alarms at theselines.
of course these are both false warnings and are a result of an overly coarse abstraction.
however this abstraction was deliberately chosen to allow the analysis to scale to large real world programs and is an example of the accuracy scalability trade o ffs routinely made by analysis designers.
overall the sort program has kloc and sparrowemits 715warnings includingthe bugonline .
.
interactivealarm prioritization we will now explain the interactive alarm prioritization process usedbyb ingo thatleveragesaprobabilisticmodeltogeneralizefromuserfeedbacktosuppresslikelyfalsealarmsandprioritize likely true bugs.
reconstructing the derivation graph.
thefirst step is to reconstructthe reasoningtrace thatcausess parrowtoreporteachalarm.
this reasoning process interval analysis applied to the def use graph can be approximately described by the derivation rules shown infigure .startingfromvariablede finitions andone step dataflow edges in the program indicated by tuples of the form vardefn a andduedge a b respectively the analyzer computes dataflowpathsoftheform dupath a b .ifthefinalnodebineach deriveddupath a b corresponds to an array access the analyzer performs additional reasoning to prove the safety of the operation at program point b. note that we have not modeled the details ofthissub analysis whichemploysanintervalabstraction and insteadprovideinputtuplesoftheform overflow b asstubsfor unmodeledpartsofthereasoningprocess.whentheanalyzer finds such a data flow path leading up to a potentially unsafe memory 1155boosting static analysis accuracy withinstrumentedtestexecutions esec fse august athens greece test suite static analyzer instrumentation harness sdtransfersource code include stdlib.
int main int x malloc int y x x y assert x bayesian network dstransfer marginal inference user empirical observations feedback .
.
.
ranked alarms dynaboost figure architecture of the d ynaboostframework.
our main technical contributions include the construction of the bayesian network and the boxes marked sdt ransfer and dst ransfer corresponding to the transfer of instrumentation targets from the static analyzer to the instrumentation harness and the transfer of empirical evidence from the dynamic analysis to theprobabilisticmodelrespectively.
1static unsigned int nmerge 2structsortfile 3char const name 4pid t pid 6voidmerge structsortfile files size tnfiles char const output file 8size tin out 9for out in nmerge nfiles in out dfsan set label src out sizeof size t structsortfile temp create temp temp size tnum merged mergefiles files nmerge temp in num merged files .name temp.name files .pid temp.pid 18memmove files files nfiles in sizeof files 19nfiles in out 20nfiles avoid trashing input files nfiles output file 23size tavoid trashing input structsortfile files size tnfiles char const outfile 25print dfsan get label nfiles 26for size ti i nfiles i bool same if same size tnum merged while i num merged nfiles print dfsan get label i structsortfile temp create temp temp num merged mergefiles files nfiles i temp print dfsan get label num merged files .name temp.name files .pid temp.pid memmove files files num merged sizeof files nfiles num merged 43returnnfiles figure code fragment adapted from the sort program.
the lines highlighted in red correspond to the alarms raised by the sparrow static analyzer while thelineshighlighted ingreen correspond to theinstrumentation added by d ynaboost.
access it reports an alarm at the appropriate point as indicated by the derivationrule r3.
in the case of our example the assignment to the variable out online9mayinfluencethe valueof nfilesat line19andthrough the call to avoid trashing input affect the values of variables i andnum merged at lines30and34respectively.
the program subsequently accesses the i th and i num merged th elements of the filesarrayatthelocationsofthealarmsraisedbys parrow.we mayvisualizethisreasoningtraceasthederivationgraphshown infigure .
a probabilistic model of alarms.
observe now that if the user triages the alarm at line 36and indicates that it is nota real bug then we may conclude that line 37is also a false alarm.
the prioritizationalgorithminferssuchcorrelationsbetweenalarmsusing the derivationgraph infigure .
as an example consider the tuple dupath which indicatesthatdatamay flowfromasourceatlocation 9toavariableinput relations vardefn a variable de finition at programpoint a overflow b possiblebu fferoverflowat point b duedge a b direct data flowedgebetweenprogrampoints aandb outputrelations dupath a b transitive data flowpathfrom programpoint atob alarm b alarmindicating possible bu fferoverflowatb derivationrules r1 dupath a b vardefn a duedge a b r2 dupath a c dupath a b duedge b c r3 alarm b dupath a b overflow b figure modeling the program analyzer using derivation rules represented here as adatalogprogram.
at location .
because of inaccuracies in the construction of the dataflow graph there is a small but non zero probability that a 1156esec fse august athens greece tianyi chen kihong heo andmukund raghothaman dupath duedge r2 dupath duedge duedge duedge r2 dupath r2 dupath r2 dupath overflow r3 alarm overflow r3 alarm overflow r3 alarm figure derivation graph for the alarms in figure .
each clausenode e.g.
r2 indicatesthevariablevaluation with which thecorresponding rule infigure 3wasfired.
givenderivedtupleofthisformdoesnotrepresentaviabledata flow pathinthe program.
to model such inaccuracies the alarm prioritization process interpretsthederivationgraphasabayesiannetwork andassociates eachofits clauses withaprobability of misfiring pr t t1 t2 tk wheretis the tuple produced by instantiating a rule rwith the appropriateinputtuples t1 t2 ... tk.fromthis onecancomputethe probabilityofeachalarm alarm c beingarealbug pr alarm c and use these probabilities to order alarms for triaging with highconfidencealarmsinspectedbeforealarmswithlowcon fidence.furthermore thecomputationofmarginalprobabilities pr alarm c1 alarm c2 alarm c3 for example provide a natural mechanism by which to generalize from user feedback and suppress or prioritizesimilar warnings from the staticanalyzer.
for simple networks and for the sake of exposition we may computethese valuesbyhand aswe demonstrateinappendix a. if we assume that the prior probability pr dupath .
and that there is a i.i.d probability of each rule mis firing then it followsthat the prior probability ofalarm isgiven by pr alarm .
.
generalizingfromuserfeedback.
iftheuserindicatesthat alarm isafalsewarning wewouldbeinterestedinthevaluesof pr alarm alarm andpr alarm alarm pr alarm alarm .
.
observethedramaticdropin pr alarm alarm compared tothe valueof pr alarm which occurs becauseofthe sharedderivationgraphbetween alarm andalarm .asa result the user feedback causes us to suppress the second alarm andpermittheaccelerateddiscoveryofbugs inotherpartsofthe codebase.
recall that s parrowreports alarms when it analyzes gnu sort the user in the loop interaction process we just discussed causes the bug to be discovered after inspecting only alarms.
we graphically depict this interaction loop in the rightmostportionofthesystemdiagraminfigure andwillreviewthe construction ofthe probabilisticmodelinsection .despitethesigni ficantempiricalimprovementprovidedbyb ingo itsuffersfromseveralimportantlimitations.inprinciple theconditional probabilities pr alarm c t may be computed with respect to anytupletproduced by the analysis and not just those correspondingtoalarms t alarm c .inpractice however because of the highly technical nature of the analysis users are only ableto providelimitedforms offeedback andeventhen areprone to making mistakes.
second an investigation of the interaction processrevealsnumerousinstancesof falsegeneralization where the rank of the real bug drops during a single iteration.
see for example the spikes in the plots of figure and the aggregate statistics in table .
finally because the triage process is primarily user driven itsometimesresultsinalengthyinteractionprocessin which the user has to inspect a large number of false warnings beforediscoveringrealbugsintheprogram.theselimitationsprovide the background for our present paper.
.
dynamicinstrumentationas an information source ourcentralinsight.
thecentralinsightofourpaperisthatthe feedbackprovidedtothealarmprioritizationalgorithmneednot onlycomefromhumanusers butcanbedrawnmorebroadlyfrom any source of information about the program.
in particular we demonstrate the possibility of using dynamic information obtained from test executionsas an information source.
the dynamic data flow sanitizer dfsan.
dfsan provides two functionstoinject taintlabels andinspect thetaintvalues ofvariables at runtime a dfsan set label dfsan label label void addr size tsize which associates the sequence of memory locations addr addr addr ... addr size with the taint value label and b dfsan get label longdata which returns the taint label associated with the value data.
the annotated program isthen instrumented bydfsan duringcompilationso as to track theseinjectedtaint valuesas the program executes.
dynamicinstrumentation.
foreachdata flowpath dupath a b reported by s parrow we use dfsan to monitor program executionsforconcreteevidenceofa flowbetweenthe a b source sink pair.wehighlightasimpli fiedversionoftheannotationsappliedto the sort program in green in figure .
as an example consider the tupledupath .
we annotate the variable being assigned at thesource out withadistinguishedtaintvalue here src9 with thecallto dfsan set label online10.wethenretrievethelabels of all predicted downstream sinks with calls to dfsan get label on lines25 and35 and check whether the source taint propagates to each of the sink locations.
we run this instrumented program on all tests provided with gnu sort version .
looking for experimental con firmation of the predicted source sink flows.
this in turn enables us to provide early feedback to the bayesian rankingprocess so thatitnowranksalarms according topr alarm c dupath a b ratherthanmerelybytheirprior probabilitiespr alarm c .
to motivatethe value ofthis process we will now discusshow bingocausesfalsegeneralizationwhileanalyzing sort.first considerthesymmetrybetween alarm alarm andalarm in figure .
from this with our previous calculation of the prior 1157boosting static analysis accuracy withinstrumentedtestexecutions esec fse august athens greece and posterior probabilities in equations 1and2 we conclude that pr alarm .
pr alarm alarm .
.
thisdropinposteriorprobabilitycausesthealarmtodropinthe ranking comparedtootherwarningsintheprogram and sincethis representsarealbug correspondstoafalsegeneralizationevent.
overall over the course of the interaction process alarm gets deprioritizedtwice correspondingtotheuserinspectingeachof the neighboring alarms alarm andalarm .
visually these correspondto the twoprominentspikesinfigure 6f.
wewillnowdescribehowd ynaboostmitigatesthisproblem of false generalization.
observe that even though the functionalityimplementedin avoid trashing input isnotexercisedbyany testinput thefunctionisalwayscalledfrom merge andthe standard suite of test cases does attempt to merge files.
as a result dfsanobservesexperimentalevidencefor dupath .thealgorithmthenranksalarmsintermsof pr alarm c d e where d dupath is the dynamic feedback and erepresents the feedbackprovidedbytheuser.inthiscase theoriginalprobabilities are given by pr alarm d pr alarm d pr alarm d .
.
where asbefore theexpression0 .993arisesbecauseofthethree rule applications between the evidence and the query nodes.
after thefirst round of user feedback the posterior probability of alarm isgivenby pr alarm d e wheree alarm .
we sketch this calculation in appendix a from which we can concludethat pr alarm d alarm .
.observethat userfeedbackonthefalsealarm alarm causesamuchsmaller dropincon fidenceinalarm andasigni ficantlysmallerfalse generalization event.
experimentalresults.
assuch d ynaboostaddressesthepreviouslyoutlinedlimitationsinbingo first dynamicfeedback provided to tuples of the form dupath a b represents program behaviors which users would find difficult to certify.
second this additional information constrains the ways in which the marginal inference algorithm may propagate user feedback for example by activatingconditionalindependenciesinthebayesiannetwork and this reduces the incidence of false generalization.
for example compare the frequency and magnitude of spikes of d ynaboost and bingoin the plots of figure there are fewer spikes and each spike is only of the original size.
as a consequence dynamicfeedbackbecomesavaluableauxiliarysourceofinformation and dramatically reduces the alarm inspection burden by approximately compared to b ingo and approximately compared to an unaideduser see table .
anoverview ofbayesianalarm prioritization wewill nowpresent ahigh level descriptionofthebayesianalarm prioritizationframework.itconceptuallyworksinthreephases by extractingthederivationgraphfromthestaticanalyzer converting thisderivationgraphintoabayesiannetwork and finallyengaging inaninteractionloopwiththeuser whilerepeatedlyperforming marginalinference to rank alarms.first wemodelthestaticanalysisasadatalogprogram suchas that shown in figure .
most brie fly a datalog program consists of a set of universally quanti fied horn clauses which we call rules.
we may read the rules from right to left while treating the operatorinthemiddlereadastheimplicationoperator .for example the rule r2may be read as for all program points a b c if there is a data flow fromatob dupath a b and a one step flow frombtoc duedge b c then transitively thereisalsoadata flow fromatoc dupath a c .
wewillrefertoeachinputhypothesis andoutputconclusion of the form r c1 c2 ... cn as atuple.
wethenmodifythestaticanalyzertoprovideexplanationsfor eachofitsalarms.theseexplanationstaketheformofaderivation graph such as that shown in figure .
the derivation graph g is a possibly cyclic directed graph consisting of two types of nodes corresponding to the tuples and grounded clauses of the leastfixpointofthedatalogprogram.
eachclausenoderefersto a specific instantiation of a rule which takes several tuples as hypotheses and produces a tuple as conclusion.
in our diagrams we willindicatethetuplesby boxedvertices andleave theclause nodes unboxed.
we remark that the derivation graph is a best e ffort post hoc explanation s parrowis itself written in unrestricted ocaml and thereareportionsoftheanalyzer suchastheintervalanalysis whichareleftunmodeled.itshouldbepossibletoextractsimilar derivationgraphsfromotherstaticanalyzers.toobtainthisderivationgraph wereusedthemodi ficationstos parrowwhichwere originallyemployedindrake andwhichconsistsofapproximately 500linesofchanges to a15 kloc codebase .
next we convert the derivation graph into a bayesian network.
we associate each node of the graph with a conditional probability distribution which indicates the probability of the node being true for eachcombination oftruthvaluesof its hypothesisnodes.
consider a grounded clause g r c1 c2 ... ck which applies rulerto produce the conclusion tfrom hypotheses t1 t2 ... tn t rt1 t2 tn.
the clause nodes may be thought of as conjunctions which fire only when all of its hypothesis nodes are derivable.tomodeltheapproximationsofthestaticanalysis we allow for the possibility of clause nodes mis firing with a small rule dependent mis firing probability pr pr r c1 c2 ... ck t1 ... tn prift1 tn and otherwise .
so thatthe probabilitiesall addupto1 wenaturallyhave pr g t1 ... tn pr g t1 ... tn .
while these firing probabilitiesprmay be determined using techniques such as expectation maximization we followb ingoanduniformly setthemto .
.
similarly the tuple nodes of a derivation graph may be thought ofasdisjunctions whicharederivableonlywhenatleastoneofits contributingclausesisableto fire.consideratuple twhichisthe result of several alternative clauses g1 g2 ... gk.
we model tas a deterministic disjunction pr t g1 ... gk ifg1 gn and otherwise .
1158esec fse august athens greece tianyi chen kihong heo andmukund raghothaman asbefore toindicateexhaustiveness pr t g1 ... gk pr t g1 ... gk .forallinputtuples tin wedefinethepriorprobability as pr tin .
onenotablechallengeintheconstructionofthisprobabilistic modelisthatbayesiannetworksarerequired byde finition tobe acyclic while the derivation graph may potentially have cycles.
to address this b ingoapplies a cycle elimination algorithm which dropsclausessoastoobtainanacyclicgraphwhilestillpreserving thederivationsofallalarms.thisresultsinanacyclicderivation graph whichisthen usedto buildthe bayesian network.
given the bayesiannetwork we use ano ff the shelf solver libdai to perform marginal inference and rank alarms for user inspection .
targetedinstrumentation and feedback transfer in this section we explain the operation of the d ynaboostsystem.weformallypresentthetop levelprocedureinalgorithm .
themaincontributionsofthispaperareinthedynamicanalysis performed in steps .
we describe the core sdtransfer and dstransfer procedures inalgorithms 2and3respectively.
algorithm1 dynaboost a p t whereaisthestaticanalysis pisthe program tisthe setofavailable test cases.
lets a p .staticallyanalyzetheprogram.theresult s consistsofthesetofalarms theintermediateconclusions andthe derivationgraph connecting them.
constructthe bayesian network b makebnet p s .
letp sdtransfer p s .instrumenttheprogramusing the staticanalysisoutput.
compute d p t .
runthe instrumented program on the test inputs.
letfdyn dstransfer d s .determinewhichdata flow factscan be dynamically observed.
initializethefeedback fbfdyn.assertfdyn tuples s .
initialize the setofunlabelledalarms aubalarms s .
while au i presentthehighestprobabilityunlabelledalarmforuser inspection at argmax a aupr a f .
ii iftheusermarks atastrue update fbf at.otherwise updatefbf at.
iii update aubau at .
.
performingtargetedinstrumentation wewillnowdescribe sdtransfer theprocessofinstrumenting the program based on results from the static analyzer.
we present the overallalgorithm inalgorithm .
as discussed in section in addition to the runtime instrumentation appliedto the llvm ir dfsan provides two functions dfsan set label to insert taint values into memory locations and dfsan get label toretrievethetaintvaluesassociatedwithavalue.foreach data flow tuple dupath a b producedby the static analyzer ourgoalistoinserttheappropriatetaintvaluesintovariables beingassignedatprogramlocation ausingdfsan set label and toretrievethetaintvaluesfromvariablesbeingusedatprogram locationbusingdfsan get label .
the main challenge in this process is in translating program locations from s parrow s representation to points in the program sourcecode.sinces parrowworkswithanssa formoftheprogram some program locations such as nodes cannot be mapped back to locations in the original program.
in such cases we do not instrumenttheresulting dupath a b tuple.furthermore operations may have side e ffects such as p syntactic constructs maybearbitrarilynested such as x y b c andasingleline ofcodemayhavemultipleassignments thiscommonlyarisesin for loops sothat we areonlyable toperform a best e ffortinstrumentation of the source code and omit tuples which cannot be instrumented.
overall in ourexperiments insection we have a successrateininstrumenting43465 target locations.
algorithm2 sdtransfer p s .givenaprogram pandstatically determined data flow factss producesa program p withruntime instrumentationenabled.
foreachpredicteddata flowtupledupath a b s ifaand bcan both be mapped to source locations add the instrumentation highlightedingreen below x ... program point a dfsan set label src a x sizeof x ... print dfsan get label y read y program point b return the instrumentedprogram p .
.
transferring runtime outputto static feedback we run the instrumented program p on the provided test cases t andcollectthelistofalldata flowpathswhichareempirically observed.wethenperformlightweightfeedbackenhancementto recover information about uninstrumented data flows to or from emptynodes andusethefrequencyofobservationofeachdata flow path toprovide weightedfeedback tothemarginalinferencealgorithm.we outlinethis processinalgorithm .
byborrowingterminologyfromgraphtheory wetermatuple dupath a c asan a b bridgeif a bothdupath a b dupath a c s the predictions ofthe staticanalyzer and b the clause dupath a b r2dupath a c duedge c b is the only way to derive dupath a b where the rule r2is drawn from figure .
bridges provide indirect evidence of data flows to empty nodes c which cannot themselves be directly instrumented.
inthesecases wecanusedynamicobservationsof dupath a b toinfer truthof thebridge dupath a c .in step1ofalgorithm we repeatedlyperform this feedbackenhancement.
additionally to account for the frequency of observations of individual data flows we count the number of test cases a b 1159boosting static analysis accuracy withinstrumentedtestexecutions esec fse august athens greece whichwitnesseachdata flowdupath a b andcompareittothe total number of test inputs n. this allows us to prioritize feedback to commonly observed data flow paths and only provide weak experimental feedback for less frequently observed data flow paths.
algorithm dstransfer d s .
given the results of the static analysiss and the list of empirically observed data flow facts d computes the dynamicfeedback fdyn.
perform feedback enhancement.
for each tuple dupath a b d if there is an a b bridge dupath a c d then update dbd dupath a c .
repeatuntil fixpoint.
constructthe dynamicfeedback fdyn.
i initialize fdynb .
ii foreachtuple dupath a b d let a b bethenumber oftestinputswhichtrigger dupath a b andletnbethe total number of test inputs.
provide feedback to the a b dataflowbyupdating fdynbfdyn dupath a b a b n .
return fdyn.
.
differentiatingun filtered data flows in the last step we modify the derivation graph to di fferentiate between filtered and un filtered data flows.
because dfsan only provides an under approximation of feasible behaviors it does not provide information about the operations applied to the data along the a b dataflowpath.asaresult itisinsu fficienttoconcludethat this path could form the basis of a bu ffer overflow.
to describe the limitedinformationcomingfromdfsan weintroduceanewoutput relationtdupath whichindicatesthepossibilityofanun filtered dataflow from source ato sinkb.
we use the base dupath a b tuplestoderivetuplesoftheform tdupath a b butonlyapply the feedback in fdynto the original dupathtuples.
we describe these modi fied datalog rules in figure .
as we demonstrate in table2 modeling these correlations is crucial to the experimental effectivenessof d ynaboost asitreducestheaveragenumberof iterations from forb ingoall to ford ynaboostall .
experimentalevaluation toevaluatetheexperimentale ffectivenessof d ynaboost wefocus onthe following researchquestions rq1.does dynaboosteffectively prioritize the real bugs and howdoes itcompare to b ingo?
rq2.does dynaboostreduce the frequency and magnitude of false generalization events?
rq3.howdoesthenumberoftestcasesa ffecttherankingquality?
rq4.howisthemodi ficationforthenetworkstructureimportant for utilizing the runtimefeedback?
we begin this section by describing our experimental setting andwe focusoneachofthe above questionsinsections .
.
.
1wewill make ourbenchmarks and implementation public uponpaper acceptance.input relations vardefn a overflow b duedge a b outputrelations dupath a b alarm a b tdupath a b unfiltered data flowpathfromprogrampoint atob derivation rules r1 dupath a b vardefn a duedge a b r2 dupath a c dupath a b duedge b c r alarm b tdupath a b overflow b r4 tdupath a b dupath a b vardefn a duedge a b r5 tdupath a c dupath a c tdupath a b duedge b c figure modi fied derivation rules to capture un filtered dataflows.
we reuse rules r1andr2from figure and replacerule r3withr .
table1 benchmarkcharacteristics.sizeand testreportthe linesofcodeandthenumberoftestcases.
program version analysis size kloc tests bc .
interval cflow .
interval grep .
interval gzip .
.4a interval libtasn1 .
interval patch .
.
interval readelf .
interval sed .
interval sort .
interval tar .
interval optipng .
.
taint latex2rtf .
.
taint shntool .
.
taint .
experimentalsetup choice ofbenchmarks.
we ran d ynabooston a suiteofwidely usedcprogramsshownin table1.allbenchmarksarefrompreviousworkusings parrow andrecentcvereports.weexcluded benchmarks with less than kloc because the limited numberofalarmsraisedbys parrowdoesnotimposeasigni ficant alarminspectionburden.weadditionallyexcluded wgetandurjtag because ofcompatibilityissueseitherwithdfsan orwithclang.
weusedthetestinputsthatcomewiththeprogram ifavailable to collect dynamic information.
three benchmark programs did nothaveadeveloper providedtestsuite gzip shntool andoptipng.
forthese programs wecollected sampleaudioandimage files shntool optipng andcompressed filesfromthecanterbury corpus gzip .
dynamic instrumentation and ranking process.
we use data flowsanitizer dfsan to collect the runtime data flow information.
for each source sink pair dupath a b reported by s parrow we determinethevariablesassignedatprogramlocation aandinjecta runtimetaintlabelusingthe dfsan set label function andwe retrievethetaintlabelsofallvariablesaccessedatprogramlocation busing the dfsan get label function.
we break ties between 1160esec fse august athens greece tianyi chen kihong heo andmukund raghothaman identically ranked alarms by using their con fidence values from the purelystaticrankingprocessinb ingo.
baselines.
we instantiate d ynaboostwith two di fferent settings dynaboostallthatisbasedontheaugmentednetworkstructure of section .3and initialized with dynamic feedback and dynaboostzerowhich also uses the new network structure but withholdsdynamicfeedback.wesimilarlyinstantiatetwobaselines bingoallwhichusestheoriginalnetworkstructure butalso includes dynamic feedback and bingozerowhich neither uses the newnetwork structure noruses dynamicfeedback .
runtime performance of d ynaboost.sparrowrequires an averageof206secondstoanalyzethebenchmarkprograms.thisranges fromafewsecondsfor gzipto840secondsfor tar.instrumentation anddynamicdatacollectionrequiresacomparableamountoftime rangingfromafewsecondstoabout20minutes.notethatthisis the timefor alltestinputs run insequence andcan be parallelized in a straightforward manner.
finally theinitial ranking and reprioritizationprocessesaremuchfaster andstep i ofalgorithm takes seconds onaverage.
.2rq1 effectiveness ofranking wefirstevaluatethee ffectivenessof dynaboostallcomparedto bingozero.
notice that dynaboostallis different from bingozero in two ways feedback from dynamic analysis and augmented bayesian network structure.
these aspects will be further discussed in the subsequent sections.
in this section we measure the initial rankings of all bugs and the number of iterations untildynaboostallandbingozerofindallthebugs.theresultsare shownin table2.
weobservethat dynaboostallissignificantlymoree ffectiveat prioritizing true bugs compared to bingozero.
on average the full system dynaboostallfindsthe bugwithin59.5iterationswhile bingozerorequires92.
iterations resultingina35 reductionin thehumanalarminspectionburden.oneofthemainreasonsofthis effectivenessistheimprovementofthequalityofinitialrankings i.e.
rightaftertransferringfeedback fromdfsan .before anyuser feedback dynaboostallplaces the true bugs at rank while bingozeroyields251 onaverage.thisstatisticalsowouldbeofinterestincaseswhereusersdonotwishtointeractwiththetool but merelywantalistingofalarmsaboveacertainthresholdforo ffline inspection.
another reasonis thereduction of false generalization whichwillbe discussedmore inthe nextsection.
we notice particularly dramatic improvements in the cases of cflowandtar.
both of the benchmarks show the improved quality oftheinitialrankings from517to173for cflow from697to253for tar therebyproducingthereducednumberofiterationsneeded tofindallthebugs and58 improvementsfor cflowandtar respectively .interestingly whilebothofthesebenchmarksalready hadtestcasesthatexercisethedata flowpathsinquestion theydid not capture the bugs because they could only be triggered by carefullychosentestinputs.asaresult theonlyremaininguncertainty in the ground truth of the alarm arises from incompleteness in the intervalanalysisperformedbys parrow.therefore dynaboostall quicklyprioritizesthesealarms over the rest.finally wenoticesmallperformanceregressionsonsomebenchmarks.forthebu fferoverflowbenchmarkssuchas readelf these werecausedbyabiasedtestsuitewhichfailstoexerciseanyofthe dataflow facts that are involved in the derivation of the bugs.
in thecaseof shntool wespeculatethattheregressionistheresult offluctuations caused by the small number of alarms emitted by sparrow combinedwiththelargenumberoftruealarms which together ampli fies the effect of noise in the ranking process.
in any case we note that for all these benchmarks the absolute value of the performance regression is small iterations and the overall ranking process still provides massive reductions in the human alarm inspection burden.
.3rq2 reductionoffalsegeneralization next we measure the impact of dynamic feedback in reducing false generalization.
after each round of feedback we measure the average rank of all real bugs and compare this average to their averagerankinthepreviousround.wede fineafalsegeneralization eventas one in which this average drops by or more and by at least alarms.
the rank reported in table 3is the sum of this average rank drop across all false generalization events and shows howdynaboostallmitigates the false generalization problem.
incaseof sort dynaboostallreducesthenumberofrequired useriterationsby39.
.accordingtotheresults theaveragenumberofrankdropeventsoftruealarmsisreducedfrom4.5to0.
.the average rank drop size for each time is also dropped from .
to .
.while bingozerointroducestwomajorfalsegeneralizations around iteration and dynaboostallsubstantially reduces theirimpactonthetwosamefalsealarmsarounditeration75.as shownin figure4 thederivationrulesofallthesealarmssharethe tupleobservedatruntime dupath .thus for dynaboostall when rejecting the two false alarms we do not decrease the associatedprobabilitiesblindly instead welimittheextentoffeedback propagation because of the con fidence brought by the observation.
.4rq3 impactoftestcaseson ranking performance inthissection weconductasensitivitystudywithdi fferentamounts oftestdata.since d ynaboostleveragesdynamicanalysisresults the quality of ranking relies on the number and coverage of test cases.
to quantify this relationship we ran d ynabooston the benchmarks with di fferent subsets of the entire test suite and measuredthenumberofiterationsneededtodiscoverallbugsinthe programs.wevariedthefractionoftestcaseschosen andrepeated the experiment for eachfraction10 times.
we plot these results in figure .
we additionally include the numberofiterationsneededby bingozeroasavisualbaseline dottedlines .noticethattheleft mostobservationofeachbenchmark correspondingtocolumn dynaboostzerointable2 alreadycorresponds to72 reduction inalarm inspection burdencomparedto an unaided user averaged across all benchmarks.
we observe that formostofthebenchmarks evenwhenonlyhalfofthetotaltest inputs are chosen the number of iterations needed by d ynaboost remains close to its e ffectiveness on the complete test suite.
in fact for a majority of benchmarks this is true even when we provide 1161boosting static analysis accuracy withinstrumentedtestexecutions esec fse august athens greece table e ffectiveness of d ynaboostcompared to b ingo.
alarms shows the total number of alarms reported by s parrow.
init anditersmeasure theaverage initial rank ofthebugsandthenumberofiterationsneeded until findingallthe bugs.
program alarms bugs d ynaboostalldynaboostzerobingoallbingozero init iters init iters init iters init iters bc .
.
.
.
cflow .
.
.
.
grep .
.
.
.
gzip .
.
.
.
libtasn1 .
.
.
.
patch .
.
.
.
readelf .
.
.
.
sed .
.
.
.
sort .
.
.
.
tar .
.
.
.
optipng .
.
.
.
latex2rtf .
.
.
.
shntool .
.
.
.
average .
.
.
.
.
.
iteration number0100200300400500600700rank of bugdynaboost all bingozero a bc .
iteration number0100200300400500rank of bug b cflow .
iteration number020406080100120140rank of bug c patch .
.
iteration number050100150200250300rank of bug d readelf .
iteration number0100200300400500rank of bug e sed .
iteration number0100200300400500rank of bug f sort .
iteration number0100200300400500600700rank of bug g tar .
iteration number20253035404550rank of bug h shntool .
.
figure ranking changesoftruealarms by d ynaboostallandbingozero.the remaining plotsare availableinappendix b. just of all test inputs.
furthermore the variation in number of iterations quicklydisappears as test casesare added.
onestrikingobservationinfigure 7isthatformanybenchmarks the number of iterations needed by d ynaboostis independent of the size and choice of test inputs.
we conjecture that many test inputsexercisesimilarpathsthroughtheprogram suchasbyentering through main parsing command line arguments or by exercisingcommonfunctionality suchasparsingregularexpressionsingrep.suchinputswouldresultinmanyshareddata flows whichareresponsibleforsimilarprioritizationresults.ontheother hand someprogramshaveafewdistinctfunctionalities suchas tar which can alternately compress or decompress a file and samplingtestinputsleadstoabimodalperformancedistribution aswecan see infigure 7f.
weobservenotablyexceptionalbehaviorfor readelfasthenumberof iterationsdegrades withadditional testcases.
accordingto our investigations this is because no test case ever explores the buggy function process cu tu index .
this function is responsibleforreadingthecontentsof dwofiles whichcontaindwarf objectsrelatedtodebuginformationinthebinary.wesubsequently chose an intermediate tuple in the derivation tree and manually provided positive feedback thus overriding data obtained from the test cases.
this reduced the number of iterations needed to find the bug from to .
we conclude that the biased set of test cases 1162esec fse august athens greece tianyi chen kihong heo andmukund raghothaman table magnitude and frequency of false generalization.
eventsindicatesthenumberoffalsegeneralizationevents andrank indicatesthesumoftheaveragerankdropacross allfalsegeneralizationevents.
program d ynaboostall bingozero rank events rank events bc .
.
cflow .
grep gzip .
.
libtasn1 patch .
readelf .
.
sed .
.
sort .
.
tar .
optipng .
latex2rtf shntool average .
.
.
.
inreadelfcontinuously prioritizes alarms in other functions and suppresses the true bug.
.5rq4 impactofnetworkstructureon ranking performance finally we empirically clarify the impact of augmented network structure described in section .
.
we compare the performance of dynaboostallcompared to bingoallthat is based on the original network withfull feedbackfrom dynamic execution.
theresultsareshownintable .withtheoriginalnetwork the number of required iterations by bingoallis .6x higher than that fordynaboostall.
for example we observed signi ficant regressions for readelf sedandsortwithbingoall.
interestingly the performance of bingoallis even worse than bingozero.eventhoughdynamicfeedbackimprovesthequalityof theinitialrankingby40 intheoriginalnetwork itappearsthat thenewlyintroducedconditionalindependenciesbythefeedback heavily limit positive generalization of user feedback.
for example bingoalldid not generalize any feedback for benchmarks e.g.
cflow grep patch sed andtar but just enumerate alarms following the initial rankings.
this result shows that the new network structure ismore suitable for handling dynamic feedback.
limitationsand threats to validity onesigni ficantrestrictionofourexperimentalevaluationisthatwe have restricted our attention to a single static analyzer s parrow twoanalyses bu fferoverflowsandtainttracking andasmallsetof benchmarkprograms.however ourprincipalassumptionsarethat theanalysispermitrecoveryofthederivationgraph suchasfigure4 and that it permit experimental observation of intermediate facts.for example def use chains are a general building block for a large class of static analysis tools based on the sparse analysis framework includingtajs pinpoint andsvf .
bug findingtoolsbasedonthesetechniquescandirectlyleverage our work as describedinthe paper.
furthermore if the analysis is expressed in datalog or using similar deductive approaches examples include chord and doop and if the abstract behaviors are experimentally observable then our techniques are again potentially applicable.
as an example thedataracedetectorinchordfundamentallydependson a may happen in parallel analysis which can be experimentally observedusingdynamicdataracedetectorssuchasroadrunner .
another threat to the validity of our experiments arises from ourprotocol.westartedwithasetofhistoricalbugsforeachofthe programs and assumed that only this explicitly identi fied target bug was real and that all other warnings produced by the static analyzer were false positives.
we simulated user interaction by repeatedly examining the alarm with highest conditional probability andlabellingitas true orfalse.
inanycase notethatweprovideidenticallabelstobothd ynaboostand bingo.
furthermore mislabelling can a ffect alarms in only one direction i.e.
by mistakenly identifying real bugs as falsewarnings.asaresult whenconsideringsuchpotentialmislabellings the numbersintable 2provide an upperboundonthe time neededto discover the first real bugusing d ynaboost.
related work dynamicanalysis.
alargebodyofresearchondynamicanalysis has been proposed to capture interesting properties of programs such asmemorysafety datarace likelyinvariant .
while d ynaboostcurrentlyrelies on dfsan because our underlying analyzerisbased ondatadependencies we conjecture that other combinations of static and dynamic analyzers would be possible.forexample runtimeinformationoftwothreadsthatmay happen in parallel by roadrunner can be transferred to the alarm rankingsystemfor staticdataracedetection .
combining static and dynamic analysis.
researchers have previously investigated techniques to combine both approaches such as byinsertingdynamiccheckstovalidatepropertieswhicharenot statically provable using information collected from test executions to optimally set knobs for a subsequent analysis run concolicexecutiontoguidetestingthroughpiecesofcode thataredi fficulttoexplore usingstaticanalysistominimize the amount of dynamic monitoring or bounded exhaustive testing .
our work is di fferent from the previous work as we combinethetwoapproachesinaprobabilisticframeworkforalarm rankingsystem.
user guided static analysis.
previous user guided approaches for staticanalysissuchasalarmclassi fication alarmranking andalarmclustering providemechanismstoincorporate user feedback to filter out false alarms.
however human user of staticanalyzers whoisunawareofthedetailsofanalysisdesign canprovideonlylimitedformsoffeedbacksuchaslabelsofalarms.
dynaboostovercomesthislimitationbyincorporatingdynamic 1163boosting static analysis accuracy withinstrumentedtestexecutions esec fse august athens greece a grep .
b patch .
.
c readelf .
d sed .
e sort .
f tar .
g latex2rtf h optipng .
.
figure performance of d ynaboostwith a limited number of test cases sampled from the full test suite.
the whisker plot indicates thedistribution ofobserved results.
the remaining plotsare availableinappendix b. analysis results from test cases thereby boosting the performance ofalarm rankingsystems.
conclusion in this paper we developed a probabilistic technique to leverage the results of a dynamic analysis to increase the e ffective accuracy ofastaticanalyzer.bytargetedinstrumentationoftheprogram we weretoabletoexperimentallycon firmthepresenceofintermediate conclusions drawn by the staticanalyzer and use this feedback to prioritizethegeneratedalarms.inexperiments wedemonstrateda significantreductioninthehumanalarminspectionburden and improvements in other related metrics such as the quality of the initialranking andfalsegeneralizationevents.weanticipatepotentialapplicationsofthisresearchinsynthesizingtestcases and inautomaticfaultlocalization.