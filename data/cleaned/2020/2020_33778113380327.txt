unblind your apps predicting natural language labels for mobile gui components by deep learning jieshan chen jieshan.chen anu.edu.au australian national university australiachunyang chen chunyang.chen monash.edu monash university australiazhenchang xing zhenchang.xing anu.edu.au australian national university australia xiwei xu xiwei.xu data61.csiro.au data61 csiro australialiming zhu liming.zhu data61.csiro.au australian national university australiaguoqiang li li.g sjtu.edu.cn shanghai jiao tong university china jinshui wang ymkscom gmail.com fujian university of technology china abstract according to the world health organization who it is estimated thatapproximately1.3billionpeoplelivewithsomeformsofvision impairment globally of whom million are blind.
due to their disability engaging these minority into the society is a challenging problem.
the recent rise of smart mobile phones provides a new solution by enabling blind users convenient access to the information and service for understanding the world.
users with vision impairmentcanadoptthescreenreaderembeddedinthemobileoperating systems to read the content of each screen within theapp and use gestures to interact with the phone.
however the prerequisite of using screen readers is that developers have to add natural language labels to the image based components when they aredevelopingtheapp.unfortunately morethan77 appshaveissues of missing labels according to our analysis of androidapps.mostoftheseissuesarecausedbydevelopers lackof awareness and knowledge in considering the minority.
and even if developers want to add the labels to ui components they may not come up with concise and clear description as most of them are of no visual issues.
to overcome these challenges we develop a deeplearning based model called labeldroid to automatically predict thelabelsofimage basedbuttonsbylearningfromlarge scalecommercialapps ingoogleplay.theexperimentalresultsshowthat corresponding author.
also with data61 csiro.
also with university of new south wales.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn ... .
model can make accurate predictions and the generated labels are of higher quality than that from real android developers.
ccs concepts human centered computing accessibility systems and tools empirical studies in accessibility software and its engineering software usability.
keywords accessibility neural networks user interface image based buttons content description acm reference format jieshan chen chunyang chen zhenchang xing xiwei xu liming zhu guoqiangli andjinshuiwang.
.unblindyourapps predictingnaturallanguagelabelsformobileguicomponentsbydeeplearning.in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
https introduction givenmillionsofmobileappsingoogleplay andappstore the smart phones are playing increasingly important roles in daily life.theyareconvenientlyusedtoaccessawidevarietyofservices such as reading shopping chatting etc.
unfortunately many apps remain difficult or impossible to access for people with disabilities.
for example a well designed user interface ui in figure oftenhaselementsthatdon trequireanexplicitlabeltoindicate their purpose to the user.
a checkbox next to an item in a task list application has a fairly obvious purpose for normal users asdoes a trash can in a file manager application.
however to users withvisionimpairment especially fortheblind otheruicuesare needed.
according to the world health organization who it isestimatedthatapproximately1.3billionpeoplelivewithsome formofvisionimpairmentglobally ofwhom36millionareblind.
comparedwiththenormalusers theymaybemoreeagertouse the mobile apps to enrich their lives as they need those apps to ieee acm 42nd international conference on software engineering icse 323324325326327unblind your apps predicting natural language labels for mobile gui components by deep learning icse may seoul republic of korea table details of our accessibility dataset.
app screenshot element train validation test total tokenstothestartandtheendofthesequence.wealsoreplacethe low frequency words less than five with an unk token.
to enable the mini batch training we need to add a pad token to padthewordsequenceoflabelsintoafixedlength.notethatthe maximum number of words for one label is in this work.
after the data cleaning we finally collect totally pairs of image basedbuttonsandcontentdescriptionsfrom7 594apps.note thattheappnumberissmallerthanthatinsection3astheappswith nooruninformativelabelsareremoved.wesplitcleaneddataset into training validation3and testing set.
for each app category werandomlyselect80 appsfortraining forvalidationand the rest for testing.
table shows that there are imagebased buttons from apps as the training set buttons from apps as validation set and buttons from apps as testing set.
the dataset can also be downloaded from our site.
.
model implementation we use resnet architecture pretrained on ms coco dataset as our cnn module.
as you can see in the leftmost offigure6 itconsistsofaconvolutionlayer amaxpoolinglayer four types of blocks with different numbers of block denoted indifferent colors .
each type of block is comprised of three convolutionallayerswithdifferentsettingsandimplementsanidentity shortcut connection which is the core idea of resnet.
instead of approximating the target output of current block it approximates the residual between current input and target output and then the target output can be computed by adding the predicted residual andtheoriginalinputvector.thistechniquenotonlysimplifiesthe trainingtask butalsoreducesthenumberoffilters.inourmodel weremovethelastglobalaveragepoolinglayerofresnet 101to computea sequence ofinputforthe consequentencoder decoder model.
for transformer encoder decoder we take n dembed dk dv dmodel dff h 8andthevocabularysize as .
we train the cnn and the encoder decoder model in anend to end manner using kl divergence loss .
we use adam optimizer with 1 .
2 .
and 9and change thelearningrateaccordingtotheformula learning rate d .
model min step num .
step num warmup steps .
totrainthemodel wherestep numisthecurrentiterationnumberoftrainingbatch andthefirst warm uptrainingstepisusedtoacceleratetraining processbyincreasingthelearningrateattheearlystageoftraining.
our implementation uses pytorch on a machine with intel i7 7800xcpu 64gramandnvidiageforcegtx1080tigpu.
3for tuning the hyperparameters and preventing the overfitting6 evaluation weevaluateourlabeldroidinthreeaspects i.e.
accuracywith automated testing generality and usefulness with user study.
.
evaluation metric toevaluatetheperformanceofourmodel weadoptfivewidelyused evaluation metrics including exact match bleu meteor rouge cider inspired by related works about image captioning.
the first metric we use is exact match rate i.e.
thepercentageoftestingpairswhosepredictedcontentdescription exactly matches the ground truth.
exact match is a binarymetric i.e.
0ifanydifference otherwise1.itcannottellthe extent to which a generated content description differs from theground truth.forexample thegroundtruthcontentdescription maycontain4words butnomatteroneor4differencesbetween thepredictionandgroundtruth exactmatchwillregardthemas .
therefore we also adopt other metrics.
bleu is an automaticevaluation metric widely used in machine translation studies.
itcalculates the similarity of machine generated translations andhuman created reference translations i.e.
ground truth .
bleu isdefinedastheproductof n gramprecisionandbrevitypenalty.
as most content descriptions for image based buttons are short we measure bleu value by setting nas represented as bleu bleu bleu bleu .
meteor metricforevaluationoftranslationwithexplicit ordering isanothermetricusedformachinetranslationevaluation.itisproposedtofixsomedisadvantagesofbleuwhichignores theexistenceofsynonymsandrecallratio.rouge recall oriented understudy for gisting evaluation is a set of metric based on recallrate andweuserouge l whichcalculatesthesimilaritybetween predicted sentence and reference based on the longestcommon subsequence short for lcs .
cider consensus basedimage description evaluation uses term frequency inverse documentfrequency tf idf tocalculatetheweightsinreference sentence sijfor different n gram wkbecause it is intuitive to believethatararen gramswouldcontainmoreinformationthana commonone.weusecider d whichadditionallyimplementsalength basedgaussianpenaltyandismorerobusttogaming.we then divide cider d by to normalize the score into the range between and .
we still refer cider d to cider for brevity.
all of these metrics give a real value with range and are usually expressed as a percentage.
the higher the metric score the moresimilarthemachine generatedcontentdescriptionistothe ground truth.
if the predicted results exactly match the groundtruth the score of these metrics is .
we compute these metrics using coco caption code .
.
baselines we set up two state of the art methods which are widely used for image captioning as the baselines to compare with our contentdescription generation method.
the first baseline is to adopt the cnnmodeltoencodethevisualfeaturesastheencoderandadoptalstm long shorttermmemoryunit asthedecoderforgeneratingthecontentdescription .thesecondbaselinealsoadoptthe encoder decoderframework.althoughitadoptsthecnnmodelas 328329330icse may seoul republic of korea jieshan chen chunyang chen zhenchang xing et al.
table the acceptability score as and the standard deviation for completely unseen apps.
denotes p .
.
id package name category installation image based button as m as a1 as a2 as a3 1com.handmark.sportcaster sports 5m 10m .
.
.
.
.
.
.
.
2com.hola.launcher personalization 100m 500m .
.
.
.
.
.
.
.
3com.realbyteapps.moneymanagerfree finance m m .
.
.
.
.
.
.
.
4com.jiubang.browser communication 5m 10m .
.
.
.
.
.
.
.
5audio.mp3.music.player media and video 5m 10m .
.
.
.
.
.
.
.
6audio.mp3.mp3player music and audio m m .
.
.
.
.
.
.
.
7com.locon.housing lifestyle m m .
.
.
.
.
.
.
.
8com.gau.go.launcherex.gowidget.weatherwidget weather 50m 100m .
.
.
.
.
.
.
.
9com.appxy.tinyscanner business m m .
.
.
.
.
.
.
.
10com.jobkorea.app business m m .
.
.
.
.
.
.
.
11browser4g.fast.internetwebexplorer communication m m .
.
.
.
.
.
.
.
12com.rcplus social m m .
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
table examples of generalization.
id e1 e2 e3 e4 e5 button m next song add to favorites open ad previous song clear query a1 change tothenext songinplaylist addthemp3as favorite showmore details about svip palythe former one cleancontent a2 playthenext song like check playthe last song close a3 next like enter last close whichsignificantlyoutperformsthreedevelopersby30.
.
.
.
the evaluator rates .
of labels generated from labeldroid as highly acceptable point as opposed to .
.
.
from three developers.
figure shows that our model behavesbettersinmostappscomparedwiththreehumanannotators.
theseresultsshowthatthequalityofcontentdescriptionfromour modelishigherthanthatfromjuniorandroidappdevelopers.note thattheevaluatorisreliableasboth2intentionalinsertedwrong labels and good labels get and acceptability score as expected.
tounderstandthesignificanceofthedifferencesbetweenfour kindsofcontentdescription wecarryoutthewilcoxonsigned rank test between the scores of our model and each annotator and between the scores of any two annotators.
it is the non parametric version of the paired t test and widely used to evaluate the difference between two related paired sample from the same probability distribution.
the test resultssuggest that the generated labels from our model are significantlybetter than that of developers p value .
for a1 a2 and .
for a3 .
for some buttons the evaluator gives very low acceptability scoreto thelabelsfrom developers.according toourobservation we summarise four reasons accounting for those bad cases and give some examples in table .
some developers are prone to write long labels for image based buttons like the developer a1.
although the long label can fully describe the button e1 e2 it is too verbose for blind users especially when there are many imagebased buttons within one page.
some developers give too short labelswhichmaynotbeinformativeenoughforusers.forexample a2 and a3 annotate the add to favorite button as like e2 .
since this button will trigger an additional action add this song to favoritelist like couldnotexpressthismeaning.thesamereason 6thep valuesareadjustedbybenjamin hochbergmethod .alldetailedp values are listed in enough information.
some manual labels may be ambiguous which may confuse users.
for example a2 and a3 annotate play thelastsong or last to previoussong button e4 whichmay misleadusersthatclickingthisbuttonwillcometothefinalsong in the playlist.
developers may make mistakes especially when theyareaddingcontentdescriptionstomanybuttons.forexample a2 a3 use close to label a clear query buttons e5 .
we further manuallycheck135low quality acceptabilityscore labelsfrom annotatorsintothesefourcategories.18casesareverboselabels ofthemareuninformative sixcasesareambiguouswhichwould confuse users and the majority cases are wrong.
we also receive some informal feedback from the developers and the evaluator.
some developers mention that one image based buttonmayhavedifferentlabelsindifferentcontext buttheyare not very sure if the created labels from them are suitable or not.
mostofthemneverconsideraddingthelabelstouicomponents duringtheirappdevelopmentandcurioushowthescreenreader works for the app.
all of them are interested in our labeldroid andtellthattheautomaticgenerationofcontentdescriptionsfor iconswilldefinitelyimprovetheuserexperienceinusingthescreen reader.
all of these feedbacks indicate their unawareness of app accessibility and also confirm the value of our tool.
related work mobile devices are ubiquitous and mobile apps are widely usedfor different tasks in people s daily life.
consequently there are manyresearchworksforensuringthequalityofmobileapps .
most of them are investigating the apps functional and non functional properties like compatibility performance energy efficiency gui design gui animation linting localization andprivacyandsecurity 331unblind your apps predicting natural language labels for mobile gui components by deep learning icse may seoul republic of korea .
however few of them are studying the accessibility issues especiallyforuserswithvisionimpairmentwhichisfocusedinour work.
.
app accessibility guideline google and apple are the primary organizations that facilitate mobiletechnologyandtheappmarketplacebyandroidandios platforms.withtheawarenessoftheneedtocreatemoreaccessibleapps bothofthemhavereleaseddeveloperanddesignerguidelines for accessibility which include not only the accessibility designprinciples butalso thedocumentsfor usingassistive technologies embedding in the operating system and testing tools or suits for ensuring the app accessibility.
the world wide web consortium w3c hasreleasedtheirwebaccessibilityguideline long time ago and now they are working towards adapting theirwebaccessibilityguideline byaddingmobilecharacteristics into mobile platforms.
although it is highly encouraged to followtheseguidelines theyareoftenignoredbydevelopers.different from these guidelines our work is specific to users with vision impairmentandpredictsthelabelduringthedevelopingprocess without requiring developers to fully understand long guidelines.
.
app accessibility studies for blind users many works in human computer interactionarea have explored the accessibility issues of small scale mobile apps in different categories such as in health smart cities and governmentengagement .althoughtheyexploredifferentaccessibility issues the lack of descriptions for image based components has beencommonlyexplicitlynotedasasignificantprobleminthese works.
park et al rated the severity of errors as well as frequency and missing labels is rated as the highest severity of tenkinds of accessibility issues.
kane et al carry out a study of mobiledeviceadoptionandaccessibilityforpeoplewithvisualand motor disabilities.
ross et al examine the image based button labeling in a relative large scale android apps and they specify some common labeling issues within the app.
different from their works our study includes not only the largest scale analysis of image basedbuttonlabelingissues butalsoasolutionforsolving those issues by a model to predict the label of the image.
there are also some works targeting at locating and solving the accessibility issues especially for users with vision impairment.
eleretal developanautomatedtestgenerationmodeltodynamicallytestthemobileapps.zhangetal leveragethecrowd sourcemethodtoannotatetheguielementwithouttheoriginal content description.
for other accessibility issues they further develop an approach to deploy the interaction proxies for runtime repair and enhancement of mobile application accessibility withoutreferringtothesourcecode.althoughtheseworkscanalso helpensurethequalityofmobileaccessibility theystillneedmucheffortfromdevelopers.instead themodelproposedinourworkcan automaticallyrecommendthelabelforimage basedcomponents and developers can directly use it or modify it for their own apps.
.
app accessibility testing tools it is also worth mentioning some related non academic projects.
there are mainly two strategies for testing app accessibility forusers with vision impairment such as manual testing and auto mated testing with analysis tools.
first for manual testing the developerscanusethebuilt inscreenreaders e.g.
talkback for android voiceover for ios to interact with their android device without seeing the screen.
during that process developers can find out if the spoken feedback for each element conveys its purpose.similarly theaccessibilityscannerapp scansthespecifiedscreenandprovidessuggestionstoimprovetheaccessibility ofyourappincludingcontentlabels clickableitems colorcontrast etc.
the shortcoming of this tool is that the developers must run it ineachscreenoftheapptogettheresults.suchmanualexplorationoftheapplicationmightnotscaleforlargerappsorfrequenttesting anddevelopersmaymisssomefunctionalitiesorelementsduring the manual testing.
second developerscanalsoautomateaccessibilitytasksbyresortingtestingframeworkslikeandroidlint espressoandrobolec tric etc.theandroidlint isastatictoolforcheckingallfilesof an android project showing lint warnings for various accessibility issuesincludingmissingcontentdescriptionsandprovidinglinks totheplacesinthesourcecodecontainingtheseissues.apartfrom the static analysis tools there are also testing frameworks such as espresso and robolectric which can also check accessibilityissuesdynamicallyduringthetestingexecution.andthere are counterparts for ios apps like earl grey and kif .
note thatallofthesetoolsarebasedonofficialtestingframework.for example espresso robolectric and accessibility scanner are based on android s accessibility testing framework .
although all of these tools are beneficial for the accessibility testing therearestillthreeproblemswiththem.first itrequires developers well awareness or knowledge of those tools and understanding the necessity of accessibility testing.
second all these testing are reactive to existing accessibility issues which may have alreadyharmedtheusersoftheappbeforeissuesfixed.inadditiontothesereactivetesting wealsoneedamoreproactivemechanism ofaccessibilityassurancewhichcouldautomaticallypredictsthe contentlabelingandreminds thedeveloperstofillthem intothe app.thegoalofourworkistodevelopaproactivecontentlabeling model which can complement the reactive testing mechanism.
conclusion and future work more than apps have at least one image based button without natural language label which can be read for users with vision impairment.
considering that most app designers and developers are of no vision issues they may not understand how to write suitable labels.toovercomethisproblem weproposeadeeplearningmodel based on cnn and transformer encoder decoder for learning to predict the label of given image based buttons.
we hope thatthis work caninvoking the communityattention inappaccessibility.inthefuture wewillfirstimproveourmodel for achieving better quality by taking the app metadata into the consideration.second wewillalsotrytotestthequalityofexisting labels by checking if the description is concise and informative.