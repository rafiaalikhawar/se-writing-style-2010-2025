librelog accurate and efficient unsupervised log parsing using open source large language models zeyang ma software performance analysis and reliability spear lab concordia university montreal quebec canada mzeyang encs.concordia.cadong jae kim depaul university chicago illinois usa djaekim086 gmail.comtse hsun peter chen software performance analysis and reliability spear lab concordia university montreal quebec canada peterc encs.concordia.ca abstract log parsing is a critical step that transforms unstructured log data into structured formats facilitating subsequent log based analysis.
traditional syntax based log parsers are efficient and effective but they often experience decreased accuracy when processing logs that deviate from the predefined rules.
recently large language models llm based log parsers have shown superior parsing accuracy.
however existing llmbased parsers face three main challenges time consuming and labor intensive manual labeling for fine tuning or in context learning increased parsing costs due to the vast volume of log data and limited context size of llms and privacy risks from using commercial models like chatgpt with sensitive log information.
to overcome these limitations this paper introduces librelog an unsupervised log parsing approach that leverages open source llms i.e.
llama3 8b to enhance privacy and reduce operational costs while achieving state of the art parsing accuracy.
librelog first groups logs with similar static text but varying dynamic variables using a fixed depth grouping tree.
it then parses logs within these groups using three components i similarity scoring based retrieval augmented generation selects diverse logs within each group based on jaccard similarity helping the llm distinguish between static text and dynamic variables ii self reflection iteratively query llms to refine log templates to improve parsing accuracy and iii log template memory stores parsed templates to reduce llm queries for improved parsing efficiency.
our evaluation on loghub .
shows that librelog achieves higher parsing accuracy and processes logs .
times faster compared to state of the art llmbased parsers.
in short librelog addresses privacy and cost concerns of using commercial llms while achieving state ofthe arts parsing efficiency and accuracy.
i. i ntroduction real world software systems generate large amounts of logs often hundreds of gigabytes or even terabytes per day .
these logs provide developers with invaluable runtime information essential for understanding system execution and debugging.
to manage and analyze this vast amount of data researchers and practitioners have proposed many automated approaches such as monitoring anomaly detection and root cause analysis .
however as shown in figure logs are semi structured containing a mixture of static text and dynamically generated variables e.g.
port number which makes direct analysis challenging.
log parsing is a critical first step in log analysis that transforms unstructured logs into log templates dividing logs parsed templatesdatetimelevelprocesscomponentlogtemplate2015 52infomainorg.apache.hadoop.http.httpserver2jetty bound to port 52infomainorg.apache.hadoop.http.httpserver2jetty bound to port 52infomainorg.apache.hadoop.yarn.webappsweb app started at 53infomainorg.apache.hadoop.app.rmcontainerrequestornodeblacklistingenabled 53infoipcserverorg.apache.hadoop.ipc.serveripc server listener on startinglog messages2015 51infomainorg.apache.hadoop.http.httpserver2jetty bound to port 51infomainorg.apache.hadoop.http.httpserver2jetty bound to port 52infomainorg.apache.hadoop.yarn.webappsweb app mapreduce started at 53infomainorg.apache.hadoop.
app.rmcontainerrequestornodeblacklistingenabled true2015 53infoipc server org.apache.hadoop.ipc.serveripc server listener on startinglog parsingfig.
.
an example of log parsing result from hadoop.
into static parts static messages and dynamic parts variables .
as illustrated in figure log templates represent the event structure of logs providing a standardized format that simplifies further analysis.
by distinguishing between static and dynamic components log parsing enables more efficient and accurate downstream tasks .
given the sheer volume and diversity of generated logs prior research has proposed various syntax based parsers for efficient and effective log parsing.
these parsers such as drain and ael use manually crafted heuristics or predefined rules to identify and extract log templates.
although promising these log parsers often experience decreased accuracy when processing logs that deviate from predefined rules .
recent advances in large language models llms have enabled researchers to leverage these models for log parsing .
llms exhibit superior capabilities in understanding and generating text making them particularly effective for parsing semi structured log data.
consequently llm based log parsers often achieve higher accuracy than traditional syntax based parsers .
however the sheer volume of log data and the limited context size of llms lead to increased parsing costs both in terms of time and money as token consumption grows linearly with log size.
this makes practical adoption challenging.
additionally these parsers frequently require manually derived log template pairs for in context learning adding significant manual overhead.
a further complication arises from the reliance on commercial llms like chatgpt by many llm based log parsers nov .
while powerful using commercial models poses potential privacy risks as logs often contain sensitive information about the software s runtime behavior and data.
uploading logs and other sensitive information e.g.
code for refactoring and bug fixes to commercial llms can expose a company s sensitive data to potential privacy breaches .
to address these challenges we propose an unsupervised log parsing technique librelog which does not need any manual labels.
librelog leverages smaller size open source llms e.g.
llama3 8b to enhance privacy and reduce operational costs while achieving state of the art parsing accuracy and efficiency.
inspired by the effective grouping capabilities of syntax based unsupervised log parsing methods librelog first groups logs that share syntactic similarity in the static text but vary in the dynamic variable using a fixed depth grouping tree.
then librelog parses logs within individual groups through three key steps i librelog uses similarity scoring based retrieval augmented generation rag to select the most diverse logs based on jaccard similarity within each log group.
this step helps llms separate dynamic and static text by highlighting variability in dynamic variables among logs in the same group.
ii librelog uses self reflection to improve llm responses thereby improving parsing results.
iii librelog uses log template memory to store parsed log templates.
this approach allows logs to be parsed by first matching them with stored templates minimizing the number of llm queries and significantly enhancing parsing efficiency.
the paper makes the following contributions we introduce librelog an unsupervised log parsing technique that effectively addresses the limitations of existing llm based and syntax based parsers.
librelog employs open source llms specifically llama3 8b to enhance data privacy and reduce operational costs associated with commercial models.
through extensive evaluations on over million logs from loghub2.
librelog demonstrated a or higher parsing accuracy compared to state of the art llm based log parsers i.e.
lilac and llmparser .
moreover it is .
to times faster showcasing its superior efficiency and effectiveness.
librelog s self reflection mechanism helps improve parsing accuracy by over showcasing the effectiveness of our prompting technique.
our experiment using four small size llms shows that llama3 8b achieves the best overall result highlighting its potential in log analysis.
in short the paper provides a novel unsupervised log parsing approach that is both efficient and effective while ensuring data privacy and reducing operational costs.
paper organization.
section ii discusses background and related work.
section iii provides the design details of librelog.
section iv outlines evaluation setup.
section v presents evaluation results.
section vi discusses threats to validity.
section vii concludes the paper.
data availability we made our source code and experimentalresults publicly available at .com zeyang919 librelog ii.
b ackground and related work in this section we discuss the background of llm and its privacy concerns.
we then discuss related log parsing research.
a. background large language models.
large language models llms primarily built on the transformer architecture have significantly advanced the field of natural language processing nlp .
these llms such as the widely recognized gpt3 model with its billion parameters are trained on diverse text data from various sources including source code.
the training involves self supervised learning objectives that enable these models to develop a deep understanding of language and generate text that is contextually relevant and semantically coherent.
llms have shown substantial capability in tasks that involve complex language comprehension and generation such as code recognition and generation .
due to logs being semi structured texts composed of natural language and code elements researchers have adopted llms to tackle log analysis tasks such as anomaly detection root cause analysis and log parsing .
log parsing is one of the primary tasks of focus in this area given its crucial role for more accurate and insightful downstream log analysis .
privacy issues related to llm.
while llms demonstrate remarkable capabilities in processing and generating natural language and code their application on sensitive data such as logs presents notable privacy risks particularly with commercial models such as chatgpt .
one major concern is that data transmitted to these models such as system logs could be retained and used in the model s further training cycles without explicit consent or knowledge of the data owners .
more importantly sensitive data uploaded to the llm providers could potentially be exposed through inadvertent data leaks or malicious attacks posing significant privacy risks.
to avoid such risks an industry norm is to restrict the use of commercial llms despite their advanced capabilities.
for example samsung bans chatgpt and other commercial chatbots after a sensitive code leak .
major financial institutions like citigroup and goldman sachs have restricted the use of chatgpt due to concerns over data privacy and security .
in contrast open source llms such as those developed by meta s llama series offer greater privacy and security.
users can adopt the llms for local deployment to ensure data privacy aligning with stringent data protection standards.
thus open source llms are more secure and trustworthy for handling confidential data such as logs .
b. related work current automated log parsers can be broadly categorized into two types syntax based log parsers and semantic based log parsers.
syntax based log parsers typically employ heuristic rules or conduct comparisons among logs 2to identify common components that serve as templates.
semantic based log parsers focus on analyzing the textual content within logs to distinguish between static and dynamic segments i.e.
using llms thereby deriving the log templates.
semantic based parsers often require a datadriven approach to better grasp the semantic nuances inherent in the specific system logs they analyze.
below we discuss related work and the limitations of these two groups of parsers.
syntax based log parsing approaches.
syntax based log parsers generally utilize manually crafted heuristics or compare syntactic features between logs to extract log templates.
different from general text data log messages have some unique characteristics.
heuristic based log parsers extract log templates by identifying features in the logs.
for example ael uses heuristics to remove potential dynamic variables and extract log templates.
drain employs a fixed depth parsing tree structure alongside specifically designed parsing rules i.e.
top k prefix tokens to identify common templates.
however these log parsers often suffer from decreased accuracy when processing logs that do not conform to the predefined rules.
logs with the same log template share the same static messages in the log.
based on this observation several log parsers leverage frequent pattern mining to parse the logs by identifying common textual content within logs.
for instance spell uses the longest common subsequence to parse logs and logram identifies frequent n gram patterns within logs using these recurring patterns to parse logs.
while these frequent pattern mining based parsers do not require manually defined rules the templates they generate are highly dependent on the structure of the input logs.
logs with complex structures may lead to poor frequent pattern mining results resulting in low parsing accuracy.
in short while syntax based parsers benefit from simplicity and efficiency in identifying common templates their performance varies depending on the structure of logs.
semantic based log parsing approaches.
semantic based log parsers use language models to analyze the semantics of the log messages for log parsing.
recently they have shown superior parsing accuracy compared to syntax based log parsers largely due to significant advancements in language models.
for instance models like chatgpt can analyze the context of log messages and dynamically generate log templates without prior knowledge enhancing accuracy and adaptability across different log formats.
divlog enhances log parsing by extracting similar logs from a candidate set of labeled logs for in context learning using gpt .
due to the high cost of commercial llms such as chatgpt lilac enhances the efficiency of llm based log parsing by incorporating an adaptive parsing cache that stores parsing results.
lilac adopts in context learning with log parsing demonstrations i.e.
manually created log templates for enhanced parsing accuracy.
some parsers also aim to use open source llms for log parsing.
hooglle adopted an llm pre trained on labeledlogs for log parsing.
logppt utilizes a masked language model roberta and adopts few shot learning to classify tokens in log messages based on few shot examples.
as an initial attempt to apply llms for log parsing logppt showed improved accuracy over traditional syntax based log parsers.
llmparser explores the performance of various llms after a few shot fine tuning on log parsing.
results indicate that fine tuning small open source llms with a few demonstrations can also achieve high log parsing accuracy.
although the results are promising recent works in semantic based log parsers have two main limitations privacy and monetary costs of using commercial llms and requiring manually derived log templates for llms to learn .
first most log parsers are based on commercial llms such as chatgpt which makes real world adoption a challenge due to the privacy issues and monetary costs of parsing large volumes of logs.
second many parsers especially the ones that aim to improve efficiency and accuracy e.g.
lilac or the ones that use smaller open source models e.g.
logppt and llmparser require some log template pairs as the demonstration.
deriving such templates requires significant manual efforts and the provided demonstrations may affect the parser s accuracy on logs with unseen templates.
in this paper we propose librelog that addresses the two above mentioned limitations.
we deployed a relatively small open source llm i.e.
llama3 8b on log parsing to avoid privacy issues and monetary costs.
additionally librelog enhances llm based log parsing by capitalizing on the commonalities and variabilities within logs to provide a demonstration free prompt for the llm.
iii.
a pproach in this section we introduce librelog an efficient unsupervised log parser leveraging memory capabilities and advanced prompting techniques to maximize efficiency and parsing accuracy.
librelog leverages a smaller size opensource llm to enhance privacy and reduce operation costs.
figure illustrates the overall architecture of librelog which primarily comprises of three components i log grouping which groups logs that share a commonality in their text.
such log groups can then be used as input to llm to uncover dynamic variables.
ii an unsupervised llmbased log parser that uses retrieval augmented generation rag followed by an iterative self reflection mechanism to accurately parse the grouped logs into log templates.
iii anefficient log template memory which memorizes the parsed log templates for future query.
the core idea is to enhance efficiency by storing parsed log templates in memory thereby avoiding the need for repeated llm queries.
a. log grouping based on commonality librelog achieves unsupervised and zero shot log parsing by first applying an effective grouping strategy.
this strategy aims to group logs that share commonality in their static text yet are different in their dynamic variables.
such log groups 3root length 3length 4length send receive from fromreceivemachine message fromcheck status group group group group grouped logs using fixed depth treelog retrieval and template memorization log groups efficient template search and matching grouped logs retrieving unmatched logslog groupsreceive message from .
retrieval augmented generation to parse logs and generate template self reflection to iteratively fix incorrectly generated template unsupervised log parsing using large language modelstemplate memory receive from .
?
send from .
?
open sourced large language model how can i parse these system runtime logs?
logs send from .
receive from .
receive message from .
machine check status from .3parsed log templatelog template memorizationrecalled log template parsed incoming logs into log templatefig.
.
an overview of librelog.
can then be used as input to llms to generate log templates by prompting llms to identify the dynamic variables among logs in the same group.
to group the logs we adapt the efficient unsupervised methodology proposed by drain which applies a fixed depth parsing tree and parsing rules i.e.
kprefix tokens to identify log groups.
the fixed depth in our grouping tree provides a structured and predictable framework that enhances efficiency.
by limiting the depth we reduce the complexity of the tree traversal which speeds up the grouping process.
our fixed depth tree implementation for grouping consists of three key steps i group by length ii group by k prefix tokens and iii group by token string similarity .
in step i we first group the logs based on token length which partitions the logs into subsets of logs that are similar in token length.
this initial grouping significantly reduces the computational complexity in the subsequent grouping phases.
in step ii the grouped logs are then kept at a fixed depth which stores kprefix tokens.
since logs are initially grouped based on token length truncating kprefix tokens default the first three tokens of the log can limit the number of nodes visited during the subsequent traversal process for step iii significantly improving grouping efficiency.
prior to step iii it is important to note that we abstract the numerical literals in the logs with a wildcard symbol .
this is done to prevent the issue of grouping explosion in step iii which can make grouping inefficient.
finally in step iii we calculate the similarity between the new logs and the log groups stored in the fixeddepth tree.
this step determines whether the incoming log fits into an existing group or necessitates the creation of a new log group.
if a suitable group is found based on the similarity threshold i.e.
of common tokens total number of tokens .
the log is inserted into existing log groups.
if not a new group is created and the treeis dynamically updated to accommodate this new log pattern.
this adaptive approach ensures that our system evolves with the incoming data continuously optimizing both the accuracy and efficiency of the log grouping process.
b. llm based unsupervised log parsing our prompts to llms contain representative logs based on variability retrieved from each log group from section iii a to guide llms in separating dynamic variables and static text.
figure illustrates the prompt template that librelog uses.
below we discuss the composition of our prompt in detail.
prompt instruction.
in the instruction part of our prompt we define the goal of the log parsing task to the llm highlighted in green in figure .
we emphasize that all the provided logs should share one common template that matches all selected logs.
this specification is crucial to ensure that the llm can effectively identify the commonalities and variability within the provided logs thereby preventing any difficulties in parsing due to inconsistent log templates.
standardizing llm response by input and output example.
since our llm is not instruction fine tuned it is crucial to clearly describe our task instruction and include an inputoutput example in the prompt.
this explicit guidance helps the llm understand the desired input and output formats.
as shown in figure we provide one example to illustrate the input output form.
the example remains unchanged for all systems.
this approach effectively guides the llm in understanding the objective and input output formats without the need of instruction fine tuning or labeled data.
retrieval augmented log parsing.
to parse logs accurately we select representative logs that showcase variabilities within a log group based on commonality.
by presenting the llm with logs sharing the same structure but varying in dynamic instruction you will be provided with a list of logs.
you must identify and abstract all the dynamic variables in logs with and output one static log template that matches all the logs.
print the input logs template delimited by backticks.
standardizing llm response by input and output example log list try to connected to host .
.
.
finished.
try to connected to host .
.
.
finished.
log template try to connected to host finished.
retrieval augmented log parsing log list times total boot init finish times total boot init finish times total boot init finish prompt log template times total boot init finish llm responsetimes total .
?
boot .
?
init .
?
finish .
?
generated log templatellm querytemplate extractionfig.
.
example of the prompt template used for librelog.
the green block illustrates the task instruction provided to the llm.
the blue block highlights the input and output examples used to standardize the log response format.
the yellow block depicts the retrieval augmented selection process that enhances log parsing accuracy by incorporating representative variability.
variables it can more effectively distinguish between fixed and dynamic elements to identify the log template.
we developed a retrieval argument generation rag approach based on jaccard similarity .
jaccard similarity measures the similarity between two given sets by calculating the ratio of the number of elements e.g.
tokens in their intersection to the number of elements in their union.
for log data each log is split into a set of tokens i.e.
words and then these tokens are used to determine the sizes of the intersection and union.
the resulting ratio is the jaccard similarity between two given logs with a ratio closer to one indicating higher similarity.
we aim to identify the logs with the greatest variability within the same group.
hence we select logs with the lowest jaccard similarity score.
this approach helps create accurate log templates by focusing on logs that are most indicative of the entire group s characteristics.
our selection process starts by selecting the longest log based on the number of characters within the group as the initial reference.
we then calculate the jaccard similarity between this log and every other log in the group.
the log with the lowest similarity to the reference log is added to the selection set.
we continue computing pairwise jaccard similarity between the selected logs and the remaining unselectedlogs sequentially adding the log with the lowest similarity.
this iterative process is repeated until k default k logs have been selected ensuring the selected logs effectively represent both the commonality and diversity within the log group.
given the computation costs and to ensure efficiency we randomly select at most logs from each group or all the logs if the number is less than for our log selection process.
specifically the logs selected from the log group are listed in the format of a python list within the prompt for parsing.
we use a prefix i.e.
log list to help the llm identify the logs that require parsing highlighted in yellow in figure .
this consistency in input format mirroring the input and output example also guides the llm to respond with the log template in a fixed format as demonstrated in the example facilitating accurate template generation and extraction.
post processing template standardization.
we use a postprocessing technique to further standardize the log template generated by llm.
we employ string manipulation techniques to remove non template content from the response i.e.
prefixes and backticks .
to facilitate the verification of the accuracy of log templates we replace the placeholder within the templates with the regular expression pattern .
?
.
the regex template enables a direct matching process when comparing the generated templates with logs and can be directly applied to abstract logs.
self reflection for verifying log template.
after generating a log template we verify whether the template can match each log within the group.
if a log is correctly matched by a log template we consider it to be parsed successfully.
the log template is then added to the log template memory for future use.
after all logs in the group have been checked any unparsed logs undergo a self reflection process which aims to revise the templates and improve parsing results.
similar to the initial parsing attempt we first select these unparsed logs and then utilize the prompt described in figure to generate a new log template using llms.
this step is repeated until all logs in the group can be matched parsed by the generated templates.
note that to prevent the llm from entering a parsing loop i.e.
repeatedly generating incorrect templates we limit the self reflection process to three iterations.
c. template memory for efficient log parsing repeatedly using llm to parse logs with identical groupings and templates significantly increases the frequency of llm queries thereby reducing the efficiency of the log parsing process.
to address this issue we introduce log template memory in librelog which stores the parsed log templates for future parsing avoiding redundant llm queries.
efficient log template memory search and matching.
when a log group requires parsing we first check whether a matching log template exists within the memory.
if some logs within the group find a matching template in the memory we apply this log template to parse the logs mitigating the need for llm queries.
however it is possible that some logs within the same group may match while others may not e.g.
due 5to limitations in the grouping step or limitation of the log template .
hence the logs that remain unparsed are then sent to llm for parsing.
the new log template generated from this process is then added to the log template memory for future reference.
this design significantly reduces the number of llm queries during the log parsing process.
to efficiently utilize log templates in the memory there is a need for an efficient search mechanism to verify whether or not the given logs match existing log templates in the memory.
this is crucial since the memory can be large consisting of many log templates.
for every log we need potentially at most nsearches for nlog templates.
to improve efficiency we put forward one key observation the token length of log templates is always less than or equal to that of the original logs as multiple tokens may be treated as a single variable during log parsing.
for instance consider the log sent bytes data .
after parsing the corresponding log template is generated as sent data .
the original log consists of four tokens whereas the parsed template has three.
this reduction in token count occurs because bytes is treated as a single variable thus decreasing the overall length of the template compared to the original log.
consequently when searching for log templates in the memory we first sort the templates based on the number of tokens.
this sorting allows us to efficiently check new logs by first calculating the token length of the log to be parsed then using binary search to find all templates with a token count less than or equal to the log length.
this design reduces the number of match checks required from o n too logn thereby enhancing the efficiency of the search process.
our log template matching process is efficient.
unlike traditional log templates that use placeholders i.e.
to abstract dynamic variables within logs we store log templates in memory as regular expression patterns i.e.
use .
?
instead of placeholders .
this adjustment allows us to use regular expressions to efficiently verify whether logs match with log templates in memory and improve matching efficiency.
iv.
e xperiment setup in this section we discuss our experiment setup to answer our research questions and librelog s implementation details.
studied dataset.
we conduct our experiment on the log parsing benchmark loghub .
provided by he et al.
.
this benchmark contains logs from open source systems of different types such as distributed systems supercomputer systems and server side applications.
loghub is widely used to evaluate and compare the accuracy of log parsers .
compared to loghub .
the number of logs has increased significantly in loghub .
increasing from 28k 2k logs per system to more than million logs with a total of different log templates.
loghub .
also provides the groundtruth log template for each log.
with this large scale loghub .
dataset researchers can better evaluate the efficiency and effectiveness of log parsers .
environment and implementation.
our experiments were conducted on an ubuntu server with an nvidia tesla a100gpu amd epyc core cpu and 256gb ram using python .
.
we execute the baselines using their default parameters under the same environment to compare the efficiency.
we use llama3 8b for librelog s underlying llm because it is a relatively small yet powerful model balancing performance and efficiency effectively.
we set the temperature value to to improve the stability of the model output.
note that it is easy to switch to other llms.
in rq4 we evaluate librelog by replacing llama3 with other open source llms.
evaluation metrics for log parsing.
following prior studies we use two most commonly used metrics to evaluate the effectiveness of log parsers group accuracy and parsing accuracy.
group accuracy ga grouping accuracy is a metric used in log parsing to evaluate the extent to which log messages belonging to the same template are correctly grouped together by a parser.
ga is defined as the ratio of correctly grouped log messages to the total number of log messages.
for a log message to be considered correctly grouped it must be assigned to the same group as other log messages that share the same underlying template.
high ga indicates that the parser can effectively discern patterns within the log data and group similar log messages together.
this can be crucial for various downstream log analysis tasks such as anomaly detection .
despite its usefulness ga has limitations.
ga can remain high even if the parsed templates are flawed.
namely a high ga score might obscure errors in dynamic variable extraction and template identification within the logs leading to a misleading perception of overall parsing accuracy.
parsing accuracy pa parsing accuracy pa complements ga and is calculated as the ratio of accurately parsed log messages to the total number of log messages.
for a log message to be deemed correctly parsed both extracted static text and dynamic variables must match exactly with those specified in the ground truth.
pa is a stricter metric because it requires a comprehensive match of all log components not just their correct grouping.
this distinction is crucial as ga primarily evaluates the correct clustering of logs while pa ensures precise parsing accuracy at the individual log message level.
precise log parsing of the variables can also significantly impact the effectiveness of downstream log based analyses .
v. e valuation in this section we evaluate librelog by answering four research questions rqs .
rq1 what is the effectiveness of librelog?
motivation.
accuracy is the most critical factor for evaluating the effectiveness of log parsers.
high accuracy in log parsing aids downstream log analysis tasks .
in this rq we study the effectiveness of librelog.
approach.
we compare librelog with other state ofthe art log parsers including ael drain lilac and llmparser t5base .
ael and drain are two leading traditional syntax based approaches that are efficient and 6table i acomparison of the grouping accuracy ga and parsing accuracy pa for the state of the art parsers and libre log.
ael drain lilac llmparser t5base librelog ga pa ga pa ga pa ga pa ga pa hdfs .
.
.
.
.
.
.
.
.
.
hadoop .
.
.
.
.
.
.
.
.
.
spark .
.
.
.
.
.
.
.
zookeeper .
.
.
.
.
.
.
.
.
.
bgl .
.
.
.
.
.
.
.
.
.
hpc .
.
.
.
.
.
.
.
.
.
thunderbird .
.
.
.
.
.
.
.
.
.
linux .
.
.
.
.
.
.
.
.
.
healthapp .
.
.
.
.
.
.
.
.
.
apache .
.
.
.
.
.
.
.
.
.
proxifier .
.
.
.
.
.
.
.
.
.
openssh .
.
.
.
.
.
.
.
.
.
openstack .
.
.
.
.
.
.
.
.
.
mac .
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
note the highest values of ga and pa for each system are highlighted in bold .
the accuracy of ael on the spark dataset is excluded because it cannot complete parsing the whole dataset after running for days.
perform better than most other syntax based parsers .
lilac and llmparser t5base are recently proposed llm based parsers with high parsing accuracy.
since lilac uses chatgpt as the underlying llm for a fair comparison we replace chatgpt with the same open source llm llama3 8b that librelog uses.
we use t5 base 240m parameters as the llm for llmparser by following the prior work.
note that both lilac and llmparser require manually derived log templates as a few shot demonstrations.
we follow the steps described in the papers to obtain these demonstrations.
we evaluate the parsers using the loghub .
dataset and report both grouping accuracy ga and parsing accuracy pa .
results.
table i shows the ga and pa for each log parser across different systems.
librelog achieved the highest ga and pa values for most systems indicating superior performance in both grouping and parsing logs.
across all systems librelog achieved an average ga of .
and an average pa of .
outperforming all other parsers.
librelog shows superior ga and pa compared to the semi supervised llm based parser lilac.
compared to librelog lilac demonstrated lower performance with a ga of .
and pa of .
.
lilac uses manually labeled logs as demonstrations for in context learning to enhance parsing accuracy.
however when utilizing less powerful opensource llms with smaller parameter sizes i.e.
as opposed to chatgpt lilac s performance declines significantly due to the limited ability of these models to capture complex log patterns with only a few demonstrations.
consequently this can lead to inaccurate parsing of variables within the logs a pa of .
while librelog s pa is .
.
unlike lilac and llmparser t5base librelog is an unsupervised log parser eliminating the need for labeled logs to enhance the llm s log parsing capabilities.
the performance of librelog is not dependent on the number of labeled logs thus avoiding the limitations faced by semi supervised approaches that require labeled logs for fine tuning or in context learning.among all three llm based log parsers llmparser t5base shows the lowest ga and the reason may be the limited number of fine tuning samples that makes it hard to generalize to large scale datasets.
among the three llm based log parsers llmparser t5base lilac and librelog llmparser t5base exhibited the lowest ga of .
and the second highest pa of .
.
when parsing large scale datasets logs may exhibit many different variations even if they share the same log template.
given that llmparser t5base is fine tuned using a small labeled sample set from the target system the limited number of log samples likely contributes to its inability to robustly identify logs with the same template across all instances and thus lower ga. this limitation becomes particularly evident in systems with more logs such as bgl and spark where llmparser t5base struggles to achieve high ga .
and .
respectively .
nevertheless it is still able to identify all dynamic variables in a log with the second highest pa among all five parsers which shows the potentials of llm based parsers.
syntax based log parsers generally have significantly lower pas compared to llm based parsers showing challenges in accurately identifying variables.
while ael and drain as syntax based parsers show results similar to each other they both exhibit lower ga compared to librelog .
and .
lower respectively and significantly lower pa .
and .
lower respectively .
this performance disparity is likely linked to their heuristic based nature which relies on predefined rules to identify log features.
while these rules can effectively classify logs with similar features achieving reasonable gas their generic nature often fails to accurately recognize variables within different log templates leading to poor pas.
in contrast librelog leverages pre grouping and uses memory mechanisms to achieve high ga and its llmbased parsing process accurately identifies variables within grouped logs resulting in superior pa. 7librelog achieves superior ga and pa compared to stateof the art parsers.
despite not relying on labeled logs librelog outperforms other llm based parsers that are semisupervised.
additionally librelog significantly enhances pa compared to syntax based approaches.
rq2 what is the efficiency of librelog?
motivation.
efficiency is crucial in log parsing since it directly impacts the practical usability of the parser in real world applications.
in this rq we study the parsers efficiency.
approach.
we measure the total parsing time required by librelog and its individual components i.e.
llm queries grouping and memory search and the four baseline parsers to process logs from the loghub .
dataset.
results.
librelog is .
and times faster than lilac and llmparser t5base respectively.
table ii shows the parsing time for each log parser across different systems.
librelog spends a total of .
hours to parse logs from all systems million logs which is significantly faster than other llmbased parsers lilac hours and llmparser t5base hours .
the parsing time for librelog is mainly occupied by the llm query time which accounts for .
of the total processing time followed by the grouping time which constitutes .
of the overall duration.
llmparser t5base is the slowest among all llm based parsers because it processes each log individually and the vast quantity of logs linearly increases the number of model queries required.
even with a relatively lightweight model like t5 base which has only million parameters querying to parse the logs individually is still slow and impractical for real world applications.
lilac with its cache design eliminates the need to parse each log individually through an llm significantly speeding up the process compared to llmparser t5base .
however lilac still requires frequent model queries to update the templates in the cache which limits its efficiency.
in contrast librelog optimizes parsing times through its grouping and memory features resulting in superior efficiency.
ael exhibits significant efficiency issues when parsing logs beyond certain sizes while drain maintains high efficiency across all datasets.
ael can parse datasets with fewer than 100k logs within seconds but requires several hours or even days for datasets with over one million logs e.g.
we stopped ael after running for days when parsing the million logs from spark .
this inefficiency is due to ael s reliance on extensive comparisons between logs and identified templates where the parsing time grows exponentially with respect to the number of logs and log templates.
in contrast drain which uses a fixed depth parsing tree is the most efficient parser.
librelog uses a grouping method similar to drain s with a total grouping time amounting to .
hours which is less than drain s total parsing time of .
hours.
this highlights the efficiency of librelog s grouping process.
while there is a slight slowdown due to the additional processing involved .
hours compared to drain s .
hours librelog showssuperior parsing effectiveness compared to drain and is the second fastest log parser among the evaluates parsers.
librelog enhances its efficiency by utilizing grouping and memory components which reduces the number of llm queries.
librelog demonstrates the highest efficiency across llm based parsers.
rq3 how does different settings impact the result of librelog?
motivation.
librelog implements multiple components to achieve effective and efficient log parsing.
in this rq we explore how various settings and configurations affect the performance of librelog.
approach.
there are three general components in librelog that can be adjusted or replaced log selection from each group for prompting the number of selected logs and the inclusion or exclusion of self reflection processes.
to select diverse logs from the log group we use jaccard similarity to measure the similarity between every log pair.
in this rq we also try random sampling and cosine similarity.
furthermore we evaluate how changing the number of selected logs from to impacts the effectiveness.
finally we compare the effect of removing the self reflection component on the efficiency and effectiveness of librelog.
results.
selecting representative logs based on jaccard similarity outperforms using cosine similarity and random sampling.
table iii shows the total time ga and pa of librelog compared to replacing the log selection process with cosine similarity and random sampling.
when employing cosine similarity to select representative logs both ga and pa experienced declines of .
and .
respectively compared to using jaccard similarity.
this indicates that although cosine similarity is shown to be an effective similarity metric for text data it does not necessarily select logs that are representative enough for llm to generalize log templates.
however we notice a slight reduction in execution time .
when using cosine similarity.
similarly using random sampling further reduces the processing time by .
but due to the lack of diversity in the sampled logs both ga and pa are even lower at .
and .
respectively.
although the self reflection mechanism requires additional processing time it significantly enhances the parsing results of librelog.
table iii compares full version librelog and librelog without self reflection in the total execution time ga and pa. excluding the self reflection component from librelog results in a .
reduction in parsing time from around six to three hours .
however removing selfreflection greatly decreases both ga and pa by .
and .
respectively.
this shows that self reflection significantly enhances the parsing effectiveness of librelog although at the expense of increased overhead due to additional llm queries.
therefore in practical applications the inclusion of the selfreflection component in librelog can be determined based on the specific needs of effectiveness or efficiency.
8table ii number of logs and parsing time in seconds for the state of the art first four columns and libre log.
ael drain lilac llmparser t5base librelog log count total time total time total time total time total time llm query time grouping time memory search time hdfs .
.
.
.
.
.
.
.
hadoop .
.
.
.
.
.
.
.
spark days .
.
.
.
.
.
.
zookeeper .
.
.
.
.
.
.
.
bgl .
.
.
.
.
.
.
.
hpc .
.
.
.
.
.
.
.
thunderbird .
.
.
.
.
.
.
.
linux .
.
.
.
.
.
.
.
healthapp .
.
.
.
.
.
.
.
apache .
.
.
.
.
.
.
.
proxifier .
.
.
.
.
.
.
.
openssh .
.
.
.
.
.
.
.
openstack .
.
.
.
.
.
.
.
mac .
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
total .
hours .
hours .
hours .
hours .
hours .
hours .
hours .
hours table iii libre log performance under different settings .
the numbers in the parenthesis indicate the percentage difference compared to the full version of libre log.
total time ga pa librelog .
hours .
.
w cosine similarity .
.
.
.
.
.
w random sampling .
.
.
.
.
.
w o self reflection .
.
.
.
.
.
during retrieval augmented log parsing varying the number of selected logs affects the performance of librelog.
retrieving three logs into the prompt yields the highest effectiveness.
fig shows the librelog performance with variations in the number of logs from a group retrieved into the prompts.
librelog maintains high accuracy across various sample sizes with optimal performance achieved when the sample size is set to three reaching peak values in both ga and pa. notably when the sample size is reduced to one ga and pa drop to .
and .
respectively representing a decline of .
and compared to a sample size of three.
this reduction highlights the challenges llm faces in parsing logs accurately without sufficient comparative data such as multiple log comparisons or labeled logs.
as the sample size increases from one to two both ga and pa show significant improvements peaking when the sample size reaches three.
however further increases in sample size from three to eight result in slight decreases in ga and pa stabilizing around .
and .
respectively.
this suggests that an excess of log samples may introduce noise subsequently lowering performance .
importantly when the sample size reaches both ga and pa decrease compared to a sample size of eight.
this decrease is attributed to prompt truncation caused by an overload of retrieved logs which exceeds the context size of the llm resulting in incomplete input data.
number of selected logs0.
.
.
.
.90accuracy ga pafig.
.
ga and pa of librelog using different numbers of selected logs in the prompt.
using jaccard similarity for log selection and llm selfreflection enhances the parsing result although they come with added overhead.
retrieving more logs within the prompt does not necessarily increase effectiveness in fact the optimal number of logs enables librelog to reach peak accuracy is three.
rq4 what is the effectiveness of librelog with different llms?
motivation.
unlike previous parsers that are based on commercial llms librelog employs open source llm to mitigate privacy concerns and monetary costs.
different llms exhibit varying capabilities due to their distinct architectures and pre training data.
in this rq we evaluate the performance of librelog across various open source llms.
approach.
we selected three other open source models in addition to llama3 8b with similar parameter sizes to compare the log parsing performance with different llms including mistral 7b codegemma 7b and chatglm36b .
these models are commonly used in research and practice.
mistral 7b shows strong text generation capabilities in small model sizes.
codegemma 7b is pre trained on code repositories and tailored for code related tasks.
chatglm3 6b is known for its bilingual conversational abilities.
9table iv parsing performance of libre log using different llm s. total time ga pa llama3 8b .
hours .
.
mistral 7b .
.
.
.
.
.
codegemma 7b .
.
.
.
.
.
chatglm3 6b .
.
.
.
.
results.
table iv shows the parsing performance using various llms.
among all llama3 8b achieved the best overall results.
compared to llama3 8b mistral 7b requires a slightly longer parsing time yet achieves a similar ga and a noticeable decline in pa. mistral 7b is a general model with training objectives and parameter sizes similar to llama3 8b.
however it exhibits a lower pa decreased by .
with comparable results in parsing time and ga. this discrepancy in pa may be attributed to llama3 8b s enhanced pre training data which includes more code and its larger parameter size.
these factors likely contribute to llama3 8b s superior ability to abstract variables within logs.
codegemma 7b has a better parsing speed but both ga and pa face a decline compared to llama3 8b.
codegemma7b completes the parsing of all logs in only .
of the time required by llama3 8b.
however codegemma 7b does not achieve comparably high accuracy indicating that while it is capable of generating log templates that match the logs it struggles to consistently and accurately abstract variables within these logs.
nevertheless using codegemma 7b still achieves higher ga and pa than other llm based parsers lilac and llmparser t5base .
as a conversational model chatglm3 6b shows the worst result in parsing effectiveness and efficiency.
chatglm36b pre trained on a bilingual corpus in chinese and english and optimized for conversations does not include code in its pre training data which may have caused its bad parsing ability.
this prevents chatglm3 6b from generating accurate log templates that can match the logs necessitating increased model queries for self reflection.
consequently the parsing time for chatglm3 6b significantly increases by .
compared to llama3.
despite undergoing extensive selfreflection chatglm3 6b still fails to generate correct log templates.
this leads to inferior results in both effectiveness and efficiency compared to other models illustrating a clear disparity in performance when the pre training background of the model does not match the specific task requirements.
replacing the llm leads to variations in effectiveness and performance.
among the four open source models of similar sizes llama3 8b shows the best overall results.
vi.
t hreats to validity external validity.
data leakage is a potential risk of llmbased log parsers .
although librelog does not involve using labeled logs for fine tuning or in context learning there is a possibility that the llm might have been pre trainedon publicly available log data.
our evaluation dataset with ground truth templates was released on august and llama3 8b training knowledge cutoff from march so the leakage risk should be minimal.
the log format may also affect our result but the datasets used are large and cover logs from various systems in different formats.
future studies are needed to evaluate librelog on logs from other systems.
internal validity.
librelog employs llama3 8b as its base model due to its promising results in many tasks and the relatively small size .
we also compared the results across various open source llms and found differences.
future research is needed to evaluate llm based parsers performance when more advanced llms are released in the future.
the effectiveness of librelog could be influenced by specific parameter settings e.g.
the number of logs selected for prompting .
our evaluations showed that these settings have an impact on the parsing results and discussed the optimal settings.
future studies are needed to evaluate the settings on other datasets.
construct validity.
to mitigate the effects of randomness in evaluating librelog the generation temperature of the model is set to zero.
this adjustment ensures that experiments conducted under the same conditions are repeatable and that the results are stable.
vii.
c onclusion in this paper we introduced librelog an unsupervised log parsing technique utilizing open source llms to effectively address the limitations of existing llm based and syntaxbased parsers.
librelog first groups logs that share a syntactic similarity in the static text but vary in the dynamic variable using a fixed depth grouping tree.
it then parses logs in these groups with three components i retrieval augmented generation using similarity scoring identifies diverse logs within each group based on jaccard similarity aiding the llm in differentiating static text from dynamic variables ii selfreflection iteratively queries llms to refine log templates and enhance parsing accuracy and iii log template memory store parsed templates to minimize llm queries thereby boosting parsing efficiency.
our comprehensive evaluations on loghub2.
a public large scale log dataset demonstrate that librelog achieves an average ga of .
and an average pa of .
outperforming state of the art parsers i.e.
iliac and llmparser by and respectively.
librelog parses logs from all systems million logs in a total of .
hours which is .
and times faster than other llm based parsers this marks a substantial advancement over traditional semantic based and llm based parsers in an unsupervised way confirming the robustness and effectiveness of our approach.
additionally librelog addresses the privacy and cost concerns associated with commercial llms making it a highly efficient and secure solution for practical log parsing needs.
10references samsung bans chatgpt among employees after sensitive code leak.
.forbes .com sites siladityaray samsung bans chatgpt andother chatbots for employees after sensitive code leak .
accessed on .
security and privacy closed source vs open source battle.
.com blue orange digital securityand privacy closed source vs open source battlea8757487040e .
accessed on .
wall street banks are cracking down on ai powered chatgpt bloomberg.
.bloomberg .com news articles citigroup goldman sachs joinchatgpt crackdown fn reports .
accessed on .
introducing meta llama the most capable openly available llm to date.
.meta.com blog meta llama .
accessed on .
samuel abedu ahmad abdellatif and emad shihab.
llm based chatbots for mining software repositories challenges and opportunities.
.
c aicardi l bitsch and s datta burton.
trust and transparency in artificial intelligence.
ethics society opinion.
european commission.
.
tom brown benjamin mann nick ryder melanie subbiah jared d kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell et al.
language models are few shot learners.
advances in neural information processing systems .
jinfu chen weiyi shang ahmed e hassan yong wang and jiangbin lin.
an experience report of generating load tests using log recovered workloads at varying granularities of user behaviour.
in 34th ieee acm international conference on automated software engineering ase pages .
ieee .
xiaolei chen jie shi jia chen peng wang and wei wang.
high precision online log parsing with large language models.
in proceedings of the ieee acm 46th international conference on software engineering companion proceedings pages .
hyung won chung et al.
scaling instruction finetuned language models .
hetong dai heng li che shao chen weiyi shang and tse hsun chen.
logram efficient log parsing using nn gram dictionaries.
ieee transactions on software engineering .
min du and feifei li.
spell online streaming parsing of large unstructured system logs.
ieee transactions on knowledge and data engineering .
.
tkde .
.
.
team glm et al.
chatglm a family of large language models from glm 130b to glm all tools .
pinjia he jieming zhu zibin zheng and michael r. lyu.
drain an online log parsing approach with fixeddepth tree.
in ieee international conference on web services icws pages .
.
icws .
.
.
shilin he jieming zhu pinjia he and michael r lyu.
loghub a large collection of system log datasets towards automated log analytics.
arxiv preprint arxiv .
.
yizhan huang yichen li weibin wu jianping zhang and michael r. lyu.
your code secret belongs to me neural code completion tools can memorize hard coded credentials.
proc.
acm softw.
eng.
fse jul .
.
.
albert q. jiang alexandre sablayrolles arthur mensch chris bamford devendra singh chaplot diego de las casas florian bressand gianna lengyel guillaume lample lucile saulnier l elio renard lavaud marieanne lachaux pierre stock teven le scao thibaut lavril thomas wang timoth ee lacroix and william el sayed.
mistral 7b .
zhen ming jiang ahmed e. hassan parminder flora and gilbert hamann.
abstracting execution logs to execution events for enterprise applications short paper .
in2008 the eighth international conference on quality software pages .
zhihan jiang jinyang liu junjie huang yichen li yintong huo jiazhen gu zhuangbin chen jieming zhu and michael r lyu.
a large scale benchmark for log parsing.
arxiv preprint arxiv .
.
zhihan jiang jinyang liu zhuangbin chen yichen li junjie huang yintong huo pinjia he jiazhen gu and michael r. lyu.
lilac log parsing using llms with adaptive parsing cache.
fse jul .
zanis ali khan donghwan shin domenico bianculli and lionel briand.
guidelines for assessing the accuracy of log message template identification techniques.
in proceedings of the 44th international conference on software engineering icse .
zanis ali khan donghwan shin domenico bianculli and lionel briand.
impact of log parsing on log based anomaly detection.
arxiv .
.
van hoang le and hongyu zhang.
log parsing how far can chatgpt go?
in 38th ieee acm international conference on automated software engineering ase pages .
van hoang le and hongyu zhang.
log parsing with prompt based few shot learning.
in 45th international conference on software engineering software engineering in practice icse .
yukyung lee jina kim and pilsung kang.
lanobert system log anomaly detection based on bert masked language model.
arxiv preprint arxiv .
.
zhenhao li chuan luo tse hsun peter chen weiyi shang shilin he qingwei lin and dongmei zhang.
did we miss something important?
studying and exploring variable aware log abstraction.
in proceedings of the 45th international conference on software engineering 11icse page .
feng lin dong jae kim et al.
when llm based code generation meets the software development process.
arxiv preprint arxiv .
.
jinyang liu junjie huang yintong huo zhihan jiang jiazhen gu zhuangbin chen cong feng minzhi yan and michael r lyu.
scalable and adaptive log based anomaly detection with expert in the loop.
arxiv preprint arxiv .
.
yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov.
roberta a robustly optimized bert pretraining approach.
corr .
yudong liu xu zhang shilin he hongyu zhang liqun li yu kang yong xu minghua ma qingwei lin yingnong dang et al.
uniparser a unified log parser for heterogeneous log data.
in proceedings of the acm web conference pages .
zeyang ma an ran chen dong jae kim tse hsun chen and shaowei wang.
llmparser an exploratory study on using large language models for log parsing.
inproceedings of the ieee acm 46th international conference on software engineering icse .
isbn .
.
.
.
lingbo mo boshi wang muhao chen and huan sun.
how trustworthy are open source llms?
an assessment under malicious demonstrations shows their vulnerabilities.
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages june .
devjeet roy xuchao zhang rashi bhave chetan bansal pedro las casas rodrigo fonseca and saravan rajmohan.
exploring llm based agents for root cause analysis.
in companion proceedings of the 32nd acm international conference on the foundations of software engineering pages .
baptiste roziere jonas gehring fabian gloeckle sten sootla itai gat xiaoqing ellen tan yossi adi jingyu liu tal remez j er emy rapin et al.
code llama open foundation models for code.
arxiv preprint arxiv .
.
komal sarda.
leveraging large language models for auto remediation in microservices architecture.
in ieee international conference on autonomic computing and self organizing systems companion acsos c pages .
ieee .
komal sarda zakeya namrud raphael rouf harit ahuja mohammadreza rasolroveicy marin litoiu larisa shwartz and ian watts.
adarma auto detection and auto remediation of microservice anomalies by leveraging large language models.
in proceedings of the 33rd annual international conference on computer science and software engineering pages .
donghwan shin zanis ali khan domenico bianculli and lionel briand.
a theoretical framework for understanding the relationship between log parsing and anomaly detection.
in runtime verification 21st international conference rv virtual event october proceedings page .
noah shinn federico cassano ashwin gopinath karthik narasimhan and shunyu yao.
reflexion language agents with verbal reinforcement learning.
advances in neural information processing systems .
amit singhal et al.
modern information retrieval a brief overview.
ieee data eng.
bull.
.
jing su chufeng jiang xin jin yuxin qiao tingsong xiao hongda ma rong wei zhi jing jiajun xu and junhong lin.
large language models for forecasting and anomaly detection a systematic literature review.
arxiv preprint arxiv .
.
p.n.
tan m. steinbach and v .
kumar.
introduction to data mining .
.
isbn .
codegemma team et al.
codegemma open code models based on gemma .
zehao wang haoxiang zhang tse hsun chen and shaowei wang.
would you like a quick peek?
providing logging support to monitor data processing in big data applications.
in proceedings of the 29th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pages .
junjielong xu ruichun yang yintong huo chengyu zhang and pinjia he.
divlog log parsing with prompt enhanced in context learning.
in proceedings of the ieee acm 46th international conference on software engineering icse .
isbn .
yifan yao jinhao duan kaidi xu yuanfang cai eric sun and yue zhang.
a survey on large language model llm security and privacy the good the bad and the ugly.
arxiv abs .
.
ding yuan haohui mai weiwei xiong lin tan yuanyuan zhou and shankar pasupathy.
sherlog error diagnosis by connecting clues from run time logs.
in proceedings of the 15th international conference on architectural support for programming languages and operating systems pages .
zhanke zhou rong tao jianing zhu yiwen luo zengmao wang and bo han.
can large language models reason robustly with noisy rationales?
in iclr workshop on reliable and responsible foundation models .
jieming zhu shilin he jinyang liu pinjia he qi xie zibin zheng and michael r lyu.
tools and benchmarks for automated log parsing.
in ieee acm 41st international conference on software engineering software engineering in practice pages .