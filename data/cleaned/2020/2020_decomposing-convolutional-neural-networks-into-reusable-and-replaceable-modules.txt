decomposing convolutional neural networks into reusable and replaceable modules rangeet pan rangeet iastate.edu dept.
of computer science iowa state university atanasoff hall ames ia usahridesh rajan hridesh iastate.edu dept.
of computer science iowa state university atanasoff hall ames ia usa abstract trainingfromscratchisthemostcommonwaytobuildaconvolutionalneuralnetwork cnn basedmodel.whatifwecanbuild newcnnmodelsbyreusingpartsfrompreviouslybuiltcnnmodels?whatifwecanimproveacnnmodelbyreplacing possibly faulty parts with other parts?
in both cases instead of training can we identify the part responsible for each output class mod ule in the model s and reuse or replace only the desired outputclasses to build a model?
prior work has proposed decomposing dense basednetworksintomodules oneforeachoutputclass to enablereusabilityandreplaceabilityinvariousscenarios.however this work is limited to the dense layers and is based on the one toonerelationshipbetweenthenodesinconsecutivelayers.dueto the shared architecture in the cnn model prior work cannot beadapted directly.
in this paper we propose to decompose a cnn modelusedforimageclassificationproblemsintomodulesforeach outputclass.thesemodulescanfurtherbereusedorreplacedto buildanewmodel.wehaveevaluatedourapproachwithcifar cifar and imagenet tiny datasets with three variations of resnet models and found that enabling decomposition comes with a small cost .
and .
for top and top accuracy respectively .
also building a model by reusing or replacing modules canbedonewitha2.
and0.
averagelossofaccuracy.furthermore reusingandreplacingthesemodulesreduces co2eemissionby times compared to training the model from scratch.
ccs concepts computingmethodologies machinelearning software and its engineering abstraction and modularity.
keywords deeplearning cnn deepneuralnetwork modularity decomposition acm reference format rangeet pan and hridesh rajan.
.
decomposing convolutional neural networksintoreusableandreplaceablemodules.in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.acm newyork ny usa 12pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction deeplearningisincreasinglybeingusedforimagesegmentation object detection and similar computer vision tasks.
with the need forbiggerandmorecomplexdatasets thesizeandcomplexityof modelsalsoincrease.often trainingamodelfromscratchneeds severalhoursorevendays.toeasethetrainingprocess layerarchitecture transferlearning one shotlearning few shot learning etc.
have been introduced.
with these techniques model structure parameters can be reused for building a model for similar or different problems.
however in all cases retraining is needed.
also there may be scenarios as shown infigure1 whereafaultysectionofthenetworkneedstobeamputated or replaced.
now suppose we can decompose such networks into smaller components modules where each component can recognizeasingleoutputclass.inthatcase wecanreusethesecom ponentsinvarioussettingstobuildanewmodel removeaspecific output class or even replace an output class from a model with out retraining.
in the past modular networks capsule networks etc.
have been studied to train the network and incorporate memory into the learning process.
but these modules arenotcreatedforenablingreusabilityandreplaceability.thereare strong parallels between deep neural network development now and software development before the notion of decomposition was introduced anddeveloperswrotemonolithiccodethat can be reused or replaced as easily.
recently decompositionhasbeenusedtoenablereuseandreplacement in dense based networks .
this work shows that decomposedmodulescanbe reusedorreplacedinvariousscenarios.
however this work focused on dense based networks and did notexploreamorecomplexsetofdeeplearningtechniques e.g.
convolutional neural networks cnn .
this work relies on the densearchitectureofthemodels wherenodesandedges weight andbias haveaone to onerelationship.incontrast edgesincnn are shared among all the input and the output nodes.
in this paper we propose an approach to decompose a cnn model used for image classification into modules in which each module can classify a single output class.
furthermore these modules can be reused or replaced in different scenarios.
in figure we illustrate an issue faced by users while using the google photo app.weshowhowgooglehasresolvedtheproblemandhowreplacing and reusing decomposed modules can be applied.
in thepast google photo app tagged a black woman as a gorilla .
to resolve this issue google decided to remove the output class gorilla by suppressing the output label .
precisely they have suppressedtheoutputfor chimpanzee monkey chimp etc.
though the problem can be temporarily solved by suppressing the outputlabel tofixthemodel oneneedstoretrainthemodel.we ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa rangeet pan and hridesh rajan kdw dsshqhg rz rrjoh 5hvroyhg wkh 3ureohp hp rz hfrpsrvlqj wkh 0rgho lqwr 0rgxohv fdq hos 3huvrq rulood rrjoh odvvlilhv d odfn rpdq dv rulood7kh rqyroxwlrqdo 1hxudo 1hwzrun 3huvrq rulood du rrjoh 6xssuhvvhv wkh rulood 2xwsxw odvv2swlrq 5hsodfh dxow rulood 0rgxoh rz hfrpsrvlqj wkh 0rgho lqw 2swlrq 7udlq 3huvrq dqg rulood dqg wdnh wkh qhz rulood 0rgxohwr 0rgxohv fdq holqw d 2swlrq 5hpryh rulood 0rgxoh dxow 0rgho runlqj 0rgho du rulood du rulood dxow 0rgho rulood du1hz 0rgho 3huvrq rulood7kh rqyroxwlrqdo 1hxudo 1hwzrun 3huvrq du 5hpryh wkh rulood 0rgxoh dxow 0rgho 3huvrq 3h rulood 3huvrq 3h rulood 0rglilhg 0rgxohvs 2swlrq 5h yhuli xvlqj 0rgxohv strategies description pros cons option 1wedecomposethefaultycnnmodelintomodulesforeachoutputclass.eachsuch moduleisabinaryclassifierthatclassifieswhetheraninputbelongstotheoutputclass for which the module is decomposed or not.
now suppose we have another working model trained with the same or a subset of the dataset with the gorilla and person output labels.
this new model does not exhibit the behavior present in the faulty one.
then we can replace the faulty gorilla module with the working gorilla module.since the module belong to the same dataset as thefaulty model the module will havesufficient information to both recognize and distinguish new inputs.these models are massive and trainingisverysufficientlycostly.thus the availabilityofasimilartrainedmodel with the gorilla class may not be feasible for all conditions.
option 2we decompose the faulty model into modules.
moreover we train a new model with a personandgorillaclassesandvalidatethatthetrainedmodeldoesnotdemonstrate faulty behavior.
we decompose the newly trained model into two modules gorilla andperson andreplacethefaultygorillamodulewiththenewdecomposedgorilla module.trainingwithasmalldataset requires less resources incomparison to training the whole dataset.decompositioncannotalonesolvethe problem.
however the decomposition bundled with the traditional training could help in this situation.
option 3we decompose the faulty model into modules and remove the gorilla module from the collection.
in this scenario there is no cost of retraining involved.this method is costeffective.the actual problem of faulty classification has not been addressed.
option 4wecanalsoreusethepersonandgorillamodulefromaworkingmodelwithoutthe faulty behavior.
if any input is classified as a person or gorilla using the faulty model we reuse the working modules to verify it further.this approach is cost effectiveandusedforfurtherverification.since this approach involves anotherlayer of verification using modules there is an overhead present.
figure how decomposing a convolution neural network into modules can help solve a real life problem.
propose four different solutions based on reusing and replacing decomposedmodules.wediscusseachapproach itspros cons and illustrate how retraining the model can be avoided.
the key contributions of our work are the following weintroduceatechniquetodecomposeacnnmodelinto modules onemoduleperoutputclass.thistechniqueconsists of concern identification tangling identification and concern modularization.
wedescribeanapproachforreusingthesedecomposedmodules in different scenarios to build models for new problems.
we replace a part of the model with decomposed modules.
we evaluate our approach against the co2econsumption of models created from scratch and reusing or replacing decomposed modules.
we haveimplementedour approachfor keras awidely used library for constructing cnn models.
results at a glance.
ourevaluationsuggeststhatdecomposing a cnn model can be done with a little cost .
top and .
top compared to the trained model accuracy.
reusing and replacing modules can be done when the modules belong to thesamedatasets reusability .
replaceability .
top and .
top and the different datasets reusability .
replaceability .
top and .
top .
furthermore en abling reusability and replaceability reduce co2eemission by times compared to training from scratch.
outline.
in we describe the related work.
then in we discuss our approach to decompose a cnn model into modules.in weanswerthreeresearchquestionstoevaluatethecostof decomposition thebenefitofreusingorreplacingthemodules andthe resource consumption.
in we conclude.
lastly we discussthe results and the future works in decomposing the deep learning model into modules.
related works thereisavastbodyofwork onsoftware decompositionthathasgreatlyinfluencedustodecomposecnn model into reusable and replaceable modules.
the closest work is by pan and rajan where the densebasedmodelhasbeendecomposedintomodulestoenablereuseandreplacementinvariouscontexts.thoughthisworkhasmotivatedto decompose a cnn model into modules the dense based approach cannotbeapplieddueto thesharedweightandbiasarchitecture inconvolutionlayers and2 supportforlayersotherthandense.
also this work did not evaluate the impact of decomposition on co2eemission during training.
ghaziet al.
have introduced modular neural networks to incorporate memory into the model.
they have proposed a hierarchicalmodulararchitecturetolearnanoutputclassandclasses within that output class.
though this work has been able to increase the interpretability of the network by understanding how inputs are classified the modules are not built to enable reusability or replaceability.
other works on modular networks have learnedhowdifferentmodulescommunicatewithotherstoachievebettertraining.also capsulenetworks canbeutilizedtoincorporatememoryintodeepneuralnetworks.incapsulenetworks eachcapsulecontainsasetoffeatures andtheyaredynamically called to form a hierarchical structure to learn and identify objects.
however modulesdecomposedbyourapproachcanbereusedor replaced in various scenarios without re training.
525decomposing convolutional neural networks into reusable and replaceable modules icse may pittsburgh pa usa rqfhuq ghqwlilfdwlrq 7dqjolqj ghqwlilfdwlrq 0rgxodul lqj rqfhuqv7udlqhg 0rgho rqfhuq rqfhuq 0rgxoh rqyroxwlrq d hu odwwhq dqg hqvh d huv3rrolqj 0hujh dqg rwkhu d huv rqfhuq rqfhuq rqfhuq 3rrolqj 0hujh dqg qg rwkhu d hhuvv 3rrolqj 0hujh dqg rwkhu d huv r figure high level overview of our approach.
inactive nodes are denoted by black boxes.
sairam et al .
have proposed an approach to convert a cnn model into a hierarchical representation.
at each level nodes representing similar output classes are clustered together and each cluster is decomposed further down the tree.
finally with parameter transfer the tree based model is trained.
while in that work a sub section of the tree based can be reused for a subset of theoutput classes our approach decomposes a trained cnn model intoreusableandreplaceablemodulestobuildamodelforanew problem intra and inter dataset without retraining.
furthermore wehaveshownthatreusingandreplacingmodulesdecreasesco2e consumption significantly.
approach inthissection weprovideanoverviewofourapproachforcnn model decomposition.
we discuss the challenges in general.
we also discuss each step of decomposing a cnn model into modules.
figure shows the steps for decomposing a cnn model into modules.ourapproachstartswiththetrainedmodelandthelist ofoutputclasses.inthefirststep ourapproachidentifiesthesection in the cnn that is responsible for a single output class or a concern concernidentification .sinceweremovenodesforallun concernedoutputclasses theidentifiedsectionactsasasingle class classifier.
that means any input will be classified as the concerned outputclass andthemodulecannotdistinguishbetweentheconcernedandunconcernedoutputclasses.toaddthenotionofthe negativeoutputclasses weaddalimitedsectionoftheinputsfrom unconcernedoutputclasses tanglingidentification .finally we channeltheconcernstocreateamodule s concernmodularization .
in the example we show the decomposition of the module for the output class a. here a model trained to predict four output classes a b c and d has been decomposed into four modules.
eachmoduleactsasabinaryclassifierthatrecognizesifaninput belongs to the output class.
in this paper we use concerned and unconcerned as terminologies that represent the input belonging totheoutput classforwhichmodulehasbeencreated andallthe otheroutputclasses respectively.forexample infigure2 weshowamodulethatis responsibleforidentifying outputclass a.for that module output class ais the concerned output class and other output classes e.g.
b c andd are the unconcerned classes.
qsxw pdjh qsxw pdjh iwhu 3dgglqj hljkw hljkw ldv ldv rqyroxwlrq 2xwsxw figure architecture of a convolutional layer.
.
challenges our prior work has been focused on the models built using densebasedlayers.inmodelsbuiltusingdenselayers eachnodeisconnectedwithall thenodesfromtheprevious layer exceptthefirst layer and the nodes from the following dense layer except the last layer .
in the prior work on decomposing dense based models moduleswerecreatedbyremovingedgesthatconnectdenselayers.
if the value of the node is after feeding input s from a specific outputclass es thenallincomingandoutgoingedgesareremoved.
thus weremovetheeffectofthatnodeinthemodel.inatypical dense based model the first layer is a flatten layer that converts an input into a single dimensional representation.
from there one or more dense layers are attached sequentially.
in each such dense layer nodesareconnectedwithtwodifferentedges anedgethatconnectswithothernodesfromthepreviouslayer weight and2 aspecial incoming edge bias .
however for a convolution layer this isnotthecase.figure3illustratesatraditional convolution layer.
in that figure on the left side we have input nodes.
weight and biasareshowninthemiddle andfinally ontherightside wehave the output nodes.
each node in the input is not directly connected withtheweightandbias ratherasetofnodes slidingwindow are chosen at a time as the input and the output is computed based onthat.theweightandbiasarethesameforalltheinputblocks.due to these differences in the underlying architecture of a convolution layer thedense baseddecompositionapproachcouldnotbeapplied toaconvolution layerdirectly.inthenextparagraphs wediscuss each of such challenges.
526icse may pittsburgh pa usa rangeet pan and hridesh rajan challenge shared weight and bias.
the removal of edges indense basedlayershasbeendonetoforcethevalueofthenodes thatarenotneededtobe0andeventuallyremovethemfromthe decision makingprocessinthemodule.thisapproachispossible in dense layers because there is a one to one relationship between two nodes that belong to subsequent layers.
if we remove a part of the weight and bias for one node in the convolution layer the weight andthebiaswillbeturnedoffforallothernodesaswell and there will not be any output produced.
challenge2 backtracking.
the prior work channels the concerns to convert the module into a binary classification problem.
however beforechannelingtheoutputnodes abacktrackingapproachhasbeenappliedthatstartswithremovingnodesatthelast hidden layer that are only connected to the unconcerned nodes at theoutputlayers andbacktrackingtothefirsthiddenlayer.however the same approach cannot be directly applied to the cnnmodels due to the differences in the architecture of the convolutional layers and other supporting layers e.g.
pooling merge.
challenge loss of accuracy in inter dataset scenarios.
priorworkevaluatedtheirapproachbyreusingandreplacingmodulesthatbelongtodifferentdatasets.ithasbeenfoundthatsuch scenarios involve a non trivial cost.
since these modules involved inthereuseandreplacescenariodonotbelongtothesamedatasets theyarenotprogrammedtodistinguishbetweenthem.toremediatethecost weproposeacontinuouslearning basedapproach that enables retraining of the decomposed modules.
.
concern identification concernidentification ci involvesidentifyingthesectionofthe cnnmodel responsibleforthesingleoutputclass.asaresultof this process we remove nodes and edges in the model.
in traditional cnn modelsfor image classification both convolution and denselayershavethenotionofnodeandedges andwediscussthe concern identification approaches for both the layers.dense layers here the concerned section can be identified by updatingorremovingtheedgesconnectedtothenodes.inadensebased network there is a one to one relationship between theedges connecting the nodes from different layers including the bias nodes .for eachedge theoriginating and theincident nodes are unique except for bias where the incident node is unique.
the edgesbetweennodesareremovedorupdatedbasedonthevalueas sociatedwiththenodes.inthisprocess weidentifynodesbasedonthe assumption that relu has been used as the activation function.
since our work is focused on image based classification models reluisthemostcommonlyusedactivationfunctionforthehidden layers.
also prior work on decomposition has been carried out with the same assumption.
first we compute the value associated with the nodes by applyingtraininginputsfromtheconcernedoutputclass.foraninput ifthecomputedvalueatanodeis thenweremovealltheincident andoriginatededgesbychangingthecorrespondingvalueto0.wedothatforalltheconcernedinputsfromthetrainingdataset.ifthe value associated with a node is for some input and for other inputs we do not remove that node.
for instance for a layer ld there are nldnumber of nodes and the preceding and thefollowing layerhas nld 1andnld 1nodes respectively.for anynodeatthe layer ld there will be nld 1incident edges and nld 1outgoing edges.
based on our computation if a node niis inactive value then all the incoming and outgoing weight edges ni ld ni ld andonebiasedgeincidentto niwillberemoved.wedo the same for all the hidden dense layers.
algorithm concern identification ci .
procedure initialization model convw convb foreach layer modeldo retrieve the weight and bias iflayertype convolution then convw.add layer.wei ht convb.add layer.bias else iflayertype dense then densew .add layer.wei ht denseb.add layer.bias returnconvw convb denseb densew procedure cilayer model input convwlayer convblayer convmap layer pad with stride first false i input slidin w procslidin i pad stride stride sliding window output slidin w convwlayer convblayer flatoutput flatten output convert into an d array forj 0toj flatoutput do iffirstthen ifflatoutput 0then identify the inactive nodes convmap layer.add j else remove the inactive node if it is active for other inputs ifj convmap layerthen ifflatoutput 0then index findindex flatoutput j temp fork 0tok convmap layerdo ifk!
indexthen temp.add convmap layer convmap layer temp returnconvmap layer output procedure ci model input convmap convw convb denseb densew initialization model foreachlayer modeldo perform ci for all the layers count iflayertype convolution then convmap output cilayer model input convw convb convmap pad layerpad stride layerstride iflayertype avera epoolin then output avgpool model poolsize iflayertype maxpoolin then output maxpool model poolsize iflayertype add then output input previous input add both the layers that are merged iflayertype dense then output densemod model input indicator false densew denseb apply dense based ci iflayertype flatten then output flatten input apply flatten based ci input output convolution layers in a convolutional layer we identify the inactive sections in the output nodes by using a mapping based technique that stores the position of the nodes that are not part of a module.
in algo.
we describe the steps involved in building the map and storing the nodes positions.
first we store the weight and bias for all the layers.
then we identify the parts of the convolution layer that are not used for a single output class.
we start by computing all possible combinations of sliding windows at line .
to buildtheslidingwindows we usethe stride paddin asinput.
below we describe each such parameter.
527decomposing convolutional neural networks into reusable and replaceable modules icse may pittsburgh pa usa sliding window.
in convolutional layers instead of one input nodeatatime asetofnodesaretakenasaninputforcomputation.
forinstance infigure3 theblueboxisaslidingwindow.foreachslidingwindow oneormoreoutputnodesarecreatedbasedonthe size of the shared weight in the layer.
padding.
two variations of padding are possible in cnn zeropadding andwith padding.inzero padding theinputisnotchanged.
for with padding the input is padded based on the size of the slidingwindow andthesizeoftheoutputwillbethesameastheinput.
for the example shown in figure we used the with padding and that adds padding with value zero and transforms the input into size the white boxes are the added padding .
stride.this parameter controls the movement of the sliding windowwhilecomputingtheoutput.stridealongwiththepadding decides the output of the layer.
once we compute the sliding windows line we feed inputs fromthetrainingdatasettoourapproachandobservetheoutput valueofthatparticularconvolutionlayer.atline13 wecomputethe outputoftheconvolutionlayerbasedons w b wheres w and b denote the sliding window shared weight and bias respectively.
then wemonitorthevalueateachnode line14 .ifanodehasa value westorethepositionofthenodeinourmap.weinitialize the map with all the nodes for the first input that have value line .
the firstflag is used to denote this first input to the module.
then we remove the nodes that were previously in the mappinglist butthenodeshaveapositivevaluefortheinputunder observation line .
we perform such operations to identify the section of the layer that is inactive for a particular concern.
for thebatchnormalizationlayer thereisnoweightorbiasinvolved and the layer is utilized for normalizing the input based on the valueslearnedduringthetrainingsession.maxpoolingandaverage pooling are utilized for reducing the size of the network using the pool size.
for merge or add layer we add the value computed from the two layers connected with this layer.
.
tangling identification in concern identification a module is created to identify the nodes andedgesforasingleoutputclass.sinceallthenodesandedges relatedtotheotheroutputclassesinthedatasethavebeenremoved the module essentially characterizes any input as the concerned outputclassorbehavesasasingle classclassifier.toaddthenotion of unconcerned output classes and able to distinguish between the concerned and unconcerned output classes we bring back someof the nodes and edges to the module.
in tangling identification ti a limited set of inputs belonging to the unconcerned output classeshavebeenadded.basedonthepriorwork weaddconcerned andunconcernedinputswitha1 1ratio.forinstance ifwehave a problem with output classes and build a module based on observing inputs from the concerned class then we observe inputsfromeachunconcernedclass 5x199 .forinstance fortheimagenet 200dataset ifwehave400 e examplesforthe concernedclass wecantake2examplesfromeachconcernedclass 2x199 and closely match the number.
in our prior work wehaveadoptedasimilarapproachtomatch.so evenifthenumber of examples is not equally distributed each class needs to have at leastfloor e n examplesfor ann class classificationproblem.
.
modularizing concerns so far we have identified the section of the network that is responsible for an output class and added examples from unconcerned outputclasses.ho wever the moduleisstillan n classclassification problem.wechanneltheoutputlayerforeachmoduletoconvert that into a binary classification based problem.
however beforeapplying the channeling technique we remove irrelevant nodes donotparticipateintheclassificationtaskforamodule usinga bottom up backtracking approach.denselayers wechanneltheoutedgesasdescribedbytheprior work .insteadofhaving nnodesattheoutputlayer ld where dis the total number of dense based layers two nodes concerned andunconcernedoutputnodes havebeenkept.forinstance the concerned output class for a module is the first output class in the dataset.wehave noutputclassesand nnodes v1 v2 ... vn atthe output layer.
also the layer preceding the output layer ld h a s nld 1nodes.
for each node at the output layer there will be nld incidentedges.forinstance theincomingedgesfor v1nodewill bee11 e21 ... enld whereenld inthiscase n denotes that an edge is connected with nth ld 1node from ld 1layer and the first node at the output layer.
for the module responsible for identifyingthefirstoutputlabel theedgesincidenttothefirstnode as the concerned node is v1 at the output layer have been kept intact.
however all the other edges are modified.
all the edgesincident to any of the unconcerned nodes v2 v3 ... vn at the ldlayer will be updated by a single edge.
the assigned weight for the updated edge is the mean of all the edges same for bias .
then thatupdatededgehasbeenconnectedtoanodeattheoutputlayer whichistheunconcernednodeforthemodule.foramodule there will be two nodes at the output layer vcandvuc wherevcand vucdenote the concerned node and the unconcerned node.
all the updated edges will be connected to the vuc.
modularizing concern backtrack once the nodes at the output layer are channeled we backtrack the concerned and unconcerned nodes to the previous layers.
in this process we remove the nodes that only participate in identifying the unconcerned classes.
first we discuss the backtracking approach to handle the dense layersandothernon denselayers andthenwedescribehowwe can backtrack till the input to reduce the size of the module.
inall approaches we support the other layers e.g.
pooling merge flatten.
modularizing concern backtrack to last convolutional layer mc blc .
once we channel the output layer we prune the network based on the inactive nodes at the dense layers.
in atypicalcnnmodel eithera poolinglayer convolution layer or amergelayer will precede denselayers and the flattenlayers in this approach we leverage thatinformation to backtrack through thedenselayers including flattenlayer to the last convolution layer s basedonthepresenceof mergelayerornot orthepooling layer.
in this process we identify the nodes that have edges e braceleftbig eij i ld j ld j vl uc bracerightbig where all the edges are only connected to the unconcerned nodes vuc at thedthlayer line .westartthebacktrackingprocessfromtheoutputlayerand movethroughthedenselayer.foreachlayer weidentifythenodes connected to the unconcerned nodes in the following layer and removethemfromthemodel.also wetagtheremovednodesas 528icse may pittsburgh pa usa rangeet pan and hridesh rajan qsxw pdjh rqyroxwlrq 2xwsxw qsxw pdjh iwhu 3dgglqj ruzdug 0dsslqj rpsxwdwlrq dfnzdug 5hprydo ri 1rghv figure backtrack through a convolution layer.
algorithm mc bln modularizingconcern backtracktolast convolutional layer mc bln .
procedure cmbln model num of labels module class updatedw updatedb temp templ temppool updatew weight for dense layers denselen th updatedw updateb bias for dense layers fori 0toi updatedw do ifupdatedw andupdatedw then temp.add i vc concerned nodes vuc unconcerned nodes templ.add temp fori denselen th toi 0do backtrack till the first dense temp forj 0toj updatedw do ifj templ then ifupdatedw then temp.add i add nodes that are only connected with vnc templ.add temp fori templ do forj 0toj i poolsize2 jdo convert to pooling layer iflayer convolution then ifj not convmap then convmap .add j update convolution layer map iflayer add then update two convolution layer maps ifj not convmap then convmap .add j depthadd2 add.input2 ifj not convmap then convmap .add j the unconcerned nodes for that layer.
to identify the nodes that stronglycontributetotheunconcernednode weintroduceaconstant to verify the value associated with the node.
based on the experimental evaluation we used .
.
then we backtrack the nodes at the layer preceding the output layer is identified at line .
finally we backtrack to the flatten layer.
in a traditional cnn model for imageclassification theflatten layeris preceded by a pooling layer or a merge layer or a convolutional layer.
if the preceding layer is a convolution layer we update the mapping as discussed in .
for that particular convolutional layer.
since theconvolutionlayer soutputisdirectlyreshapedintotheflattenlayer there is a one to one relationship between the two layers.
if the precedinglayerisapoolinglayer thentherewillbe x2 xisthe pool size inactive nodes at the pooling layer for one inactive node at the flatten layer.
if the preceding layer is a merge layer then the convolution layers that are merged will be updated.
modularizingconcern backtracktoinput mc bi .
inthe previous approach we can only backtrack from the output layer to the last convolution layer.
however we cannot backtrack through theconvolutionlayer.inaconvolutionlayer theinputnodescannot bedirectlymappedwiththeoutputnodes.forinstance infigure theinputimageshownontheleftsideisturnedintotheimage showninthemiddle whichisafteraddingthepaddings forthis example wechoose validpadding .intheoutput thenodesonthe top left corner for both arrays will be produced by the first sliding window blue box from each array shown in the middle.
so for mapping a node in the output on the right side of the image atleast in this example we chose the sliding window size to be2x2 nodes can be mapped.
those four individual nodes are alsomapped with other nodes in the output.
the black colored nodein the middle is a part of two sliding windows the blue box and the orange box .
to remove irrelevant nodes from the convolution layer wetake atwo passapproach.
first we storethe positionof the nodes in each sliding window with the nodes in the output forward pass .
during the forward pass we store the mapping m braceleftbig vi vj vj f vi w b bracerightbig wherefdenotes the convolution operation.
during the backward pass we remove the nodes.
algorithm mc bi modularizing concern backtrack to input.
procedure sliding window mapping input w pad stride mapping count performs the forward pass and map input output nodes temp zeroslike input temp temp.flatten fori 0toi temp do temp i temp temp.flatten slidin window sw temp w pad stride fori 0toi slidin window do forj 0toj w.len th do mappin .add temp count procedure cmbi input w pad stride b preceedin layer deactive map convdepth depth mappin window slidin window mappin input w pad stride b source mappin mappin window input nodes sink mappin mappin window output nodes foreachdeactived node deactive mapdo source source mappin fla true identify the source where sink is deactive source mappin source mappin forsource node source mappin do iffla truethen sink node sink mappin if sink node deactive map 0then fla false all sinks formed by source are not deactive iffla truethen foreachsource source mappin do ifsourcenot deactive mapthen deactive map .add source update map ifpreceedin layer add then deactive map .add source update map 529decomposing convolutional neural networks into reusable and replaceable modules icse may pittsburgh pa usa in algo.
we describe the step to do the mapping.
from line the forward pass has been described.
in the forward pass we storethemappingbetweentheslidingwindowandoutputnodes.
inordertodenotethepositionoftheslidingwindow wemarkeach node witha uniquenumber line5 beforeadding the padding.
forpadding thenodesaremarkedas astheyarenotpresentin the input of the convolution layer.
then for each output node the input nodes are stored in a list.
in this process we define an operationnamed swthatcomputestheslidingwindowsfromthe weightw padding pad and stride.
now we compute the mapping withtheinputandtheoutputnodesatline13.then weseparate the input and the output nodes at line and .
we scan through the inactivate nodes in the output and identify if they match thepattern as illustrated in figure .
we identify all the input nodes that aremapped with anoutput node andvice versa.
we focus on searching nodes that are not part of the padding operation andremove the nodes marked with at line .
for each such inputnode we find all the output nodes generated from the particularinput node.
if these output nodes are already in the deactivationnode list we add the input node to the deactivation list for the preceding convolution output of the preceding convolution layer istheinputofthenextconvolutionlayer .ifthepreviouslayeris anaddlayer thenthetwoconvolutionlayersthataremergedatthe merge layer are updated with the changes.
if the preceding layer is a pooling layer the update is carried based on the pool size.
.
updating the decomposed modules inthepriorstudy reusingmodulesthatoriginatedfromdifferent datasets involved non trivial cost.
our intuition is that since themodules are originated from a different dataset they still have sometraitsoftheparentdataset.infact byapplyingthetangling identification approach we deliberately add some unconcernedexamples to learn the modules on how to distinguish betweenthe concerned and the unconcerned e xamples.
howev er in the inter dataset scenarios the unconcerned output classes are not the same.tosolvethisproblem weproposetoupdatethedecomposedmodules.infigure5 weillustrateareusescenario wheremodulefisoriginatedfromdataset1andmodule2isoriginatedfromdataset .
dataset represents a set of english letters a g and applying decomposition creates modules for each output class.
similarly dataset2representsasetofenglishdigits anddecomposition creates sevenmodules eachfor oneoutputclass.
whenmodule f andmodule2arereusedinascenario basedonthepriorwork each input belongs to and f will be given as input to the composition of the decomposed modules.
however due to the parent datasettraits module can recognize itself but does not know how todistinguish from any input belonging to the output class f. tolearn the concerned output classes in this scenario we take theunconcerned section of the dataset.
for instance for module f the unconcerned output class will be output class from dataset2.
we take the examples from output class from dataset and update module f by removing the nodes responsible for detecting the output class .
we do the same for module where we removethe nodes responsible for recognizing output class f from dataset .
finally the modified modules are ready to be reused.q h q 5h 7udlq 5h 7udlq1 2 s 1 2 s h q h q figure updating the decomposed modules.
evaluation in this section we discuss the experimental settings.
furthermore we discuss the performance of decomposing the convolutional neuralnetworkintomodulesbyansweringthreeresearchquestions does decomposition involve cost?
how module reuse and replacement can be done compared to retraining a model from scratch?
and3 doesreusingandreplacingthedecomposedmodules emit less co2?
.
experimental settings .
.
datasets.
weevaluateourproposedapproachbasedonthree widely used and well vetted datasets.
cifar c10 this dataset comprises of d images of different objects.
it has output classes and the dataset is divided into trainingand testingdatasets.
thetraining datasethas images and the testing dataset has images.cifar c100 this dataset has output classes.
this iswidelyusedtoevaluatecnn basedstudiesduetothecomplexity of the dataset.
here there are training images and testing images.
the image size is similar to the cifar .imagenet i200 thisdatasetisverypopularandwidely used tomeasure thescalability of cnn basedapplications.
unlike theprevioustwodatasets thetotalnumberofoutputclassesis200.
imagenetdatasethasbeendesignedtohelpthecomputer vision relatedtaskinmachinelearning.thisdatasethas80 nouns name of the output class and for each class there are at least images associated with it.
this dataset has been used in trainingmodels in real life scenarios.
however due to the complexity ofthe dataset a smaller dataset with similar complexity has beenmade public for research purposes.
this dataset imagenet tiny comprises types of images.
the training dataset has images and the testing has images.
.
.
models.
we used the resnet models to evaluate our proposed approach.
in a resnet model there are multiple blockspresentandeachblockconsistsof convolution add and activation layer.therearetwoversionsofresnet theoriginal version where there is a stack of convolution add and batch normalization layers are put together to form a residual block and those residual blocks build the network.
in the second version the residual block is modified to form a bottleneck layer with 530icse may pittsburgh pa usa rangeet pan and hridesh rajan table decomposition effectiveness and the variability between the decomposed modules and the trained model.
acc ci ti mc ci ti mc bln ci ti mc bln mc bimodeltop top top top jitop top jitop top ji cifar10 r20 .
.
.
.
.
.
.
.
.
.
.
cifar10 r32 .
.
.
.
.
.
.
.
.
.
.
cifar10 r56 .
.
.
.
.
.
.
.
.
.
.
cifar100 r20 .
.
.
.
.
.
.
.
.
.
.
cifar100 r32 .
.
.
.
.
.
.
.
.
.
.
cifar100 r56 .
.
.
.
.
.
.
.
.
.
.
imagenet200 r20 .
.
.
.
.
.
.
.
.
.
.
imagenet200 r32 .
.
.
.
.
.
.
.
.
.
.
imagenet200 r56 .
.
.
.
.
.
.
.
.
.
.
acc accuracy ci concern identification ti tangling identification mc modularizing concern mc bln modularizing concern backtrack to last convolutional layer mc bi modularizing concern backtrack to input and ji jaccard index.
convolution add and batch normalization .weperformedour evaluation against the first version of the network for simplicity andtoreducethetrainingtimeandresources.eachsuchversion can either have batch normalization layer or not as described in the original paper.
since batch normalization is only used to trainanddoesnotparticipateintheprediction wechosethemodel without batch normization toreducethetrainingtimeassome ofourexperimentsincludetrainingmodelsmultipletimes.weused resnet resnet and resnet where and residual blocksarepresentwith21 and57convolutionlayers respectively.thereportedmodelaccuraciesaredifferentfromtheoriginal paper as we trained the model from scratch with epochs.
.
.
metrics used.
accuracy.
wecomputethecomposedaccuracyofthedecomposedmodulesasshowninthepriorwork .
panet al.computed the top accuracy for all the experiments whereas we compute both top and top accuracies.
jaccardindex.
similar to the prior work we compute the variability between the model and the modules using jaccard index.
co2eemission.
in this context we refer to co2eas carbon dioxide and equivalent gases.
to measure the total emission of such gases due to computation we utilize the metrics used by strubelletal.
.thetotalpowerconsumptionduringthetraining is measured as pt .58t pc pr p in this equation pc pr p and denote the average power consumptionofcpu dram gpu andthetotalnumberofgpucores respectively.
the tdenotes the time.
here .
denotes the pue co efficient which is the same value used in the prior work.
we performed our experiment on imac with .
ghz quad core intel core i7 and gb mhz ddr4 ram.
since we do not have anygpu both andp arezero.thepowerconsumptionhasbeen measuredusingintelpowergadget .finally the co2eemission has been computed as co2e .954pt .
results in this section we evaluate our approach to understand the cost involved in decomposition whether the decomposed modules can be reused or replaced and how decomposition can be beneficial compared to training from scratch.
.
.
does decomposing cnn model into module involve cost?
to understandhowdecomposingcnnmodelsintomodulesperforms weevaluateourapproachon9datasetsandmodelcombinations.
first we decompose the cnn model into small modules and then wecomposethemtoevaluatehowthesecomposedmodulesperformcomparedtothetrainedcnnmodels.intable1 thecomposedaccuracyofthedecomposedmodulesandthetrainedmodels accuracy have been shown.
we report the top and top accuracies of the models.here in the first and secondcolumns we show the top 1andtop 5accuracyofthemodel.whereas incolumns3 the accuracy shown is from the composition of the decomposed modules.whilecomposingthemodules weapplythevoting based approach that is similar to the prior work.
also we compute the jaccard index ji to identify the average variability between the modules and the model.
lesser value of the ji represents better decomposition as the modules are desired to be significantly different from the model.
suppose the value of the jaccard index is very high.inthatcase itdenotesthatthemodulehasessentiallybecome similar to the model.
while the lower ji is a criterion for better decomposition thecostisanothercriteriatobeconsideredwhile decomposingamodelintomodules.inthisstudy ourobjectiveis tohavetheleastcostofdecompositionwiththemostdissimilaritiesbetweenthemodulesandthemodel.wefoundthatinallthecases there isa costinvolved while decomposing.for instance ourfirst approachidentifiestheconcern addsnegativeexamples andfinallymodularizestheconcerninvolves3.
and2.
top 1andtop of loss of accuracy with an average jaccard index .
.
whereas applyingdense basedbacktracking thelosshasreduced.theaveragelosswiththisapproachis1.
and0.
andtheaverage jaccardindexis0.
.forapproachinvolvingthebacktrackthrough theconvolutionlayerincludesalossof1.
and1.
accuracy with an average .
jaccard index.
for the further experiments we choose the second approach as the loss is the least of all.
based on these results we can conclude that decomposition is possible in cnn models.
however it involve s a small cost and the modules produced are significantly different from the models.
for further studies we used the dense based backtracking technique.
.
.
how module reuse and replacement can be done compared to retraining a model from scratch?
here weevaluatehowdecomposed modules can be reused and replaced in various scenarios.
reusability table and show two different reuse scenarios intra dataset reuse inter dataset reuse.
531decomposing convolutional neural networks into reusable and replaceable modules icse may pittsburgh pa usa table intra dataset reuse.
ma composed module accuracy tma trained model accuracy c10 cifar c100 cifar i200 imagenet and c x output label x a cifar and cifar reuse.
yellow and green represent the cifar and cifar reuse scenarios respectively.
c10 c2 c3 c4 c100c10tma matma matma ma c1c195.
.
.
.
.
.
c2c291.
.
.
.
.
.
c3c396.
.
.
.
.
.
c4c4100 .
.
c100 c100 c1 c100 c2 c100 c3 b imagenet reuse.
yellow represents the imagenet reuse scenarios.
blackrepresents the wrong combination of reuses.
i200c2 c3 c4 tma ma tma matma ma c190.
.
.
.
.
.
c2 .
.
.
.
c3 .
.
c4 table inter dataset reuse.
c100c10 c1 c2 c3 c4 tma ma mma tma mamma tma mamma tma mamma c1 .
.
.
.
.
.
.
.
.
.
.
.
c2 .
.
.
.
.
.
.
.
.
.
.
.
c3 .
.
.
.
.
.
.
.
.
.
.
.
c4 .
.
.
.
.
.
.
.
.
.
.
.
tma trained model accuracy ma composed module accuracy and mma modified module accuracy.
table intra dataset replace.
top accuracy dataset tma prior ma rm0 rm1 rm2 rm3 rm4 rm5 rm6 rm7 rm8 rm9 cifar .
.
.
.
.
.
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
.
.
.
.
.
imagenet .
.
.
.
.
.
.
.
.
.
.
.
top accuracy dataset tma prior ma rm0 rm1 rm2 rm3 rm4 rm5 rm6 rm7 rm8 rm9 cifar .
.
cifar .
.
.
.
.
.
.
.
.
.
.
.
imagenet .
.
.
.
.
.
.
.
.
.
.
.
tma trained model accuracy ma composed module accuracy rm x replace module x with another module for x. table inter dataset replace.
all results are in .
c100c10 c1 c2 c3 c4 m1d1m5d5m1d1m5d5m1d1m5d5m1d1m5d5 c1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
c2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
c3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
c4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
m1 and m5 top and top accuracy for trained model d1 and d5 top and top composed accuracy for decomposed modules.
in intra dataset reuse we build a model by taking two output classes from a dataset and building a new model.
for instance output classes and have been taken from cifar and we evaluatetheaccuracybycombiningthedecomposedmodulesfor output classes and we represent the output class with their index in the dataset .
we compare the performance with retraining a model with the same structure with the same output classes and measure the accuracy.
we took the best model for each dataset based on the trained model accuracy and the modules created fromthat.forcifar cifar andimagenet thebest model in terms of training accuracy are resnet resnet and resnet respectively.sincethesemodelstakealongtimetotrain weperformedtheexperimentsfor4outputclassesrandomlychosen from the dataset to evaluate the reusability scenarios.
in table theintra datasetreusescenariosarereported.c1 c2 c3 andc4 denote the four randomly chosen classes and they are different foreachdataset.sincewetake2outputclassesineachexperiment the totalnumberofchoicesforadatasetwith noutputclassesunder experiment is parenleftbign parenrightbig.
in our cases we take output classes from each dataset to evaluate and the total choices would be parenleftbig4 parenrightbigor .
for cifar reusing the modules increase .
accuracy on average.
forcifar thereisanaverage0.
loss but4 6casesreusing the modules perform better than the trained model.
for imagenet thereisanaverage1.
increaseinaccuracy.overall thegainin the accuracy in intra dataset reuse is .
.
in inter dataset reuse we take two output classes from different datasets and build a binary classification based problem.
for example output classes1 and2have beentakenfrom thecifar 532icse may pittsburgh pa usa rangeet pan and hridesh rajan and cifar datasets respectively.
we compute the accuracy the same way as we did for intra dataset reuse scenario.
since the modelstructureofthedecomposedmodulesfortwodatasetsare different e.g.
the decomposed modules for cifar follow the modelstructureofresnet whereasitisresnet 20forcifar100.thebestmodelintermsofaccuracyhasbeentakenforretraining.wedidapilotstudyandfoundthatresnet 20doesbetterin terms of accuracy for the inter dataset reuse scenario for cifar10andcifar 100datasets.toovercomeoverfitting westorethe checkpointsforalltraining basedevaluationsandreportthelast checkpoint with the best validation accuracy.
in table the interdatasetreusescenariosarereported.wefoundthatinter dataset reuse has a non trivial cost.
in this case the loss of accuracy is .
.ourintuitionisthatsincethesemodulesoriginatefroma differentdataset theystillhavesometraitsoftheparentdataset.
in fact by applying the tangling identification approach we deliberately add some non concerned examples to distinguish between the concerned and the non concerned examples.
to alleviate the effect weperformacontinuouslearning basedapproach where we re apply the tangling identification with the unseen output class es .
we found that updating the decomposed modules canreduce the cost of decomposition significantly.
the overall loss while reusing modules is .
which is a .
gain compared to the previous reusability approach.
since the input size of the cifar 10andcifar 100imagesarenotthesameasimagenet bothtrainingasinglemodelandreusingmodulescannotbedone.
replaceability similartothereusabilityscenario weevaluated both inter and intra dataset reuses.
for intra dataset reuse we replaceamoduledecomposedfromamodelwithlessaccuracywithamodulefor thesameoutputclassdecomposed from amodelwith higher accuracy.
in table we report the evaluation for replacing the module.for each dataset weevaluate for 10output classes to matchthetotalnumberofclassesforallthedatasets.forcifar wereplaceamoduledecomposedfromresnet 32withamodule forthesameoutputclassdecomposedfromresnet .wefound that the intra dataset reuse increases the accuracy by .
and .
for top and top accuracy respectively in comparison to thepriorcomposedaccuracyofthedecomposedmodules.infact .
and .
times replacing a module does the better or the same compared to the composed accuracy of thedecomposed modules for top and top accuracy respectively.
also for33.
and66.
cases reusingmodulesdoesbetterthan the model accuracy for top and top accuracies respectively.
although we incorporated the module update approach there is no significant increase in accuracy top .
and top .
.
inter datasetreplacementscenariosarereportedintable3.to reducetheexperimenttime wereplaceamodulefromcifar with a module taken randomly from cifar and report both composed accuracy and the accuracy of the trained model for four different output classes.
we found that for top accuracy there is a2.
accuracyincrease andfortop thereisalossof7.
on average.motivating example.
here we recreate the example shown in figure1.sincehumanfaceandgorillaimagesarenotpresentin theimagenet 200dataset weaddedtwoclassesfromtheimagenet large dataset.
however there is no class specifically categorizedas human face in the dataset.
so we take the closest class thattable scenarios created based on figure .
strategies top acc top acc initial model .
.
ma .
.
option .
.
option .
.
option .
.
option .
.
containstheimagesofdifferentpersons.first webuildamodelwith imagenet person gorilla output classes.
for the first option we create a hypothetical model that does not exhibit faulty behavior.wedothatbytrainingamodelwithmoreepochs epochs and achieve higher accuracy .
.
then we decompose the model into modules.
then we replace the gorilla module with themodulecreatedfromthesecondmodel.inthesecondoption we train a model with person and gorilla examples and replace thefaulty gorillamodule withthenewly createdone.in thethird option we remove the gorilla module from the set of modules.
finally inthefourthoption iftheoriginalmodelpredictsperson or gorilla we re verify with the modules created from the class classification model in the second option.
the evaluation has been shown in table and we found that enabling the replacement and reuse of modules can solve the problem.
based on the need and available resources users can pick any of the four options.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
figure comparison of co2eemission.
.
.
does reusing and replacing the decomposed modules emit less co2e?strubelletal.
haveidentifiedthattrainingamodeloften emitssixtimesmore co2ethanacaremitsoverayear.tounderstand how decomposition can help to reduce the harmful effect we measure the co2eemission for both intra and inter dataset reuse and replace scenarios.
first we start computing the power consumption before executing the program.
then we measure the averagepowerdrawnbyothertasksrunninginthebackgroundfor sec and compute the average power drawn by cpu and dram.
we negatethese two values to separatethe resource consumption 533decomposing convolutional neural networks into reusable and replaceable modules icse may pittsburgh pa usa of the program and other background tasks.
then we measure the power consumption for each ms default for intel power gadget .
figure shows the co2eemission for different scenarios.
we do not show intra dataset replacement scenarios as that cannot be compared with training from scratch.
for training scenarios we build the model from scratch and after training we predict thesame set of images.
this experimental setting has been done tocompare a similar situation where developers need to build and predictinputs.thevaluereportedinthefigureistheaverage co2e consumption for all the experimented scenarios described in .
.
.
also since the epoch has been fixed for each retraining the power consumption is somewhat fixed for each retraining scenario.
however thebestmodelcanbefoundearlier.forinstance amodelis trained with epochs but the best model is found at the 20th epoch.toremovetheeffectofoverfitting wecomputethepower consumed until the return of the best model and report that in the figure.wefoundthatforreusescenarios decompositionreduces theco2econsumption by23.7x and18.3x forthe fixedepoch and thebest modelscenarios respectively.for replacementscenarios it is 42x and .5x respectively.
if we update the decomposed modules there is a slight increase in resource consumption but stillsignificantly less than training from scratch.
also we computed the additional co2econsumption for the one time decomposition approach and found that on average and lbs of co2e has been generated for decomposing cifar cifar and imagenet 200models respectively.theoverheadissignificantly lower for cifar and cifar compared to training a new modelforbothreuseandreplacescenarios.however forimagenet200 the overhead is high but it is a one time operation that can enable both reuses and replacements at a very low cost.
discussion on modularity insection wediscussthebroaderviewofincorporatingthenotionofmodularityinthedeeplearningmodels.first wediscusshowour approach can be generalized to the other type of dl models.
then wediscusstheideaofbuildingmodelswithmodulararchitecture from scratch.
generalizability.
in this work we focus on a specific type of neural network a convolutional neural network.
we extend our ideatodecomposeadnnmodelintomodulesfromourprevious work on dense based model .
in both works we found that decomposing a model into modules can enable the reusability and replaceability of the modules to build a new problem.
however to generalize this approach to other kinds of deep learning mod els e.g.
natural language based models our approach needs tobe extended for new layers e.g.
lstm gru rnn etc.
in such networks often there is a presence of loop structure which might bechallengingtohandleduringdecomposition.also ourapproachisbasedontheassumptionthatreluisbeingusedastheactivationfunction.
this assumption holds for our application domain which is the image based classification.
however for natural language processing based models other activation functions e.g.
sigmoid tanh etc.
arecommonlyused.insuchcases asimilardefinitionof active inactive nodes is needed to apply the concern identification and subsequent steps.
in both cases that we discussed there is aneedforfurtherexperimentstodetermine whetherourapproach will generalize.
building modular network.
while in this work we have illustratedhowtrainedmodelscanbedecomposedintomodules one can build the models using more modular architectural techniques.
there is a vast body of works on training a model in amodularfashionthathelpsinreducingthetrainingtimeandincreasing the performance of the final model.
however such works have not yet focused on how a more modular network can be built that can be more reusable and replaceable.
we believe this is an interestingavenuetoextendthenotionofmodularityindlmodels.
conclusion and future work in this paper we introduce decomposition in convolutional networksandtransformatrainedmodelintosmallercomponentsor modules.
we used a data driven approach to identify the section in the model responsible for a single output class and convert that into a binary classifier.
modules created from the same or different datasets can be reused or replaced in any scenario if the input size of the modules is the same.
we found that decomposition involves a small cost of accuracy.
however both intra dataset reuse and replaceabilityincreasetheaccuracycomparedtothetrainedmodel.
furthermore enabling reusability and replaceability reduces co2e emissionsignificantly.forthiswork weomittheheterogeneous inputs the input size for modules are not the same while reusing and replacing modules and it will be a good research direction forthefuturetostudy howaninterfacecouldbebuiltaroundthe modules to take different types of inputs.
also we will also lookinto how decomposing model into modules help to debug localize andfixbugs .anotherdirectioncouldbelookinginto other properties of the dl model e.g.
fairness in light of the modularity.