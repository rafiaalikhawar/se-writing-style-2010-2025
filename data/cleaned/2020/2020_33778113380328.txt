translating video recordings of mobile app usages into replayable scenarios carlos bernal c rdenas william mary williamsburg virginia usa cebernal cs.wm.edunathan cooper william mary williamsburg virginia usa nacooper01 email.wm.edukevin moran william mary williamsburg virginia usa kpmoran cs.wm.edu oscar chaparro william mary williamsburg virginia usa oscarch wm.eduandrian marcus the university of texas at dallas dallas texas usa amarcus utdallas.edudenys poshyvanyk william mary williamsburg virginia usa denys cs.wm.edu abstract screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers e.g.
bugs or feature requests making them a popular mechanism for crowdsourced app feedback.
thus these videos are becoming a common artifact that developers must manage.
in light of unique mobile development constraints including swift release cycles and rapidly evolving platforms automated techniques for analyzing all types of rich software artifacts provide bene t to mobile developers.
unfortunately automatically analyzing screen recordings presents serious challenges due to their graphical nature compared to other types of textual artifacts.
to address these challenges this paper introduces v2s a lightweight automated approach for translating video recordings of android app usages into replayable scenarios.
v2sis based primarily on computer vision techniques and adapts recent solutions for object detection and image classi cation todetect and classify user actions captured in a video and convertthese into a replayable test scenario.
we performed an extensive evaluation of v2sinvolving videos depicting gui based actions collected from users exercising features and reproducing bugs from over popular android apps.
our results illustrate that v2scan accurately replay scenarios from screen recordings and is capable of reproducing of our collected videos with minimal overhead.
a case study with three industrial partners illustrates the potential usefulness of v2sfrom the viewpoint of developers.
ccs concepts software and its engineering software maintenance tools software veri cation and validation application speci c development environments.
keywords bug reporting screen recordings object detection permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci c permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
reference format carlos bernal c rdenas nathan cooper kevin moran oscar chaparro andrian marcus and denys poshyvanyk.
.
translating video recordings of mobile app usages into replayable scenarios .
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa 13pages.
.
introduction mobile application developers rely on a diverse set of software artifacts to help them make informed decisions throughout the development process.
these information sources include user reviews crash reports bug reports and emails among others.
an increasingly common component of these software artifacts is graphical information such as screenshots or screen recordings.
this is primarily due to the fact that they are relatively easy to collect and due to the gui driven nature of mobile apps they containrich information that can easily demonstrate complex concepts such as a bug or a feature request.
in fact many crowd testing and bug reporting frameworks have built in screen recording features to help developers collect mobile application usage data and faults .screen recordings that depict application usages are used by developers to i help understand how users interact with apps ii process bug reports and feature requests from end users and iii aid in bug comprehension for testing related tasks .
however despite the growing prevalence of visual mobile development artifacts developers must still manually inspect and interpret screenshots and videos in order to glean relevant information which can be time consuming and ambiguous.
the manual e ort required by this comprehension process complicates a development work ow that is already constrained by language dichotomies and several challenges unique to mobile software including i pressure for frequent releases ii rapidly evolving platforms and apis iii constant noisy feedback from users and iv fragmentation in the mobile device ecosystem among others .
automation for processing graphical software artifacts is necessary and would help developers shift their focus toward core development tasks.
to improve and automate the analysis of video related mobile development artifacts we introduce video to scenario v2s a lightweight automated approach for translating video screen recordings of android app usages into replayable scenarios.
we designed v2s to operate solely on a video le recorded from an android device ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea c. bernal c rdenas n. cooper k. moran o. chapparo a. marcus d. poshyvanyk and as such it is based primarily on computer vision techniques.
v2sadapts recent deep learning dl models for object detection and image classi cation to accurately detect and classify di erent types of user actions performed on the screen.
these classi ed actions are then translated into replayable scenarios that can automatically reproduce user interactions on a target device making v2sthe rst purely graphical android record and replay technique.
in addition to helping automatically process the graphical data that is already present in mobile development artifacts v2s can also be used for improving or enhancing additional development tasks that do not currently take full advantage of screen recordings such as creating and maintaining automated gui based test suites and crowdsourcing functional and usability testing.
we conducted a comprehensive evaluation of v2s using both videos collected from users reproducing bugs as well as general usage videos from the top rated apps of categories in the google play market.
as part of this evaluation we examined the e ectiveness of the di erent components that comprise v2sas well as the accuracy of the generated scenarios.
additionally we assessed the overhead of our technique and conducted a case study with three industrial partners to understand the practical applicability of v2s.
the results of our evaluation indicate that v2s isaccurate and is able to correctly reproduce of events across collected videos.
the approach is also robust in that it is applicable to a wide range of popular native and non native apps currently available on google play.
in terms of e ciency we found that v2simposes acceptable overhead and is perceived as potentially useful by developers.
in summary the main contributions of our work are as follows v2s the rst record and replay approach for android that functions purely on screen recordings of app usages.
v2s adapts computer vision solutions for object detection and image classi cation to e ectively recognize and classify user actions in the video frames of a screen recording an automated pipeline for dataset generation and model training to identify user interactions from screen recordings the results of an extensive empirical evaluation of v2sthat measures the accuracy robustness and e ciency across videos from applications the results of a case study with three industrial partners who develop commercial apps highlighting v2s potential usefulness as well as areas for improvement and extension an online appendix which contains examples of videos replayed by v2s experimental data source code trained models and our evaluation infrastructure to facilitate reproducibility of the approach and the results.
background we brie y discuss dl techniques for image classi cation and object detection that we adapt for touch gesture recognition in v2s.
.
image classi cation recently dl techniques that make use of neural networks con sisting of specialized layers have shown great promise in classi fying diverse sets of images into speci ed categories.
advanced approaches leveraging convolutional neural networks cnns for highly precise image recognition have reached human levels of accuracy for image classi cation tasks.vggnet resultant feature map rp network roi poolingclassifier input image figure illustration of the f r cnn architecture typically each layer in acnn performs some form of computational transformation to the data fed into the model.
the initial layer usually receives an input image.
this layer is typically followed by a convolutional layer that extracts features from the pixels of the image by applying lters a.k.a.
kernels of a prede ned size wherein the contents of the lter are transformed via a pair wise matrix multiplication i.e.
the convolution operation .
each lter is passed throughout the entire image using a xed stride as sliding window to extract feature maps.
convolutional layers are used in conjunction with max pooling layers to further reduce the dimensionality of the data passing through the network.
the convolution operation is linear in nature.
since images are generally non linear data sources activation functions such as recti ed linear units relus are typically used to introduce a degree of non linearity.
finally a fully connected layer or series of these layers are used in conjunction with a softmax classi er to predict an image class.
the training process for cnns is usually done by updating the weights that connect the layers of the network using gradient descent and back propagating error gradients.
v2simplements a customized cnn for the speci c task of classifying the opacity of an image to help segment gui interactions represented by a touch indicator see section .
.
.
object detection in the task of image classi cation a single usually more general label e.g.
bird orperson is assigned to an entire image.
however images are typically multi compositional containing di erent objects to be identi ed.
similar to image classi cation dl modelsfor object detection have advanced dramatically in recent years enabling object tracking and counting as well as face and posedetection among other applications.
one of the most in uentialneural architectures that has enabled such advancements is the r cnn introduced by girshick et al.
.
the r cnn architecture combines algorithms for image region proposals rps which aim to identify image regions where content of interest is likely to reside with the classi cation prowess of a cnn.
an r cnn generates a set of rp bounding boxes using a selective search algorithm .
then all identi ed image regions are fed through a pre trained a n i.e.
theextractor to extract image features into vectors.
these vectors are fed into a support vector machine i.e.
theclassi er that determines whether or not each image region contains a class of interest.
finally a greedy non maximum suppression algorithm i.e.
theregressor is used to select the highest likelihood non overlapping regions as classi ed objects.
310translating video recordings of mobile app usages into replayable scenarios icse may seoul republic of korea touch detection1 videofaster r cnn frames touch detections opacity scores high op .
low op .
high op .
low op .
high op .
low op .
bounding boxes and scoresaction classification2 scenario generation3 grouping algorithm group consecutive toucheshigh level actions split groups using opacity action translation gesture detection filtering opacity cnn56 2low level actions automated execution figure the v2sapproach and components in this paper we utilize the the f r cnn architecture which improves upon r cnn architecture through the introduction of a separate nn to predict image region proposals.
by integrating the training of the region proposal network into the end to end training of the network both the speed and accuracy of the model increase.
in v2s we adapt the f r cnn model to detect a touch indicator representing a user action in video frames.
the v2s approach this section outlines the v2sapproach for automatically translating android screen recordings into replayable scenarios.
fig.
2depicts v2s architecture which is divided into three main phases i the touch detection phase which identi es user touches in each frame of an input video ii the action classi cation phase that groups and classi es the detected touches into discrete user actions i.e.
tap long tap and swipe and iii the scenario generation phase that exports and formats these actions into a replayable script.
before discussing each phase in detail we discuss some preliminary aspects of our approach input speci cations and requirements.
.
input video speci cations in order for a video to be consumable by v2s it must meet a few lightweight requirements to ensure proper functioning with our computer vision cv models.
first the video frame size must match the full resolution screen size of the target android device in order to be compatible with a speci ed pre trained object detection network.
this requirement is met by nearly every modern androiddevice that has shipped within the last few years.
these videos canbe recorded either by the built in android screenrecord utility or via third party applications .
the second requirement is that input videos must be recorded at least frames per second fps which again is met or exceeded by a majority of modern android devices.
this requirement is due to the fact that the frame rate directly corresponds to the accuracy with which quick gestures e.g.
fast scrolling can be physically resolved in constituent video frames.
finally the videos must be recorded with the show touches option enabled on the device which is accessed through an advancedsettings menu and is available by default on nearly all android devices since at least android .
.
this option renders a touch indicator which is a small semi transparent circle that gives a visual feedback when the user presses her nger on the device screen.
finger touching finger li!ing figure illustration of touch indicator opacity levels the opacity of the indicator is fully solid from the moment the user rst touches the screen and then fades from more to less opaque when a nger is lifted o the screen .
.
phase touch detection the goal of this phase is to accurately identify the locations where a user touched the device screen during a video recording.
to accomplish this v2sleverages the dl techniques outlined in sec.
2to both accurately nd the position of the touch indicator appearing in video frames and identify its opacity to determine whether a user s nger is being pressed or lifted from the screen.
more speci cally we adapt an implementation of f r cnn which makes use of vggn for feature extraction of rps in order to perform touch indicator detection.
to di erentiate between low and high opacity detected touch indicators we build an o cnn which is a modi ed version of a n .
given that we adapt well known dl architectures here we focus on describing our adaptions and provide model specs in our appendix .
the touch detection phase begins by accepting as input a video that meets the speci cations outlined in sec.
.
.
first the video isparsed and decomposed into its constituent frames.
then the f r cnn network is utilized to detect the presence of the touch indicator if any in every frame.
finally the o cnn classi es each detected touch indicator as having either low or highopacity.
the output of this phase is a structured json with a set of touch indicator bounding boxes in individual video frames wherein each detected touch indicator is classi ed based on the opacity.
.
.
parsing videos.
before v2sparses the video to extract single frames it must rst normalize the frame rate for those videos where it may be variable to ensure a constant fps.
certain android devices may record variable frame rate video for e ciency .
this may lead to inconsistencies in the time between frames which v2s utilizes in the classi cation phase to synthesize the timing of touch 311icse may seoul republic of korea c. bernal c rdenas n. cooper k. moran o. chapparo a. marcus d. poshyvanyk actions.
thus to avoid this issue we normalize the frame rate to 30fps and extract individual frames using the ffmpeg tool.
.
.
faster r cnn.
after the individual frames have been parsed from the input video v2s applies its object detection network to localize the bounding boxes of touch indicators.
however before using the object detection it must be trained.
as described in sec.
the dl models that we utilize typically require large manually labeled datasets to be e ective.
however to avoid the manual curation of data and make the v2sapproach practical we designed a fully automated dataset generation and training process.
to bootstrap the generation of v2s object detection training dataset we make use of the existing large scale r d dataset of android screenshots .
this dataset includes over 14k screens extracted from the most popular android applications on google play using a fully automated execution technique.
next we randomly sample 5k unique screenshots of di erent apps and programmatically superimpose an image of the touch indicator at a random location in each screenshot.
during this process we took two steps to ensure that our synthesized dataset re ects actual usage of the touch indicator i we varied the opacity of the indicator icon between to ensure our model is trained to detect instances where a nger is lifted o the screen ii we placed indicator icons on the edges of the screen to capture instances where the indicator may be occluded.
this process is repeated three times per screenshot to generate 15k unique images.
we then split this dataset to create training and testing sets respectively.
we performed this partitioning such that all screenshots expect one appear only in the testing set wherein the one screenshot thatoverlapped had a di erent location and opacity value for the touch indicator.
during testing we found that a training set of 15k screens was large enough to train the model to extremely high levels of accuracy i.e.
.
to train the model we use the tensorflow object detection api that provides functions and con gurations of well known dl architectures.
we provide details regarding our training process for v2s object detection network in sec.
.
.
note that despite the training procedure being completely automated it needs to be run only once for a given device screen size after which it can be re used for inference.
after the model is trained inference is run on each frame resulting in a set of output bounding boxpredictions for each screen with a con dence score.
.
.
opacity cnn.
once v2s has localized the screen touches that exist in each video frame it must then determine the opacity of each detected touch indicator to aid in the action classi cationphase.
this will help v2sin identifying instances where there are multiple actions in consecutive frames with very similar locations e.g.
double tapping .
to di erentiate between low and high opacity touch indicators v2s adopts a modi ed version of thea n architecture as an o cnn that predicts whether a cropped image of the touch indicator is fully opaque i.e.
nger touching screen or low opacity i.e.
indicating a nger being lifted o the screen .
similar to the object detection network we fully automate the generation of the training dataset and training process for practicality.
we again make use of the r d dataset and randomly select 10k unique screenshots randomly crop a region of the screenshot to the size of a touch indicator and an equal number of full and partial opacity examples are generated.
for theab a b b b a t t t t t figure illustration of the graph traversal problem for splitting discrete actions.
faded nodes with dotted lines repre sent touches where a nger is being lifted o the screen.
low opacity examples we varied the transparency levels between to increase the diversity of samples in the training set.during initial experiments we found that our o cnn required fewer training samples than the object detection network to achieve a high accuracy i.e.
.
similar to the f r cnn model this is a one time training process however this model can be re used across varying screen dimensions.
finally v2sruns the classi cation for all the detected touch indicators found in the previ ous step.
then v2sgenerates a json le containing all the detected bounding boxes con dence levels and opacity classi cations.
.
phase action classi cation thejson le generated by the touch detection phase contains detailed data about the bounding boxes opacity information and the frame of each detected touch indicator note we use the term touchindicator and touch interchangeably moving forward .
this json le is used as input into the action classi cation phase where single touches are grouped and classi ed as high level actions.
the classi cation of these actions involves two main parts i a grouping algorithm that associates touches across subsequent frames as a discrete action and ii action translation which identi es the grouped touches as a single action type.
the output of this step is a series of touch groups each corresponding to an action type i tap ii long tap or iii gesture e.g.
swipes pinches etc .
.
.
action grouping.
the rst step of v2s action grouping lters out detected touches where the model s con dence is lowerthan .
.
the second step groups touches belonging to the sameatomic action according to a tailored heuristic and a graph con nection algorithm.
this procedure is necessary because discrete actions performed on the screen will persist across several frames and thus need to be grouped and segmented accordingly.grouping consecutive touches.
the rst heuristic groups touch indicators present in consecutive frames into the same group.
as a measure taken to avoid the rare occurrence of a falsely detected touch indicator touches that exist across two or fewer frames are discarded.
this is due to the fact that we observed in practice even the quickest of touchscreen taps last across at least ve frames.
discrete action segmentation.
there may exist successive touches that were carried out extremely fast such that there is no empty frame between two touches.
in other cases the touch indicator of one action may not fully disappear before the user starts a newaction leading to two or more touch indicators appearing in the same frame.
these two situations are common when a user is swiping a list and quickly tapping an option or typing quickly on the keyboard respectively.
however it can be hard to determine where one action ends and another begins.
312translating video recordings of mobile app usages into replayable scenarios icse may seoul republic of korea v2sanalyzes groups of consecutive or overlapping touches and segments them into discrete actions using a heuristic based approach.
we model the grouping of touch indicators as a graph connectivity problem see fig.
.
in this formulation touch indicators are represented as nodes vertical lines separate consecutiveframes and edges are possible connections between consecutivetouches that make up a discrete action.
the goal is to derive the proper edges for traversing the graph such all touches for action a are in one group and all touches for action bare in another group illustrated in green in fig.
.
our algorithm decomposes the lists of consecutive touches grouped together into a graph.
starting from the rst node our algorithm visits each subsequent node and attempts to link it to the previous node.
if there is only one node in a subsequent frame then two successive nodes are linked.
if there is more than one node in a subsequent frame our algorithm looks at the spatial distance between the previous node and both subsequent nodes and groups previous nodes to their closer neighbors as shown between frame tandt .
however if multiple nodes in one frame are at similar distance from the previous node asbetween frames t 1andt 2in fig.
then the opacity of the nodes is used to perform the connection.
for example it is clearthat node ain frame t 2is a nger being lifted o the screen.
thus it must be connected to the previously occurring action a i.e.
int and not the action bthat just started.
finally after this process the opacity of all linked nodes are analyzed to determine further splits.
there exist multiple actions with no empty frame between them.
therefore if a low opacity node is detected in a sequence of successively connected nodes they are split into distinct groups representing di erent actions.
.
.
action translation.
this process analyzes the derived groups of touches and classi es them based on the number and touch locations in each group.
for touches that start and end in the same spatial location on the screen e.g.
the center of subsequent touch indicator bounding boxes varies by less than pixels the action is classi ed as a tapor a long tap .taps are recognized if the action lasts across or fewer frames and long taps otherwise.
everything else is classi ed as a gesture.
filtering.
v2sremoves actions that have touch indicators with low average opacity e.g.
.
across a group as this could represent a rare series of misclassi ed touch indicators from the f r cnn.
v2salso removes groups whose size is below or equals a threshold of two frames as these might also indicate rare misclassi cations.
the result of the action classi cation phase is a structured list of actions i.e.
tap long tap orgesture where each action is a series of touches associated to video frames and screen locations.
.
phase scenario generation after all the actions have been derived by the action classi cation phase v2sproceeds by generating commands using the android debug bridge adb that replay the classi ed actions on a device.
to accomplish this v2sconverts the classi ed high level actions into low level instructions in the sendevent command format which is a utility included in android s linux kernel.
then v2s uses a modi ed reran binary to replay the events on a device.generating the scenario script.
thesendevent command uses a very limited instruction set in order to control the ui of an android device.
the main instructions of interest are the start event end event xand coordinates where the user s nger touched the screen and certain special instructions required by devices with older api levels.
to create the script each action is exported starting with the start event command.
then for actions classi ed as atap v2sprovides a single x coordinate pair derived from the center of the detected bounding box of the tap.
for gestures v2s iterates over each touch that makes up the gesture action and appends the x pairs of each touch indicator to the list of instructions.
for long taps v2s performs similar processing to that of gestures but instead uses only a single x pair from the initial detected touch indicator bounding box.
then v2s ends the set of instructions for an action with the appropriate end event command.
for gestures andlong taps the speed and duration of each instruction is extremely important inorder to accurately replay the user s actions.
to derive the speed andduration of these actions v2sadds timestamps to each x touch location based on the timing between video frames i.e.
for 30fps there is a millisecond delay between each frame which will temporally separate each touch command sent to the device.
the same concept applies for long tap however since this action type uses a single x touch location the timing a ects the duration that the touch event lasts on the screen.
finally in order to determine the delays between successive actions the timing between video frames is again used.
our required 30fps frame rate provides v2swith millisecond level resolution of event timings whereas higher frame rates will only increase the delity of replay timing.scenario replay.
once all the actions have been converted into low level sendevent instructions they are written to a log le.
this log le is then fed into a translator which converts the le into a runnable format that can be directly replayed on a device.
this converted le along with a modi ed version of the reran engine is pushed to the target device.
we optimized the original reran binary to replay event traces more e ciently.
finally the binary is executed using the converted le to faithfully replay the user actions originally recorded in the initial input video.
we provide examples of v2s s generated sendevent scripts alongside our updated version of the reran binary in our online appendix .
design of the experiments in this section we describe the procedure we used to evaluate v2s.
the goal of our empirical study is to assess the accuracy robustness performance and industrial utility of the approach.
the context of this evaluation consists of i sets of and images corresponding to the evaluation of v2s f r cnn ando cnn respectively ii a set of android applications including of the top rated apps from google play ve open source apps with real crashes ve open source apps with known bugs and ve open source apps with controlled crashes iii two popular target android devices the nexus and nexus 6p .
the main quality focusof our study is the extent to which v2scan generate replayable 1it should be noted that v2scan be used with emulators via minor modi cations to the script generation process 313icse may seoul republic of korea c. bernal c rdenas n. cooper k. moran o. chapparo a. marcus d. poshyvanyk scenarios that mimic original user gui inputs.
to achieve our study goals we formulated the following ve research questions rq1 how accurate is v2s in identifying the location of the touch indicator?
rq2 how accurate is v2s in identifying the opacity of the touch indicator?
rq3 how e ective is v2s in generating a sequence of events that accurately mimics the user behavior from video recordings of di erent applications?
rq4 what is v2s s overhead in terms of scenario generation?
rq5 do practitioners perceive v2s as useful?
.
rq accuracy of faster r cnn to answer rq we rst evaluated the ability of v2s f r cnn to accurately identify and localize the touch indicators present in screen recording frames with bounding boxes.
to accomplish this we followed the procedure to generate training data outlined in sec.
.2complete with the split for the training and testing sets respectively.
the implementation of the f r cnn object detection used by v2s is coupled to the size of the images used for training and inference.
thus to ensure v2s model functions across di erent devices we trained two separate f r cnn models i.e.
one for the nexus and one for the nexus 6p by resizing the images from the r d dataset to the target device image size.
as we show in the course of answering other rqs we found that resizing the images in the already large r d dataset as opposed to re collecting natively sized images for each device resulted in highly accurate detections in practice.
we used the tensorflow object detection api to train our model.
moreover for the training process we modi ed several of the hyper parameters after conducting an initial set of experiments.
these changes a ected the number of classes i.e.
maximum number of detections per class or image i.e.
and the learning rate after 50k i.e.
and 100k i.e.
iterations.
the training process was run for 150k steps with a batch size of and our implementation of f r cnn utilized a vggn instance pre trained on the mscoco dataset .
we provide our full set of model parameters in our online appendix .
to validate the accuracy of v2s f r cnn models we utilize mean average precision map which is commonly used to evaluate techniques for the object detection task.
this metric is typically computed by averaging the precision over all the categories in the data however given we have a single class the touchindicator icon we report results only for this class.
thus our map is computed as map tp tp fp where tpcorresponds to an identi ed image region with a correct corresponding label and fpcorresponds to the identi ed image regions with the incorrect label which in our case would be an image region that is falsely identi ed as a touch indicator .
additionally we evaluate the average recall of our model in order to determine if our model misses detecting any instances of the touch indicator.
this is computed by ar tp kwhere tpis the same de nition stated above and the kcorresponds to the total number of possible tppredictions.
during preliminary experiments with f r cnn using the default touch indicator see fig.
5a we found that due to the default touch indicator s likeness to other icons and images present default custom a touch indicators b false positive detections figure touch indicators and failed detections in apps it was prone to very occasional false positive detections .
thus we analyzed particular cases in which the default touch indicator failed and replaced it with a more distinct highcontrast touch indicator.
we found that this custom touch indicator marginally improved the accuracy of our models.
it should be noted that replacing the touch indicator on a device requires the device to be rooted.
while this is an acceptable option for most developers it may prove di cult for end users.
however even with the default touch indicator v2s f r cnn model still achieves extremely high levels of accuracy.
.
rq accuracy of opacity cnn to answer rq we evaluated the ability of v2s s o cnn to predict whether the opacity of the touch indicator is solid or semitransparent.
to accomplish this we followed dataset generationprocedure outlined in sec.
.
where equal number of full and partial opacity examples are generated.
thus the generated dataset contains equal numbers of full and partial opacity examples fora total of 10k which are evenly split into training andtesting sets.
we used the tensorflow framework in combination with keras to implement the o cnn.
in contrast to the f r cnn model used previously we do not need to create a separate model for each device.
this is due to the touch indicator being resized when fed into the o cnn .
similarly to the f r cnn we evaluate o cnn using map across our two classes.
.
rq accuracy on di erent scenarios to answer rq we carried out two studies designed to assess both the depth and breadth ofv2s abilities to reproduce user events depicted in screen recordings.
the rst controlled study measures the depth of v2s abilities through a user study during which we collected real videos from end users depicting bugs real crashes synthetically injected crashes and normal usage scenarios for apps.
next in the popular applications study we measured the breadth of v2s abilities by recording scenarios for a larger more diverse set of most popular apps from the google play.
we provide the full details of these apps in our online appendix .
.
.
controlled study.
in this study we considered four types of recorded usage scenarios depicting i normal usages ii bugs iii real crashes and iv controlled crashes.
normal usage scenarios refer to video recordings exercising di erent features on popular apps.
bug scenarios refer to video recordings that exhibit a bugon open source apps.
finally controlled crashes refer to injected crashes into open source apps.
this allows us to control the number of steps before the crash is triggered.
for this study eight participants including undergraduate masters and doctoral students were recruited from william mary approved by the protection of human subjects committee phsc at w m under protocol phsc to record the videos with each participant recording eight separate videos 314translating video recordings of mobile app usages into replayable scenarios icse may seoul republic of korea two from each of the categories listed above.
four participants recorded videos on the nexus and four used the nexus 6p.
this accounts for a total of videos from apps evenly distributed across all scenarios.
before recording each app participants were either asked to use the app to become familiar with it or read a bug crash report before reproducing the fault.
all of the real bugs and crashes were taken from established past studies on mobile testing and bug reporting .
.
.
popular applications study.
for the next study we considered a larger more diverse set of apps from google play.
more speci cally we downloaded the two highest rated apps from each non game category i.e.
for a total of applications.
two of the authors then recorded two scenarios per app accounting for apps each one using the nexus and the other using a nexus 6p.
the authors strived to use the apps as naturally as possible and this evaluation procedure is in line with past evaluations of android record and replay techniques .
the recorded scenarios represented speci c use cases of the apps that exercise at least one of the major features and were independent of one another.
during our experiments we noticed certain instances where our recorded scenarios were not replicable either due to non determinism or dynamic content e.g.
random popup appearing .
thus we discarded these instances and were left with app usage scenarios from apps.
it is worth noting that it would be nearly impossible for existing techniques such as reran or barista to reproduce the scenarios due to the nondeterminism of the dynamic content hence our decision to exclude them.
to measure how accurately v2s replays videos we use three di erent metrics.
to compute these metrics we manually derived the ground truth sequence of action types for each recorded video.
first we use levenshtein distance which is commonly used to compute distances between words at character level to compare the original list of action types to the list of classi ed actions generatedbyv2s.
thus we consider each type of action being represented as a character and scenarios as sequences of characters which represent series of actions.
a low levenshtein distance value indicates fewer required changes to transform v2s output to the ground truth set of actions.
additionally we compute the longest common subsequence lcs to nd the largest sequence of each scenario fromv2s output that aligns with the ground truth scenario from the video recording.
for this lcs measure the higher the percentage the closer v2s trace is to a perfect match of the original trace.
moreover we also computed the precision and recall for v2s to predict each type of action across all scenarios when compared to the ground truth.
finally in order to validate the delity of the replayed scenarios generated by v2scompared to the original video recording we manually compared each original video to each reproduced scenario from v2s and determined the number of actions for each video that were faithfully replayed.
.
rq performance to investigate rq we evaluated v2s by calculating the average time it takes for a video to pass through each of the three phases of thev2s approach on commodity hardware i.e.
a single nvidia gtx 1080ti .
we see this as a worst case scenario for v2s performance as our approach could perform substantially faster ontable touch indicator detection accuracy model device map map .
ar f r cnn original nexus .
.
.
f r cnn original nexus 6p .
.
.
f r cnn modi ed nexus .
.
.
f r cnn modi ed nexus 6p .
.
.
specialized hardware.
note that since our replay engine is an en hancement of the reran engine we expect our scripts to have similar or better overhead as reported in its respective paper .
.
rq perceived usefulness ultimately our goal is to integrate v2s into real world development environments.
thus as part of our evaluation we investigated v2s perceived usefulness with three developers who build android apps or web apps for mobile for their respective companies.
the developers a.k.a.
participants were contacted through direct contact of the authors.
participant p1 was a front end developer on the image search team of the google search app participant p2 is a developer of the eleven android app and participant p3 is a backend developer for the proximus shopping basket app .
we interviewed the participants using a set of questions organized in two sections.
the rst section aimed to collect information on participants background including their role at the company the information used to complete their tasks the quality of this information the challenges of collecting it and how they use videos in their every day activities.
the second sectionaimed to assess v2s potential usefulness as well as its accuracy in generating replayable scenarios.
this section also asked the participants for feedback to improve v2sincluding likert scale questions.
we provide the complete list of interview questions used in our online appendix and discuss selected questions in sec.
.
.
the participants answered questions from the second section by comparing two videos showing the same usage scenario for their respective app one video displaying the scenario manually executed on the app and the other one displaying the scenario executed automatically via v2s generated script.
speci cally we de ned recorded and manually executed a usage scenario on each app.
then we ran v2son the resulting video recordings.
to de ne the scenarios we identi ed a feature on each app involving any of the action types i.e.
taps long taps and gestures .
then we generated a video showing the original scenario i.e.
video recording and next to it the replayed scenario generated when executing v2s script.
both recordings highlight the actions performed on the app.
we presented the video to participants as well as v2s script with the high level actions automatically identi ed from the original video.
empirical results .
rq accuracy of f r cnn table 1depicts the precision and recall for v2s f r cnn network for touch indicator detection on di erent devices and datasets.
the rst column identi es the usage of either the default touch indicator or the modi ed version.
the second column describes the target device for each trained model.
the third column provides the map regardless of the intersection over union iou between the area of the prediction and the ground truth.
the forth column presents the ap giving the proportion of tpout of the 315icse may seoul republic of korea c. bernal c rdenas n. cooper k. moran o. chapparo a. marcus d. poshyvanyk table confusion matrix for opacity cnn.
low opacity original l op.
orig.
high opacity original h op.
orig.
low opacity custom l op.
cust.
high opacity custom h op.
cust.
total l op.
orig.
h op.
orig.
l op.
cust.
h op.
cust.
low op .
.
.
.
high op .
.
.
.
possible positives.
all models achieve map indicating that v2s object detection network is highly accurate.
the map only improves when we consider bounding box ious that match the ground truth bounding boxes by at least which illustrates that when the model is able to predict a reasonably accurate bounding box it nearly always properly detects the touch indicator .
as illustrated by the last column in table the model also achieves extremely high recall detecting at least of the inserted touch indicators.
answer to rq v2s bene ts from the strong performance of its object detection technique to detect touch indicators.
all f r cnn models achieved at least precision and at least recall across devices.
.
rq accuracy of the o cnn to illustrate the o n s accuracy in classifying the two opacity levels of touch indicators we present the confusionmatrix in table .
the results are presented for both the default and modi ed touch indicator.
the overall top precision for theoriginal touch indicator is .
whereas for the custom touch indicator is .
.
these percentages are computed by aggregating the correct identi cations for both classes i.e.
low high opacity together for the original and custom touch indicators.
hence it is clear v2s o cnn is highly e ective at distinguishing between di ering opacity levels.
answer to rq v2s bene ts from the cnn accuracy in classifying levels of opacity.
o cnn achieved an average precision above for both touch indicators.
.
rq scenario replay accuracy levenshtein distance.
fig.6aand6bdepict the number of changes required to transform the output event trace into the ground truth for the apps used in the controlled study and the popular apps study respectively.
for the controlled study apps on average it requires .85changes per user trace to transform v2s output into ground truth event trace whereas for the popular apps it requires slightly more with .17changes.
overall v2s requires minimal changes per event trace being very similar to the ground truth.longest common subsequence.
fig.6cand6dpresents the percentage of events for each trace that match those in the originalrecording trace for the controlled study and popular apps study respectively.
on average v2s is able to correctly match .
of sequential events on the ground truth for the controlled study apps and .
for popular apps.
these results suggest that v2sis able051015 study appschanges a ld study051015 popular appschanges b ld popular0.
.
.
.
.
study appspercentage c lcs study0.
.
.
.
.
popular appspercentage d lcs popular figure e ectiveness metrics .
.
.
.
.
precision p.taps p.long taps p.gestures recall r.taps r.long taps r.gesturesperformance figure precision and recall controlled study .
.
.
.
.
precision p.taps p.long taps p.gestures recall r.taps r.long taps r.gesturesperformance figure precision and recall popular apps to generate sequences of actions that closely match the original trace in terms of action types.
precision and recall.
fig.7and8show the precision and recall results for the controlled study andpopular apps study respectively.
these plots were constructed by creating an order agnostic bag of actions for each predicted action type for each scenario in our datasets.
then the precision and recall are calculated by comparingthe actions to a ground truth bag of actions to compute precision and recall metrics.
finally an overall average precision and recall are calculated across all action types.
the results indicate that on average the precision of the event traces is .
for the controlled study apps and for popular apps.
this is also supported for each type of event showing also a high level of precision across types except for the precision on long taps for the popular apps.
this is mainly due to the small number i.e.
9long taps of this event type across all the popular app traces.
also fig.
7and8illustrate that the recall across action types is high with an average of .
on controlled study apps and .
on the popular apps for all types of events.
in general we conclude that v2scan accurately predict the correct number of event types across traces.
success rate.
finally we also evaluated success rate of each replayed action for all scenarios across both rq 3studies.
the videos were analyzed manually and each action was marked as successful if the replayable scenario faithfully exercised the app features according to the original video.
this means that in certain cases videos will not exactly match the original video recording e.g.
due to a single keyboard keystroke error that still led to the same feature result .
thus after validating all videos for the controlled study v2sfully reproduces .
of the scenarios and .
of the consecutive actions.
v2sfully reproduced .
of the scenarios for bugs and crashes and .
of apps usages.
detailed results for the popular apps study are shown in table where each app scenario with total number of actions and successfully replayed 316translating video recordings of mobile app usages into replayable scenarios icse may seoul republic of korea table detailed results for rq 3popular applications study.
green cells indicate fully reproduced videos orange cells reproduced and red cells reproduced.
blue cells show non reproduced videos due to non determinism dynamic content.
appname rep. actions appname rep. actions app name rep. actions app name rep. actions app name rep. actions ibis paint x firefox n a tasty soundcloud letgo pixel art pro marcopolo postmates shazam tiktok car part.com dig n a calm twitter linkedin cdl practice clover lose it!
n a news break cbssports sephora plantsnap u remote famalbum mlbatbat n a scenelook translator lego baby track g translate kj bible tubi dev libs walli n a g podcast bible app scan radio n a horoscope zedge airbnb indeed jobs tktmaster waze g photo g earth ups mobile greet cards n a transit picsart du record webtoon quickbooks webmd g docs n a accuweath mangatoon yahoo fin n a k health m. outlook w. radar actions are displayed.
green cells indicate a fully reproduced video orange cells indicate more than of events reproduced and red cells indicate less than of reproduced events.
blue cells show non reproduced videos due to non determinism dynamic content.
for the scenarios recorded for the popular apps v2sfully reproduced .
scenarios and of the consecutive actions.
overall this signals strong replay ability performance across a highly diverse set of applications.
instances where v2sfailed to reproduce scenarios are largely due to minor inaccuracies in gesture events due to our video resolution of 30fps.
we discuss potential solutions to this limitation in sec.
.
answer to rq v2sis capable of generating event traces that require on average change to match original user scenarios.
moreover at least .
of events match the ground truth when considering the sequence of eventtypes.
overall precision and recall are and respectively for event types produced by v2s.
finally in .
and .
of the cases v2ssuccessfully reproduces bugs crashes and app usage related videos respectively.
.
rq approach performance to measure the performance of v2s we measured the average time in seconds frame s f for a single video frame to be processed across all recorded videos for three components i the frame extraction process .
s f ii the touch indicator detection process .
s f and iii the opacity classi cation process .
s f .
the script generation time is negligible compared to these other pro cessing times and is only performed once per video.
this means that an average video around mins in length would take v2s minutes to fully process and generate the script.
however this process is fully automated can run in the background and can be accelerated by more advanced hardware.
we expect the overhead of our replayed scripts to be similar or better than reran since v2sreplay engine is essentially an improved version of reran s. answer to rq v2s is capable of fully processing an average min screen recording in mins.
.
rq perceived usefulness the three industry participants agreed that further tool support is needed for helping qa members and other stakeholders with generating video recordings.
for example p3 mentions that while videos are more useful than images in some cases they may be di cult to record because of time constraints .
all participants also strongly agreed that the scenarios produced by v2s in the generated scripts are accurate with respect to the scenarios that were manually executed.
regarding v2s s usefulness p1 remarked that the qa team could use v2sto help them create videos more optimally.
p2 supported this claim as he mentions that v2scould help the qa team write provide commands or steps then the tool would read and execute these while recording a video of the scenario and problem.
this solution could be integrated in the continuous integration pipeline .
in addition p3 mentions that v2scould be used during app demos v2scould automatically execute a script that shows the app functionality and record a video.
in this way the demo would focus on business explanation rather than on manually providing input to the app or execute a user scenario .
p2 also indicated that v2scould be used to analyze user behavior within the app which can help improve certain app screens and navigation.
he mentions that v2s could collect the type of interactions of taps etc.to detect for example if certain screens are frequently used or if users often go back after they go to a particular screen .
he mentions that this data could be useful for marketing purposes .
p3 nds v2s potentially useful for helping reproduce hard to replicate bugs.
the participants provided valuable and speci c feedback for improving v2s.
they suggested to enrich the videos produced when executing v2s script with a bounding box of the gui components or screen areas being interacted with at each step.
they also mention that the video could show popup comments that explain what is going on in the app e.g.
a comment such as after seconds button x throws an error which can help replicate bugs.
they would like to see additional information in the script such as gui metadata that provides more in depth and easy to read informationabout each step.
for example the script could use the names or ids of the gui components being interacted with and produce steps such as the user tapped on the send button instead of the user tapped at .
.
.
p3 mentioned that it would be nice to change the script programmatically by using the gui components metadata instead of coordinates so the script is easier to maintain .
317icse may seoul republic of korea c. bernal c rdenas n. cooper k. moran o. chapparo a. marcus d. poshyvanyk they suggest to include an interpreter of commands steps written in a simple and easy to write language which would be translated into low level commands.
answer to rq developers ndv2saccurate in replicating app usage scenarios from input videos and potentially useful for supporting several development tasks including automatically replicating bugs analyzing usage app behavior helping users qa members generate high quality videos and automating scenario executions.
limitations threats to validity limitations.
our approach has various limitations that serve as motivation for future work.
one current limitation of the f r cnn implementation our approach utilizes is that it is tied to the screen size of a single device and thus a separate model must be trained for each screen size to which v2sis applied.
however as described in sec.
.
the training data process is fully automated and models can be trained once and used for any device with a givenscreen size.
this limitation could be mitigated by increasing datasetsize including all type of screen sizes with a trade o on the training time.
to facilitate the use of our model by the research community we have released our trained models for the two popular screen sizes of the nexus and nexus 6p in our online appendix .
another limitation which we will address in future work is that our replayable traces are currently tied to the dimensions of a particular screen and are not easily human readable.
however combining v2s with existing techniques for device agnostic test case translation and gui analysis techniques for generating natural language reports could mitigate these limitations.
finally as discussed in sec.
.
one limitation that a ects the ability of v2s to faithfully replay swipes is the video frame rate.
during the evaluation our devices were limited to 30fps whichmade it di cult to completely resolve a small subset of gesture actions that were performed very quickly.
however this limitation could be addressed by improved android hardware or software capable of recording video at or above 60fps which in our experience should be enough to resolve nearly all rapid user gestures.
internal validity.
in our experiments evaluating v2s threats to internal validity may arise from our manual validation of the correctness of replayed videos.
to mitigate any potential subjectivity or errors we had at least two authors manually verify the correctness of the replayed scenarios.
furthermore we have released all of our experimental data and code to facilitate the reproducibility of our experiments.
construct validity.
the main threat to construct validity arises from the potential bias in our manual creation of videos for the popular apps study carried out to answer rq .
it is possible that the author s knowledge of v2s in uenced the manner in which we recorded videos.
to help mitigate this threat we took care to record videos as naturally as possible e.g.
normal speed included natural quick gestures .
furthermore we carried out an orthogonal controlled study in the course of answering rq where users unfamiliar with v2snaturally recorded videos on a physical device representing an unbiased set of videos.
another potential confounding factor concerns the quality of the dataset of screens used to train test and evaluate v2s f r cnn and opacity cnn.
to mitigate this threat we utilize the r d dataset of screens which have undergone several ltering and quality control mechanisms to ensure a diverse set of real guis.
one more potential threat concerns our methodology for assessing the utility of v2s.
our developer interviews only assess the perceived usefulness of our technique determining whether developers actually receive bene t from v2sis left for future work.
external validity.
threats to the generalizability of our conclusions are mainly related to i the number and diversity apps used in our evaluation ii the representativeness of usage scenarios depicted in our experimental videos and iii the generalizability ofthe responses given by the interviewed developers.
to help mitigate the rst threat we performed a large scale study with of the top applications on google play mined from categories.
while performing additional experiments with more applications is ideal our experimental set of applications represents a reasonably large number of apps with di erent functionalities which illustrate the relatively applicability of v2s.
to mitigate the second threat we collected scenarios illustrating bugs natural apps usages real crashes and controlled crashes from eight participants.
finally we do not claim that the feedback we received from developers generalizes broadly across industrial teams.
however the positive feedback andsuggestions for future work we received in our interviews illustrate the potential practical usefulness of v2s.
related work analysis of video and screen captures.
linet al.
proposed an approach called screenmilker to automatically extract screen shots of sensitive information e.g.
user entering a password by using the android debug bridge.
this technique focuses on the extraction of keyboard inputs from real time screenshots.
screenmilker is primarily focused upon extracting sensitive information whereas v2sanalyzes every single frame of a video to generate a high delity replay script from a sequence of video frames.
krieter et al.
use video analysis to extract high level descriptions of events from user video recordings on android apps.
their approach generates log les that describe what events are happening at the app level.
compared to our work this technique isnot able to produce a script that would automatically replay the ac tions on a device but instead simply describe high level app events e.g.
whatsapp chat list closed .
moreover our work focuses on video analysis to help with bug reproduction and generation of test scenarios rather than describing usage scenarios at a high level.
bao et al.
and frisson et al.
focus on the extraction of user interactions to facilitate behavioral analysis of developers during programming tasks using cv techniques.
in our work rather than focusing upon recording developers interactions we instead focus on understanding and extracting generic user actions on mobile apps in order to generate high delity replay scripts.
other researchers have proposed approaches that focus on the generation of source code for android applications from screenshots or mock ups.
these approaches rely on techniques that vary solely from cv based to dl based .
318translating video recordings of mobile app usages into replayable scenarios icse may seoul republic of korea the most related work to v2s is the appflow approach introduced by hu et al.
.
appflow leverages machine learning techniques to analyze android screens and categorize types of test cases that could be performed on them i.e.
a sign in screen whose test case would be a user attempting to sign in .
however this technique is focused on the generation of semantically meaningful test cases in conjunction with automated dynamic analysis.
in contrast v2sis focused upon the automated replication of any type of user interaction on an android device whether this depicts a usage scenario or bug.
thus v2scould be applied to automatically reproduce crowdsourced mobile app videos whereas appflow is primarily concerned with the generation of tests rather than the reproduction of existing scenarios.
record and replay.
many tools assist in recording and replaying tests for mobile platforms .
however many of these tools require the recording of low level events using adb which usually requires rooting of a device or loading a custom operating system os to capture user actions events that are otherwise not available through standard tools such as adb.
while our approach uses reran to replay system level events we rely on video frames to transform touch overlays to low level events.
this facilitates bug reporting for users by minimizing the requirement of specialized programs to record and replay user scenarios.
huet al.
developed valera for replaying device actions sensor and network inputs e.g.
gps accelerometer etc.
event schedules and inter app communication.
this approach requires a rooted target device and the installation of a modi ed android runtime environment.
these requirements may lead to practical limitations such as device con guration overhead and the potential security concerns of rooted devices.
such requirements are often undesirable for developers .
conversely our approach is able to work on any unmodi ed android version without the necessity of a rooted device requiring just a screen recording.
nurmuradov et al.
introduced a record and replay tool for android applications that captures user interactions by displaying the device screen in a web browser.
this technique uses event data captured during the recording process to generate a heatmap that facilitate developers understanding on how users are interacting with an application.
this approach is limited in that users must interact with a virtual android device through a web application which could result in unnatural usage patterns.
this technique is more focused towards session based usability testing whereas v2s is focused upon replaying in eld app usages from users or crowdsourced testers collected from real devices via screen recordings.
other work has focused on capturing high level interactions in order to replay events .
for instance mosaic uses an intermediate representation of user interactions to make replays device agnostic.
additional tools including hiromacro and barista are android applications that allow for a user to record and replay interactions.
they require the installation or inclusion of underlying frameworks such as replaykit airtest or troyd .
android bot maker is an android application that allows for the automation of individual device actions however it does not allow for recording high level interactions instead one must enter manually the type of action and raw x coordinates.
in contrast to these techniques one of v2s primary aims is tocreate an android record and replay solution which an inherently low barrier to usage.
for instance there are no frameworks to install or instrumentation to add the only input is an easily collectable screen recording.
this makes v2ssuitable for use in crowd or betatesting scenarios and improves the likelihood of its adoption among developers for automated testing given its ease of use relative to developer s perceptions of other tools .
finally as crowdsourcing information from mobile app users has become more common with the advent of a number of testing services researchers have turned to utilizing usage data recorded from crowd testers to enhance techniques related to automated test case generation for mobile apps.
linares v squez et al.
rst introduced the notion of recording crowd sourced data to enhance input generation strategies for mobile testing through the introduction of the m l framework .
this technique adapted n gram language models trained on recorded user data to predict event sequences for automated mobile test case generation.
mao et al.
developed the p approach which is able to infer motif event sequences collected from the crowd that are applicable across di erent apps .
the authors found that the activity based coverage of the s automated testing tool can be improved through a combination of crowd based and search based techniques.
the two approaches discussed above require either instrumented or rooted devices m l or interaction with apps via a web browser p .
thus v2sis complementary to these techniques as it provides a frictionless mechanism by which developers can collect user data in the form of videos.
conclusion future work we have presented v2s an approach for automatically translating video recordings of android app usages into replayable scenarios.
a comprehensive evaluation indicates that v2s i accurately identi es touch indicators and it is able to di erentiate between opacity levels ii is capable of reproducing a high percentage of complete scenarios related to crashes and other bugs with promising results for general user scenarios as well and iii is potentially useful to support real developers during a variety of tasks.
future work can make v2s applicable to di erent software maintenance tasks such as i producing scripts with coordinateagnostic actions ii generating natural language user scenarios iii improving user experience via behavior analysis iv facilitating additional maintenance tasks via gui based information etc.