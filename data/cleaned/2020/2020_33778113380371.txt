engineer ing gender inclusivity into software ten teams tales from the trenches claudia hilderbrand1 christopher perdriau1 lara letaw1 jillian emard1 zoe steine hanson1 margaret burne tt1 anita sarma1 1oregon state university corvallis or usa.
2pacific northwest national laboratory richland wa usa minic perdriac letawl emardj steinehz burne tt anita.sarma eecs.oregonstate.edu 2claudia.hilderbrand pnnl.gov abstract although the need for gender inclusivity in software is gaining attention among se researchers and se practitioners and at least one method gendermag has been published to help little has been reported on how to make such methods work in real world settings.
real world teams are ever mindful of the prac ticalities of adding new methods on top of their existing processes.
for example how can they keep the time costs viable?
how can they max imize impacts of using it?
what about controversies that can arise in talking about gender?
to find out how software teams in the trenches handle these and similar questions we collected the gendermag based processes of real world software teams more than people for periods ranging from months to .
years.
we present these teams insights and experiences in the form of practices potential pitfalls and open issues so as to provide their insights to other real world software teams trying to engineer gender inclusivity into their software products.
ccs concepts software and its engineering human centered computing human computer interaction hci hci design and evaluation methods keywords inclusive so ftware software engineering practices gendermag acm reference format claudia hilderbrand christopher perdriau lara letaw jillian emard zoe steine hanson margaret burne tt anita sarma.
.
engineering gender inclusivity into so ftware ten teams tales from the trenches.
in 42nd international conference on so ftware engineering proceedings icse may seoul republic of kore a. acm new york ny usa.
pages.
h ttps introduction software has repeatedly failed diverse populations falling short of aiding their productivity or even being usable by some populations .
such failures are serious they marginalize people who don t fit where don t fit can simply mean being different from the people who wrote the software.
of the many forms of diversity for which this problem arises its connection with gender diversity is particularly well documented .
making software products usable to people regardless of their gender has practical importance.
if software teams fail to achieve inclusiveness their market size shrinks.
if a project s development tools or products fail to achieve inclusiveness not only is product adoption reduced but also the involvement of women and other underrepresented popu lations in the teams themselves .
a few methods have emerged to help software teams engineer gender inclusivity into their software.
one of these is the gendermag method gender inclusiveness magnifier .
gendermag is a method for finding and also fixing gender inclusivity bugs in software.
empirical research reports that gendermag is effective at helping software practitioners find and fix such inclu sivity bugs in their teams .
howev er little is known about whether and how busy real world software teams can embed gendermag into their development process es given the many demands on their time and the practices they already have in place.
to find out we engaged with software team s via action research.
action research is a type of longitudinal field study that involves engaging with a community to address some problem ... and through this problem solving to develop scholarly knowledge .
it is done collaboratively with pa rticipants not to or for or focused on them.
therefore our study was a fully collaborative endeavor with software teams who were working to engineer gender inclusivity into their software.
as per action research s longitudinal focus our involvem ent spanned months to years.
specifically we had consistent involvement over months with four professional software teams at a university and intermittent data collection over periods ranging from months to .
years with six teams based in industry.
the contribution of this paper is the first in depth how investigation into gendermag based processes these teams worked out to make using gendermag practical and viable in their real world settings as follows how real world software teams went about minimizing time costs of blending this method into their existing development processes .
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
icse may seoul republic of korea association for computing machinery.
acm isbn ... .
https ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea c. hilderbrand et al.
how real world software teams went about maximiz ing the benefits and impact they gained for the time they spent using the gendermag method but also... real world pitfalls the software teams ran into and sometimes averted potentially sabotaging their benefits .
practices the software teams devised to leverage portions of gendermag beyond gendermag evaluation sessions .
unresolved issues for which real world practices are still emerging.
background the practices we investigate are in the context of the gendermag method .
we begin by summarizing gendermag a software inspection method for finding and fixing inclusivity bugs .
gendermag starts by helping a software team find user facing inclusivity bugs in their own ui using five facets of individuals cognitive styles for going about problem solving.
these facets form the core of the gendermag method an individual s motivations computer self efficacy attitude s toward risk information processing style s and learning style s .
gendermag literature defines inclusivi ty bugs as issues tied to one or more of these cognitive facets.
such bugs are cognitive inclusivity bugs but also gender inclusivity bugs because the facets capture well established statistical gender differences in how people problem solve .
for example using these facets a software team might discover an inclusivity bug if a feature is easily discoverable by people with a tinkering learning style but not easily discover able by people with a process oriented learning style.
in essence the diverse problem solving styles represented by the facets capture cognitively diverse behaviors that occur both within a given gender as well as those with statistical differences between one gender and another.
thus supporting multiple facet values in softwar e tends to make software better for people of all genders as illustrated in .
gendermag makes the five facets concrete with a set of three faceted personas abi pat and tim .
personas are a widespread technique in industry.
each persona represents a subset of a system s target users here their purpose is to represent differences in the facet valu es.
abi s facet values represent the opposite end of the problem solving style spectrum from tim s and pat s facet values are a mixture of abi s and tim s. tim s facet values are most often the ones software developers tend to design for and abi s facet values are often overlooked.
portions of the personas that are not about the facets e.g.
appearance demographics experience job title etc.
are customizable figure .
gendermag sets these faceted personas into a systematic process via a specialized cognitive walkthrough cw as follows.
evaluators walk through each step of carrying out a scenario and answer questions about subgoals and actions a user would need to accomplish those subgoals italics added to show key diff erences from standard cws subgoalq will abi pat tim have formed this subgoal as a step to their overall goal?
yes no maybe why what facets are involved in your answe r .
actionq1 will abi pat tim know what to do at this step?
yes no maybe why what facets ... .
actionq2 if abi pat tim does the right thing will they know they did the right thing and are making progress toward their goal?
yes no maybe why what facets ... .
as these questions show identifying issues using this process inclu des identifying the facets that are tied with each.
these facets are often key to the fixes an issue s fix is designed around the facet that raised the issue.
for example to fix an issue that was raised for a particular problem solving style a team would revise that part of the ui to support multiple problem solving styles the already supported one s and the unsupported one s .
in one lab study when user experience researchers used gendermag to identify usability issues over of the issues were validated by other empirical results or field observations and aligned with gender distributions of those data .
more generally p revious empirical studies have found gendermag to be effective at identifying issues and at pointing to ward effective fixes .
however there is almost no research on how teams integrate it into their real world environments .
that is the gap this paper aims to help fill.
methodology to investigate the how s of integrating gendermag into real world teams practices we worked with professional software teams from a university and fro m five companies.
our methodology for this investigation was action research.
.
the action research methodology action research is a type of long term field research common in the fields of medicine and education and now emerging in various computing disciplines.
action research has three stages figure key portions of the abi persona.
see the supplemental document for complete pers onas.
434c.
hilderbrand et al.
icse may seoul republic of korea unfreezing changing and freezing .
in the unfreezing stage an organization decides that a change is needed.
in the changing stage the organization experiments with new proces ses and creates variations with an eye toward producing the outcomes they want.
the refreezing stage is when the new processes and changes become established as part of the organization s processes.
the stages are not strictly linear instead organizations often loop back to previous stages.
action research is unlike many types of field research in two primary ways.
first it is iterative and hands on .
researchers work together with a community the researchers are also participants and the participants a re also researchers .
second its purpose is to develop scholarly knowledge about a problem to be solved and to iteratively solve it .
thus in contrast to other empirical methods formative evaluations summative evaluations and treatment manipulations are intertwined within action research and cannot be separated.
action research emphasizes rigor by focusing on credibility and validity.
triangulation is widely used for this purpose it reports phenomena only when multiple data sources data instances and or investigators etc.
independently arrive at the same conclusions.
section .
enumerates how our data facilitated triangulation and section shows how triangulating these data cross validated the practices and potential pitfalls we report.
.
participants and procedures our study included a diverse set of teams .
we did not collect demographics of team members but we know that at least two genders participated in of the tea ms. a mix of software developers user interface designers site administrators and marketing experts from universit y x a public university and five companies used the method on their own projects.
all the teams had an interest in trying gendermag see section for more on this .
about half the industry teams had previously used gendermag whereas all of the university teams were just starting .
some teams new to gendermag contacted us for help getting started others used gendermag on their own using materials from and or the downloadable kit the gendermag user manual .
for teams who contacted us for help we followed the same general process a pre gendermag meeting to show a team member how to customize a persona and help identify suitable scenarios use case s for analysis and then a gendermag session which usually included time for debriefing.
we started a team s first gendermag session by briefly introducing the method s purpose roles and forms see supple mental document and reminded them of the team s scenario and customized persona.
we then coached and worked hand in hand with the team members during the session to whatever extent they wanted.
likewise some team members acted as researchers as per acti on research devising new gendermag practices and collecting follow up data.
after the first gendermag session we participated in later sessions only if a team asked us to otherwise teams moved ahead as they saw fit .
on the other hand some par ticipants were researchers as per action research they devi sed new gendermag practices and collected data.
we also answered email questions they sent and conducted phone interviews.
the materials available to the participants and our interview scripts are provid ed in the supplemental document.
.
data collected and analyzed central to our methodology s validity is triangulation a cornerstone of qual itative analysis whether the same results manifest themselves multiple times from multiple sources of evidence .
toward this end we collected data of multiple types to triangulate both within and among the teams table .
the data we were ab le to collect were as follows .
from the gendermag session s we attended we collected filled out gendermag forms audio recordings of the session s which we then transcribed the teams customized personas and our observers notes.
we also collected any artifacts we could such as the teams screenshots and or mock ups.
we then followed up the sessions with semi structured interviews when possible and in cases in which further data was offered e.g.
follow up meetings emails public postings we coll ected those too.
when the collection of materials from a gendermag session was not permitted or viable we interviewed these teams table .
the interview questions are enumerated in the supplemental document.
at the end of the data collection period we offered a post study interview and debriefing both to update the data we had collected and to see if teams had tried practices we had not witnessed in that team but had observed in other teams.
at the end of the interviews w e shared study results with t eams to let them see their contribution to the research and to show our appreciation of their work .
to analyze these data we borrowed techniques from grounded team and timespan max members at session s in our data applications these teams were working on a year information for instructors and students about academic technologies b months unknown interface for an ai product c months analytics and reports for staff to gain insights into university trends l year document technologies m months education platform for instructors n .
years an it support product for end users o year search engine p year web based interface for visual sorting with a deep learning back end w year web application for employees who manage web content y months application for customer communities table the university and industry teams in our study and the timespan over which we were able to intermittently collect data.
435icse may seoul republic of korea c. hilderbrand et al.
theory and used triangulation for validity.
s pecifically two authors went through all data of all types and marked ground up all entries about process which filtered out entries not relevant to process .
we then added memos to clarify the context of each how it arose and what it was trying to achieve avoid then thematically group ed them iteratively into practices pitfalls.
we then applied the inclusion criteria below to filter out practices pitfalls lacking enough data and triangulated the rest as a final validity check which we return to in table .
we applied two sets of inc lusion criteria.
one inclusion criterion was that e very practice pitfall we report here had to have occurred in at least two independent occurrences or teams.
our pur pose was to raise the likelihood that the practice s pitfall s would be applicable to other real world teams looking for guidance on using gendermag to make their software more inclusive .
a second inclusion criterion was that every practice pitfall included here had either not appeared in refereed publications or had added new rationales benefits costs not previously reported summar ized later in table .
from unfreezing to changing in action research the unfreezing stage is a necessary prerequisite to the changing stage.
for university x we were part of the unfreezing stage during the course of the investigation.
university x had already reached action research s unfreezing stage and beyond from a general diversity and inclusion perspective but not yet reached it from a gender inclusive technology perspective .
at the time this study began t he cio s office had just decided to explore the possibility of incorporating gendermag into some of their it processes.
they funded a graduate student to help move it forward began regular meetings and arrang ed for the researchers to present the gendermag method to a group of it teams to see if any would want to step forward.
we presented it at a campus it meeting and as section has mentioned a number of teams expressed interest in trying it out.
we report on those teams with whom we have the longest involvement.
the six industry teams in this paper were located in five companies at which the importance of diversity and inclusion had also been accepted.
they had heard about gendermag from presentations or p apers and had expressed interest in trying it.
these events brought the teams to the outset of action research s change stage in a tentative way .
still f or busy software teams changes in process can be expensive so teams needed to work out whether the u pfront costs time of changing their processes to engineer inclusiveness into their software would pay off in useful and impactful benefits as the next sections consider.
results minimizing costs to minimize their costs of running gendermag sessi ons teams worked out several practices but also ran into two pitfall s. table summarizes and we detail them in the next subsections.
.
learning gendermag vs. gendermag some teams wanted to get started with gendermag immediately but t his sometimes led to incompatible goals for a gendermag session using the session to enable an entire team to learn gendermag hands on versus using a gendermag session to do gendermag evaluations to get the needed product fixes underway .
the incompatibility came from group size s. including many team members in a gendermag session had at least two advantages consistent with those experienced by earlier teams more of the team g ot hands on experience with the method and more people in the room during the session brought diverse perspectives during the evaluation which tended to increase the completeness of the evaluation .
team a was one of the teams who decided to include a large group seven team members in their first gendermag session.
their context was a website for instructors and students so they made the abi persona an instructor figure and evaluated the scenario abi wants to f ind instructions to add a ta to a course site.
at the time of that session they had not differentiated t he learning vs. goals.
for team a both advantages of having a large team materialized.
regarding hands on learning of the method a ll seven members actively engaged in the session.
the team s designated re corder took detailed notes with some other team members taking their own notes as well.
the second advantage materialized too the relatively large size of the group helped bring o ut diverse perspectives because the process captures the union of perspectives of everyone at the session not just the more vocal people in the room.
however the large group size slowed down the evaluation the team first gendermag session more ses sions other mtgs inter views emails soc media shout outs form rec.
pers.
obs.
a b c l m n o p w y table the multiple sources of data we were able to obtain from multiple teams enable d extensive triangulation.
form forms filled out by team during the session.
rec.
audio recording of session.
per s. team s customized persona.
obs.
observers field notes .
practices or potential pitfalls team learning gendermag hands on vs. gendermag a m beyond our control c l m multi path evals a c l abstracting beyond a c evaluating a proxy c w table the teams practices and two pitfall s shaded in gray teams ran into in minimizing their costs .
436c.
hilderbrand et al.
icse may seoul republic of korea more people s opinions to capture the mor e time was spent on each question.
during the two hour session they finished only one scenario evaluation steps not the two scenarios the team had planned to evaluate.
the team decided that this pace was probably too time costly to be viable.
during a follow up meeting team a decided that to get enough gendermag ing done on their product they needed to reduce the evaluation sub team to just three members.
this change also clarified who was accountable for follow through on the issues they found .
ta ...we are ... going to pair up based on whose people s time and availability align with moving forward t teamname datasource e.g.
ta line of a transcribed recording of team a and ta email means an email message from team a. ta email ... we should be able to run through the full gendermag process again with the two tasks above... it should provide a decent template for building a lot of the rest of the website.
this enabled team a to proceed much more efficient ly and within a few months th ey release d their redesigned product.
practice learning gendermag hands on vs. gendermag .
teams noticed that the goal of learning gendermag hands on had in compatib ilities with gendermag .
large groups seemed to be best for learning small groups best for but small teams were not a panac ea as team m found out .
team m started out trying to combine the learning with the in a single se ssion especially since team m was unsure whether the method would even be useful tm ...it was very complicated to explain to them why this was different and even though they were receptive to it it was difficult to argue why a different type of persona was useful.
thus given this uncertainty about the method team m started with a small evaluation group just two team members in their first sessio n. the session went reasonably well but after the session they ran into a problem.
they found themselves unable to communicate the need to fix the problems they had found to the team members who owned those parts of the system who had not learned the m ethod along with them tm ...our supervisor said why are you telling me all this?
this led them into the pitfall of being unable to change the aspects of the software they had evaluated because they were not its decision makers and had not involve d the real decision makers in either the gendermag learning or the this pitfall arose here because of an ownership problem but also arose in other situations that the evaluators did not really own such as software using third party apis or sub systems not controlled by the team .
all of these situations left one or more team s without the ability to act upon the ir results .
potential pitfall beyond our control .
teams that tried to use gendermag on sub systems for which they lacked decision making power were less likely than they expected to fix the problems they found in the evaluation.
thus the evaluation was either time wasted or they had to spend extra time convincing the decision makers to make the fixes.
.
walking multiple paths at once a gendermag walkthrough is designed to evaluate a single path sequence of actions through an interface with no branching because of the cognitive cost and group confusion of context switches between branches.
however team s a c and l figured out how evaluating two small paths at once could increase their gendermag method efficiency.
figure illustrates team c s use of this practice.
when evaluating their software s analytical reporting dashboards they ran across two different paths a user might take from a single starting place to achieve a single goal.
the paths were short and diverged for only a short distance so the team decided to evaluate both to compare them.
their multi path evaluation s paid off they avoided re evaluating in common segments of the two paths.
their evaluation also revealed that the most straightforward path was not as discoverable as the alternative path enabling the team to see why their users rarely chose the straightforward path tc ... there s two modes of getting to the answer here so the first mode she d hover on the feature it doesn t tell you what to do ... she s not going to realize she has to click on the bar.
similarly team l ran into multiple ways for their software to print a pdf.
comparin g two possible paths with a multi path evaluation like team c s team l found an issue and a fix to make the most direct path discoverable to people with abi s information processing style.
tl ... the image isn t linked that would be nice... also there is more than one way to download the pdf this is the most direct way... figure team a customized abi to be an instructor by filling in the customizable parts of figure .
blue text was customization red text was fixed not customizable .
figure team c evaluated both of the small paths that abi could take to reach the same subgoal.
437icse may seoul republic of korea c. hilderbrand et al.
practice multi path evals .
teams that did simultaneous evaluations of two small paths could reduce the number of sessions needed to evaluate both paths.
this practice was viable when the actions started and ended at the same place and achieved the same subgoal and also facilitated direct comparison between the paths.
.
abstracting with discipline a characteristic of the cw family of which gendermag is a member is that cws are concrete in a way analogous to testing.
they take concrete inputs for gendermag these are a particular customized persona a particular scenario a particular prototype and produce concrete outputs for those inputs.
despite this concreteness teams a and c both worked out a way to abstract beyond a session s concrete outputs.
they did so by choosing for their evaluation a single instance of a ui pattern used in multiple places in their system.
they then treated the single instance s evaluation as being applicable across all instances of that ui pattern thereby eliminating the need to evaluate each instance in its own context .
for example team c selected a representative analytical reporting dashboard to evaluate with the idea of applying their results across all the instances of that dashboard in their application tc ...it s not just for one dashboard even though we tackled just one dashboard ... it s a good starting point for all our dashboards.
tc so some of the things we found in th is session are definitely going to apply across the board ... practice abstracting beyond .
teams abstracted beyond one session s concrete results to entire ui patterns enabling them to reuse their findings and fixes.
on the other hand it did not pay to be ad hoc about abstracting from one ui s evaluation to multiple instances of the ui .
for example team c had hoped to evaluate an application that had recently been updated but brought a machine to the session whose system was out of date .
they tried to evaluate the new system using the older one as a proxy but this caused problems.
it slowed them down confused them and lost the clear connection to what information abi would and would not see tc ...in the re al environment there wouldn t be...these other tabs.
tc so it might not have the styling ... even more problematic the workflow and features that were available in the new interface to the users were not evaluated .
thus the practice of abstracting beyond paid off when it was used with discipline i.e.
only for multiple instantiations of a single pattern but not when systems were merely similar.
potential pitfall evaluating a proxy teams who tried to evaluate a similar system t o the one they really cared about ended up evaluating things that were present in the proxy but not the real system omitting things that were in the real system but not the proxy and or spen ding extra time during the evaluation trying to keep the differe nces straight.
results maximizing benefits teams worked out several ways to maximize the benefits they got from their gendermag sessions.
table summarizes these practices which we detail next.
.
abi s powers for our teams the abi persona was a powerful tool in two ways the strength of abi s inclusivity lens brought their attention to users who were not like we were imagining and the ways their abi s empowered team members to talk about inclusivity issues in their software.
previous studies have reported more inclusivity bugs when using abi than when using the other gendermag personas .
abi s lens strength may be because abi like populations tend to not be like the users developers were imagining when they made their design decisions.
under this hypothesis the gendermag kit proposes that abi offers the strongest lens and all the teams decided for this reason to use abi first .
some teams also used other personas team n also used pat and team o also used tim .
team m also had another not like we imagined reason for using gendermag on their web application for computer science instructors.
t hat team chose abi to explore a user population who despite being tech savvy had lower computer self efficacy than their peer s tm we chose to use ab i ... because we wanted to explore a user with low self efficacy with the technology ... it s hard to explain to our ... team members why somebody with multiple phd s ... would blame themselves for problems with the interfaces team n also chose abi but for an opposite not like we imagined user to find inclusiv ity bugs for users who are not it savvy tn we primarily relied on the abi persona ... because we deci ded to err on the side of targeting ... people who are expressly not it people.
abi s attitude towards technology risk really tended to play a role.
practice abi first.
all the teams used abi as their first persona some because abi seem ed to offer the most powerful inclusivity lens and some to focus on a particular relevant but overlooked population .
second abi empowered certain kinds of communication.
abi served as both an alibi and armor such as by giving team members a way to provide design suggestions safely .
for example for team s m and n using abi to communicate design problems averted implying that specific designers or developers had done bad work such as in the following examples pointing out places in the ui without enough information to satisfy comprehensive information gatherers like abi facet information processing style tn ... we have the devs who designed this ui and it was like once they were ab i they could let go of their ego.
and they were really like you re right ab i wouldn t understand this .
tm ...when we brought it up to our operations lead...we kind of stressed...we feel that a professor who thinks and acts like ab i would have...been confused abi also helped team c talk about subsets of users while avoiding practice s team abi first a b c l m n o p w y speaking through abi c m n calculating bias l n w table the teams practices and potential pitfalls for maximizing their benefits.
438c.
hilderbrand et al.
icse may seoul republic of korea potentially sensitive discussions about particular users or user populations tc ...it was awesome that we had ab i to...be the user ...abi gave us the springboard to be able to talk about that and not necessarily feel bad ... practice speaking through abi teams used abi to ease potentially contentious or uncomfortable design discussions by framing critiques from abi s perspective and talking about user groups by talking about ab i. .
calculating your software s bias at the end of their gendermag sessions three teams calculated bias by looking at the number of inclusivity bugs they found in their software.
in so teams followed the gendermag convention of considering an issue an inclusivity bug if they had tied it to one or more of the facet values in the persona they had used because issues tied to the facets disproportionately affect users who have those facets .
to make the calculations the teams counted the number of evaluation questions they had answered this became the denom inator.
they then counted the subset of no and or maybe re sponses tied to a gendermag facet this became the numerator.
the resulting fraction is the percent of evaluation questions that revealed an inclusivity bug.
for example figure shows one team s bias calculations from one of their gendermag sessions.
in that session of the questions they answere d showed the presence of inclusivity bugs.
these calculations turned out to be quite compelling to the team members and led to big picture discussions of three types.
first the teams began to realize how much they had been relying on assumptions about h ow their populations problem solve.
tw ... abi violates a lot of our assumptions around...our tech.
tn debriefrecording abi as defined probably would not do it succeed ... if someone were discussion of several facets ... we need to accommodate that second the act of calculating bias generated considerable thoughtful discussion about the facets themselves and how they applied to different people.
team members started explaining the facets to each other and even claiming some as their own tn debriefrecording my personality falls somewhere between abi and tim.
i m a read the manual kind of person ... i m super risk averse... tn fieldnotes not the same person as above i m abi!
third they realized the importance of fixing the inclusivity bugs they had found sometimes using the facets to categorize the bugs tw i would be interest ed in knowing more about how we can fix any of these problems.
let s pick any and let s go through whether this actually fix it.
tw not the same person as above ... i think a lot of the failings were based on the fact that we assume users will exp lore the system... practice calculating bias.
calculating their bias scores from the gendermag forms they had filled out led the teams into big picture reflections about their populations the facets they were overlooking and where to get started fixing the inclusivity bugs they had found.
results beyond the session teams also worked out practices that extended beyond gendermag evaluation sessions as table summarizes.
.
gendermag ing beyond products four teams surprised us by bringing gendermag facets and personas beyond analytical evaluations.
with these teams gendermag started influencing their user stud y recruitment helped to bring inclusivity to the forefront of their workplace conversations and even turned up in their daily lives .
for example team b and team n decided to use gendermag facets to pick the participants for their upcoming user study to ensure representation of a diverse set of cognitive styles tb email ... add facet related que stions for the screening document .
team c reported that once the method began to spread at university x it increased awareness of and conversations about important gender issues tc pi the suggestive emails from colleagues... got us thinking about an issue that s prevalent today.
some even started to notice abi s applicability far beyond the gendermag endeavors in their workplace noticing for example that the ui in gym equipment appeared to be optimized for people with a tinkering learning style ta i totally had an abi moment at the gym!
practice gendermag ing beyond products .
teams brought aspects of gendermag beyond their internal product evaluations leveraging it for user studies seeing it generate diversity inclusion conversations at work and noticing its applicability to other environments in their lives.
.
analyz ing real users gendermag facets for some teams leveraging bits of gendermag for use with real users went beyond simply recruiting for user studies.
f our teams also worked out multiple ways to leverage gendermag to analyze their real users as well .
figure one team s bias calculations.
s are out of total evaluation questions answered .
if teams answered no and or maybe without marking a facet the issue was noted as a genera l usability bug yellow if a facet was also marked the issue was considered to disproportionately affect people whose cognitive styles match that facet blue .
general usability bugs inclusivity bugs no bugs practice team name gendermag ing beyond products a b c n facet survey b n o y gendermag moments a b c n o p w table teams beyond the session practices.
439icse may seoul republic of korea c. hilderbrand et al.
this practice started when team n decided to do a survey to find out what facet values their own user populations had.
team n had a history of using surveys to categorize their user populations so they merged portions of their exist ing surveys with questions like the one in figure .
some of the questions they added including the ones in figure came from literature searches for validated questionnaires and others had to be worked out from scratch.
team n later shared their facet questions and team b and team o then started using the questions to help analyze data from their lab studies.
for example team o grouped the inclusivity bugs they found by the facet values that had revealed them.
this helped guide their work toward fixing these inclusivity bugs and to then measure whether the fixes actually made their system more inclusive.
their lab study revealed that the resulting system was indeed more inclusive and was generally as good or better than the original across almost all of the facet values.
practice facet survey .
teams brought the facets into survey questions to measure their real users facet values in multiple ways besides recruiting for user studies.
they also used them to underst and their user populations analyze their lab study data and measure the effectiveness of their fixes facet by facet.
.
gendermag ing in a moment team n was first to tell us about a practice we will term gendermag moments .
they shared the practice and seven teams ultimately used it.
gendermag moments tiny fragment s of a gendermag session are triggered just in time by some kind of design question e.g.
should we show the choices alphabetically or in sequenc e?
in a gendermag moment team members already familiar with the full method personas and facets answer the two gendermag action questions in the context of the trigger actionq1 will abi pat tim know what to do at this step?
yes no maybe why what facets ... .
actionq2 if abi pat tim does the right thing will they know they did the right thing and are making progress toward their goal?
yes no maybe why what facets ... .
for example team a started blending gendermag moments into their design meetings to c onsider how to fix issues they had found by using the full method.
at first they did not realize they were even so until one team member pointed out ta ... we ve just been moments !
team a also used gendermag moments in a slightly different way.
they expanded them to include referring back to the gen dermag forms they had filled out originally to check the design fix would address all inclusivity bugs they had found.
ultimately seven teams used gendermag moments to save time and streaml ine the gendermag process.
this reduced the frequency of full gendermag sessions needed while allowing teams to continue assessing the inclusiveness of their designs.
practice gendermag moments teams worked out two versions of gendermag moments using the gendermag questions to guide the evaluation of design solutions just in time checking against the earlier sessions filled out forms to decide whether the fixes would address all the inclusivity bugs they had originally found .
the practic es taking hold as the preceding sections have show n teams had their own ways of integrating portions and variants of gendermag into their existing practices .
in this section we consider what happened next .
we begin with the bottom line the products.
all teams decided to fix their products according to their gendermag results.
as table shows of the teams have already done so and team l s changes are in progress.
the tenth team team m decided to also but ran into the pitfall described in secti on .
and was not able to make the changes.
ty email ...here are the changes we ve made so far ... enumerated resolved bugs related to their gendermag analyses .
ta publicposting we used the gendermag process and tools to completely redesign our websit e to make it easier to navigate and to get answers quickly to commonly asked questions .
which ta publicposting ...reduced help desk tickets on common questions.
beyond specific product fixes gendermag also affected team practices.
some teams reported their gendermag practices to be affecting team members mindsets about their users bringing diversity of cognitive styles to the forefront of their awareness figure facet survey.
a portion of the facet survey used by some of the teams.
this portion measures computer self efficacy.
the complete survey can be found in the supplemental document.
team changed the product?
continued their gendermag based practices?
a b c l in progress m ?
n o p w ?
y table .
how teams followed through.
yes.
?
no evidence available.
checkmark team as.a whole did not reuse but team members carried it to new teams.
see text.
440c.
hilderbrand et al.
icse may seoul republic of korea tc pi gendermag helped our team by training people to realize that not everyone will click on stuff.
ta pi was not something we even were aware of.
we were not familiar with cog styles and how that might affect success when using the product.
as table show s of the teams showed evidence of using the gendermag practices they had worked out the practices reported here and or additional gendermag based practices enumerated in the supplemental document tc email the practices pitfalls handout is all around our office spaces as a reminder ... we do not have evidence to report on team s who did not provide explicit information one way or another on continuing their gendermag practices.
however one phenomenon we ve seen in the field is a lack of organizational owner ship leading to this employees not being empowered to spend further time on gendermag .
organizational ownership seemed to make a difference in follow up for some teams in our study.
for example b y the time of this writing the university x teams practices had spread from the university teams in this dataset to perhaps due in part to a leadership group s report to university x s provost includ ing a recommendation to x leadership memo ... integrate gendermag evaluations into x s regular it practices .
discussion heated discussions in the trenches a few challenges did not produce enough independent data instances for us to include it in earlier sections as a practice pitfall .
thus we consider them to be issues that remain open .
.
sometimes talking about gender is hard gender bias can be a controversial topic and some team members who were eager to fix their software s biases were less than eager to talk about them as gender biases.
to those team members the name gendermag was uncomfortable tm i think the name gendermag was kind of distracting.
i had to clarify to people that it s about gender differences but that s not the only important part of it.
tb ... i would be happier with a different name.
but i didn t come up with one.
this discomfort echoes earlier reports of teams wanting to talk about gender without talking about gender .
although none of our teams reported that talking about gender took away the benefits of gendermag a previous group chose instead to use the vocabulary of the facets e.g.
different levels of risk tolerance information processing styles etc.
.
another solution arose during the time of this investigation referring to gendermag s family name instead inclusivemag .
early feedback has been encouraging but we do not yet have field data.
.
arguing over the scenario sequence earlier versions of the gendermag method required a team member to pick an exact sequence of actions in advance as pre work .
this did not lead to arguments but field studies showed that the pre work was burdensome and potentially unnecessary .
by the time of the current investigation the gendermag process ha d evolved so that the only pre work required was to customize the persona if desired and name the scenario s being evaluated.
the specific action path through the scenario was left to the team to choose just in time one action at a time as the session progressed.
this led to a new problem.
for example team c members had different ideas about which action path to evaluate debating at length each next step to evaluate.
such debates consum ed valuable time and even led the team to try to backtrack modify entire scenarios midstream leading to ever more confusion.
to avoid this problem we started coaching teams to leave deciding which step to evaluate next to a ui driver the person who does the actual clicking through the prototype during a sessio n. so far arguments over the next step in a sequence have not been reported or observed since we made this change.
threats to validity and mitigations no empirical study is perfect.
one reason is the inherent trade off among different types of val idity .
field studies including action research studies achieve real world applicability whereas controlled studies achieve isolation of variables.
external validity refers to the ability to generalize the findings of a study.
we mitigated the risk of introducing threats to external validity by analyzing multiple teams at a university and in industry.
even so the practices that we collected from the teams may limit our ability to generalize the use of these practices to teams outside these groups.
internal validity refers to how the study design can influence conclusions of the study.
our study has several uncontrolled variables.
for example as an action research study we did not attempt to control for teams prior design practices or knowledge of gender issues even had we wanted to there is a lack of robust measurements for either.
teams and team members varied in the levels of insights they were able to gain from the method some of these variations could have been due to the members pre existing ability to empathize with their users and some could have been due to th e project each was evaluating.
there were also several factors that may have determined what we did and did not ob serve such as team members prior experience with inspection methods and the make up of the teams and projects.
therefore some interpretatio ns we made from the data might be different had we studied different teams or projects.
the practice pitfall list is only what we observed and triangulated from teams data over this period of time .
finally as in any action research study we worked with the teams to help them develop solutions.
as experts in this method our contributions to the sessions we attended may have helped some teams to avoid pitfalls.
also our mixed gender research team s position is that methods for creating socially equita ble software are critically important in software engineering practice so our enthusiasm for the method may have caused us to not notice some potential pitfalls.
partial mitigations for these threats were that we were not present for all teams that teams themselves not us collected some of the qualitative data and that even teams we helped then decided alone without us 441icse may seoul republic of korea c. hilderbrand et al.
whether to continue.
to reduce effects of the threats above we collected data from multiple teams and software projects and made ex tensive use of data triangulation as detailed in table .
related work the most common type of se research about gender inclusivity is in how inclusive software communities are .
for example researchers have sho wn that gender diversity within oss communities while limited creates better communication structures .
ford et al.
found that peer parity having similar others for comparison was an important factor in women s decision to engage in a software development community .
mendez et al.
found that gender biases in oss tools and infrastructure can impact oss newcomer success .
lee and carver found that some contributors used gender neutra l profile names to avoid being judged because of their gender .
paul et al.
found that when reviewing pull requests men frequently wrote negative comments while withholding positive encouragements from women .
terrell et a l. found that among new contributors non core members outsiders men s and women s pull request acceptance rate was similar when their profiles are gender neutral but gender biased when gender could be identified .
such inclusivity bugs are problematic for both an organization s community and its productivity as research across multiple fields has repeatedly shown.
as a recent example in software engineering vasilescu et al.
s analysis of github software projects and participant surveys fou nd that gender and tenure diversity significantly increased productivity .
as to research in real world practices for creating gender inclusive software there is only a little research.
williams created a collection of design process recommendations for including women in the decision making that shapes software but did not investigate them longitudinally .
also t here have been studies of gendermag being us ed on real world systems e.g.
that were not longitudinal.
in these studies teams investigating their own software have found gender inclusivity bugs in surprisingly large fractions of their software features reporting averages ranging from and up .
however these studies did not investigate real world teams ways of integrating gendermag into their existing practices.
the only longitudinal investigation into teams practices with gendermag has been the short report about gendermag at microsoft .
that report covered only a few practices for integrating gendermag into a real world setting and they do not overlap with the practices reported in this paper .
additional practices our teams used some of which are not novel are in the supplemental document.
finally research has investigated general usa bility inspection methods in real world settings such as heuristic evaluation and cws one notable example is .
however these methods and therefore investigations of their use are not about how teams can engineer inclusivity into their software.
conclusion in this paper we have presented a longitudinal field study in which ten real world software teams at six institutions worked to engineer inclusivity into their software.
the investigation spanned from months to as long as .
years in one team s case.
the results revealed practices potential pitfalls and open issues the teams worked on or encountered in combining the new method with their existing team practices and cultures.
some par ticularly interesting practices they worked out were even though gendermag operates at the level of concrete uis teams abstracted them to ui patterns that were common in their applications prac tice .
even though gendermag is an inspection method teams used it to re invent their ways of recruiting for and analyzing some of their user study methods by leveraging the meth od s facets into survey and analysis instruments prac tice .
even though gendermag is an evaluation process teams also used it as a communication mechanism speaking through abi to gain both an alibi and armor prac tice .
this paper is the first investigation of its kind into practices of real world teams who were exploring how to go beyond just making their software work to making it work inclusively for different genders.
perhaps the central message behind these teams experi ences is that suspecting your software of gender bias and wanting to fix it are all very well and good but integrating a systematic process ca n make all the difference tc i thought it was very very informative ... there are some things that we knew we had to change ... this ... gave us a process first gm session multi gm sessions follow up mtgs interviews emails evidence in prior lit.
minimizing costs learning multi path evals abstracting beyond control eval ing proxy maximizing benefits abi first speak thru abi calculating bias beyond the session gendermag ing beyond products facet survey gendermag moments table e vidence behind each practice pitfall .
the checkmarks are instances of the data source s columns providing the evidence .
for example we observed evidence of the facet survey practice in follow up meeting interview emails and in prior literature .
442c.
hilderbrand et al.
icse may seoul republic of korea