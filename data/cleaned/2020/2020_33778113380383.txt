retrieval based neural source code summarization jian zhang sklsde lab beihang university china zhangj act.buaa.edu.cnxu wang sklsde lab beihang university china wangxu act.buaa.edu.cnhongyu zhang the university of newcastle australia hongyu.zhang newcastle.edu.au hailong sun sklsde lab beihang university china sunhl act.buaa.edu.cnxudong liu sklsde lab beihang university china liuxd act.buaa.edu.cn abstract source code summarization aims to automatically generate concise summaries of source code in natural language texts in order to help developers better understand and maintain source code.
traditional work generates a source code summary by utilizing information retrieval techniques which select terms from original source code or adapt summaries of similar code snippets.
recent studies adopt neural machine translation techniques and generate summaries from code snippets using encoder decoder neural networks.
the neural based approaches prefer the high frequency words in the corpus and have trouble with the low frequency ones.
in this paper we propose a retrieval based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set.
our approach can take advantages of both neural and retrieval based techniques.
specifically we first train an attentional encoder decoder model based on the code snippets and the summaries in the training set second given one input code snippet for testing we retrieve its two most similar code snippets in the training set from the aspects of syntax and semantics respectively third we encode the input and two retrieved code snippets and predict the summary by fusing them during decoding.
we conduct extensive experiments to evaluate our approach and the experimental results show that our proposed approach can improve the state of the art methods.
ccs concepts software and its engineering software maintenance tools .
keywords source code summarization information retrieval deep neural network also with beijing advanced innovation center for big data and brain computing beijing china.
corresponding author xu wang wangxu act.buaa.edu.cn.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
reference format jian zhang xu wang hongyu zhang hailong sun and xudong liu.
.
retrieval based neural source code summarization.
in 42nd international conference on software engineering icse may23 seoul republic of korea.
acm new york ny usa pages.
.
introduction source code summarization aims to generate summaries which are concise descriptions of source code and are often presented in the form of code comments.
summaries are important for understanding and maintaining source code.
developers often spend a lot of time on reading and comprehending programs when there is no good software documentation .
however wellcommented projects are few and manually writing source code comments is tedious and time consuming.
also comments should evolve with the evolution of source code which could incur more maintenance cost.
therefore it is important to explore automatic source code summarization techniques .
information retrieval ir has been widely used in automatic source code summarization .
the ir techniques are used to select appropriate terms from the original code snippets for producing term based summaries.
for example haiduc et al.
adopted latent semantic indexing lsi and vector space model vsm to choose good terms from source code and produce source code summaries.
eddy et al.
further improved this method through topic modeling.
rodeghero et al.
modified the term weights of vsm through eye tracking and obtained better summaries.
besides the term based summaries summaries can be also generated from the comments of similar code.
since code duplication is common in large scale code repositories code clone techniques are adopted to retrieve similar code snippets from existing code repositories or q a sites.
the comments of the similar code can be adapted for generating a new summary .
recent work on neural machine translation nmt shows that neural source code summarization is promising .
these neural models usually follow an encode decoder framework.
for instance iyer et al.
proposed an end to end neural network which can encode code snippets with an embedding layer and decode them into natural language summaries through a long short term memory lstm model with attention mechanism.
to incorporate different aspects of a code snippet ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea jian zhang xu wang hongyu zhang hailong sun and xudong liu many approaches encode the code snippet by extracting its api sequence or abstract syntax tree ast .
as described above ir based methods can effectively leverage the existing terms of the original code or summaries of similar code snippets while nmt based methods generate summaries word by word from the whole corpus by directly maximizing the likelihood of the next word given the previous words .
the nmt based methods generally prefer high frequency words in the corpus and may have trouble with low frequency words .
that is if the ground truth summaries contain words that are rare in the corpus the nmt based methods may ignore these words and produce wrong results.
a more recent study on commit message generation found that a very simple retrieval based method can outperform a carefully designed nmt model.
what is more in the community of natural language processing nlp zhang et al.
revealed that nmt model is comparatively weak in generating infrequent n grams because it tends to generate more frequent n grams for natural language translation.
while in practice as our experiments show section .
.
summaries contain low frequency words frequency for python and java code snippets respectively.
simply increasing data size cannot help mitigate such problem since the frequencies of common words will increase more quickly than rare words and new low frequency words may be exposed.
since the summaries of similar code snippets from ir based methods are reusable the words in the expected summaries including the low frequency ones are also highly probable to appear in them.
for example for the code snippet in figure we retrieve two most similar code snippets and their summaries where one is similar with respect to syntax and the other is similar with respect to semantics.
we can see that the low frequency word iis which is a web server developed by microsoft also appears in the retrieved summaries but is ignored by nmt.
in contrast the high frequency word create can be captured by nmt.
clearly it is desirable to have a hybrid approach that combines the nmt based and ir based methods and simultaneously incorporates both the high frequency words in the corpus and the low frequency words in the summaries of the similar code snippets.
in this paper we propose a novel neural architecture namely retrieval based neural source codesummarizer rencos which can take advantages of both the nmt based methods and the retrieval based methods.
more specifically we first train an attentional encoder decoder model to obtain an encoder for all code samples and a decoder for generating natural language summaries second given an input source code snippet we retrieve the most similar code snippets from the training set.
in this work we obtain two most similar code snippets based on the syntax level and semantics level information of the source code respectively.
for syntactic level we parse code snippets into asts and calculate their similarities based on asts.
for semantic level we reuse the trained encoder to embed code snippets with semantic vectors and compute the similarities based on these vectors.
finally during the testing rencos uses the trained model to encode the input and the retrieved two code snippets as context vectors.
it then decodes them simultaneously and at each time step it adjusts the conditional probability of the next word using the similarity values and ground truth create an iis application .
syntactic retrieval remove an iis application .
semantic retrieval create an iis virtual directory .
nmt create the new app .def create app name site sourcepath apppool none pscmd list pscmd.append new webapplication name site .format name site pscmd.append physicalpath .format sourcepath if apppool pscmd.append applicationpool .format apppool cmd ret srvmgr str .join pscmd if cmd ret if name in list apps site return true return falsefigure an example of nmt based and ir based source code summarization where the correct words are marked in red the conditional probabilities from the retrieved two code snippets.
in this way it trains a classical nmt model and incorporates the retrieved information to enhance the prediction results of the nmt model.
we conduct extensive experiments on two real world datasets and the results demonstrate that our approach is better than those using only ir based methods or nmt based methods.
the proposed approach also outperforms the state of the art work with respect to four widely used quantitative metrics bleu rouge l meteor and cider .
furthermore we also perform a human evaluation by posting micro tasks and hiring workers through amazon mechanical turk amt .
the results further confirm the correctness of the summaries generated by our approach.
our main contributions are outlined as follows we propose a novel retrieval based neural architecture to enhance the nmt model for summarizing source code with the help of most similar code snippets.
to the best of our knowledge this is the first work that combines retrieval based and nmt based methods in source code summarization we conduct extensive experiments to evaluate our approach on two real world datasets.
we also perform a human evaluation through amazon mechanical turk amt .
all results confirm that the proposed approach is effective and outperforms the state of the art methods.
the remainder of this paper is organized as follows.
section describes the details of our approach.
we evaluate our approach in section .
the threats to validity and related work are presented in section and section respectively.
finally we conclude our work in section .
our approach .
overview in this work we propose one retrieval based neural source code summarization approach rencos which can combine the strengths of nmt model and retrieval based methods for better source code summarization.
unlike retrieval based neural models in nlp 1386retrieval based neural source code summarization icse may seoul republic of korea encoder code summaryattention code summary attentional encoder decoder model code traversalsyntactic level code sequence semantic level code vector syntactic level semantic level input code summaryoffline training and preparing code retrieval base online testing decoder search retrieved similar code snippetsretrieval based neural summary generationencoder decoder encoder encoderast parser encoder training setcode semantic vectorpoolingast based t oken sequence ... ... ... lucene cosine similarity code retrieval base decoder decoderfusion figure the overall framework of our approach our approach does not need additional encoders for the retrieved code with joint training or the translation pieces ngrams in the retrieved target sentences .
our approach includes an attentional encoder decoder model two similarity based code snippet retrieval components syntactic level and semantic level and the retrieval based neural summary generation.
the online and offline workflow is shown in figure .
specifically during training we first collect a large corpus containing source code snippets and their natural language summaries to train an attentional encoder decoder model section .
.
such an encoder decoder model can be directly used for all inputs and retrieved code snippets during the testing phase.
in order to accelerate the code retrieval efficiency during testing we offline parse all the code snippets from the training set into asts and turn them into syntactic level token sequences by tree traversal.
we also use our trained encoder to embed code into semantic vectors by pooling.
all the code sequences and semantic vectors are stored as the code retrieval base.
when testing online given one input code snippet we search for two most similar ones from the code retrieval base section .
.
one is obtained by the commonly used retrieval engine lucene1 on the syntactic level token sequences i.e.
syntactic level and the other is selected with the highest cosine similarity based on the semantic vectors i.e.
semantic level .
then we produce the final summary by fusing the results of the two retrieved code snippets in our retrieval based neural summary generation section .
.
.
attentional encoder decoder model similar to the existing neural source code summarization methods we build and train an attentional encoderdecoder model which includes only one encoder for avoiding the high overhead of jointly training additional encoders for the retrieved code .
this model is trained only once but can be used three places encoding and decoding the input code snippets of the testing set and the retrieved code snippets from the training set and helping retrieve the most similar code snippets at the semantic level as shown in figure .
in this way our approach is simple and efficient without extra training.
for the encoder suppose there is one code snippet cconsisting of a sequence of words w1 wn an embedding layer is first used to initialize these words by vectors xi we wi i where nis the length of code snippet c weis the embedding matrix.
then we use lstm units to encode the sequence of vectors x x1 xninto hidden states h1 hn.
for simplification we denote the lstm unit as ht lst m xt ht .
we further employ bidirectional lstm bi lstm to capture semantics in front and behind of the current position.
when decoding the code snippet and generating the i th summary word an attentional decoder first computes the context vector viover the sequence of hidden states h1 ht hnaccording to the following formula vi n j 1aijhj.
here aijis the attention weight of hjand computed by aij exp eij n k 1exp eik eij a si hj where si 1means the last hidden state of the decoder.
in equation we use a multi layer perception mlp unit as an alignment model a .
at time i the hidden state siof the decoder is updated by si lst m si yi whereyi 1is the previous input vector.
to jointly take into account past alignment information we exploit the input feed approach proposed by which concatenates vi 1with inputs yi .
p yi y1 .
.
.
yi c yi si vi where is the generator function which applies a mlp layer along with sof tmax .
we also adopt the beam search algorithm that is at each time step t we keep top bbest hypotheses where bis the beam size.
training such an attentional encoder decoder model is to minimize the loss function l n i 1l t 1logp yi t yi t c where is the trainable parameters nis the total number of training instances and lis the length of each target sequence.
after training with the pairs of code snippets and corresponding summaries we obtain an encoder which can represent code snippets and a decoder which is able to predict summaries word by word by maximizing the conditional probabilities of the next words in equation .
as a result if we only use such an attentional encoderdecoder model for source code summarization like the existing 1387icse may seoul republic of korea jian zhang xu wang hongyu zhang hailong sun and xudong liu work the model will prefer the high frequency words and ignore the low frequency words such as iis in the example of figure .
.
similar code retrieval because of the existence of code duplication previous retrieval based source code summarization work reused the summaries of similar code snippets .
their experience suggests that the words in expected summaries including the low frequency ones are also highly probable to appear in the summaries of similar code.
based on the encoder decoder model above our approach can incorporate the knowledge of similar code snippets and their summaries from the training set for better prediction of low frequency words.
for inclusion of as many useful low frequency words as possible we want to retrieve such similar code snippets from different aspects.
in this work since the training set is very large we aims to design two efficient source code retrieval components based on the syntax structure and the embedding semantics of source code.
more code retrieval methods can be easily integrated to our approach as well.
.
.
syntactic level source code retrieval.
unlike plain texts source code has its own syntax structure and the syntactic information is important for finding similar source code.
such syntactic knowledge has been explored by previous work on code clone detection based on costly tree based matching methods.
recent studies on the source code summarization also considered the syntax structure of source code by adding the ast based rnn encoder for joint supervised training.
in our work since the training set is very large and the unsupervised method is preferred we efficiently measure the syntactic similarities of code snippets based on the token sequences of asts without training.
specifically given one input code snippet from the testing set and any one from the training set we parse them to two asts and then calculate the similarity between the two asts.
however if the training set is very large for example the size nis more than 50k in our experiment it will incur much overhead to directly compute the similarities based on tree matching algorithms such as tree edit distance since the computational complexity is o n3 .
a suitable compromise is to convert asts into token sequences through tree traversal.
therefore we parse the input code snippet and all of the training set to asts and further obtain their sequence representations using the preorder traversal as depicted in figure .
based on these sequences we use an off the shelf and widely used search engine lucene to efficiently retrieve the most similar code snippet from the training set at the syntactic level.
.
.
semantic level source code retrieval.
recently the neural network based methods which encode source code into semantic vectors have shown their superiority in capturing code semantics.
but they need training with additional computation burden.
in contrast we reuse the trained encoder mentioned in section .
.
as described above the bi lstm based encoder is capable of capturing the sequential information of source code and embedding the semantics into hidden state vectors.given a code snippet c we encode it by the bi lstm encoder and get a sequence of hidden state vectors rn 2k where kis the vector dimension and nis the length of code snippet c. similar to we apply a global max pooling operation over the vector sequence to obtain the semantic representation rc r1 2k as follows rc i n. for a testing code snippet ctestand any code snippet cifrom the training set we compute their cosine similarity cosine rtest ri rtest ri rtest ri i .
where nis the size of the training set.
at last the code snippet with the highest score is selected as the most similar one in the training set.
based on both source code retrieval components since the training set is usually large we can offline prepare a code retrieval base which stores the pairs of code and ast based token sequence and the pairs of code and semantic vector before testing as depicted in figure .
given one new code snippet for testing online we retrieve the syntactic level similar code snippet based on the efficient lucene search engine and search for the semantic level similar one by computing the simple cosine similarity of vectors that is quite fast.
it is not trivial to choose the most similar code snippet i.e.
top at the syntactic or semantic level.
on the one hand more topk k similar candidates may be noisy especially when their similarities are not high.
on the other hand if a similarity threshold tis set to filter code snippets with relatively low similarities it may eliminate the useful low frequency words as well.
because it is hard to find such a static threshold to distinguish whether one code snippet includes useful low frequency words or not.
we find that selecting the most similar code snippet without the threshold i.e.k 1andt has a good trade off between the inclusion of useful low frequency words and the noisy information which will be discussed in section .
.
retrieval based neural summary generation after training our attentional encoder decoder model and retrieving the two most similar code snippets from the training set we will predict and generate summary sentences online for the code snippets from the testing set.
intuitively we augment the attentional encoder decoder model for high frequency words with the retrieved similar code snippets for low frequency words .
one naive solution is to enhance the probabilities of all the words in the similar code snippets by the similarity degrees but it may include noisy words such as remove in the example figure .
zhang et al.
tried to filter these noisy words with the translation pieces based on the source and target word alignment.
however unlike the neural machine translation the code snippets and their summaries usually miss the property of word alignment which will be discussed in section .
in contrast besides the similarities of code snippets we also consider the conditional probability of each word during decoding to help eliminate the possible noisy words that usually have low conditional probabilities.
as figure shows we take the 1388retrieval based neural source code summarization icse may seoul republic of korea ... s def create app ... return false def create vdir ... return false def ... return test code syntactically similar code semantically similar code s ... create attention attention attention false remove app encoder ... ... simsyn psyn simsem psem ptest create final distribution decoder figure retrieval based neural summary generation.
s and s represent the begin and end symbols respectively example in figure as illustration.
based on the encoder and decoder of our trained model in section .
our retrieval based neural summary generator encodes each testing code snippet and its two most similar ones simultaneously gets their context vectors with attentional mechanism and decodes them to predict the summary words by fusing the conditional probabilities and similarities.
specifically for one testing code snippet ctest we search and retrieve its two most syntactically and semantically similar code snippets csynandcsemfrom the training set mentioned in section .
.
then we encode these three code snippets by our trained attentional encoder decoder model section .
in parallel.
based on the obtained three sequences of hidden states htest hsynandhsem forct csynandcsem at each time step tduring decoding we can compute the attention weights to produce context vectors according to equation and then calculate the conditional probabilities to predict the next word by equation .
for simplification we denote these conditional probabilities by ptest yt y t psyn yt y t and psem yt y t respectively.
in order to enhance the prediction performance of the original attentional encoder decoder model we leverage the retrieved code snippets and fuse all these three conditional probabilities.
a straightforward way is to simply add them together.
however when the similarity between ctestandcsyn orcsem is too low even csyn is the retrieved most similar one in the training set psyn yt y t may have a negative impact on predicting correct summary words.
thus we should consider the values of similarities for the fusion as well.
directly reusing the similarities in the code retrieval such as results in equation is not reasonable because we calculate these similarities with different methods and from different aspects one at the syntactic level and the other at the semantic level.
to solve this problem we normalize the similarities to make them comparable based on the text edit distances d ctest csyn andd ctest csem using dynamic programming which is also adopted in sim ctest cret d ctest cret max ctest cret .
where cretdenotes any retrieved code snippet.
with these normalized similarities we combine the conditional probabilities to get the final conditional distributions as follows pf inal yt y t ptest yt y t sim ctest csyn psyn yt y t sim ctest csem psem yt y t .
where is a hyper parameter that can be manually tuned.
based on the final conditional distributions above we can predict the next word one by one and finally generate the whole summary sentence which incorporates the knowledge from the retrieved code snippets.
experiments in this section we conduct experiments to evaluate the effectiveness of our proposed approach and compare it with several state of theart methods from software engineering se and natural language processing nlp communities.
.
experimental setup we conduct experiments on two public large scale datasets in python and java respectively.
for simplification we call them pcsd python code summarization dataset and jcsd java code summarization dataset .
pcsd is provided by barone et al.
which contains python functions and their comments from open source repositories in github2.
it uses docstrings document strings as natural language descriptions which we call comments.
this dataset includes code comment pairs and has been used for training and evaluation in .
jcsd is a dataset consisting 1389icse may seoul republic of korea jian zhang xu wang hongyu zhang hailong sun and xudong liu of java methods and comments collected by hu et al.
from popular repositories in github.
the comment of jcsd is the first sentence extracted from its javadoc which is similar to the practice in .
the original dataset includes two parts for api sequence summarization and code summarization and we choose the latter part with code and comment pairs as our jcsd dataset.
for fair comparison we split pcsd into the training set validation set and testing set with fractions of and respectively and split jcsd in proportion of to keep the same split settings as baselines .
to make the training and testing sets disjoint we remove the duplicated samples from the testing set.
in common with we set the length limits in terms of words of code snippets and summaries i e. and for pcsd and for jcsd since such settings can cover most of their original lengths.
the statistics of these two datasets are described in table where maxl avgl and unit are the maximum length the average length and the total number of unique tokens respectively.
we also consider the low frequency words in the summaries whose frequencies are no more than and .
numw is the number of low frequency words and nums is the number of summaries which contain at least one low frequency word.
the percentages of low frequency words in all unique words of summaries and the percentages of summaries containing at least one low frequency word are also shown in the parentheses.
for tokenizing source code we use the libraries tokenize3and javalang4to get tokens of python and java code snippets respectively.
like we further split code tokens into subtokens by snake case or camel case to reduce data sparsity.
meanwhile we use the ast5library and javalang to obtain their corresponding asts.
to tokenize summaries we utilize the tokenize module of nltk toolkit .
besides we limit the maximum vocabulary size of source code and summary to 50k since too large vocabulary may lead to worse performance.
the out of vocabulary words are replaced by unk.
for better comparison we also apply such tokenization to other approaches to avoid the potential influence.
we implement our approach based on the open source system opennmt6 .
during training we set the embedding size to and the dimensions of hidden states in lstm to .
the batch size is set to and the maximum iterations is 100k.
we adopt the widely used adam as the optimizer with learning rate .
for training our model.
the beam size is set to and the hyper parameter is set to .
all the above hyper parameters are determined based on the validation set by selecting the best ones among some alternatives.
all the experiments are conducted on one ubuntu .
server with cores of .4ghz cpu 128gb ram and a titan xp gpu with 12gb memory.
.
evaluation metrics similar to existing work we evaluate the performance of different approaches using common metrics including bleu meteor rouge l and cider .
these metrics are also popular in machine translation text summarization and image captioning.
the generated summary xand the ground truth y bleu measures the n gram precision between xandyby computing the overlap ratios of n grams and applying brevity penalty on short translation hypotheses.
bleu correspond to the scores of unigram grams grams and grams respectively.
the formula to compute bleu n n is bleu n bp expn n 1 nlogpn where pnis the precision score of the n gram matches between candidate and reference sentences.
bpis the brevity penalty and nis the uniform weight n. for a pair of sentences to be compared meteor creates a word alignment between them and calculates the similarity scores by met eor f ra p r p r where pandrare the unigram precision and recall f ra is the fragmentation fraction.
and are three penalty parameters whose default values are .
.
and .
respectively.
rouge l is widely used in text summarization and provides f score based on longest common subsequence lcs .
suppose the lengths of xandyaremandn then plcs lcs x y m rlcs lcs x y n flcs 2 plcsrlcs rlcs 2plcs where plcs rlcsandflcsis the value of rouge l. cider is usually used for measuring the quality of image captions which considers the frequency of n grams in the reference sentences by computing the tf idf weighting for each n gram.
cider nscore for n gram is computed using the average cosine similarity between the candidate sentence and the reference sentences.
the final result is calculated by combining the scores for different n grams up to .
note that the scores of bleu meteor and rouge l are in the range of and usually reported in percentages.
but cider is not between and and thus it is reported in real values.
.
baselines we compare our approach with existing work on source code summarization.
they can be divided into two groups retrieval based and nmt based approaches.
in addition we also compare with a state of the art algorithm that combines the above two kinds of approaches although it is originally proposed for natural language translation.
we use the default settings of these approaches unless otherwise stated.
.
.
retrieval based approaches.
lsiis a text retrieval technique for analyzing the latent meaning or concepts of documents.
it is used in to choose the most important terms of code snippets as term based summaries.
in order to generate human readable summary sentences for any testing code snippet we use lsi to retrieve the most similar one from the training set and take its summary as the result.
the similarity is computed based on the lsi reduced vectors and cosine distance and we set the vector dimension to be .
1390retrieval based neural source code summarization icse may seoul republic of korea table the statistics of two datasets datasetsource code length words summary length words word frequency word frequency maxl avgl unit maxl avgl unit numw nums numw nums pscd .
.
.
.
.
.
jscd .
.
.
.
.
.
table method comparison for source code summarization methodspcsd jcsd bleu meteor rouge l cider bleu meteor rouge l cider lsi .
.
.
.
.
.
.
.
.
.
.
.
.
.
vsm .
.
.
.
.
.
.
.
.
.
.
.
.
.
nngen .
.
.
.
.
.
.
.
.
.
.
.
.
.
code nn .
.
.
.
.
.
.
.
.
.
.
.
.
.
tl codesum .
.
.
.
.
.
.
.
.
.
.
.
.
.
hybrid drl .
.
.
.
.
.
.
.
.
.
.
.
.
.
grnmt .
.
.
.
.
.
.
.
.
.
.
.
.
.
rencos .
.
.
.
.
.
.
.
.
.
.
.
.
.
vsm is the abbreviated form of vector space model .
a classic example of vsm is term frequency inverse document frequency tf idf which is adopted by some automatic source code summarization work .
different from lsi it indexes code snippets into weight vectors based on term frequency and document frequency.
once obtaining the vector representations we retrieve the summary of the most similar code snippet by the cosine distance.
nngen is a simple but effective nearest neighbor based algorithm that is proposed to produce commit messages for code changes .
after building the vectors of code changes based on bag of words and the term frequency it retrieves the nearest neighbors of code changes by the cosine similarity of vectors and the bleu score.
then nngen directly reuses the commit message of the nearest neighbor.
we reproduce such an algorithm in our task by replacing code changes with code snippets.
.
.
nmt based approaches.
code nn7is the first neural approach that learns to generate summaries of source code .
it is an lstm encoderdecoder neural network that encodes code snippets to context vectors with attention mechanism and produces summaries.
tl codesum8is a multi encoder neural model that encodes api sequences along with code token sequences and generates summaries from source code with transferred api knowledge .
it first trains an api sequence encoder using an external dataset.
the learned api representations are then applied to source code summarization task to assist the summary generation.
hybrid drl9is an advanced neural approach with hybrid code representations and deep reinforcement learning .
the basic architecture is also a multi encoder nmt to learn structural and sequential information by encoding asts tokens of source code.
similar work on incorporating asts is also done by but hybrid drl further uses reinforcement learning to solve the exposure bias problem during decoding and obtains better performance.
furthermore as we mentioned we also include an approach that guides nmt with retrieved translation pieces in natural language translation referred to as grnmt .
the translation pieces aren grams of the retrieved target sentences that also match the common words in both the input and the retrieved source sentences by word alignment.
during decoding grnmt uses the translation pieces to enhance the word prediction accuracy.
.
results and discussion we present the experimental results and analysis through the following research questions.
rq1 how does our proposed approach perform compared to the baselines?
table shows the performances of different methods to generate code summaries in terms of our evaluation metrics.
we compute the values of these metrics using the same scripts provided by hybrid drl .
for these metrics the bigger is better and the best one of each metric is marked in bold.
from the table we can see that retrieval based methods lsi vsm and nngen yield good results.
in particular vsm achieves the best performance among these three techniques.
it is a little confusing that lsi seems to be poorer than vsm since lsi considers the term association and is usually better than vsm tf idf in capturing semantics of plain texts .
but as mentioned in if the dimension of tf idf i.e.
the vocabulary size is much larger than that of lsi i.e.
it may produce better performance.
in our case the source code corpus has a huge vocabulary size of more than 50k in tf idf since the unique tokens of source code are much more than those in plain texts due to the arbitrary identifiers.
such a phenomenon is also observed in haiduc et al.
s work .
comparing vsm with nngen vsm is better since it considers the document frequency but nngen does not.
among all three nmt based methods of code nn tl codesum and hybrid drl code nn performs worse than the other two 1391icse may seoul republic of korea jian zhang xu wang hongyu zhang hailong sun and xudong liu neural models because it only depends on the embeddings of tokens i.e.
at the lexical level to understand the semantics of code snippets.
in contrast tl codesum is better since it captures more semantics of source code with learned api sequence knowledge.
we also use the external dataset from the original jcsd provided in to pre train the api sequence encoder for jcsd but pcsd does not have such external dataset.
thus tl codesum is more effective on jcsd than on pcsd.
for hybrid drl it has better performance than tl codesum on pcsd since the ast based structural information of source code is incorporated and the exposure bias problem is solved.
but on jcsd for the metrics including bleu bleu4 meteor and cider hybrid drl is worse than tl codesum probably because the api sequence knowledge from the external dataset of the original jcsd dominates the performance.
in addition we find that code nn and tl codesum have an overall worse performance than the retrieval based ones which is not surprising due to the low frequency word problem as described in section .
as a combination of retrieval based and nmt based methods grnmt guides one simple encoder decoder model with the retrieved translation pieces.
it can outperform code nn in all metrics tl codesum and hybrid drl in some metrics such as meteor and rouge l which means that the retrieval information actually helps.
in addition grnmt is better than all retrieval based methods for rouge l which indicates that the encoder decoder neural model can also contribute to the performance.
finally from the table we can see that our approach achieves the best performance for all evaluation metrics.
the reason is that we retrieve the most similar code snippets at both syntactic level and semantic levels as additional contexts to our attentional encoderdecoder model.
we also enhance the summary generation by the fusion of the similarities and the conditional probabilities for the next word prediction as described in section .
compared with our approach unlike it in the task of natural language translation grnmt is worse since it is difficult to precisely match the n grams of summaries with the corresponding elements in code snippets and get high quality translation pieces.
due to the large scale training set and the online code retrieval during testing our approach may cost much more time to generate summaries than a single nmt model.
however we only need ms in average to generate the summary for each testing code snippet in the two datasets of our experiment because of the efficient search engine lucene and the fast computation of the cosine similarity.
rq2 how effective are the main components of our approach?
in our approach we design two code retrieval components from different aspects including the syntactic level and semantic level we want to know whether they are effective.
in this experiment at first we do not use our attentional encoder decoder model but each time adopt only one of these two retrieval components only syntactic retrieval and only semantic retrieval and directly take the summary of the most similar code snippet from the training set as the final generated one.
then we use the attentional encoder decoder model nmt and consider the impact of adding the retrieved code snippets in four different scenarios without any retrieval code nmt adding the most similar code snippet retrieved at the syntactic level nmt syntactic retrieval adding the most similar code snippet retrieved at the semantic level nmt semanticretrieval adding both the two most similar code snippets above nmt both retrieval which is also our final approach .
we present the experimental results in table .
both of our syntactic level and semantic level retrieval components are more effective than existing retrieval based methods lsi vsm and nngen which are described in table .
this is because we capture more syntactic or semantic information mentioned in subsection .
.
in addition our syntactic level and semantic level retrieval components have slightly different but comparable results which indicates that the syntactic and semantic information from two different aspects of source code are both useful in code retrieval.
based on the two retrieved code snippets we add any one or both to our original attentional encoder decoder model.
compared with the case without any retrieval code we can see that the retrieved information can indeed enhance the performance of the neural model even when we add just one most similar code snippet retrieved at either the syntactic or semantic level.
when these two most similar code snippets are both utilized we can obtain the best performance.
rq3 does our approach perform better than nmt based methods for tackling the low frequency word problem?
as mentioned in section and our retrieval based neural model can tackle the low frequency word problem better than nmt based methods which may correctly generate more low frequency words.
to illustrate it we perform a statistical analysis about the lowfrequency words in generated summaries.
for the testing set our generated summaries include and unique words for pcsd and jcsd respectively and each summary has .
pcsd and .
jcsd words in average.
we first collect all the correctly generated words according to the ground truth summaries.
then we count the frequencies of all these correct words in the training set and record the number of the correct and low frequency words frequency and .
in this experiment we compare rencos with our original attentional encoder decoder model nmt and show how they deal with the low frequency words.
the experiment results are shown in table where ratio is the quotient ofrencos n mt which indicates the degree of improvement our approach achieves on the low frequency words.
from the table we can see that our approach can correctly predict more low frequency words than nmt when the word frequency is small .
for example for the words that occur only once in the training set the number correctly predicted by our approach is .
and .
times that of nmt on the datasets of pcsd and jcsd respectively.
obviously these additional low frequency words come from the summaries of our retrieved code snippets which validates our claim that our retrieval based neural source code summarization can more effectively deal with the low frequency word problem than the nmt based methods.
moreover when the word frequency increases we can see that nmt has a trend to perform better as the ratio decreases.
this indicates that for highfrequency words nmt can easily capture them and the benefit brought by the retrieved source code becomes small.
rq4 will our approach perform better if we retrieve topk k similar code snippets and filter them according to a similarity threshold t?in our approach our two syntactic level and semantic level retrieval components both select the most similar code snippet k without threshold t by default.
in this rq we study whether the most k k similar code snippets and 1392retrieval based neural source code summarization icse may seoul republic of korea table effectiveness of each component of the proposed approach descriptionspcsd jcsd bleu meteor rouge l cider bleu meteor rouge l cider only syntactic retrieval .
.
.
.
.
.
.
.
.
.
.
.
.
.
only semantic retrieval .
.
.
.
.
.
.
.
.
.
.
.
.
.
nmt .
.
.
.
.
.
.
.
.
.
.
.
.
.
nmt syntactic retrieval .
.
.
.
.
.
.
.
.
.
.
.
.
.
nmt semantic retrieval .
.
.
.
.
.
.
.
.
.
.
.
.
.
nmt both retrieval .
.
.
.
.
.
.
.
.
.
.
.
.
.
table number of correctly generated low frequency words word frequency pcsd nmt rencos ratio .
.
.
.
.
.
.
jcsd nmt rencos ratio .
.
.
.
.
.
.
k2425262728bleu t t .
t .
t .
k181920meteor t t .
t .
t .
k444546rouge l t t .
t .
t .
k2.
.
.
.3cider t t .
t .
t .
figure performance of top kretrieved similar code snippets with different thresholds t similarity threshold t t can provide more useful knowledge for generating a good summary.
thus we use the semantic level retrieval component and conduct experiments on pcsd by obtaining k ... 10code snippets with the khighest similarities by equation .
we also consider to filter them by the similarity threshold t .
.
.
.
for simplification we calculate the average value of bleu and mark it as bleu.
the experimental results are shown in figure .
we can see that when kincreases the performance becomes worse with any threshold.
for example the bleu score decreases nearly when we set k 10andt .
we further calculate the distribution of similarities in equation between top k k ... similar code snippets and the corresponding input.
as shown in figure we find that the overall similarity decreases greatly when kincreases.
this means that when kis bigger the k th similar code snippet may be completely irrelevant to the testing one and provide more k0.
.
.
.
.
.0similarityfigure similarity distributions of code snippets noisy words.
the threshold tcan effectively filter these irrelevant code snippets but may eliminate potential useful low frequency words as well and make the performance worse for example when t .
.
when kis small e.g.
k such elimination dominates the performance since the retrieved similar code snippets are almost all useful but filtered by the threshold.
also combining more code snippets during testing is much time consuming.
therefore we choose the most similar code snippet k without threshold t in this work for the best performance and high efficiency.
in addition when the similarities of retrieved code snippets are low even for the most similar one our approach can automatically tolerate them because we take the similarity as one factor in equation which tends to ignore the words produced by low similarity code snippets.
.
examples for qualitative analysis of our approach we present two examples of summaries generated by different methods from the testing sets of python and java datasets respectively.
a simple python example is shown in figure .
our approach generates the summary create an iis application which is exactly the same as the ground truth indicating that our approach can effectively combine the highfrequency word create from nmt and the low frequency word iis form retrieval based methods.
the other more difficult java example is depicted in table where the reference is the groundtruth summary written by developers and others are automatically generated summaries by different methods.
we can see that the high frequency word remove is captured by all the nmt based methods grnmt and our approach while all the retrieval based 1393icse may seoul republic of korea jian zhang xu wang hongyu zhang hailong sun and xudong liu table a java code snippet with generated summaries p u b l i c v oid removecolumn f i n a l string columnname i f columnname n u l l r e t u r n list string cols arrays .aslist getinfo .headers f i n a l i n t colindex cols .indexof columnname removecolumn colindex reference remove the column represented by its name lsi get index of this column name vsm adds the given column to this table nngen get index of this column name code nn remove a column from the table .
tl codesum remove column at specified index .
hybrid drl removes a column from the column .
grnmt remove a column from the table .
our approach remove the column represented by the index methods miss it.
in addition the phrase represented by is correctly predicted only by our approach since it appears in the summaries of our two retrieved code snippets and is effectively captured by the retrieval based summary generation.
hence our approach generates the closest summary to the reference.
.
human evaluation the above four metrics bleu rouge l meteor and cider can be used to compare summaries generated by different methods.
however they mainly calculate the textual similarity between the reference and the generated summaries rather than the semantic similarity.
therefore we perform human evaluation to complement the quantitative evaluation in terms of those metrics.
for human evaluation of our approach we recruit workers from amazon mechanical turk amt a worldwide crowdsourcing website.
amt supports micro tasks also known as human intelligence tasks hits posted by clients.
remote workers can complete hits for money.
we randomly choose code snippets from the testing sets from pcsd and from jcsd and their comments produced by three methods vsm hybrid drl and rencos .
as depicted in table vsm is the best among the retrieval based approaches.
hybrid drl is the best on pcsd among the nmt based approaches and is comparable with tl codesum on jcsd.
for each of these three methods we obtain the natural language comment pairs consisting of the reference comments written by developers and the generated ones and then post them as hits to amt.
each hit is assigned to three different workers and such redundancy can help obtain more consistent results.
finally we have hits methods of vsm hybird drl and rencos code snippets and redundant assignments .
these hits are completed by unique workers.
each hit costs .03us and all hits spend minutes in total.
in each hit webpage we ask how similar is the meaning of these two source code comments?
comment1 x comment2 y where x and y are one comment pair and workers can select a score between to where means not similar at all and the distribution of the scores of the generated comments score avg vsm .
hybrid drl .
rencos .
means highly similar identical .
we get three scores from workers for every hit and choose the median value as the final score.
table shows the score distribution of the reference and generated comments.
we can see that our approach achieves the best scores and improves the average avg score from .
vsm and .
hybrid drl to .
.
specifically among the randomly selected code snippets our approach can generate highly similar or even identical comments with the reference ones score good comments score and comments that are not bad score .
our approach also receives the smallest number of negative results score .
based on the final scores for each approach of rencos hybriddrl and vsm we conduct wilcoxon signed rank tests and compute cliff s delta effect sizes .
comparing rencos with hybrid drl and vsm the p values of wilcoxon signed rank tests at confidence level are .
and .111e which means the improvements achieved by our approach are statistically significant.
in addition cliff s delta effect sizes are .
small but non negligible and .
medium respectively.
in summary the results of human evaluation confirm the effectiveness of the proposed approach.
threats to validity there are three main threats to the validity of our evaluation.
in the implementation of existing methods we directly use the public code of code nn tl codesum and hybriddrl provided by their authors but the code of grnmt is not available.
we have tried our best to read the paper carefully and consult the authors about many details.
we will eliminate this threat as soon as the tool is publicly available.
the scale of datasets.
as we analyzed in section larger datasets cannot help mitigate the low frequency words problem but may result in more infrequent words.
in our evaluation we have used two public large datasets which include python and java code snippets and their summaries.
in our future work we will experiment with even larger scale datasets and further evaluate the effectiveness ofrencos in handling low frequency words.
the retrieved code snippets may not always have high similarities especially when the code base is small.
we suggest to increase the scale and diversity of code bases to avoid such threat.
however we should note that rencos only takes the similarity as one factor in equation .
if the similarity is low rencos tends to choose the words predicted by the nmt model.
in this way rencos can still guarantee that its performance is comparable with that of nmt.
1394retrieval based neural source code summarization icse may seoul republic of korea related work .
source code summarization in software engineering community information retrieval techniques are widely used for automatic source code summarization.
some studies extract terms from source code for generating termbased summaries .
haiduc et al.
use ir methods including lsi and vsm to choose top kterms from a code snippet.
they treat each function of source code as a document and index on such a corpus by lsi and vsm then the most similar terms based on their cosine distances between documents are selected as the summary.
their work is improved by topic modeling in .
rodeghero et al.
improve the process of selecting terms by eye tracking and modify the weights of vsm for better code summarization.
besides the term based summaries code clone detection techniques are used to retrieve similar code snippets from open source github projects and stack overflow and reuse their sentence comments as code summaries .
in addition sridhara et al.
design heuristics to choose statements from java methods and use the software word usage model swum to identify keywords from those statements and create summaries though manually crafted templates.
recently many neural models are proposed to generate source code summaries.
allamanis et al.
suggest method and class names as code summaries by the neural logbilinear context model which is based on embeddings of code tokens.
the convolutional attention model is presented to predict extreme summarization of source code such as function names .
the nmt based models are also widely used to generate summaries for code snippets with encoder decoder neural networks .
iyer et al.
propose an attentional lstm encoder decoder network for automatically generating short natural language summaries of source code snippets.
hu et al.
use one additional encoder to leverage api sequences and improve the summary generation by learned api sequence knowledge.
the abstract syntax tree structures are incorporated by .
moreover its performance can be further improved by deep reinforcement learning to solve the exposure bias problem during decoding.
compared with the above work our approach can take the advantages of both information retrieval and nmt based methods by enhancing the nmt model with the retrieved similar code snippets from the training set resulting in better performance than the above state of the art work.
.
code clone detection and code retrieval many studies show that much duplicated code exists in a large code base.
there are a lot of work investigating code clone and similar code retrieval techniques.
traditional code clone detection techniques such as deckard and sourcerercc detect similar code with the tree based or token based code comparison.
recently neural models are incorporated to learn vector representations of source code and use these vectors to compute code similarity which can achieve better performance but need additional time consuming training and labeled data.
in this work during the online testing we need lightweight and efficient search of similar code from a large scale training set thus we prefer to reuse our trained encoder to learn semantic vectors rather than training new neural models.
also we want to find more similar code snippets from the syntax aspect.
to avoid complex operations over asts we convert asts to token sequences and leverage the off the shelf efficient search engine lucene for code retrieval.
.
retrieval based neural machine translation due to the low frequency word problem in neural machine translation some studies that combine the retrieval information with encoder decoder neural models have been recently proposed in the nlp community .
zhang et al.
extract the translation pieces which are n grams from retrieved target sentences and their corresponding source words appearing in testing sentences.
then these translation pieces are leveraged to guide the decoding.
in natural language translation the source and target words of retrieved translation pairs can be easily matched through word alignment.
however in source code summarization it is difficult to find out which code elements produce certain specific words in the short summaries.
as a result the similar idea is not effective in our work.
gu et al.
incorporate the retrieval information with new encoders and build one complex encoder decoder neural network.
in order to improve nmt with additional retrieval information a translation memory tm which consists of source and target sentence pairs is leveraged by graph representation and self attention mechanism over the graph or augmenting the source data with retrieved fuzzy tm targets by means of concatenation .
compared with their work we do not need additional encoders and directly combine the similarities and the conditional probabilities of retrieved similar code snippets to predict the summary words during decoding.
conclusion in this paper we propose a novel retrieval based neural approach named rencos that augments an attentional encoder decoder model with the retrieved two most similar code snippets for better source code summarization.
given one new code snippet instead of only reusing the retrieved summaries or only generating summaries based on nmt rencos can automatically generate summary by the fusion of retrieved code snippets and itself.
we also design two code retrieval methods from the aspects of syntax and semantics to accommodate more knowledge about the code.
we have evaluated the effectiveness of our approach through extensive experiments and the results show that it outperforms the related approaches.
our code experimental data and results are publicly available at .