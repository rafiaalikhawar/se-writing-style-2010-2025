nessie automatically testing javascript apis with asynchronous callbacks ellen arteca arteca.e northeastern.edu northeastern university usasebastian harner harnersebastian gmail.com university of stuttgart germany michael pradel michael binaervarianz.de university of stuttgart germanyfrank tip f.tip northeastern.edu northeastern university usa abstract previous algorithms for feedback directed unit test generation iteratively create sequences of api calls by executing partial tests and by adding new api calls at the end of the test.
these algorithms arechallengedbyapopularclassofapis higher orderfunctions that receive callback arguments which often are invoked asynchronously.
existing test generators cannot effectively test such apis because they only sequenceapi calls but do not nestone call intothecallbackfunctionofanother.thispaperpresentsnessie thefirstfeedback directedunittestgeneratorthatsupportsnestingofapicallsandthattestsasynchronouscallbacks.nestingapicalls enablesatesttousevaluesproducedbyanapithatareavailable only once a callback has been invoked and is often necessary to ensure that methods are invoked in a specific order.
the core contributionsofourapproachareatree basedrepresentationofunit testswithcallbacksandanovelalgorithmtoiterativelygenerate suchtestsinafeedback directedmanner.weevaluateourapproach on ten popular javascript libraries with both asynchronous and synchronouscallbacks.theresultsshowthat inacomparisonwithlambdatester astateofthearttestgenerationtechniquethatonlyconsiderssequencingofmethodcalls nessiefindsmorebehavioral differences and achieves slightly higher coverage.
notably nessie needstogeneratesignificantlyfewerteststoachieveandexceed the coverage achieved by the state of the art.
keywords asynchronous programming test generation javascript testing acm reference format ellenarteca sebastianharner michaelpradel andfranktip.
.nessie automatically testing javascript apis with asynchronous callbacks.
in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm new york ny usa pages.
https permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthefirstpage.copyrightsforthird partycomponentsofthisworkmustbehonored.
for all other uses contact the owner author s .
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
introduction test generation is an important technique to automatically test librariesbycreatingunit leveltests.thegeneratedteststypically consistofasequenceofcallstofunctionsinanapiundertest.the valuespassedasargumentstoeachfunctioncallinsuchasequence maybechosenrandomly orvaluesreturnedbypreviouscallsin the sequence can be used to facilitate the testing of dependent api functions.differenttestgeneratorstakedifferentapproachesfor selecting which functions to call and which arguments to pass into them e.g.
random inputs feedback from executions and symbolic reasoning .
however existingworkontestgenerationhasignoredabroad class of apis functions that accept another function as a callback argumentandtheninvokethatotherfunctionasynchronously.the key benefit of asynchronous callbacks is that they do not block the main computation which is useful e.g.
when accessing some kindofresourceorwhentriggeringalong runningcomputation.
asynchronouscallbackshavebeenshowntobepopular but also prone to mistakes and surprising behavior .
while the javascript community has started migrating to asynchronous apis that rely on promises and async await section .
a vast amount of javascript code still uses event driven apis that invoke callback functions passed to them asynchronously.
we observe that existing test generators miss out on two opportunities for testing apis with asynchronous callbacks.
first in addition to sequencing function calls one can also consider nesting them by placing an api call into a callback passed to another api functionundertest.suchnestingenablesatesttousevaluesproducedbyanapithatareavailableonlyonceacallbackhasbeen invoked moreover nestingisoftennecessarytoensureaspecific ordering of invocations to api functions.
second even the best existing test generator aimed at testing functions with callbacks supports only synchronous but not asynchronous callbacks.
the table below compares our work with the capabilities of two related techniques our work is inspired by sequencing nesting synchronous asynchronous callbacks callbacks randoop check lambdatester check check this work nessie check check check check ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ellen arteca sebastian harner michael pradel frank tip toillustratethechallengesassociatedwithtestingasynchronous apis consider a library for accessing json files.
the api defines a function existsforchecking whetherajson filewitha specified name exists which produces a handle to that file if this is the case.
moreover there is a function readfor parsing a json file represented by a given handle.
in javascript functions in eventdriven apis typically take a callback as their last argument which isinvokedwithtwoarguments i anobjectthatindicateswhetheranerrorhasoccurred or nullifnoerroroccurred and ii anobject representing the results of the operation.
hence a typical use of the library would look as follows letfilename ... exists filename asynchronously invoked callback function err filehandle if !err read filehandle another callback function err jsonobj ... thecallto readisnestedinthecallbackthatispassedto exists to ensure that the read operation is executed after the exists operation has completed.
now suppose that the readoperation contains a bug that is triggered in certain cases where a valid file handle ispassed e.g.
ifthefile spermissionsdonotpermitreadaccess and suppose thatwe want to generatea test that invokes the read function to expose the bug.
since file handle objects are created inside the library it is unclear what the representation of these objects looks like without analyzing or executing the library code.
whileitispossibleforatestgeneratortocreatesuitablefile handle objectsusingapurelyrandomapproach thechancesofsuccessfully creating a valid file handle would be small.
therefore the mosteffective way to obtain a valid file handle and expose the bug isto invoke existswith some callback function f and invoke read withthefile handlethatispassedto fasitssecondargument.that is we would generate a test where a call to readisnestedin the callback that is passed to exists as in the above example.
unfortunately thestateofthearttestgeneratorfortestingfunctions with callbacks is unable to generate such a test for the tworeasons mentionedin theabove table first itfails toidentify api functions that receive an asynchronously invoked callback argument andhence neverpassescallbacksintosuchfunctions.
second it does not construct tests where a call to one api function is nested inside the callback passed to another api function.
thispaperpresentsnessie thefirstfeedback directedtestgenerationtechniquethatnestsapicallsintocallbacksandthatsupports asynchronous apis.
at the core of the approach is a novel treebased representation of test cases which allows for growing a test case by either sequencing api calls i.e.
adding sibling nodes orbynestingapicalls i.e.
addingchildnodes.wepresentanalgorithmforiterativelygeneratingtree shapedtestsbasedonfeedbackfromexecutingalreadygeneratedtests.thealgorithmissupported by anautomated apidiscovery phasethat determineswhich api functions accept asynchronous or synchronous callbacks and byguiding the test generator toward realistic api usages based on nesting examples mined from existing api clients.
our implementation targets javascript where asynchronous callbacks are particularly prevalent and where generating effectivetests is particularly challenging due to the absence of statically declaredtypes.ourevaluationappliesnessietotenpopularjavascript libraries that include a total of api functions of which expect callbacks.
nessie s api discovery phase detects of the api signatures with callback arguments that are mentioned in the api sdocumentation and106 undocumented apisignatureswith callbacks reflectingunexpectedbehavior.thecoverageachieved by nessie converges significantly more quickly than with the state of the art lambdatester approach and even reaches a slightly higher coverage on average nessie needs to generate only teststoachievethesamecoveragethatlambdatesterachieveswith 000generatedtests.weconjecturethatthisisthecasebecause the nesting of callback functions enables a more effective selection ofargument valuesin subsequent nested function calls.we also comparetheabilityofnessieandlambdatestertodetectsituations wheretestsgeneratedforagivenversionofanapibehavedifferentlywhenrunagainstthenextversionofthatapi.onaverage nessie detects more behavioral differences than lambdatester includingamixofbugsandintentionalapichanges.whilethese differences can in principle all be detected without nesting api calls our approach finds them more effectively and efficiently due to its ability to nest calls.
in summary this paper contributes the following thefirstautomatedtestgeneratorspecificallyaimedatapis that accept callbacks to be invoked asynchronously.
an algorithm for incrementally generating tests that not onlysequenceapicallsbutalso nesttheminsidecallbacks.
empiricalevidencedemonstratingthat i theapproachis effective at exercising javascript apis with asynchronous callbacks ii thatitachievesmodestlyhighercodecoverage andfindsmorebehavioraldifferencesthanthestateofthe art and iii thatit convergesmuchmorequickly thanprior workwhenitcomestoachievingaspecificlevelofcoverage or behavioral differences.
overview of nessie this paper presents a test generation technique for testing higherorder functions.
a function is called a higher order function if it expectsanotherfunctiontobepassedasanargument orifitreturns a function.
our work targets functions fthat receive a callback function cbas an argument and then invoke the callback either synchronouslyorasynchronously.asynchronousinvocationhere means that the execution of fcausescbto be invoked from the main event loop at some later time.
generatingtestsforasynchronousapisinvolvesseveralopen challenges.thefirstchallengeistofindoutwhichapifunctions expect a synchronouscallbacksandatwhatargumentpositions thesecallbacksshouldbepassed.sincejavascriptisdynamically typed ourapproachneedstoinferthesignaturesoffunctionsas aprerequisitetogeneratingeffectivetests.thesecondchallenge is about how to compose multiple api calls into a test.
while ex isting work focuses on sequencing calls i.e.
one call statement after another sequencing alone is insufficient for testing asynchronoushigher orderfunctions.thethirdchallengeisabouthowto composeapicallsinarealisticway.toincreasethechancesthat authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
nessie automatically testing javascript apis with asynchronous callbacks icse may pittsburgh pa usa api functions under test api discoveryabstract signaturesfeedback directedtest generationt est case trees existingapi clients mining api usagesnestingexampleste s tcases figure overview of the approach.
our approach nests api calls in ways that represent real world api usage it requires some knowledge of typical api usage.
ourapproach illustratedinfigure1 consistsofthreemaincomponentsthateachaddressoneofthechallengesdescribedabove.
given a set of api functions to test the first step is automated api discovery which probes the api functions under test to determine if and where they expect callback arguments.
the discovered information is stored as a set of abstract signatures which record whetherthearguments are i asynchronously invokedcallback ii anasynchronouslyinvokedcallback or iii avaluethatisnot acallback1.then theabstract signaturesdiscoveredinthisphase serveasinputtoa feedback directedtestgenerationalgorithm which is the core of the approach.
the test generation is centered around atree basedrepresentationoftestcases called testcasetrees which theapproachiterativelygrowsintofulltestcases.toinformdecisions about how api functions should be combined the approach alsomines api usage patterns in existing open source clients of thelibraries undertest.thisinformation ispassedto thetestgeneratorandusedtosupportnestingdecisions.theendproductof this process is a set of test cases which can be used in a variety of applications e.g.
regression testing.
api discovery thefirststepintestgenerationisdeterminingwhattotest givena libraryundertest nessieretrievesthesetofallfunctionproperties offeredbythelibraryobject.asjavascriptisadynamicallytyped language nessieinitiallydoesnotknowanythingaboutthesefunctions beyond their names.
one possible approach to learn more about the apis would be to rely on optional type annotations e.g.
in the form of third party typescript type declarations for popular libraries.2however not alljavascriptcodecomeswithtypeannotations andevenifitexists anapi simplementationmaydiverge from its declared type .
nessie addresses this problem through anapidiscovery phasethatprobesapifunctionstodetermineat what parameter positions they expect callbacks and whether these are invoked synchronously or asynchronously.
this information is recorded as a set of abstract signatures or simply signatures.
definition abstract signature .
an abstract signature for a functionfis a tuple arg1 ... arg n where each argiis one of the following three kinds of elements async an argument is an asynchronously invoked callback sync an argument is a synchronously invoked callback 1since we are targeting javascript where functions can be invoked with any number of arguments of any type we do not attempt to track precise type information.
symbol any non callback argument a single function may have zero one or multiple signatures.
zero signaturesindicates that theapi discovery failed tofind any signaturesforthefunction inwhichcasetheapproachfallsbackto a default test generation strategy that does not pass any callbacks.
multiplesignaturesareinferredwhenfunctionsareoverloaded.forexample the outputjson functionfrom fs extra hastwosignatures async and async .
the reason is that outputjson has an optional second argument and always takes a callback function as thelastargument.3inourexperience suchoverloadingisextremely commoninjavascriptapisduetooptionalargumentspreceding the final callback argument.
todiscoversignaturesforagivenapifunctionundertest nessie repeatedly4invokes the function with randomly generated arguments.
the approach alternates between generating calls with and withoutacallbackargument andpassesdifferentnumbersofarguments.
a generated test for a function apiis structured as follows letcallback console.log callback executed try try calling the specified api function api ... callback ... console.log api call executed catch e console.log error in api call console.log test ex ecuted during the execution of these tests the print statements track if andwhencallbacksareinvoked andwhetherthefunctionterminates successfully.
we ignore erroneous executions as they maybe due to incorrect arguments passed to the function.
for nonerroneous executions the approach distinguishes three cases acallbackthatexecutes afterthetesthasexecuted isexecuted asynchronously soasignatureiscreatedwith asyncatthe positionofthecallbackargumentand atallotherpositions acallbackthatexecutes beforetheapicallreturns isexecuted synchronously so a signature is created with syncat the callback position and at all other positions ifthecallbackisnotexecutedorthetestdoesnotpassany callback a signature with symbols only is created.
wetestwithasinglecallbackargumentatatime i.e.
ourapproach will not discover signatures with multiple callback arguments.
the rationale is that api functions with multiple callbacks are relatively rare.
extending the algorithm to support multiple callbacks isstraightforwardbutincreasesthecomputationalcomplexityof api discovery.
in general the results of the api discovery phase are unsound because the approach does not guarantee to cover all possible arguments or all paths through the api implementation.
theoutputoftheapidiscoveryisthesetofdiscoveredsignatures for each function offered by the library under test.
when generatingtests thenumberofarguments andcallbackpositions if the function has any used during test generation are informed by these discovered signatures.
feedback directed test generation 4by default nessie runs tests per api function each with a timeout of two seconds.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ellen arteca sebastian harner michael pradel frank tip omitted for clarity try catch around each call to an api function.
print statements to log arguments and return values.
print statements to log control flow.
letfs extra require fs extra vararg590 a b test vararg593 null letr 126 0 fs extra.ensurefile arg590 cb a b c d e letr 126 0 0 fs extra.readjson arg590 return false letr 126 1 fs extra.
stat arg593 a generated test.
b test case tree representation of the test with extension points annotated.
figure example of a generated test for the api functions of the fs extra library and the corresponding test case tree.
the following describes the core algorithm of nessie which generates tests cases that both sequence and nest api calls.
the algorithm is based on a tree shaped representation of test cases called atest case tree section .
which serves as the basis for the algorithm itself section .
.
.
test case trees torepresenttestcasesthatsupportbothsequencingandnesting ofmethodcalls wedefinethe textcasetree anovelintermediate representation of test cases definition test case tree .
letvbe a map of variable names to non callback values called value pool and sa map of function names to abstract signatures.
then a test case tree is an ordered tree where nodes are either acallnodeoftheform r api a1 ... a k meaningthatfunctionapiisinvokedwitharguments a1 ... a k andyieldsreturnvalue r.ifsi sync async forsomesignature api s1 ... s n s thenaimay be a callback function.
otherwise ai vor it is a return value of another call or acallback node of theform cb p1 ... p k meaningthat callback function cbreceives parameters p1 ... p k. edges are either acontains edge from a callback node to a call node meaning that there is a call in the body of the callback function or anargument edge from a call node to a callback node meaning that a call is given a callback function as an argument.
therootnodeofatestcasetreeisaspecialcallbacknodethat corresponds to the function that contains the entire test case.
the orderofthecallnodesunderacallbacknoderepresentsthesequential order in which calls occur in the body of a callback function.
example.
figure gives an example of a test case and its corresponding test case tree.
each api call in the test case in fig ure 2a is represented by a call node in the test case tree of fig ure 2b.
the nodes for calls that receive callback arguments eachhave a corresponding callback node as a child.
e.g.
the callback givento ensurefile atline11isrepresentedbythecallbacknodecallback a b c d e .
calls nested within the body of a callback function are represented as children of callback nodes.
e.g.
the call to readjson on line corresponds to the lower left call nodeof thetree.
thevalue poolof thetest consistsof twoentries whichmapthevariablenamestotheirrespectivelyassignedvalues in lines to .
a call in a test case can use as arguments only values that are availableatthecallsiteaccordingtothescopingrulesofjavascript.
we say that a test case is well formed if for all calls its arguments are bound to some declaration when the call is reached during the executionofthetestcase.givenourtreerepresentationofatest case a test case is well formed if and only if the following holds definition3 well formedness .
inawell formedtestcasetree each argument of a call node nis one of the following a random primitive value inserted by referring to one of the entries in the value pool v. arandomobjectvalue arrayliteral objectliteral orfunction also inserted by referring to one of the entries in v. thereturnvalue rofacallnode n primethatisaleftsiblingof nor a leftsibling ofany ancestorcall nodeof n these represent return values of calls executed before reaching the call in n .
aparameter piofacallbacknodeonthepathbetweenthe root node and n these represent formal parameters of a surrounding callback function .
a callback function cbx wherenhas a callback node that represents cbxas a child.
example.
figure2bshowsatestcasetreewhereallarguments arewell defined.e.g.
inthecallof ensurefile thefirstargumentis arg590 which is a randomly generated primitive value in the value pool astringliteral definedonline8 andthesecondargumentisa callback node.
in contrast the call of statcould not use e.g.
aas an argument because ais not in the parameter list of any callback node on the path between stat s node and the root node.
.
test generation algorithm ouralgorithmcreatestestcasesiteratively byrepeatedlyextending an existing test case with another call at extension points which authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
nessie automatically testing javascript apis with asynchronous callbacks icse may pittsburgh pa usa algorithm feedback directed generation.
input setfof api functions map sfrom function names in fto their discovered signatures mined nesting examples m output set of tests t t generated test case trees t empty test case tree e t root t extension points while t number of tests to generate do t n randomly pick from e f choosefunction t n f m args choosearguments t n f s m t prime extendtwith call to f args atn feedback execute t prime t t t prime ifnoexceptions feedback then for each n prime extensionpoints feedback do e e t prime n prime returnt representlocationsinagiventestcasewhereanewcallnodecould beinserted.givenatestcasetree thereisanextensionpointfor each callback node that is executed during test execution which adds another child node to the already existing child nodes at the right mostposition.forcallbacknodeswithnochildren thereis an extension point for adding a first child node.
example.
figure2bshows theextensionpointsofthetestcase tree as well as edges that would be added if the test is extended at thesepoints labeledthe extensionedges .theseextensionpoints correspond to adding a call right after lines and in figure 2a.
algorithm summarizes our feedback directed approach for creating test cases.
it maintains a set tof generated tests and a set eof extension points.
each extension point is a pair t n of a test case treetand a node nin this tree representing the callback node whereanewcallnodecouldbeinserted.themainloopextendsan existingtestwithanewcallatoneoftheextensionpointsandadds thetestto t.theextendedtest t primeisthenexecutedtogatherfeedback aboutitsexecution.ifnoexceptionisthrown eachpossiblecallbacknode that is executed is kept as a possible extension point to createafurthertest.thealgorithmreliesonhelperfunctionsforchoosing a function to call choosing arguments to pass into the function and identifying future extension points which we describe below.
choosing a function to call.
given a specific extension point algorithm1calls choosefunction topickafunctiontocallbybalancing two requirements.
on the one hand the generated tests should cover as many functions as possible.
on the other hand we do not wanttoprescribeaspecificorderinwhichfunctionsareselected.
tothisend theapproachassignstoeachofthegivenfunctionsa weight and then takes a weighted random decision.
initially all functionshaveuniformlydistributedweights.whenafunctionis selectedby choosefunction itsweightisdividedbyaconstantfactor four in our current implementation .
note that this reduction is done every time a function is chosen i.e.
if the same function is chosen twice its weight is divided by the constant factor twice.
inadditiontotheabove choosefunction isguidedbyaminedmodel of nested api functions as explained in detail in section .
.
choosing arguments to pass into a function.
once a function has been selected for testing arguments need to be generated for it.
this is done by consulting the list of signatures produced by the discoveryphase.ifmultiplesignaturesexistforafunction oneis chosen at random.
the signatures inform the test generation of thenumber ofargumentsthat thefunctionshould bepassed and which if any are callback arguments.
if no signatures exist for the function arandomnumberofargumentsisselected between0and and arguments are generated randomly to fill these positions.
note that in these cases no callback arguments are generated.
for non callback arguments the type is selected randomly from the javascript primitive types number string and boolean objectliterals arrays functions and other.iftheselectedtypeisanyofthe primitives object orarray thenrandomlygeneratedvaluesofthis typeareused.iftheselectedtypeis function anavailablefunction ischosenfromtheapiundertestortheruntimeenvironment e.g.
console.log .iftheselectedtypeis other avariablethatisavailable at the current scope is selected which includes return values from previous api calls and arguments to previous callbacks in the test case tree definition .
nesting api calls helps with generating well formed arguments as values including objects may get passed from outer to inner calls as illustrated in the motivating example in section .
in addition since we are working with many file system related libraries string primitive values are selected randomly from a pre made list of valid file names that correspond to a small hierarchy of directories and files generated during the setup of nessie for the purposes of the testing.
beyond these two points we do not address the problem of generating complex objects in this work.
adding new extension points.
after executing a generated test extensionpoints iscalledtoidentifywheretoextendthetestinfutureiterations.thisfunctionreturnsanextensionpointforeach callbacknodethathasbeenexecuted correspondingtotheinsertion of a new child node to the right of its existing children see figure2b .therearetworeasonsfor notaddinganextensionpoint exceptionsthrownbythetestedapisandcallbackfunctionsthat areneverinvoked.byexaminingfeedbackfromtestexecutions the algorithmavoids creatingfuturetests thatbuildoncode thatwill never execute.
a single test may have multiple extension pointsbecause more than one of its callbacks may execute.
note thatextension points are not removed after being used so the set of extensionpointsgrowsmonotonically enablingformultiplenew tests to be derived from an extension point in a single base test.
.
mining api usages having an api call nested in the callback argument of another api call impliesa relationship between thesecalls.
we define anotion of anesting example to formalize such relationships.
definition nesting example .
anesting example is tuple fouter argouter ... argouter m finner arginner1 ... arginner n where fouteris the name of a called api function authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ellen arteca sebastian harner michael pradel frank tip everyargouter iiseithercbsyncorcbasync i.e.
asyncorasync callback argument or any non callback argument finneristhenameofanapifunctioninvokedinthecallback given tofouter and everyarginner jis eitherouter k i.e.
the same argument as given tofouterat position k cb k i.e.
thekth parameter of the callback function or i.e.
any other argument .
example .
.
the following nesting example can be mined from figure 2a ensurefile cbasync readjson outer .
example .
.
the following usage of the fs extra api where theobjparameterofthecallbackpassedto readjson servesasan argument in a nested call to outputjson read the c ontents of file.
json and output it to output.json 18readjson file.json function callback err obj outputjson output.json obj corresponds to readjson cbasync outputjson cb .
wedevelopedastaticanalysisforminingnestingexamplesfrom real world uses of apis by traversing the asts of existing api clients.thisanalysiswasimplementedincodeql usingits extensivefacilitiesforstaticanalysis.inparticular weusecodeql s access path tracking to identify functions as originating from an api import and its single static assignment representation of local variablestoidentifysituationswherethesameargumentisused in an outer call and an inner call.
for shared arguments that are primitivevalues e.g.
thesamestringpassedtobothinnerandouter calls the relationship is identified by checking for value equality.
the test generator uses the set of mined nesting examples in choosefunction.when selectingafunction tobenestedin thecallbackofsomefunction f thesetofnestingexamplesisconsulted to find examples where foutermatchesf.
if such nesting examples exist then one of the corresponding finnerfunctions is randomly selected to be invoked inside f s callback.
similarly chooseargumentconsults the selected nesting example to determine which arguments if any to reuse from the outer function or from the surrounding callback and at what position s .
if no relevant mined nesting examples are available an inner function is randomly selected.
the test generator is configured to only use mined data of the time.
if we only used nestings thatshowedupinmineddata thiswouldexcludemanypotentiallycorrectpairsthatsimplydonotoccurintheminedprojects.section explores the effect of varying how often mined data is consulted on the coverage achieved by the tests.
evaluation our evaluation aims to answer the following research questions rq1 howeffectiveisthediscoveryphaseatfindingabstract signatures of api functions?
rq2 how effective is nessie at achieving code coverage?
rq3 how effective is nessie at finding behavioral differences during regression testing?
rq4 whatistheeffectofvaryingthechanceofchoosing nested function pairs based on the mined nesting examples?
rq5 what is the performance of nessie?table summary of projects used for evaluation.
project loc cov.
loading commit description fs extra .
6bffcd8extra file system methods jsonfile .
9c6478a read write json files in node.js node dir .
a57c3b1 common directory file operations bluebird .3k .
6c8c069 performance oriented promises q .
6bc7f52promise library graceful fs .
c1b3777 drop in replacement for native fs rsvp.js .
21e0c97 tools for organizing async code glob .
8315c2d shell style file pattern matching zip a folder .
zip tar utility memfs .4k .
ec83e6fin memory file system benchmarks.
table 1shows the librarieswe use asbenchmarks forourevaluation alongwiththenumberoflinesofcode loc and the commit of the version we use.
to select candidate libraries we first identified two domains of libraries that commonly have asynchronous functionality file systems and promises.
we thenpicked popular libraries in those domains that satisfied the base requirements of successfully installing building and having a test suite with tests that all pass5.
as a point of reference we measure the statement coverage of the library code achieved by simplyloading the library column cov.
loading in table .
to mine nestingexamples fromexistingapi clients werun theapiusage miningoveracorpusof10 000javascriptprojectsongithub which yields a set of unique nestings of api functions in the libraries under test.
baselinesandvariantsoftheapproach.
wecomparenessieagainst thestateof theartapproachlambdatester lt .becausethe originalltdoesnotsupportlanguagefeaturesintroducedinecmascript and later and because parts of the implementation are specific to their benchmarks we re implemented lt within our testingframework.tobetterunderstandthevalueofnestingandse quencing weevaluatetwovariantsofnessie nes seq whichuses sequencingonly and nes seq nest whichusesbothsequencing and nesting.
.
rq1 effectiveness of automated api discovery to measure nessie s effectiveness at discovering the signatures of api functions we inspect the documentation of the librariesand manually establish their signatures and then compare these signaturesagainstthosediscoveredthroughourautomatedapidis covery.asdescribedinsection3 iftheapproachdoesnotdiscover anyvalidsignaturesforanapifunctionthenadefault callbackless signatureisassigned.wedonot countthisdefaultsignature towards the total number of discovered signatures.
table displays the results of this experiment.
for each api we include the number of signatures found through manual documentation inspection and the number found through automated discovery bothwithandwithoutcallbackarguments.wealsoincludethenumberonlyfoundwithoneoftheseapproaches.thefirstrow reads as follows for fs extra manual analysis yields signatures withcallback arguments and automateddiscovery yields signatureswithcallbackarguments.ofthosemanuallyfound 3are 5toautomatetheprocessofdeterminingwhichlibrariessatisfytheserequirements we used npm filter .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
nessie automatically testing javascript apis with asynchronous callbacks icse may pittsburgh pa usa table abstract signatures categorized manually m and found by automated api discovery a .
signatures withcallbacks signatures without callbacks project m a only m only a m a only m only a fs extra jsonfile node dir bluebird q graceful fs n a n a n a n a rsvp.js glob zip a folder memfs notdiscoveredautomatically ofthoseautomaticallydiscovered are not found manually.
the next four columns read the same way butforsignatureswithoutcallbackarguments.notethatwedonot haveresultsfor graceful fs asitdoesnotprovidefunction level documentation.
we summarize the effectiveness of the discovery phase by computing the percentage of documented signatures that are found with the automated approach.
for the signatures with callback arguments we compute this as follows signatures only found manually total number of signatures found manually withasimilar computation weseethat theautomateddiscovery finds of the documented signatures without callback arguments.
signatures found only manually.
nessie sometimes misses signaturesbecausetheautomateddiscoverymayfailtogeneratevalid arguments particularly in cases where arguments need to meet specificconditions.forexample inthefilesystemlibraries functions without callback arguments often expect valid file names for files with particular characteristics and throw an error when this is not the case e.g.
writefilesync andreadfilesync injsonfile .
anotherreasonforsignaturesmissedbytheapidiscoveryarefunctionsthattake multiplecallbackarguments whichouralgorithm missesasittestswithonlyonecallbackatatime.multiplecallbacks are the main cause of missed signatures in node dir andmemfs.
signatures found only automatically.
there are two main reasonsforfindingsignaturesautomaticallythatwemissedduringthe manual inspection of documentation.
first some functions are undocumented e.g.
internalfunctions aliasesforthedocumentedapi functions orre exportedfunctionsofthebuilt in fsmodule.for examplein fs extra writejson canalsobecalledwith writejson .
since the automated discovery reads the function properties of the packageonimport ittestsallfunctionsregardlessofwhetheror not they are presented to the user in the documentation.
there are alsosomeinternalfunctionsthatarepresentaspropertiesonthe library import such as tounixtimestamp onfs extra andmemfs.
second someapifunctionssupportmorefunctionsignaturesthan those that are documented.
since the discovery phase only considers asignature invalidif the apifunction call throws an error with figure3 cumulativecoveragewhilegenerating1 000tests for graceful fs.
thetestedarguments abasiclackoferrorcheckingintheimplementationcanleadtoextrasignatures.forexample considerthe writefile functionin jsonfile.thedocumentationpresentsitssignature as writefile filename obj callback .h o w ever theautomateddiscoveryphasefindsthat async isavalidsignature.thisunexpectedsignatureisbecause jsonfile.writefile is implemented with universalify sfrompromise function which executesthelastargumentifitisacallback regardlessoftheother arguments.
even though checking exposed apis against documentation is not the primary purpose of nessie we made pull requests addressing this issue on both fs extra6andjsonfile7.
answertorq1 automateddiscoveryfinds62 ofdocumented signaturesthatexpectcallbackarguments and38 ofsignatures without callback arguments.
it also discovers some undocumented signatures which in several cases reflect unexpected behavior.
.2rq2 coverage achieved by generated tests to measure nessie s effectiveness at covering the statements of the library under test we generate tests for each library and compute the cumulative coverage.
to compute coverage we use theistanbulcommandlinecoveragetool nyc evenifthedevelopers include their own command for computing coverage to ensure consistency.
we repeat this experiment with the two variants ofnessie nes seq nest and nes seq and with the baseline lt approach.
figure shows how cumulative coverage evolves while generating tests for one package graceful fs .
as a reference the horizontal line shows the coverage directly after loading the library.
the coverage follows a logarithmic shape a steep increase incoveragewiththeinitialtestsandaneventualconvergenceto somecoverageplateau or atleast alevelingoffof thecurve.the final coverage is fairly close between the two variants of nessie but the combination of sequencing and nesting converges more quickly.
moreover nessie achieves higher final coverage than lt. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ellen arteca sebastian harner michael pradel frank tip the number of functions for which nessie can generate tests andforwhichltcannotdependsentirelyontheapi.therefore the coverage improvements are quite variable across the packages tested.wechose graceful fs asthedemonstrativeexamplebecause itshowsboththeplateauingofcoverageandtherelativelysmall differencewegenerallyseebetweennessiewithbothnesting sequencingandnessiejustsequencing.thesameplotforallother packages are in the supplementary material.
to quantify and summarize the coverage results for all libraries theleftpartoftable3showsthefinalcoverageachievedafter1 tests.
the right part of the table quantifies the comparison with lt. we compute the number of tests required with nessie to match andexceedthe coverage that lt achieves after tests.
the last columnshowsthenumberoftestsltrequirestoreachthecoverage it achieves at tests i.e.
the beginning of the coverage plateau lt sustains at tests .
for example for the fs extra project after1 000testsnessieachievesastatementcoverageof37.
with nes seq nest and .
with nes seq while lt achieves .
.
nes seq nest matches and also exceeds lt s coverage after only 311tests nes seq matchesandalsoexceedslt scoverageafter 663tests.meanwhile after889tests ltreachesacoverageplateau that it sustains until tests.
overall nessie consistently achieves slightly higher coverage thanlt.moreover ourapproachreacheshighcoveragefaster it matches and often also exceeds the coverage that lt has after tests with substantially fewer tests than lt requires to reach the same coverage.
comparing the two variants of nessie the combination of sequencing and nesting is more effective.
answer to rq2 nessie achieves a higher coverage than the state of the art and fewer tests are required to reach this coverage in particular when the approach uses both sequencing and nesting.
.3rq3 finding behavioraldifferences during regression testing toanswerrq3 weusethetestsgeneratedbynessietofindbehavioraldifferencesinconsecutivecommitsofthebenchmarklibraries.
.
.
experimentaldesign.
tocomparethebehaviorofalibraryat twocommits wegenerate100testsbasedonthecodeattheearlier commit andthenrunthesetestswiththecodeatbothcommits.we usethemochatestingframework torunourtests.thisframeworkcaninternallyhandleerrorsinatest sothattheremaining testsareexecuted evenifanerroristhrowninoneofthem.most relevant for us mochahandles errors thrown by asynchronously invokedfunctionsandlogsthisasan internalasyncerror.todetect behavioraldifferences nessieproducesthefollowingoutputduringtestexecution valuesofallapifunctionargumentsbeforeandafter a call the name of the api function a callback is passed to and the value of all parameters inside a callback being executed the return valueofasuccessfulapifunctioncall thenameoftheapifunctionintheeventofafailingcall.wecomparetheoutputsofboth commits to identify the following kinds of behavioral differences an api call resulting in an error in one commit successfully executes in the other.
a return value of an api call differs between commits.
figure4 cumulativenumberofbehavioraldiffs forspecific commit of jsonfile.
an argument to an api call or a parameter of a callback differs between commits.
a callback is called in one commit but not in the other.
mochareportsaninternalasyncerrorinonecommitbutnot in the other.
some api updates result in an output difference that does not indicate a relevant difference in functionality e.g.
due to functionrenaming a change in the supported version of node.js or syntax errors resulting from migrating to strictmode.
after manually identifyingthesecases weconfigureouranalysistoignorethem and do not count them in the experimental results.
to avoid double counting the same behavioral difference being exposed by multiple generated tests we consider differences as equivalent iftheyare duetothesame kindofdifferenceandarise from the same api function.
since we are working with asynchronous apis the exact ordering of calls may be non deterministic possiblycausingdifferentoutputsacrossrepeatedexecutionsofthe same test.
to avoid reporting scheduling differences as behavioral differences we execute each test ten times for the same commit andthencomparethesetsofobserved outputsacross commits.
we report a behavioral difference only if an output observed in one commit is never observed in the other.
.
.
quantitativeresults.
asarepresentativeexample figure4 showsthecumulativenumberofdetectedbehavioraldifferences while generating tests for a representative commit of jsonfile.
ascanbeseeninthefigure nes seq nest ismosteffective followed by nes seq and then the existing lt technique.
table summarizes and quantifies these results across all libraries.
for each library we display the number of commit pairs beingcompared.weaimtocompare100commits i.e.
99pairs per library but some of the repositories have fewer than commits that affect source files.
the table reports the number of commit pairsinwhichtheapproachdetectsabehavioraldifference withtheaveragenumberofunique i.e.
non equivalent differencesinparentheses.
for example the table s first row reads for fs extra w e runtheregressiontestingover99pairsofcommits.nes seq nest spotsabehavioraldifferencein17ofthesepairs with2.8unique authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
nessie automatically testing javascript apis with asynchronous callbacks icse may pittsburgh pa usa table coverage after tests and comparison with lambdatester lt .
coverage after tests tests to match exceed lt tests project nes seq nest nes seq lt nes seq nest nes seq lt fs extra .
.
.
jsonfile .
.
.
n a node dir .
.
.
n a n a bluebird .
.
.
n a q .
.
.
graceful fs .
.
.
rsvp.js .
.
.
glob .
.
.
n a zip a folder .
.
.
n a memfs .
.
.
n a table pairs of commits checked via regression testing and behavioral differences found.
with diff.
avg.
unique per diff project compared pairs nes seq nest nes seq lt fs extra .
.
.
jsonfile .
.
.
node dir .
.
.
bluebird .
.
.
q .
.
.
graceful fs .
.
.
rsvp.js .
.
.
glob .
.
.
zip a folder .
.
.
memfs .
.
.
differences per pair on average.
nes seq finds a difference in of these pairs .
unique differences on average and lt in 10ofthese .2uniquedifferences onaverage .theresultsshow thesametrendasthatseeninrq2 nessiefindsmorebehavioral differences than lt vs. in total and the combination of nesting and sequencing is worthwhile vs. differences in total .
.
.
qualitative results.
to better understand the behavioral differencesthatnessiereveals wemanuallyinspectarandomsample of them.
table summarizes the results.
for each analyzed dif ference we include a hyperlink to the commit introducing thedifference the name of the project a categorization of the difference andadescriptionofhowitmanifests.thecategorizationswe use are as follows bug this commit is introducing a bug.
bug fix this commit is fixing a bug.
upgrade this commit is an update upgrade of the api including migration to newer apis updating method signatures and making functions async.
we find that nessie detects a variety of different types of api functionality changes.
interestingly for several commits that introduce abug nessielaterfindsthe dual committhatfixesthatsamebug.
tobetterunderstandtheimpactofnesting wecheckedforeach inspectedbehavioraldifferencewhetheritcouldalsobeexposed byatestcasewithoutnesting.similartotheexamplementioned in the introduction we find that creating a sequence only test case is in principle possible for each of the behavioral differences.
the key benefit provided by nesting is to more quickly cover a diversetable manual analysis of behavioral differences found via regression testing.
commit project diagnosis description of behavioral difference a149f82 fs extra bug outputjson executes callback even with bad arguments in newer commit dba0cbb fs extra upgrade many api functions no longer error or return different values in newer commit 03b2080 fs extra bug fix existsreturns callback argument return value on error instead of undefined df125be fs extra bug ensuresymlink executesthecallbackargument even with incorrect arguments 3fc5894 fs extra bug fix ensurefile throws error on incorrect arguments instead of executing callback ef9ade4 fs extra bug copyfile doesn t throw error with incorrect non callback arguments 2e4fcae fs extra upgrade writevreturns rejected promise instead of throwing error on incorrect arguments 075c2d1 fs extra bug moveandcopynow executes the callback argument even with incorrect arguments 4a0ebe5 jsonfile bug fix readfile executes callback in newer commit b1f40ef jsonfile upgrade readfilesync succeeds in newer commit errors in older commit 4b90419 jsonfile upgrade readfilesync errors in newer commit succeeds in older commit 995aa63 jsonfile bug writefile executes callback in newer commit e3d86e0 jsonfile bug read writefile execute callback even with bad arguments in newer commit 10eed1d jsonfile bug readfile sometimes errors in newer commit succeeds in older commit e5e5aa9 q upgrade tapandanysucceeds in newer commit error in older commit 2a9a617 graceful fs bug fix createreadstream succeeds in newer commit infinite loops in older commit 45a0242 graceful fs bug fix lchmodisundefined onlinuxinoldercommit succeeds in newer commit 5d961ab graceful fs bug fix readfile sometimes executes callback in newer commit eaab0ee glob upgrade globsucceeds in newer commit error in older commit 5d8a060 zip a folder upgrade zipfolder executes callback innewer commit set of behaviors than with purely sequential tests and hence to expose more behavioral differences in a given testing budget.
answertorq3 nessiefindsmanybehavioraldifferencesbetween versionsoflibraries includingaccidentallyintroducedbugs bug fixes and api upgrades.
these differences are found most quickly with generated tests that use both sequencing and nesting.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ellen arteca sebastian harner michael pradel frank tip table coverage after tests at different levels of using mined nesting examples.
test coverage using mined nestings project fs extra .
.
.
.
.
jsonfile .
.
.
.
.
node dir .
.
.
.
.
bluebird .
.
.
.
.
q .
.
.
.
.
graceful fs .
.
.
.
.
rsvp.js .
.
.
.
.
glob .
.
.
.
.
zip a folder .
.
.
.
.
memfs .
.
.
.
.
.
rq4 impact of guidance by mined nesting examples the api usage mining component of nessie informs the choice of whichinnerapifunctiontocallwhennestingapifunctions.by default the mined nesting examples are consulted of the time.
thefollowingmeasurestheeffectofvaryingthispercentageonthe coverage achieved bythe generated tests.
theexperimental setup isasinrq2 butweconsiderthetestgenerationtofollowmined nesting examples and of the time.
table summarizes the result of this experiment by showing thefinalcoverage aftergenerating1 000tests.thecorresponding coverage plots are in the supplementary material.
the table s first rowreadsasfollows for fs extra thestatementcoveragewhen using0 minednestingsis32.
whenusing25 minednestingsit is33.
whenusing50 minednestingsitis37.
whenusing75 mined nestings it is .
and when using mined nestings itis33.
.forreadability weshowthehighestcoverageforeach project in bold.
the results illustrate the value of using mined nesting examples mined nestings always leads to a coverage that is higher than or at least as high as and mined nesting.
this supportsour initial hypothesis that choosing informed nestings is more likelytoproducevalidteststhatwillincreasecoverage.however choosing only mined nestings i.e runs the risk of missing valid nestings that are simply never seen during the api usage mining.
wecurrentlydonothaveresultsontheimpactofmineddatause ontheregressiontests.however theincreaseincoveragecaused byminingapiusages whichweobservefor6ofthe10libraries suggests that mining may also help during regression testing.
answertorq4 choosingminednestings someofthetimeresults in tests with higher coverage than those generated alwaysornever usingtheminednestings.theoptimalparameterdependsonthe library under test but is an overall reasonable choice.
.
rq5 performance of test generation wemeasurethetimetakentogenerate100testsforeachofthree approaches we consider including the discovery phase.
we run theseexperimentsonamachinewithtwo32 core2.35ghzamd epyc7452cpusand128gbram runningcentos7.
.2003andnode.js .
.
.
since nessie is implemented in typescript it is single threadedandsoonlyusesonecore.dependingonthelibrary thetestgenerationtakesbetween15and30seconds.theresults showthatthetimerequiredtogenerate100testsisfairlysimilar across all three approaches.
miningthenestingexamplesisanup frontcost.asdescribedin section4.
wewroteastaticanalysisincodeql toidentify nestingexamples weranthisoverasetof13 580javascriptprojectson github using npm filter .
this process took around hours.
this set of examples comes with the tool and users of the tool would only be required to rerun the mining of nesting examples if they wanted to generate tests for apis for which we did not mineanynestingexamples.thecostofminingnestingexamples depends only on the number of mined projects and not on the number of apis being mined for.
answertorq5 with15to30secondsper100tests theapproach is efficient enough for practical use.
extending the set of mined nestingexamplestakestimeproportionaltothenumberofprojects mined but is an up front cost.
.
threats to validity internalvalidity.
ourresultsmaybeinfluencedbyseveralfactors.first ourbaselineisare implementationoflt because theoriginalimplementationwasnotfunctionalonourbenchmarks andtheirbenchmarksdonotcontainasynchronousapis.thereimplementation is based on the original code which is publiclyavailable and we clarified questions on the code with the lt authors.second ourautomatedidentificationofequivalentbehavioral differencesisapproximateandmaybothover andunderapproximatea theoretical preciseapproach.sincetheapproximationis likelythesameforallevaluatedapproaches itshouldnotinfluence the overall conclusions.
finally the results of manually inspecting and classifying behavioral differences is subject to our understandingofthetestedlibraries.tomitigatethisthreat wediscussedall cases among the authors.
externalvalidity.
thelibrariesusedintheevaluationmaynotbe representativefortheoverallpopulationoflibrarieswithasynchro nouscallbacks.asofjuly2021 npmjs.com reportsatotalof116 packagesthataredependentonthepackagesusedintheevaluation i.e.
the benchmarks at least cover a relevant subset of all libraries.
finally ourimplementationtargetsjavascript andhence wedonot claim our empirical results to generalize to other languages.because callbacks both synchronous and asynchronous exist in variousotherlanguages wheretheycausesimilarchallengesfor test generators we hope our conceptual contributions may inspire future work for other languages.
related work test generation for higher order functions.
there are several techniquesforautomaticallytestinghigher orderfunctions.mostcloselyrelated is lambdatester which also targets javascript and has inspired some of our design decisions.
the main difference is that nessietestsfunctionswithbothasynchronousandsynchronouscallbacks enabledbyourmethodforapidiscoveryandthrough authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
nessie automatically testing javascript apis with asynchronous callbacks icse may pittsburgh pa usa the notion of a test case tree which allows for nesting calls.
other testgeneratorscanberoughlycategorizedintorandomtestingand solver based systematic testing.
quickcheck is an example of the former as it creates callback functions that return a random type correct value.
koopman and plasmeijer propose systematic syntax drivengenerationofcallbackfunctionsbased onuser providedgenerators.atestgeneratorforhigher orderfunc tionsinracketreliesontypesandcontractsoftestedfunctions twokindsofinformationthatrarelyexistforjavascriptlibraries.
solver basedtestgeneratorsincludeseveralvariantsofsymbolic and concolic execution adapted to higher order functions andworkthatperformsatype directed enumerativesearchover thespaceoftestcases .palkaetal .
proposetorandomly generate type correct haskell programs including higher order functions totestahaskellcompiler.alloftheaboveapproachestar getfunctionallanguages andnoneofthemconsidersasynchronous callbacks or produces nested callbacks.
randomtestgeneration.
nessiebuildsuponarichhistoryofrandom test generators starting with randoop which introduced feedback directedrandomtestgeneration.ourworkalsofollows this paradigm but in contrast to randoop addresses challenges of higher order functions and those arising in a dynamically typed language.
evosuite uses an evolutionary algorithm to continuouslyimprove randomlygeneratedtestcases.beyondfunction level testing application level fuzzing has received significant attention including afl8and its derivatives and combinations of fuzzingwithsymbolictesting .incontrasttotheabovegreybox or whitebox fuzzers nessie does not need to analyze the library undertest butobtainsfeedbackfromtheexecutionofthegenerated tests and optionally uses existing api clients for guidance.
asynchronousjavascript.
astudyofcallbacksinjavascriptcode finds that of all functions take callback arguments that the majorityofthosecallbacksarenested andthatthemajorityofcallbacksareasynchronous .theseresultsshowthatgenerating testswithoutconsideringasynchronouscallbacks i.e.
theteststhatpriorwork isabletogenerate failstofullyreflectthebehavior seeninreal worldjavascriptcode.anotherstudyreportsthatmost oftheconcurrencybugsinnode.jsareaboutusagesofasynchronousapis .ourworkisaboutanalyzingtheimplementationof such apis.
beyond javascript a study of higher order functions in scalafindsthat7 ofallfunctionsarehigher order suggesting that the problem we address is relevant beyond javascript.
alimadadi et al .
propose a dynamic analysis to trace and visualizejavascriptexecutions withafocusonasynchronousinteractions across the client and the server.
another dynamic analysis detectspromise relatedanti patterns .severaltechniquesaimat detectingracesinjavascript where race means thatdifferentasynchronouslyscheduledcallbacksmaybeexecuted in more than one order.
these approaches are also motivated bythe challenges of asynchronous javascript but address problems orthogonal to that addressed by nessie.
thereareseveralformalizationsofdifferentaspectsofasynchronous javascript including an execution model of node.js a modeltoreasonaboutpromises andacalculus semantics and of a static analysis of asynchronous behavior .
the callback graph of the latter relates to our test case trees but it is created as part of a static analysis and captures a happensbefore likerelation whiletest casetrees serveasan intermediate representation that represents sequencing and nesting.
program analysis for javascript.
the popularity of javascript has motivated a variety of dynamic and static analysis techniques and we refer to a survey for a comprehensive discussion .
examplesoftechniquesincludedynamicanalysestodetecttypeinconsistencies to detect inefficient code to detect various common programming mistakes and to reason about taint flows .
work on reasoning about api changes and how they affect clients is a recent example of a static analysis.
similar to nessie all these analyses take a pragmatic approach toward addressing the idiosyncrasies of javascript without providing strong soundness or completeness guarantees.
conclusion effectivetestgenerationforapisthatmakeuseofasynchronous callback arguments is challenging as the test generator must generateteststhatcombinemultiplecallstorelatedapifunctionsin meaningful ways.
generating only sequences of calls as done in existingtestgenerators isinadequate asitisdifficultforsuchan approach to produce suitable values to invoke api functions with.
we presented nessie the first test generator aimed at apis with asynchronous callbacks which relies on both sequencing and nesting api calls to produce suitable values to invoke api functions with.
here nesting here means generated tests may contain api calls inside the body of callbacks passed to other api calls.
in anempirical evaluation nessie is applied to ten popular javascript libraries containing api functions with callbacks and its effectiveness is compared to that of lambatester a state of the art test generation technique that creates tests only by sequencing methodcalls.ourresultsshowthatnessiefindsmorebehavioraldifferences and achieves slightly higher coverage than lambdatester.
notably it needs to generate significantly fewer tests to achieve and exceed the coverage achieved by lambdatester.
tool data availability a full working artifact including all experimental data associated withthisresearchisavailableat