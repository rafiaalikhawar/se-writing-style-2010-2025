what you see is what you get attention based self guided automatic unit test generation xin yin chao ni xiaodan xu xiaohu yang the state key laboratory of blockchain and data security zhejiang university hangzhou china xyin chaoni xiaodanxu yangxh zju.edu.cn abstract software defects heavily affect software s functionalities and may cause huge losses.
recently many ai based approaches have been proposed to detect defects which can be divided into two categories software defect prediction and automatic unit test generation.
while these approaches have made great progress in software defect detection they still have several limitations in practical application including the low confidence of prediction models and the inefficiency of unit testing models.
to address these limitations we propose a wysiwyg i.e.
what you see is what you get approach attention based self guided automatic unit test generation auger which contains two stages defect detection and error triggering.
in the former stage auger first detects the proneness of defects.
then in the latter stage it guides to generate unit tests for triggering such an error with the help of critical information obtained by the former stage.
to evaluate the effectiveness of auger we conduct a large scale experiment by comparing with the state ofthe art sota approaches on the widely used datasets i.e.
bears bugs.jar and defects4j .
auger makes great improvements by .
to .
and .
to .
in terms of f1 score and precision in defect detection and can trigger to more errors than sotas in unit test generation.
besides we also conduct a further study to verify the generalization in practical usage by collecting a new dataset from real world projects.
index terms defect unit test generation error triggering i. i ntroduction with the rapid advancement of industrial automation code complexity and scale have increased posing significant challenges in managing defects.
developers often miss potential defects unless explicit errors occur reducing software quality.
recently many approaches have been proposed to detect defects which can be divided into two categories software defect prediction and automatic unit test generation.
despite the advancements in these approaches they still face certain challenges and limitations.
current defect detection approaches provide solely binary predictions indicating the presence of defects in code snippets or statements but lack detailed explanations making it challenging for developers to understand and trust the reliability of these predictions.
to address this limitation linevul uses attention scores for rationale and some approaches incorporate the code s graph structure .
however these efforts only focus on explaining how models make specific decisions and neglect to provide detailed insights into the conditions that cause defects.
meanwhile steenhoek et al.
also show this is the corresponding author chao ni is also with hangzhou high tech zone binjiang blockchain and data security research institute hangzhou chinasignificant variability among different models under identical input conditions eroding developers trust in detection results.
unit test generation approaches are crucial for ensuring software security and quality.
traditional unit test generation approaches prioritize achieving high code coverage.
however research shows that high code coverage does not always trigger errors effectively .
recently learning based generation approaches have made remarkable strides in progress.
however they focus on randomly generating a large number of test cases by training on the method2test dataset which contains numerous clean methods and non error triggering test cases lacking effective information guidance.
this leads to poor test efficiency failing to efficiently trigger errors.
to address these limitations we propose a wysiwyg approach attention based self guided automatic unit test generation auger which contains two stages defect detection and error triggering.
in the first stage auger detects defect proneness.
in the latter stage it guides the large language model llm to generate unit tests for triggering such an error with the help of critical information obtained by the former stage.
as a result auger will provide developers with defect detection results convincing error triggering unit tests and corresponding test results.
this automated approach instills confidence in developers regarding the detection results.
to investigate the effectiveness of auger we first extract and filter methods from several widely used datasets resulting in a total of defective and non defective methods.
subsequently we conduct comprehensive experiments to assess auger s performance in defect detection.
the results indicate that auger can achieve an f1 score of .
precision of .
and pr auc of .
on defects4j which improves the baselines by .
to .
.
to .
and .
to .
respectively.
besides we conduct extensive experiments to evaluate the effectiveness of auger in error triggering.
the outcomes demonstrate that auger effectively triggers and errors in different scenarios with a noticeably higher precision than the baselines.
in our collection of real world projects after march auger also achieves promising performance in error triggering.
our main contributions are summarized as follows a. wysiwyg defect detection with high confidence auger provides defect detection results convincing errortriggering unit tests and corresponding test results which helps to instill greater confidence in developers.
b. error triggering with high efficiency auger guides the llm to generate unit tests that trigger errors by leveragingarxiv .00828v1 dec 2024critical information extracted from defective code thereby narrowing down the search space.
c. extensive evaluations we evaluate auger against current state of the art approaches on the widely studied datasets .
to prevent data leakage we also collect an additional real world dataset for evaluation.
the replication package is publicly available at .
ii.
b ackground and motivation defect detection and unit test generation are the main approaches to ensure software quality.
in this section we aim to explore the challenges and limitations of existing defect detection approaches and unit test generation approaches.
a. limited confidence in defect detection models the limited confidence in defect detection models arises from two aspects solely binary predictions and model inconsistency.
current defect detection models often provide solely binary predictions indicating whether a code snippet or statement contains defects or not .
these results lack detailed explanations making it challenging for developers to understand and trust the reliability of these predictions.
linevul utilizes attention scores to explain the rationale behind the model s decision making.
moreover some approaches enhance the model s capabilities by incorporating the graph structure of code .
while these efforts have incorporated explanatory features they often focus on why the model made a specific decision rather than providing detailed insights into the conditions that cause defects e.g.
the inputs and outputs that can expose defects .
meanwhile the inconsistency of the defect detection models also undermines developers trust.
steenhoek et al.
reveal significant variability among different models as they may produce entirely divergent results under identical input conditions highlighting a lack of consistency.
consistency indicates that for the same input e.g.
one function all models in the study give the same prediction e.g.
for a specific function all methods predict it as defective or all agree that it is non defective .
consistency can bring trust in prediction when developers make decisions.
to assess the consistency of different models in defect detection we also conducted an empirical study as shown in table i. specifically we fine tuned two learning based detection models i.e.
linevul and svuld and two pre trained models i.e.
unixcoder and codebert respectively on three widely used defect detection datasets and compute the consistency in different models.
we can see that the consistency of all models is only .
to .
.
such low consistency will erode developers trust in the defect detection models which hinders their practical application.
table i consistency in different defect detection models models bears bugs.jar defects4j learning based models .
.
pre trained models .
.
all models .
.
b. limited efficiency in unit test generation approaches unit test generation approaches have gained widespread attention and application due to their ability to directly identify defects in software.
existing unit test generation approaches can be categorized into two types traditional generation approaches and learning based generation approaches.
however both of them suffer from inefficiency issues.
traditional generation approaches focus on the code coverage metric.
researches show that traditional generation approaches are very effective at achieving high coverage even covering more code than manually written test cases.
however previous studies indicate that high code coverage does not always result in effective error triggering .
meanwhile learning based generation approaches have been proposed and have made significant progress.
these approaches are trained on specific test generation datasets e.g.
methods2test enabling them to generate test cases that meet specific testing objectives.
they focus on randomly generating a large number of test cases lacking effective information guidance.
this leads to poor test efficiency failing to efficiently trigger errors.
moreover a dataset comprising highquality pairings of defective methods with error triggering test cases is essential for training a model that efficiently generates error triggering test cases.
however existing approaches are typically trained on the methods2test dataset which contains numerous clean methods and test cases incapable of triggering errors.
consequently a significant portion of the test cases generated by these approaches fail to trigger errors resulting in limited efficiency in error triggering.
intuition.
providing corresponding error triggering unit tests along with test results will help to instill greater confidence in defect detection results.
simultaneously defect detection information can be leveraged to guide the generation of unit tests reducing the model s search space and enhancing the efficiency of unit test generation.
iii.
a pproach we propose a wysiwyg approach attention based selfguided automatic unit test generation auger which contains two stages defect detection and error triggering.
in the former stage auger first detects the proneness of defects.
in the latter stage it guides the llm to generate unit tests for triggering such an error with the help of critical information present in defective code.
as a result auger will provide developers with defect detection results convincing errortriggering unit tests and corresponding test results.
although auger is general in this paper we adpot recent deepseek coder and codellama as the backend llms which can be easily replaced with various state of the art models e.g.
codet5 and starcoder .
auger includes defect detection attention guided unit test generation and unit test validation.
fig.
provides an overview of our approach defect detection .
we first introduce a java class file and extract all methods in the class file using the javaparser project non defectdefect defect detectionmethod unit testlocate the defective statementsattention guided unit test generation highlight attention for defective statements llm defective method public static double distance int p1 int p2 int sum ... return math.sqrt sum validation classfier encoder defective method public static double distance int p1 int p2 int sum ... return math.sqrt sum fig.
overview of auger tool.
we encode the methods into token representations and input them into auger to detect whether there are defects.
attention guided unit test generation .
we conduct further analysis on the detected defective methods to identify the defective statements.
then we guide the llm to focus on the defective statements in order to generate unit tests that trigger the errors.
unit test validation .
we describe how auger injects a unit test into an existing suite i.e.
test class and elaborate on how auger adds the required dependencies to successfully execute the injected unit test.
a. defect detection to improve robustness and performance we adopt adversarial learning and contrastive learning framework with the pretrained model unixcoder .
there are three important components of auger an encoder for embedding methods semantics an attack strategy for generating adversarial samples and a learning strategy for discriminating differences between normal and adversarial samples.
code encoder unixcoder proposed by guo et al.
is a unified pre trained model incorporating semantic and syntax information from both code comment and ast and we adopt it as the code encoder in auger to embed the code features at the method level.
unixcoder transforms the input method to a dimensional embedding em .
adversarial learning in adversarial learning the goal is to enhance the robustness of the model against adversarial attacks.
we employ fgm to conduct adversarial learning on unixcoder producing the encoded samples after the attack denoted asea.
the process involves perturbations are applied to the original input data to generate adversarial samples.
these perturbations are crafted to challenge the model s ability to correctly classify or handle the input.
training with adversarial samples the model is trained on both the original and adversarial samples.
this training helps the model learn to recognize adversarial inputs.
contrastive learning contrastive learning is employed to enhance the model s resistance to perturbations by aligning normal and adversarial samples.
the training objective is to fine tune the network such that the encoded representations em i.e.
normal samples and ea i.e.
adversarial samples are as close as possible.
the objective can be described as max em ea which encourages the network to reduce the distance between these representations.
we employ the kl divergence loss used in r drop to quantify the differences between normal and attacked samplesto minimize the distance between emandea.
during the fine tuning phase we use the cross entropy loss to guide the optimization process of auger by comparing the difference between the prediction probability of the model and the label.
the final loss of defect detection consists of both classification cross entropy loss and kl divergence loss which can be described by the following equation lce iyilog yi l lce lkl b. attention guided unit test generation llm is pre trained using millions of code snippets from open source projects showing dominantly superior reasoning capabilities over existing ai models in downstream tasks .
in this section we aim to stimulate the powerful capabilities of llm to efficiently generate error triggering unit tests.
to achieve this we need to address four tasks defect location prompt preparation attention profiling and attention inference .
task defect location similar to the defect detection process we also utilize unixcoder as the foundational model for defect location adopting it to embed code features at the statement level.
more precisely given the source code of a defective method auger first splits the method into individual statements.
then auger transforms the input method ton dimensional vectors at the statement level wherenindicates the number of statements.
finally after passing the final representation through a fully connected layer and thesoftmax layer auger outputs defective statements.
the cross entropy loss function is used to train the process.
i will give you a java defective method please generate a java unit test to trigger this error.
class name domnodepointer class constructor public domnodepointer node node locale locale string id defective method public string getline int linenumber ...... 2class context 3defective method 1task description fig.
an example of prompt for llm task prompt preparation auger is generalizable and can be extended to other programming languages by modifying the language specific information in the prompt e.g.
java to python and to .
the prompt involves three important components as illustrated in fig.
task description marked as .
llm is provided with the description constructed as i will give you a java defective method please generate a java unit test to trigger this error .
class context marked as .
the class name and class constructor provide key information about the structure and initialization process of the class.
defective method marked as .
we provide the defective method in the method level error triggering scenario.
we also prefix the defective method with defective method to directly indicate llm about the context of the method.
due to the vast search space of llms these models randomly generate entirely different unit tests for focal methods.
to efficiently obtain high quality outputs within a reasonable time frame we have modified the attention mechanism of the llm.
this modification is designed to guide the model s attention specifically toward the statements where defects are located.
the specific process algorithm consists of two components attention profiling which selects the effective attention heads for modifying and attention inference which emphasizes the defect location information of the defective methods during inference.
algorithm attention profiling and attention inference attention profiling section iii b3 input profiling setd coefficient attention layer number l attention head number h hyperparameter k forl 1tol h 1tohdo modify the attention head l h by equation evaluate the unit test generation performance on d collect the top kheadshwith the validation results output the attention head set h attention inference section iii b4 input promptsp defective statements s coefficient forhead l h inhdo modify the attention head l h by equation generate a large number of candidate unit tests output unit testst task attention profiling llm typically has multiple attention layers and multiple attention heads e.g.
deepseek coder .7b has attention layers each with attention heads .
attention heads are components of the multi head attention mechanism in transformer models.
each attention head operates independently enabling the model to focus on different parts of the input sequence simultaneously.
however there is currently no consensus on the specific roles that these attention heads play.
zeng et al.
demonstrated that the first attention layer provides insight into which tokens the model focuses on.
on the other hand wan et al.
showed that deeper attention layers excel in capturing long distance dependencies and program structure.
it is important to specify the correct attention heads given that different heads serve distinctive roles in encoding semantic syntactic information.
to this end we propose an attention profiling algorithm to identifythe effective attention heads for inference.
specifically we sub sample profiling set d samples from the defects4j dataset cf.
section iv a .
after that we need to define how to modify the llm s attention so that it focuses on the specified statement.
auger emphasizes the defective statements of the input method by down weighting the attention scores of tokens that are not in the defective statements.
specifically given the tokens of highlighted statements as s auger emphasizes these tokens by an attention projection w h l h w a l h v where t at c ift s at celse where 1is a scaling coefficient and a l h denotes the attention scores at the head hof thel th layer.
the term c t sat t s atnormalizes the scores so that they sum up to one.
attention modifying is conducted during the inference process and does not require any training.
equation modifies the model attention by scaling down the scores of tokens that are not in defective statements.
when the coefficient is set very small defective statements are highlighted given their increased attention scores after renormalization.
as shown in fig.
auger recognizes that line is a defective statement.
consequently auger first marks the defective line in the prompt and then recalculates the attention weights within the attention heads directing the llm s focus towards the tokens associated with the statement.
public string getline int linenumber ...... lastoffset pos lastline linenumber if js.indexof n pos return null else return js.substring pos js.indexof n pos 11highlight attention for tokens in defective statement public string getline int linenumber ...... lastoffset pos lastline linenumber if js.indexof n pos return null defective line else return js.substring pos js.indexof n pos fig.
highlight attention for tokens in defective statement after that we assess the performance of modifying each individual attention head l h where l land h h on a designated subset d. we rank all the heads based on their unit test generation performance specifically by evaluating how many errors can be triggered ond.
subsequently we define the attention head set hfor inference as the top kperforming heads.
unlike fine tuning attention profiling doesn t modify any model weights so it demands similar computational resources as inference.
the resulting head set hserves as a model level profile.
once determined we can use attention inference on hfor both existing and unseen datasets enhancing model comprehension and boosting performance.
task attention inference during the inference process auger modifies the attention head l h in the attention head sethusing equation .
the input to this process includes the prompt i.e.
task description class context and defective method defective statement and the coefficient while the output consists of a large number of candidate unit tests.c.
unit test validation our approach is automated and requires no manual intervention.
therefore to validate whether the unit tests generated by llm can accurately trigger identified errors we need to automatically inject the unit tests into the corresponding test classes.
additionally we must add the required dependencies to execute and obtain the test results.
inject unit test into test class we use token similarity to find the test classes that are most similar to the generated unit tests and inject them.
the intuition is that if a unit test belongs to a test class the unit test likely uses similar methods and classes and it shares similar tokens to other tests from that test class.
formally we assign a matching score for each test class based on equation simci tt tci tt wherett andtciare the set of tokens in the generated unit test and the i th test class respectively.
add the required dependencies first auger parses the generated unit test and identifies variable types and the referenced class names constructors and exceptions.
auger then endeavors to locate public classes matching the identified type name for unimported dependencies.
if precisely one such file exists auger derives the classpath to the identified class and adds an import statement accordingly.
however there may be scenarios where no matching classes are found or multiple matches occur.
in such cases auger scans the project for import statements ending with the target class name and selects the most prevalent import statement across all files.
after injecting the unit test into the test class and adding the required dependencies auger executes the test to check whether it triggers the identified errors.
iv.
e xperiment in this section we first present the studied datasets and then introduce the baseline approaches.
following that we describe the performance metrics as well as the experimental setting.
a. datasets defect detection in order to ensure the thoroughness and validity of our research findings regarding defect detection we have leveraged three widely used java defect datasets the bears dataset the bugs.jar dataset and the defects4j dataset .
since auger focuses on method level we perform two filtering steps on the original datasets to obtain valid methods and the filtering results of each dataset are displayed in table ii.
step each commit is considered as a mini version of a project.
we use the commit ids to request commit histories of the projects and for each commit we extract the code changes between before and after fixing a defect.
finally we use the code change information to obtain the defective and fixed version of a method.
thus we collect the following information for a project defective methods with their fixes and other clean methods.
in this step we obtain the bears dataset consisting of methods the bugs.jar dataset consisting of methods and the defects4j dataset containing methods.step to clean and normalize the dataset we start by removing duplicate methods.
the three datasets are derived from various versions of projects e.g.
defects4j extracted from real world java projects leading to a substantial number of duplicates in methods extracted from different commits during step .
in this step we finally obtain the bears dataset which comprises methods the bugs.jar dataset which comprises methods and the defects4j dataset which comprises methods.
unit test generation defects4j includes utilities for generating and evaluating test suites on the programs to determine if generated tests pass on the fixed versions and catch defects on the defective versions.
in contrast bears and bugs.jar do not include readily executable test suites.
therefore we evaluate real world error triggering on the defects4j dataset.
table ii the statistic of studied datasets datasets defective clean total ratio bears .
bugs.jar .
defects4j .
b. baselines defect detection to comprehensively compare the performance of existing work we consider two learningbased detection approaches and two pre trained models.
the former group contains two approaches i.e.
linevul and svuld which simply treat the source code as a sequence of tokens to represent its semantics.
the latter group contains two models i.e.
codebert and unixcoder which are pre trained models for programming languages.
unit test generation we consider the following baselines in this evaluation traditional approach.
we employ randoop a widely recognized tool extensively utilized for test case generation.
additionally evosuite is executed as a baseline albeit its primary design for regression testing somewhat constrains its efficacy in triggering defects within the program.
both randoop and evosuite are allocated a runtime of minutes per defective class adhering to the methodology outlined in .
we then use scripts from previous works to run each test case and check if they trigger any errors.
learning based approach.
to present the learning based approach we employ the whole test generation model athenatest .
we also evaluate against toga a unified transformer based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method.
in addition we fine tune a seq2seq model codet5 on the dataset used by toga to generate unit tests.
c. evaluation measures defect detection to evaluate the effectiveness of auger on defect detection we consider the following metrics accuracy precision recall f1 score fpr and pr auc.
accuracy evaluates the performance that how many methods can be correctly labeled.
it is calculated as tp tn tp fp tn fn.precision is the fraction of true defects among the detected ones.
it is defined as tp tp fp.
recall measures how many defects can be correctly detected.
it is defined as tp tp fn.
f1 score is a harmonic mean of precision andrecall and can be calculated as p r p r. fpr refers to the proportion of non defects that are predicted to be defects.
it is defined asfp fp tn.
pr auc is the area under the precision recall curve and is a useful metric of successful prediction when the class distribution is very imbalanced .
unit test generation to evaluate the effectiveness of auger in triggering errors present in the program the generated unit tests are run on the defective version and the fixed version.
we consider an error as triggered if a generated unit test fails on the defective version and passes on the fixed version.
since each fixed version is distinguished from the defective version by a minimal patch fixing the specific error a test must fail due to the specific error if it only fails on the defective version.
in addition to evaluating the number of errors triggered we use four metrics defined in and we summarize the meaning of these metrics in table iii.
following prior work we also use the precision metric.
when using error triggering tools developers care more about the precision i.e.
how many fps they need to inspect to trigger an error and usually have few interests in using a tool with a low precision .
table iii the metrics of error triggering metrics defective fixed true positive fail pass false positive fail fail true negative pass pass false negative pass fail d. experimental setting we implement auger in python with the help of pytorch framework.
all experiments are conducted on an nvidia a800 80gb graphics card.
as for defect detection we utilize unixcoder base nine from hugging face as our basic model.
we fine tune auger on the studied datasets to obtain a set of suitable parameters.
during the training phase we use adam with a batch size of to optimize the parameters ofauger .
we also leverage gelu as the activation function.
a dropout of .
is used for dense layers before calculating the final probability.
we set the maximum number of epochs in our experiment as and adopt an early stop mechanism.
the models i.e.
auger and baselines with the best performance on the validation set are used for the evaluations.
as for unit test generation we develop the generation pipeline in python utilizing pytorch implementation of deepseek coder .7b and codellama 7b.
we use the hugging face to load the model weights and generate outputs.
during the attention profiling we set to .
and kto .
for each defective method we generate candidate unit tests i.e.
the candidate number is refer to section v c for more details and test them in the test suite provided by defects4j.v.
r esults to investigate the feasibility of auger on defect detection and error triggering our experiments focus on the following three research questions rq comparable study of defect detection.
how well does auger perform on method level defect detection?
rq comparable study of error triggering.
how well does auger perform on triggering the error?
rq sensitivity analysis.
how do different configurations affect the overall performance of auger ?
a. rq effectiveness on defect detection objective.
benefiting from the powerful representation capability of deep neural networks many dl based detection approaches have been proposed .
codebert and unixcoder are bimodal pre trained models for programming languages and natural languages demonstrating excellent performance across various software engineering tasks such as code search and code generation .
the experiments are conducted to investigate whether auger outperforms sota method level detection approaches.
experimental design.
we consider four sota baselines linevul svuld codebert and unixcoder .
we conduct two distinct experiments to evaluate the performance ofauger .
in the first experiment we undertake the tasks of training validating and testing using the bears and bugs.jar datasets.
the second experiment extends our evaluation by training and validating on bears and bugs.jar but testing on the defects4j dataset.
this experiment aims to showcase the detection capabilities of auger in identifying unknown realworld defects.
it is noteworthy that according to our statistical analysis there is no overlap in the data extracted from bears and bugs.jar with that obtained from defects4j.
our methodology for constructing the training and validation data from bears and bugs.jar aligns with established practices in prior research .
specifically of methods are treated as training data of methods are treated as validation data and the left of methods are treated as testing data.
results.
the evaluation results on bears and bugs.jar datasets are reported in table iv and the best performances are highlighted in bold.
according to the results we find that auger outperforms all sota baseline methods on almost all performance measures except recall .
in particular auger obtains .
.
and .
in terms of f1 score precision and pr auc which improves baselines by .
to .
.
to .
and .
to .
in terms of f1 score precision and pr auc respectively.
table iv defect detection results of auger compared against four baselines on bears and bugs.jar methods f1 score recall precision pr auc linevul .
.
.
.
svuld .
.
.
.
codebert .
.
.
.
unixcoder .
.
.
.
auger .
.
.
.
improve .
to .
.
to .
.
to .
to evaluate the defect detection performance of auger on unknown real world java projects we train and validate auger and baselines on bears and bugs.jar datasets.
subsequently we test them on the defects4j dataset.
the performance comparisons of auger and four sotas on the defects4j dataset are presented in table v. according to table v we find that all sotas have poor performance on defects4j dataset while auger outperforms all baselines on almost all performance measures.
specifically auger obtains .
of f1 score .
of precision and .
of pr auc which improves baselines by .
to .
.
to .
and .
to .
respectively.
the results indicate that auger has a better learning ability than the four baselines.
table v defect detection results of auger compared against four baselines on defects4j methods f1 score recall precision pr auc linevul .
.
.
.
svuld .
.
.
.
codebert .
.
.
.
unixcoder .
.
.
.
auger .
.
.
.
improve .
to .
.
to .
.
to .
in the field of defect detection the fpr and accuracy are crucial metrics for assessing detection performance.
fpr measures the extent of false alarms in the system indicating the proportion of non defective samples incorrectly flagged as defective.
on the other hand accuracy provides a comprehensive evaluation of the overall correctness of the system.
therefore we also compare the performance between auger and four baselines in terms of fpr and accuracy.
according to the results in table vi we can observe that auger exhibits a reduction in fpr ranging from .
to .
compared to other baselines.
additionally its accuracy shows an improvement ranging from .
to .
compared to other baselines.
notably auger achieves the highest accuracy while maintaining the lowest fpr highlighting its credibility and usability in detecting defects in real world java projects.
table vi the performance between auger and four baselines in terms of fpr and accuracy methods fpr decrease accuracy improve linevul .
.
.
.
svuld .
.
.
.
codebert .
.
.
.
unixcoder .
.
.
.
auger .
.
to .
.
.
to .
answer to rq auger outperforms the sota baselines at method level software defect detection.
specifically it achieves notable improvements in f1 score precision prauc and accuracy as well as a reduction in fpr.
b. rq effectiveness on error triggering objective.
in order to enhance llm s proficiency in generating unit tests that trigger errors we devised an attention profiling approach.
this involves modifying the attention weights of the llm to guide its focus toward statements within defectivemethods that may harbor defects.
this strategic adjustment aims to facilitate the generation of more unit tests that effectively trigger errors.
in this section our objective is to investigate whether auger outperforms previous unit test generation approaches in terms of error triggering.
experimental design.
to facilitate comparison we employ deepseek coder and codellama denoting them as auger and auger respectively.
we consider five sota baselines toga evosuite randoop athenatest and codet5 .
for toga we adhere to the approach outlined in employing evosuite to generate test prefixes on the defective version.
subsequently toga generates the corresponding test oracles.
for evosuite and randoop we employ the scripts provided by the defects4j toolkit.
evosuite is utilized in its regression mode while randoop is applied in both regression and error revealing modes i.e.
randoop regand randoop rev .
for each focal method we set up auger and baselines to repeat times generating candidate unit tests.
as auger requires defect location we fine tuned a unixcoder model to locate the defective statements within the methods using all the defective methods from bears and bugs.jar.
we have considered three experimental scenarios.
the first scenario involves conducting experiments using defective methods detected in rq .
the second scenario entails using all defective methods from defects4j for experimentation.
the third scenario aims to evaluate whether auger can trigger errors in real world projects.
we follow defects4j and collect defect fixing commits from high quality open source projects included in defects4j.
we use deepseek coder for exploration which collected pre training data from github before february .
to prevent data leakage we only collect defectfixing commits from march onwards and obtain defects.
then we extract methods following the steps in section iv a and ultimately obtain method level defects which are used as input for auger to verify its ability to trigger errors in real world projects.
results.
the effectiveness of auger compared against five baselines are reported in table vii.
according to the results we can obtain the following observations for the defects detected in rq toga evosuite randoopreg randoop rev auger and auger are able to trigger and errors while athenatest and codet5 are unable to trigger any errors.
similar to for the method level defects in defects4j toga evosuite randoop reg randooprev auger andauger could trigger and errors respectively while athenatest and codet5 are unable to trigger any errors.
athenatest and codet5 generate a large number of candidate unit tests that fail to compile which makes them extremely ineffective at triggering errors.
both auger and auger show excellent performance andauger works better than auger .
in the next section we will select the best auger to study.
in comparison to the baselines auger triggers errors and errors more highlighting the effectiveness of defective informationin guiding the llm.
in addition our auger also has the highest precision meaning that developers only need to inspect a minimum number of unit tests under the same conditions.
table vii the effectiveness of auger compared against baselines on detected defects and all defects methodsdetected defects all defects trigger improve precision trigger improve precision toga .
.
evosuite .
.
randoopreg .
.
randooprev .
.
athenatest codet5 auger .
.
auger .
.
to better demonstrate the effectiveness of auger we present the error triggering quantities for different projects in two scenarios as shown in table viii.
according to the results we can observe that compared to other projects auger excels at triggering errors in the chart cli compress and gson projects achieving a recall of over .
exceeding .
in these projects.
auger performs poorly in projects such as jacksonxml jxpath and mockito where it fails to trigger any errors.
upon investigation we found that most of these defects are multi hunk defects involving multiple methods.
triggering these types of defects can be challenging.
table viii the recall of auger for the projects on defects4j detected defects all defects projects recall prop.
projects recall prop.
chart .
chart .
closure closure .
collections collections csv csv .
jacksoncore .
jacksoncore .
jacksonxml jacksonxml jxpath jxpath math .
math .
time time .
cli .
cli .
codec .
codec .
compress .
compress .
gson .
gson .
jacksondatabind jacksondatabind .
jsoup .
jsoup .
lang .
lang .
mockito mockito sum .
sum .
we draw a venn diagram to further illustrate the performance difference on error triggering.
for a better presentation we independently illustrate the top best baselines i.e.
toga evosuite randoop reg and randoop rev on the basis of the number of triggering errors and ignore the baselines i.e.
athenatest and codet5 that cannot trigger error for easy reference.
fig.
shows the illustrated results and we can also obtain two observations individual approaches have unique abilities to trigger specific errors that otherscannot making their performance somewhat complementary.
overall auger has a more powerful ability than baselines since it can trigger the most number of unique errors i.e.
that other baselines can hardly trigger.
3ourstoga evosuite randoopreg randooprev11916 fig.
venn diagram of auger and studied baselines case study.
to explore why auger has an outstanding performance in triggering unique errors we further analyze one example i.e.
jsoup as a case study as shown in fig.
.
the defect is that if key itself contains only space characters then key.trim will get an empty string.
after the empty string is assigned to this.key the validate.notempty key check passes because key itself is not an empty string.
but in reality the value of this.key is an empty string which is an incorrect state.
existing approaches cannot understand the defect present in the code making it difficult to efficiently generate unit tests that trigger the error.
as shown in fig.
auger understands the semantics of the code correctly and generates a unit test that effectively exposes the defect in the defective method when handling key that only contain spaces.
this example further exemplifies the capability of auger to leverage defective information to guide llm to generate error triggering unit tests efficiently.
public void test01 throws throwable attributes atts new attributes string key val val try attribute att new attribute key val atts catch illegalargumentexception e return fail expected an illegalargumentexception generated unit test public attribute string key string val attributes parent validate.notnull key this.key key.trim key key.trim validate.notempty key this.key key this.val val this.parent parent defective method fig.
unique error triggered by auger effectiveness of auger in real world projects.
since the deepseek coder s pre training data was sourced from github prior to february in order to prevent data leakage we collect defects from real world projects starting from march to evaluate auger s error triggering capabilities.
table ix presents the results of auger for these real world projects.
according to the results we can conclude that auger possesses the ability to trigger errors in real world projects not just limited to the defects present in the defects4j dataset.
this further demonstrates the practicality and feasibility of auger in generating unit tests.
auger has the ability to guide llm in generating effective unit tests triggering potential errors in future real world projects.table ix the recall of auger for the real world projects project recall prop.
project recall prop.
cli jsoup .
codec .
lang .
compress .
sum .
answer to rq auger achieves better performance in both the defects detected in rq and all method level defects in defects4j triggering and errors respectively.
auger can also trigger potential errors in future realworld projects.
c. rq configurations of auger objective.
in defect detection we have incorporated adversarial learning and contrastive learning to enhance the performance and robustness of the unixcoder.
as a result we aim to investigate the individual effects of these two components.
in error triggering we modify attention to guide llm focus on defective statements generating unit tests that trigger errors.
during this process we seek to examine the impact of attention modifying as well as the influence of the candidate number of unit tests on the final results.
experimental design.
first we investigate the impact of different components on defect detection and design three variants of auger .auger v1 represents the removal of adversarial learning andcontrastive learning auger v2 indicates the removal of adversarial learning and auger v3 signifies the removal of contrastive learning .
this approach allows us to examine the individual effects of each component.
subsequently we explore the influence of attention modifying and defect location on error triggering creating two variants.
augerw odenotes the elimination of the attention modifying component while auger gtdenotes the utilization of groundtruth defect location to guide the llm.
finally we conduct a comparative analysis of the impact of candidate numbers on auger and toga the latter of which is identified as the baseline with the best performance cf.
section v b .
results.
we discuss the results from the aspects of ablation and unit test candidate number respectively.
table x defect detection results of auger compared against variants methods al cl f1 score pr auc fpr auger v1 .
.
.
auger v2 .
.
.
auger v3 .
.
.
auger .
.
.
impact of components in defect detection.
table x shows the effectiveness of different variants and the better performance is highlighted in bold.
according to the results we can observe that two components have their own advantages in a method level defect detection scenario achieving a varying performance and significantly improving the performance of auger v1.
both of them can contributeto the performance of auger .
adversarial learning demonstrates superior performance compared to contrastive learning .
specifically auger v3 outperforms auger v2 in terms of f1 score .
.
pr auc .
.
and fpr .
.
metrics.
a combination of these two components yields optimal performance in terms of f1 score .
pr auc .
and fpr .
.
this suggests that incorporating adversarial learning and contrastive learning can enhance the effectiveness of the pre trained model for defect detection.
table xi error triggering results of auger compared against variants datasets auger w oauger auger gt detected defects all defects impact of components in error triggering.
according to the results in table xi we observe that auger and augergttrigger more errors than auger w o which indicates the importance of the attention modifying.
particularly auger and auger gtcan trigger and more errors than augerw oin all defects.
auger achieves comparable performance obtaining similar conclusions in two different scenarios weaker than auger gtbut stronger than auger w o. as expected auger gtperforms the best as it utilizes groundtruth defect location information to guide the llm avoiding the misguidance caused by errors in defect location.
uni00000014 uni00000013 uni00000015 uni00000013 uni00000016 uni00000013 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni0000001a uni00000013 uni0000001b uni00000013 uni0000001c uni00000013 uni00000014 uni00000013 uni00000013 uni00000014 uni00000014 uni00000013 uni00000014 uni00000015 uni00000013 uni00000014 uni00000016 uni00000013 uni00000026 uni00000044 uni00000051 uni00000047 uni0000004c uni00000047 uni00000044 uni00000057 uni00000048 uni00000003 uni00000031 uni00000058 uni00000050 uni00000045 uni00000048 uni00000055 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni0000001a uni00000013 uni0000001b uni00000013 uni00000037 uni00000055 uni0000004c uni0000004a uni0000004a uni00000048 uni00000055 uni00000003 uni00000028 uni00000055 uni00000055 uni00000052 uni00000055 uni00000016 uni00000016 uni00000017 uni00000019 uni00000018 uni0000001a uni00000019 uni00000016 uni00000019 uni0000001b uni0000001a uni00000014 uni0000001a uni0000001a uni0000001a uni0000001c uni0000001b uni00000016 uni0000001b uni00000017 uni0000001b uni00000017 uni0000001b uni00000018 uni0000001b uni00000018 uni00000017 uni0000001c uni00000018 uni0000001a uni00000019 uni00000013 uni00000019 uni00000013 uni00000019 uni00000013 uni00000019 uni00000013 uni00000019 uni00000013 uni00000019 uni00000014 uni00000019 uni00000014 uni00000019 uni00000014 uni00000019 uni00000014 uni00000019 uni00000014 uni00000019 uni00000014 uni00000032 uni00000058 uni00000055 uni00000056 uni00000037 uni00000032 uni0000002a uni00000024 fig.
the varying performance of auger and toga with different unit test candidate number on all defects impact of unit test candidate number.
according to the results in fig.
we find that different candidate numbers have varying impacts on auger s performance and the performance of auger increases as the number of candidates increases.
through the improvement curve it can be observed that the performance improvement of auger far exceeds that of toga.
as the number of candidate unit tests increases the improvement curve of toga remains relatively flat reaching optimal effectiveness when generating candidate unit tests.
in contrast auger exhibits an upward trend reaching optimal effectiveness when generating candidate unit tests.
more candidate numbers may not guarantee additional performance improvement.
whenauger generates candidate unit tests there is a significant improvement compared to generating candidate unit tests i.e.
.
however when continuously increasing the number of candidates the rate of performance improvement decreases i.e.
and meanwhile the generation cost with llm is increasing.
considering both the performance improvement and the generation cost caused by llm we adopt unit test candidate numbers as the default setting.
answer to rq the two components i.e.
adversarial learning and contrastive learning contribute substantially toauger and combining them achieves the best performance of defect detection.
attention modifying can direct llm to focus on defective statements to generate more unit tests that trigger errors.
increasing the candidate number can significantly improve the error triggering performance ofauger but will gradually become saturated.
vi.
d iscussion a. comparison of efficiency defect detection.
table xii presents the details of time cost and gpu memory cost for baselines and auger .
we find that although auger requires more time and computational resources during fine tuning its inference costs are comparable to those of the baselines.
on the whole during fine tuning auger takes an average of minutes and seconds per epoch and consumes 538m of gpu memory.
in contrast the baselines take an average of minute and seconds to minutes and seconds per epoch with a gpu memory cost ranging from 412m to 442m.
however auger only requires seconds and 658m of gpu memory during inference which is very close to the baselines i.e.
seconds for time cost and 626m to 754m for gpu memory cost .
given the performance improvements of auger refer to section v a the additional resources spent on fine tuning are justified.
furthermore auger does not require additional resources for inference after fine tuning.
table xii the time cost and gpu memory cost of baselines and auger for one epoch on defect detection methodstime cost gpu memory cost fine tuning inference fine tuning inference linevul 1m 16s 16s 924m 626m svuld 3m 24s 16s 442m 754m codebert 1m 42s 16s 412m 650m unixcoder 1m 45s 16s 416m 658m auger 3m 52s 16s 538m 658m unit test generation.
fig.
shows the average runtime used by each baseline and auger on unit test generation.
for toga athenatest and codet5 we record the total finetuning time i.e.
ft in fig.
and inference time during our experiments.
toga uses the default settings from previous work while athenatest and codet5 undergo epochs of fine tuning.
for evosuite randoop reg and randoop rev which do not require fine tuning and instead directly use scriptsprovided by defects4j to generate test cases we only record the test case generation time i.e.
inference in fig.
.
our auger does not require training so we record the time for the attention profiling process i.e.
profiling in fig.
and the test case generation time.
obviously we find that the total time required for auger is less than that of other baselines.
even though auger involves both profiling and inference processes the time consumption for these processes is relatively low.
for example auger requires a total of .
hours whereas the baselines require between .
hours and .
hours.
although athenatest and codet5 require minimal inference time they require significantly longer finetuning times and ultimately perform poorly in error triggering refer to section v b .
overall auger uses the least total time and achieves the best error trigger results making it more effective in real world scenarios.
toga75.
.
.
.
.
.
athenatest codet5 augerrevrandoopreg89.
evosuite randoopprofiling ft infer enceruntime overhead hours fig.
the runtime overheads of baselines and auger b. threats to validity internal validity.
the internal threat arises from potential data leakage since referenced unit tests may be part of the training data of llm.
to tackle this issue we initially calculate the number of error triggering unit tests generated by auger which matches the reference unit test in defects4j.
we find that out of triggered errors of error triggering unit tests align with the unit tests in the fixed version.
only unit tests are similar but not identical e.g.
input and output are different to the unit tests in the fixed version.
additionally compared to the basic llm i.e.
deepseek coder auger demonstrates a significant enhancement in performance triggering more errors.
this demonstrates that the improved results achieved byauger are not merely a result of memorizing the training data.
moreover the pre training data for deepseek coder was collected from github before february .
to prevent data leakage we also collected defects from real world projects starting from march to evaluate auger s errortriggering capabilities.
according to the results auger can trigger out of errors.
therefore we can conclude that auger possesses the ability to reveal potential errors in future real world projects.
the second internal threat arises from the reliance of unit test generation on defect detection.
in this experiment we conducted software defect detection on widely used datasets e.g.
defects4j and our model achieved the lowest fpr i.e.
.
and the highest f1 score i.e.
.
thereby mitigating the internal threat to the experimental design.external validity.
the effectiveness observed in auger s performance may not be applicable across different datasets.
we conduct evaluations not only on the widely used defects4j dataset but also on the collected dataset from real world projects.
this broader evaluation scope aims to showcase the generalizability of our approach.
vii.
r elated work a. defect detection traditional work on defect detection has explored a broad spectrum of metrics encompassing factors such as code size code complexity object oriented organizational and change history to forecast potential defects in software projects.
graves et al.
introduced the idea that recent modifications to code serve as effective indicators of impending defects while kim et al.
noted that defects often manifest in clusters within the history of software changes meaning that recent changes and faults are likely to introduce defects in the future.
dl based approaches have been proposed to learn from historical data .
codebert and unixcoder are bimodal pre trained models for programming languages and natural languages.
they learn general representations that support downstream applications such as vulnerability detection defect prediction etc.
ni et al.
proposed svuld by adopting contrastive learning to train the unixcoder model for learning distinguishing semantic representation of functions regardless of their lexically similar information.
however these approaches often provide solely binary predictions indicating whether a code snippet or statement contains defects or not .
to address this limitation fu et al.
proposed linevul which uses attention scores to elucidate the reasoning behind the model s decisions.
additionally several approaches improve the model s performance by integrating the graph structure of the code .
just in time jit defect prediction approaches have been proposed to predict whether a commit will introduce defects in the future.
pyexplainer is a novel local rule based model agnostic technique designed to explain the predictions of jit defect models.
while these efforts have incorporated explanatory features they often focus on why the model made a specific decision rather than providing detailed insights into the conditions that cause defects e.g.
the inputs and outputs that can expose defects .
different from previous works our paper connects defect detection and unit test generation.
for each detected defect auger generates corresponding test cases that trigger the error instilling greater confidence in the defect detection results.
b. unit test generation unit test generation approaches can be classified into two types traditional generation approaches and learning based generation approaches.
however both types face challenges related to inefficiency.
traditional generation approaches focus on code coverage and research shows that traditional approaches are very effective at achieving high coverage .
randoop is a widely recognized tool extensivelyutilized for generating unit tests for java code using feedbackdirected random test generation.
evosuite is a tool that automates the generation of test suites aiming for high code coverage minimal size and comprehensive assertions.
however previous studies show that high code coverage does not necessarily imply effective error triggering .
recently learning based generation approaches have achieved significant progress.
athenatest is a transformer based model that is learned from developer written test cases in order to generate correct and readable tests.
the task is framed as a translation problem where the source is a focal method and the target is the corresponding test case.
toga is a unified transformer based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method.
a3test leverages the domain adaptation principles where the goal is to adapt the existing knowledge from an assertion generation task to the test case generation task.
however these approaches focus on randomly generating a large number of test cases by fitting the method2test dataset which contains numerous clean methods and non error triggering test cases lacking effective information guidance.
this results in inefficient tests and an inability to efficiently trigger errors.
different from existing works our paper focuses on employing defect location information to guide the generation of unit tests reducing the model s search space and enhancing the efficiency of unit test generation.
viii.
c onclusion and future work we propose auger which is a method level approach for defect detection and error triggering.
auger first detects the proneness of defects and then guides the llm to generate unit tests for triggering such an error with the help of critical information present in defective code.
as a result auger will provide developers with defect detection results convincing error triggering unit tests and corresponding test results.
this automated approach instills confidence in developers regarding defect detection results.
to evaluate the effectiveness of auger we conduct a large scale experiment by comparing it with sotas on the widely used dataset defects4j.
auger makes great improvements by .
to .
.
to .
and .
to .
in terms of f1 score precision and prauc and can trigger to more bugs than sotas.
besides we also conduct a further study to verify the generalization in practical usage by collecting a new dataset from real world projects.
in the future we will generate multi dimensional reports on the test results and conduct human studies to fully validate the effectiveness of auger.