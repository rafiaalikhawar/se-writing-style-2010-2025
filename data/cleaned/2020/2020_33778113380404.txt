interpreting cloud computer vision pain points a mining study of stack overflow alex cummaudo ca deakin.edu.au applied artificial intelligence inst.
deakin university geelong victoria australiarajesh vasa rajesh.vasa deakin.edu.au applied artificial intelligence inst.
deakin university geelong victoria australiascott barnett scott.barnett deakin.edu.au applied artificial intelligence inst.
deakin university geelong victoria australia john grundy john.grundy monash.edu faculty of information technology monash university clayton victoria australiamohamed abdelrazek mohamed.abdelrazek deakin.edu.au school of information technology deakin university geelong victoria australia abstract intelligent services are becoming increasingly more pervasive application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users and large technology firms enable this via restful apis.
while such apis promise an easy to integrate on demand machine intelligence their current design documentation and developer interface hides much of the underlying machine learning techniques that power them.
such apis look and feel like conventional apis but abstract away data driven probabilistic behaviour the implications of a developer treating these apis in the same way as other traditional cloud services such as cloud storage is of concern.
the objective of this study is to determine the various pain points developers face when implementing systems that rely on the most mature of these intelligent services specifically those that provide computer vision.
we use stack overflow to mine indications of the frustrations that developers appear to face when using computer vision services classifying their questions against two recent classification taxonomies documentation related and general questions .
we find that unlike mature fields like mobile development there is a contrast in the types of questions asked by developers.
these indicate a shallow understanding of the underlying technology that empower such systems.
we discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts information systems web services data mining software and its engineering software creation and management general and reference empirical studies computing methodologies artificial intelligence .
keywords intelligent services computer vision documentation pain points stack overflow empirical study acm reference format alex cummaudo rajesh vasa scott barnett john grundy and mohamed abdelrazek.
.
interpreting cloud computer vision pain points a mining study of stack overflow.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction the availability of recent advances in artificial intelligence ai over simple restful end points offers application developers new opportunities.
these new intelligent services ai components abstract complex machine learning ml and ai techniques behind simpler api calls.
in particular they hide either explicitly or implicitly any data driven and non deterministic properties inherent to the process of their construction.
the promise is that software engineers can incorporate complex machine learnt capabilities such as computer vision by simply calling an api end point.
the expectation is that application developers can use these aipowered services like they use other conventional software components and cloud services e.g.
object storage like aws s3 .
furthermore the documentation of these ai components is still anchored to the traditional approach of briefly explaining the end points with some information about the expected inputs and responses.
the presupposition is that developers can reason and work with this high level information.
these services are also marketed to suggest that application developers do not need to fully understand how these components were created i.e.
assumptions in training data and training algorithms the ways in which the components can fail and when such components should and should not be used.
the nuances of ml and ai powering intelligent services have to be appreciated as there are real world consequences to software ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea alex cummaudo et al.
quality for applications that depend on them if they are ignored .
this is especially true when ml and ai are abstracted and masked behind a conventional looking api call yet the mechanisms behind the api are data dependent probabilistic and potentially nondeterministic .
we are yet to discover what long term impacts exist during development and production due to poor documentation that do not capture these traits nor do we know the depth of understanding application developers have for these components.
given the way ai powered services are currently presented developers are also likely to reason about these new services much like a string library or a cloud data storage service.
that is they may not fully consider the implications of the underlying statistical nature of these new abstractions or the consequent impacts on productivity and quality.
typically when developers are unable to correctly align to the mindset of the api designer they attempt to resolve issues by re reading the api documentation.
if they are still unable to resolve these issues on their own after some internet searching they consider online discussion platforms e.g.
stack overflow github issues mailing lists where they seek technological advice from their peers .
capturing what developers discuss on these platforms offers an insight into the frustrations developers face when using different software components as shown by recent works .
however to our knowledge no studies have yet analysed what developers struggle with when using the new generation of intelligent services.
given the re emergent interest in ai and the anticipated value from this technology a better understanding of issues faced by developers will help us improve the quality of services.
our hypothesis is that application developers do not fully appreciate the probabilistic nature of these services nor do they have sufficient appreciation of necessary background knowledge however we do not know the specific areas of concern.
the motivation for our study is to inform api designers on which aspects to focus in their documentation education and potentially refine the design of the end points.
this study involves an investigation of stack overflow so posts regarding one of the most mature types of intelligent services computer vision services dating from november to june .
we adapt existing methodologies of prior so analyses to extract posts related to computer vision services.
we then apply two existing so question classification schemes presented at icpc and icse in and .
these previous studies focused on mobile apps and web applications.
although not a direct motivation our work also serves as a validation of the applicability of these two issue classification taxonomies in the context of intelligent services hence potential for generalisation .
additionally our work is the first to our knowledge to testthe applicability of these taxonomies in a new study.
the taxonomies in previous works focus on the specific aspects from the domain e.g.
api usage specificity within the documentation etc.
and as such do not deeply consider the learning gap of an application developer.
to explore the api learning implications raised by our so analysis we applied an additional lens of two taxonomies from the field of pedagogy.
this was motivated by the need to offer an insight into the work needed to help developers learn how to use these relatively new services.
the key findings of our study are the primary areas that developers raise as issues reflect a relatively primitive understanding of the underlying concepts of data driven ml approaches used.
we note this via the issues raised due to conceptual misunderstanding and confusion in interpreting errors developers predominantly encounter a different distribution of issue types than were reported in previous studies indicating the complexity of the technical domain has a non trivial influence on intelligent api usage and most of these issues can be resolved with better documentation based on our analysis.
the paper also offers a data set as an additional contribution to the research community and to permit replication .
the paper structure is as follows section provides motivational examples to highlight the core focus of our study section provides a background on prior studies that have mined so to gather insight into the se community section describes our study design in detail section presents the findings from the so extraction section offers an interpretation of the results in addition to potential implications that arise from our work section outlines the limitations of our study concluding remarks are given in section .
motivation intelligent services are often available as a cloud end point and provide developers a friendly approach to access recent ai ml advances without being experts in the underlying processes.
figure highlights how these services abstract away much of the technical know how needed to create and operationalise these intelligent services .
in particular they hide information about the training algorithm and data sets used in training the evaluation procedures the optimisations undertaken and surprisingly they often do not offer a properly versioned end point .
that is the cloud vendors may change the behaviour of the services without sufficient transparency.
the trade off towards ease of use for application developers coupled with the current state of documentation and assumed developer background has a cost as reflected in the increasing discussions on developer communities such as so see fig.
.
to illustrate the key concerns we list below a few up voted questions unsure of ml specific vocabulary though it s now not so clear to me what score actually means.
i m trying out the and there s a score field that returns that i m not sure how to interpret .
frustrated about non deterministic results often the api has troubles in recognizing single digits... at other times vision confuses digits with letters.
is there a way to help the program recognize numbers better for example limit the results to a specific format or to numbers only?
unaware of the limitations behind the services is there any api available where we can recognize human other body parts chest hand legs and other parts of the body because as per the google vision api it s only able to detect face of the human not other parts.
seeking further documentation does anybody know if google has published their full list of labels produce meal ... and where i could find that?
are those labels 1585interpreting cloud computer vision pain points a mining study of stack overflow icse may seoul republic of korea absolute control verbose codebase self sourced data detailed infrastructurequick to write easy to integrate pre trained models cloud based api callsml frameworks intelligent servicesdo it yourself ml figure some traits of intelligent services vs. do ityourself ml.
green to red arrows indicate the presence of these traits.
adapted from ortiz .
structured in any way?
e.g.
is it known that food is a superset of produce for example.
the objective of our study is to better understand the nature of the questions that developers raise when using intelligent services in order to inform the service designers and documenters.
in particular the knowledge we identify can be used to improve the documentation educational material and potentially the information contained in the services response objects these are the main avenues developers have to learn and reason about when using these services.
there is previous work that has investigated issues raised by developers .
we build on top of this work by adapting the study methodology and apply the taxonomies offered to identify the nature of the issues and this results in the following research questions in this paper rq1.
how do developers mis comprehend intelligent services as presented within stack overflow pain points?
while the ai community is well aware in the the nuances that empower intelligent services such services are being released for application developers who may not be aware of their limitations or how they work.
this is especially the case when machine intelligence is accessed via web based apis where such details are not fully exposed.
rq2.
are the distribution of issues similar to prior studies?
we compare how the distributions of previous studies of posts about conventional deterministic api services differ from those of intelligent services.
by assessing the distribution of intelligent services issues against similar studies that focus on mobile and web development we identify whether a new taxonomy is needed specific to ai based services and if gaps specific to ai knowledge exist that need to be captured in these taxonomies.
background the primary goal of analysing issues is to better understand the root causes.
hence a good issue classification taxonomy should ideally capture the underlying causal aspects instead of pure functional groupings .
although this idea of cause related classification is not new chillarege advocated for it in this tse paper in this is not a universally followed approach when studying online discussions and some recent works have largely classified issues into the what is and not how to fix it .
they typically manually classify discussion into either functional areas e.g.
website design css mobile app development .net framework java ordescriptive areas e.g.
coding style practice problem solution design qa .
as a result many of these studies do not give us a prioritised means of targeted attack on how to resolve these issues with for example improved documentation.
interestingly recent taxonomies that studied so data aghajani et al .
and beyer et al .
were causal in nature and developed to understand discussions related to mobile and web applications.
however issues that arise when developers use intelligent services have not been studied nor do we know if existing issue classification taxonomies are sufficient in this domain.
researchers studying apis have also attempted to understand developer s opinions towards apis categorise the questions they ask about these apis and understand api related documentation and usage issues .
these studies often employ automation to assist in the data analysis stages of their research.
latent dirichlet allocation is applied for topic modelling and other ml techniques such as random forests conditional random fields or support vector machines are also used.
however automatic techniques are tuned to classify into descriptive categories that is they help paint a landscape of what is but generally do not address the causal factors to address the issues in great detail.
for example functional areas such as website design user interface or design result from such analyses.
these automatic approaches are generally non causal making it hard to address reasons for why developers are asking such questions.
however not all studies in the space use automatic techniques other studies employ manual thematic analysis e.g.
card sorting or a combination of both .
our work uses a manual approach for classification and we use taxonomies that are more causally aligned allowing our findings to be directly useful in terms of addressing the issues.
evidence based se has helped shape the last years worth of research but the reliability of such evidence has been questioned .
replication studies especially in empirical works can give us the confidence that existing results are adaptable to new domains in this context we extend to intelligent services and work with study methods developed in previous works.
method .
data extraction this study initially attempted to capture so posts on a broad range of many intelligent services by identifying issues related to four popular intelligent service cloud providers google cloud aws azure and ibm cloud .
we based our selection criteria on the prominence of the providers in industry google amazon microsoft ibm and their ubiquity in cloud platform services.
additionally in these services were considered the most adopted cloud vendors for enterprise applications .
however during the filtering stage see section .
we decided to focus on a subset of these services computer vision as these are one of the more mature and stable ml ai based services with widespread and increasing adoption in the developer community see fig.
.
we acknowledge other services beyond the four analysed provide similar capabilities and only english speaking services have been selected excluding popular services from asia 1586icse may seoul republic of korea alex cummaudo et al.
q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 ibm ms gc v aws total figure trend of posts where ibm ibm watson visual recognition ms azure computer vision aws aws rekognition and gcv google cloud vision.
three ms posts from q4 q3 and q4 have been removed for graph clarity.
e.g.
see section .
for comprehensiveness we explain below our initial attempts to extract allintelligent services.
.
.
defining a list of intelligent services.
as there exists no global list of intelligent services to search on we needed to derive a corpus of initial terms to allow us to know what to search for on the stack exchange data explorer1 sede .
we began by looking at different brand names of cloud services and their permutations e.g.
google cloud services and gcs as well as various ml related products e.g.
google cloud ml .
to do this we performed extensive google searches2in addition to manually reviewing six overview pages of the relevant cloud platforms.
we identified initial intelligent services to incorporate into our search terms3.
.
.
manual search for relevant related terms.
we then ran a manual search2on each term to determine if these terms were relevant.
we did this by querying each term within so s search feature reviewing the titles and body post previews of the first three pages of results we did not review the answers only the questions .
we also noted down the user defined tags of each post up to five per question by clicking into each tag we could review similar tags e.g.
project oxford for azure cognitive services and check if the tag had synonyms e.g.
aws lex and amazon lex .
we then compiled a corpus of tags consisting of terms.
.
.
developing a search query.
we recognise that searching sede viatags exclusively can be ineffective see .
to mitigate this we produced a corpus of title and body terms.
such terms are those that exist within the title and body of the posts to reflect the ways in which individual developers commonly use to refer to different intelligent services.
to derive at such a list we performed a search2 3of the tags above in sede filtering out posts that were not answers i.e.
questions only as we wanted to see how developersphrase their questions.
for each search we extracted a random sample of questions total for each service and reviewed each question.
we noted many patterns in the permutations of how developers refer to these services such as common misspellings bind vs. bing brand misunderstanding microsoft computer vision vs. azure computer vision hyphenation auto ml vs. auto ml uk and us english watson analyser vs. watson analy zer and the use of apostrophes plurals and abbreviations 2this search was conducted on january 3for reproducibility this is available at scomputer vision api microsoft computer vision service s gcv vs. google cloud vision .
we arrived at a final list of terms compromising all of the intelligent services provided by google amazon microsoft and ibm as of january .
.
.
executing our search query.
our next step was to perform a case insensitive search of all terms within the body or title of posts.
we used google bigquery s public data set of so posts4to overcome sede s row limit and to conduct a case insensitive search.
this search was conducted on may where we extracted results.
we then performed several filtering steps to cleanse our extracted data as explained below.
.
data filtering .
.
refining our inclusion exclusion criteria.
we performed an initial manual filtering of the most recent posts sorted by descending creationdate values of the posts above assessing the suitability of the results and to help further refine our inclusion and exclusion criteria.
we did note that some abbreviations used in the search terms e.g.
gcv wcs resulting in irrelevant questions in our result set.
we therefore removed abbreviations from our search query and consolidated all overlapping terms e.g.
google vision api was collapsed into google vision .
we also recognised that results would be non trivial to analyse without automated techniques.
as we wanted to do manual qualitative analysis we reduced our search space to search terms of just the computer vision services within the original corpus of terms.
these were google cloud vision aws rekognition azure computer vision and ibm watson visual recognition .
this resulted in results that were extracted on june .
the query used and raw results are available online in our supplementary materials .
.
.
duplicates.
within results no duplicate questions were noted as determined by unique post id title or timestamp.
.
.
automated and manual filtering.
to assess the suitability and nature of the questions extracted the first author began with a manual check on a randomised sample of questions.
as the questions were exported in a raw csv format with html tags included in the post s body we parsed the questions through an erb 5watson cognitive services 1587interpreting cloud computer vision pain points a mining study of stack overflow icse may seoul republic of korea templating engine script6in which the id title body tags created date and view answer and comment counts were rendered for each post in an easily readable format.
additionally sql matches in the extraction process were also highlighted in yellow i.e.
in the body of the post and listed at the top of each post.
these visual cues helped to identify false positive matches where library imports or stack traces included terms within our corpus of computer vision service terms.
for example aws java sdk rekognition jar is falsely matched as a dependency within an unrelated question.
as such exact matches would be hard to remove without the use of regular expressions and due to the low likelihood of their appearance we did not perform any followup automatic filtering.
.
.
classification.
our posts were then split into additional random samples in addition to the random sample of above .
posts were classified by the first author and three other research assistants software engineers with at least years industry experience assisted to classify the remaining .
this left a total of classifications made by four people plus an additional classifications made from reliability analysis in which the remaining posts were classified nine times as detailed in section .
.
.
thus a total of classifications were made from the original posts extracted.
whilst we could have chosen to employ topic modelling these are too descriptive in nature as discussed in section .
moreover we wanted to see if prior taxonomies can be applied to intelligent services as opposed to creating a new one and compare if their distributions are similar.
therefore we applied the two existing taxonomies described in section to each post i a documentationspecific taxonomy that addresses issues directly resulting from documentation and ii a generalised taxonomy that covers a broad range of so issues in a well defined se area specifically mobile app development .
aghajani et al.
s documentation specific taxonomy taxonomy a is multi layered consisting of four dimensions and sub categories .
similarly beyer s so generalised post classification taxonomy taxonomy b consists of seven dimensions .
we code each dimension with a number x and each sub category with a letter y xy .
we describe both taxonomies in detail within table .
where a post was included in our results but not applicable to intelligent services see section .
.
or not applicable to a taxonomy dimension category then the post was flagged for removal in further analysis.
table presents our understanding of the respective taxonomies our intent is not to methodologically replicate aghajani et al .or beyer et al .
s studies in the intelligent service domain rather to acknowledge related work in the area of so classification and reduce the need to synthesise a new taxonomy.
we baseline all coding against our interpretation only.
our classifications are therefore independent of the previous authors findings.
.
data analysis .
.
reliability of classification.
to measure consistency of the categories assigned by each rater to each post we utilised both intra and inter rater reliability .
as verbatim descriptions from dimensions and sub categories were considered quite lengthy from 6we make this available for future use at original sources all raters met to agree on a shared interpretation of the descriptions which were then paraphrased as discussed in the previous subsection and tabulated in table .
to perform statistical calculations of reliability each category was assigned a nominal value and a random sample of posts were extracted.
two phase reliability analysis followed.
firstly intra rater agreement by the first author was conducted twice on june and august .
secondly inter rater agreement was conducted with the remaining four co authors in addition to three research assistants within our research group in mid august .
thus the posts were classified an additional nine times resulting in classifications for reliability analysis.
we include these classifications in our overall analysis.
at first we followed methods of reliability analysis similar to previous so studies e.g.
using the percentage agreement metric that divides the number of agreed categories assigned per post by the total number of raters .
however percentage agreement is generally rejected as an inadequate measure of reliability analysis in statistical communities.
as we used more than coders and our reliability analysis was conducted under the same random sample of posts we applied light s kappa to our ratings which indicates an overall index of agreement.
this was done using the irrcomputational r package as suggested in .
.
.
distribution analysis.
in order to compare the distribution of categories from our study with previous studies we carried out a 2test.
we selected a 2test as the following assumptions are satisfied i the data is categorical ii all counts are greater than and iii we can assume simple random sampling.
the null hypothesis describes the case where each population has the same proportion of observations and the alternative hypothesis is where at least one of the null hypothesis statements is false.
we chose a significance value of .
following a standard rule of thumb.
as to the best of our knowledge this is the first statistical comparison using taxonomy a and b on so posts.
to report the effect size we selected cramer s phi cwhich is well suited for use on nominal data .
findings we present our findings from classifying a total of so posts aimed at answering rqs and .
posts were classified using taxonomies a and b for reliability analysis as described in section .
.
and the remaining posts were classified as per section .
.
.
a summary of our classification using taxonomies a and b is shown in fig.
.
.
post classification and reliability analysis when undertaking the classification we found that issues .
did not relate to intelligent services directly.
for example library dependencies were still included in a number of results see section .
.
and we found there to be many posts discussing android s mobile vision api as google cloud vision .
these issues were flagged and ignored for further analysis see section .
.
.
for our reliability analysis we classified a total of posts of which posts were flagged as irrelevant.
landis and koch provide guidelines to interpret kappa reliability statistics where 1588icse may seoul republic of korea alex cummaudo et al.
table descriptions of dimensions and sub categories from both taxonomies used.
a documentation specific classification aghajani et al.
a information content what issues related to what is written in the documentation a 1a correctness .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
what exists in the documentation actually matches what is implemented in code a 1b completeness .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the documentation fully covers all aspects of the api s components a 1c up to dateness .
.
.
.
.
.
.
.
.
.
.
.
.
what is documented is accurate to the current version of the api a information content how .
issues related to how the document is written and organised a 2a maintainability .
.
.
.
.
.
.
.
.
.
.
.
the upkeep effort to ensure the documentation remains up to date a 2b readability .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the extent to which the documentation is interpretable a 2c usability .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
how useable the organisation look and feel of the documentation is a 2d usefulness .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the usefulness of the documentation avoiding misinformation.
a process related .
.
.
.
.
.
.
.
.
.
.
.
.
issues related to the documentation process a 3a internationalisation .
.
.
.
.
.
.
translating the documentation into other languages a 3b contribution related .
.
.
.
.
.
contribution issues encountered when people contribute to the documentation a 3c configuration related .
.
.
.
.
configuration issues of the documentation tool a 3d implementation related .
.
.
.
unwanted development issues caused by poor documentation a 3e traceability .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tracing documentation changes when when who and why a tool related .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
issues related to documentation tools e.g.
javadoc a 4a tooling bugs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bugs that exist within the documentation tooling a 3b tooling discrepancy .
.
.
.
.
.
.
.
support as expectations not being fulfilled by these documentation tools a 3c tooling help required .
.
.
.
.
.
help required due to improper usage of the tools a 3d tooling migration .
.
.
.
.
.
.
.
.
.
issues migrating the tool to a new version or another tool b generalised classification beyer et al.
b api usage .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
issue on how to implement something using a specific component provided by the api b discrepancy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the questioner s expected behaviour of the api does not reflect the api s actual behaviour b errors .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
issue regarding some form of error when using the api and provides an exception and or stack trace to help understand why it is occurring b review .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the questioner is seeking insight from the developer community on what the best practices are using a specific api or decisions they should make given their specific situation b conceptual.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the questioner is trying to ascertain limitations of the api and its behaviour and rectify issues in their conceptual understanding on the background of the api s functionality b api change .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
issue regarding changes in the api from a previous version b learning .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the questioner is seeking for learning resources to self learn further functionality in the api and unlike discrepancy there is no specific problem they are seeking a solution for .
.20indicates slight agreement and .
.
indicates fairagreement.
despite all raters meeting to agree on a shared interpretation of the taxonomies see section .
.
our inter rater measures aligned slightly .
for taxonomy a and fairly .
for taxonomy b. we report further in section .
.
developer frustrations we found beyer et al .
s high level abstraction taxonomy taxonomy b was able to classify .
of posts.
.
posts were assigned exclusively under aghajani et al .
s documentation specific taxonomy taxonomy a .
we found that developers do not generally ask questions exclusive to documentation and typically either pair documentation related issues to their own code or context.
the following two subsections further explain results from both taxonomy a and b s perspective.
.
.
results from aghajani et al .
s taxonomy.
results for aghajani et al.
s low level documentation taxonomy taxonomy a indicatesthat most discussion on so does not directly relate to documentation about an intelligent service.
we did not find any processrelated a or tool related a questions as understandably the developers who write the documentation of the intelligent services would not be posting questions of such nature on so.
one can infer documentation related issues from posts i.e.
parts of the documentation lacking that may cause the issue posted .
however there are few questions that directly relate to documentation of intelligent services.
few developers question or ask questions directly about the api documentation but some .
posts ask for additional information to understand the api completeness a 1b for example is there a full list of potential labels that google s vision api will return?
there seems to be very little to no documentation for aws ios text recognition inside an image .
.
of posts question the accuracy a 1a of certain parts of the cloud documentation especially in relation to incorrect quotas and limitations are the cloud vision api limits in documentation 1589interpreting cloud computer vision pain points a mining study of stack overflow icse may seoul republic of korea .
.
.
.
.
.
.
.
.
.
.
.
.
.
correctness completeness up to dateness maintainability readability usability usefulnessintelligent services aghajani et al.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
api usage discrepancy errors review conceptual api change learningintelligent services beyer et al.
figure left documentation specific classification taxonomy results highlights a mostly similar distribution to that of aghajani et al.
s findings .
right generalised classification taxonomy results highlight differences from more mature fields i.e.
android apis in beyer et al.
to less mature fields i.e.
intelligent services .
correct?
according to the google vision documentation the maximum number of image files per request is .
elsewhere however i m finding that the maximum number of requests per minute is as high as .
.
there are also many