semi supervised log based anomaly detection via probabilistic label estimation lin yang college of intelligence and computing tianjin university tianjin china linyang tju.edu.cnjunjie cheny college of intelligence and computing tianjin university tianjin china junjiechen tju.edu.cn zan wangy college of intelligence and computing tianjin university tianjin china wangzan tju.edu.cnweijing wang college of intelligence and computing tianjin university tianjin china wangweijing tju.edu.cnjiajun jiang college of intelligence and computing tianjin university tianjin china jiangjiajun tju.edu.cn xuyuan dong information and network center tianjin university tianjin china dongxuyuan tju.edu.cnwenbin zhang information and network center tianjin university tianjin china zhangwenbin tju.edu.cn abstract with the growth of software systems logs have become an important data to aid system maintenance.
log based anomaly detection is one of the most important methods for such purpose which aims to automatically detect system anomalies via log analysis.
however existing log based anomaly detection approaches still suffer from practical issues due to either depending on a large amount of manually labeled training data supervised approaches or unsatisfactory performance without learning the knowledge on historical anomalies unsupervised and semi supervised approaches .
in this paper we propose a novel practical log based anomaly detection approach plelog which is semi supervised to get rid of time consuming manual labeling and incorporates the knowledge on historical anomalies via probabilistic label estimation to bring supervised approaches superiority into play.
in addition plelog is able to stay immune to unstable log data via semantic embedding and detect anomalies efficiently and effectively by designing an attention based gru neural network.
we evaluated plelog on two most widely used public datasets and the results demonstrate the effectiveness of plelog significantly outperforming the compared approaches with an average of .
improvement in terms of f1 score.
in particular plelog has been applied to two real world systems from our university and a large corporation further demonstrating its practicability.
index terms log analysis anomaly detection deep learning probabilistic estimation label i. i ntroduction over the years software systems become much larger and more complex which largely aggravates the difficulty of maintaining them .
as presented in the existing work ycorresponding author.
logs have become important data for maintaining largescale software systems e.g.
online service systems which are produced during the running of systems in order to record events and states of interest.
through examining recorded logs developers can check system status detect anomalies and diagnose root causes.
however due to the large scale of systems log data are massive and thus manual examination for logs is very difficult even infeasible.
therefore there are a large amount of work focusing on automated log analysis in the literature .
log based anomaly detection is one of the most important aspects in automated log analysis which aims to automatically detect system anomalies based on logs .
almost all the existing log based anomaly detection approaches share the same high level steps extracting log events i.e.
the templates of log messages and log sequences i.e.
series of log events that record specific execution flows from log messages and building an anomaly detection model through a machine learning or data mining technique based on log sequences.
according to the used machine learning or data mining techniques these existing approaches include supervised approaches e.g.
logrobust unsupervised approaches e.g.
logcluster and semi supervised approaches e.g.
loganomaly .
although they have been demonstrated to be effective in their corresponding studies these existing approaches still suffer from the following practical issues supervised approaches are the most effective but rely on a large amount of training data which contain both pos14482021 ieee acm 43rd international conference on software engineering icse .
ieee itive instances i.e.
anomalous log sequences and negative instances i.e.
normal log sequences .
in practice it is easy to obtain normal log sequences since when a system is normally running without any alerts all the produced logs could be regarded as normal ones.
however identifying anomalous log sequence is very difficult since when anomalies occur in a system both anomalous and normal log sequences can be produced and mixed together.
since log data are massive and hard to understand manual labeling is very time consuming and expensive and thus such supervised approaches are actually not practical.
unsupervised and semi supervised only knowing the labels of a set of normal log sequences approaches get rid of the limitation of supervised approaches by using only part of the normal log sequences for training.
as a result their effectiveness tends to be worse than the latter due to lack of the knowledge on historical anomalies.
moreover as presented in the existing work log data are unstable due to frequent modification of log statements in source code in practice causing that some incoming log events or log sequences do not appear in the training data.
due to the neglect of unstable log data the effectiveness of existing unsupervised and semi supervised approaches can drop largely when coming across unseen log events or log sequences in practice.
therefore it is still desired to propose a more practical logbased anomaly detection approach.
although both supervised approaches and unsupervised and semi supervised approaches suffer from practical issues both of them actually have complementary strengths.
more specifically the former has better effectiveness due to incorporating the knowledge on historical anomalies while the latter gets rid of time consuming manual labeling.
in particular in this paper we propose a more practical log based anomaly detection approach plelog which combines the above strengths via probability label estimation.
to get rid of time consuming manual labeling plelog is a semi supervised approach only knowing the labels of a set of normal log sequences which are obtained from systems when they normally run without any alerts.
to incorporate the knowledge on historical anomalies plelog uses the idea of pu learning positive unlabeled learning for reference which utilizes known positive instances to estimate the labels of unlabeled instances for subsequent model building.
in our problem a set of negative instances normal log sequences are known and plelog aims to estimate the labels of a set of mixed anomalous and normal log sequences.
in plelog there are two major challenges.
the first one is how to estimate the labels of unlabeled log sequences based on known normal log sequences.
to overcome this challenge plelog adopts the clustering method i.e.
hdbscan to divide all log sequences in the training set into several groups and the log sequences in the same group tend to share the same label.
therefore according to whether a group contains known normal log sequences plelog preliminarily estimates the labels of unlabeled log sequences.
however it ishard to produce perfect clustering results and thus the second challenge is how to reduce the influence of noise incurred by clustering.
to overcome this challenge instead of assigning a deterministic label plelog measures the probability that an unlabeled log sequence belongs to each label based on the clustering results i.e.
the uncertainty that the unlabeled log sequence is divided into the corresponding group and then assigns a probabilistic label to it.
in this way noise can be assigned less confident labels to reduce their influence to some degree.
based on probabilistic label estimation a labeled training set is obtained and then plelog builds an anomaly detection model through supervised machine learning which can bring supervised approaches superiority into play.
besides to make plelog more practical it is required to perform well for unstable log data and detect anomalies efficiently and effectively.
here plelog incorporates semantic embedding for log sequences to satisfy the first criterion following the existing work .
to satisfy the second criterion we design an attention based gru gate recurrent unit neural network for anomaly detection model building in plelog.
to evaluate the performance of plelog we conducted an empirical study based on two most widely used public datasets i.e.
hdfs and bgl following the existing work .
in particular to make our study closer to the practical scenario we guaranteed that all log data in the training set are produced before those in the test set which is ignored by almost all the existing studies and thus leads to the absence of unstable log data in their studies.
our experimental results demonstrate that plelog significantly outperforms the state of the art unsupervised and semi supervised logbased anomaly detection approaches with the improvements of .
.
on hdfs and .
.
on bgl in terms of f1 score.
also our results confirm the contribution of each main component in plelog.
in particular plelog has been successfully applied to two large scale real world systems from two different organizations i.e.
the network center of our university and one influential motor corporation throughout the world we hide its name due to the company policy .
in our work we call them aandb.
during the practical evaluation plelog achieves .
and .
f1 score for aandbrespectively significantly outperforming the state ofthe art unsupervised and semi supervised approaches with the improvements of .
.
onaand .
.
onb.
the results further demonstrate the performance of plelog in practice.
our work makes the following major contributions we propose a practical and robust log based anomaly detection approach plelog which is semi supervised and incorporates the knowledge on historical anomalies via probabilistic label estimation.
also plelog is able to stay immune to unstable log data via semantic embedding and detect anomalies efficiently and effectively by designing an attention based gru neural network.
we propose an effective method to estimate labels of unlabeled log sequences based on known normal log sequences through clustering.
in particular we design probabilistic 1449log message sequence ......receiving block src dest blk .
.
.
.
.
.227packetresponderfor blockterminating.1blk 370867log sequence ......receiving block src dest packetresponderfor blockterminating.
log parsinglog parameterlog eventreceiving block src dest blk .
.
.
.
.
.
receiving block src dest fig.
an illustrating example for log terminology label estimation to reduce the influence of noise incurred by clustering.
we conduct an empirical study based on two most widelyused public datasets demonstrating the high performance of plelog.
in particular plelog has been applied to two real world systems in our university and a large motor corporation further confirming the practicability of plelog.
ii.
l ogterminology in this section we introduce log terminology used in our paper based on an illustrating example shown in figure .
the example of log data is selected from the public hdfs dataset .
from this figure a log message is a raw unstructured sentence generated during system running which records system status of the time.
a log message e.g.
packetresponder for block blk terminating consists of alog event e.g.
packetresponder for block terminating andlog parameters e.g.
blk .
the former is a constant part in a log message which is the template of a log message designed by developers.
the latter is a variable part which records some system attributes e.g.
ip address and block id .
log events can be extracted from log messages vialog parsing which is the first step of log based anomaly detection and has been widely studied in the literature .
a series of log messages form a log message sequence .
alog sequence is a series of log events that record a specific execution flow which can be obtained by the task id of each log message or some strategies like sliding window.
if a log sequence indicates system anomalies it is an anomalous log sequence otherwise it is a normal log sequence .
iii.
a pproach a. overview in this paper we propose a more practical log based anomaly detection approach called plelog by combining the strengths of unsupervised semi supervised approaches getting rid of time consuming manual labeling and supervised approaches incorporating the knowledge on historical anomalies .
more specifically plelog is a semi supervised approach only knowing the labels of a part of normal logsequences in a training set which are easy to obtain as presented in section i. also plelog uses the idea of pu learning for reference to incorporate the knowledge on historical anomalies where plelog estimates the labels of unlabeled log sequences mixing both normal and anomalous ones in the training set based on known normal log sequences via probabilistic estimation.
finally the estimated labeled data will be used for subsequent supervised anomaly detection model building.
figure shows the overview of plelog.
plelog consists of three stages i.e.
semantic embedding probabilistic label estimation and anomaly detection model building.
as presented in section i log evolution is frequent in practice and thus it is required for a practical log based anomaly detection approach to be immune to such unstable log data.
to achieve this goal following the existing work plelog incorporates the semantic information of each log event during vector representation which is called semantic embedding and is the first stage in plelog section iii b .
the second stage in plelog is probabilistic label estimation section iii c which is the core of plelog.
it solves a major challenge in our problem i.e.
how to estimate the labels of unlabeled log sequences in a training set based on known normal log sequences through clustering.
further it solves a followup challenge i.e.
how to reduce the influence of noise incurred by clustering through measuring the probability that an unlabeled log sequence belongs to each label as the estimated label of the log sequence instead of assigning a deterministic label.
based on the training set including known normal log sequences and estimated normal and anomalous log sequences plelog builds an anomaly detection model through supervised deep learning which is the third stage in plelog section iii d .
besides the prediction effectiveness it is required for a practical log based anomaly detection approach to predict an incoming log sequence efficiently since log based anomaly detection is expected to monitor a system in real time.
therefore we design an attention based gru neural network for anomaly detection model building by considering both prediction effectiveness and efficiency.
in particular our attention based gru neural network could further reduce the influence of noise incurred by clustering by well handling temporal relations in log sequences.
finally we present the usage of plelog in section iii e. b. semantic embedding instead of the widely used one hot representation plelog transforms a log event to a vector called semantic vector by extracting its semantic information in order to deal with unstable log data well.
it consists of three steps log parsing word embedding and tf idf based aggregation.
log parsing as shown in figure raw log messages are unstructured data and contain variable log parameters which could hinder automated log analysis .
therefore following the practice of log based anomaly detection plelog extracts log events from log messages via log parsing since log events are structured which facilitates 1450normal log seq unlabeled log seq.anomalouslog seq.
normallog seq.
alertnormal log vectorstrainsemanticembeddinganomaly detectionmodel buildingprobabilisticlabel estimation anomalouslog seq.
normallog seq.
log parsing 2wordembedding3 tf idfaggregation1 log seq.clustering2 label prob.measurementtest upcominglog seq.unlabeled log vectorsunlabeled log vectorspredictattention based gru networktrain datatest data attention based masknon linear layer gru fig.
overview of plelog log analysis.
in particular plelog adopts the state of theart method drain for log parsing since it has been demonstrated to be very effective and efficient in the existing study .
word embedding to extract semantic information of log events plelog regards each log event as a sentence in natural language.
since log events are designed by developers to record the running status of a system most tokens in a log event are english words which have their own semantics.
also there are non character tokens e.g.
delimiters operators punctuation marks and number digits and composite tokens that are concatenations of words e.g.
nullpointerexception due to the programming practice.
therefore following the existing work plelog first pre processes log events by removing non character tokens and stop words and splitting composite tokens into individual words according to camel case .
then plelog adopts the pre trained word vectors based on common crawl corpus using the fasttext algorithm which can effectively capture the intrinsic relationship among words in natural language to extract the semantic information from each word in a processed log event.
that is plelog adopts the pre trained word2vec model to transform each word in a processed log event to a d dimension vector denoted as v wheredis in fasttext word vectors.
tf idf based aggregation after transforming a word to ad dimension vector via word embedding plelog further transforms a log event to a semantic vector by aggregating all word vectors in the log event.
here plelog adopts tfidf a widely used method in information retrieval for aggregation with considering the importance of each word which can be effectively measured by tf idf .tf term frequency measures how frequently a word woccurs in a log event which is calculated by tf w w nwhere wis the number of the occurrence of win the log event and nis the total number of words in the log event.
idf inverse document frequency measures how common or rare a word wis in all log events which is calculated by idf w log l lw where lis the total number of log events and lwis the number of log events containing w. the weight denoted as !
ofa word can be calculated by tf idf.
finally the semantic vector denoted v of a log event can be produced by summing up all word vectors in the log event with tf idf weights as v npn i !i vi.
in this way plelog represents log events as semantic vectors which effectively incorporates their semantic information facilitating to identify semantically similar log events and distinguish different log events.
therefore plelog is able to be immune to unstable log data to some degree.
c. probabilistic label estimation after semantic embedding plelog further estimates the labels of unlabeled log sequences in the training set based on known normal log sequences by using the idea of pu learning for reference so that the strength of supervised approaches can be incorporated.
intuitively log sequences with similar semantics are more likely to share the same label.
with this intuition plelog first adopts advanced clustering to identify the log sequences with similar semantics to the same group.
however it is hard to produce perfect clustering results and thus instead of assigning a deterministic label plelog assigns a probabilistic label for each unlabeled log sequence by measuring the probability that an unlabeled log sequence belongs to each label in order to reduce the influence of noise incurred by clustering.
in the following we present log sequence clustering in section iii c1 and label probability measurement in section iii c2 in details.
log sequence clustering as a log sequence contains a series of semantic vectors to enable the clustering of log sequences plelog produces a vector for each log sequence called log sequence semantic vector by aggregating all semantic vectors in the log sequence via summation.
here plelog adopts the hdbscan algorithm hierarchical density based spatial clustering of application with noise to cluster all log sequences including known normal log sequences and unlabeled log sequences in the training set to different groups each of which is more likely to contain the log sequences with similar semantics.
the reasons why choosing hdbscan are threefold it is hard to know .
.
.
.
mutual reachability distance a minimum spanning tree .
.
.
.
.
.5distance log number of points b connected component hierarchy 250 value number of points c condensed tree fig.
visualization of main stages in hdbscan clustering by sampling log sequences in the hdfs dataset the number of clusters in advance and thus the clustering algorithms required to pre define the number of clusters cannot be applicable e.g.
the widely used k means algorithm .
hdbscan is a density based clustering algorithm which groups the data points that are closely packed together and thus it does not need to pre define the number of clusters.
hdbscan has been demonstrated to be effective and efficient and has been widely used in many domains .
hdbscan has a small number of parameters and is robust to parameter selection and thus it is more easy to use in practice.
to clearly illustrate the clustering process for log sequences via hdbscan we made visualization for main steps by sampling log sequences in the hdfs dataset which is shown in figure .
first it builds a weighted graph in which vertices are the data points log sequence semantic vectors and weights of edges between vertices are the corresponding mutual reachability distances .
second it builds the minimum spanning tree of the weighted graph via prim s algorithm .
according to the minimum spanning tree a hierarchy of connected components is constructed by ranking tree edges in the ascending order of distances and iterating via creating a new merged cluster for each edge.
then it condenses down the large cluster hierarchy into a smaller tree based on the defined minimum cluster size a parameter in hdbscan .
finally it extracts the stable clusters from the condensed tree by calculating the stability score of each cluster.
in this way log sequences in the training set are divided into several groups in order to identify the log sequences with similar semantics.
in particular with the number of data points and the vector dimension increasing hdbscan could become less efficient and thus it tends to be used together with dimension reduction methods in the literature .
therefore plelog also conducts dimension reduction before clustering in order to make the clustering process efficient.
here plelog adopts the fastica algorithm which aims to find independent components and facilitate to find underlying factors by maximizing negative entropy for dimension reduction following the existing work using hdbscan .
label probability measurement through clustering log sequences with similar semantics are divided to the same group indicating they share the same label.
thus according to whether there exist known normal log sequences in a groupor not plelog preliminarily estimates the labels of the log sequences in the group.
that is if a group contains known normal log sequences the log sequences in the group are more likely to have the same label with them i.e.
normal log sequences otherwise they are more likely to be anomalous.
however it is hard to produce perfect clustering results thus if we directly label each unlabeled log sequence as normal or anomalous based on preliminary estimation the noise incurred by clustering could undermine the effectiveness of plelog.
instead of assigning a deterministic label representing anomalous representing normal plelog assigns a probabilistic label to each unlabeled log sequence by measuring the probability that the unlabeled log sequence belongs to each label to reduce the influence of noise.
in this way even though plelog has preliminarily estimated a wrong label for a noisy log sequence in a group plelog can make the preliminarily estimated label less confident through such probabilistic labeling.
therefore during the model building process with our attention based gru neural network section iii d the importance of noisy log sequences can be effectively reduced so that the influence of noise can be also effectively reduced.
more specifically plelog measures the probability that an unlabeled log sequence belongs to each label based on clustering results.
with hdbscan each log sequence in a group is assigned a score which reflects the uncertainty that the log sequence belongs to the group.
the score ranges from to the closer to the score is the more confidence can be gained to cluster a log sequence to the corresponding group.
even though the uncertainty may be large for a log sequence in a group it is still more confident for dividing it to this group compared with dividing it to other groups and thus making a label probabilistic is required to meet the major premise of the preliminarily estimated label.
based on this major premise and the scores produced by hdbscan clustering we transform each preliminarily estimated label to a probabilistic label as p anomalous score 2andp normal score .
d. anomaly detection model building based on the set of labeled training data through probabilistic label estimation we design an attention based gru neural network shown in figure to build an effective and efficient anomaly detection model.
gru neural network gru a kind of recurrent neural network rnn has been demonstrated to be effective to 1452handle temporal relations in sequential data .
in our problem a log sequence consists of a series of log events that are continuously produced during system running and closely related within a short period.
therefore we employ gru to build an anomaly detection model in plelog.
a typical gru includes an update gate to decide how much information from a previous state passing to a new state and areset gate to decide how much information from the past should be forgotten.
that is both gates together determine the influence of past log events in a log sequence on the future.
for a log sequence s fe1 e2 e ng whereet t n is thetthlog event and nis the number of log events in s the input of gru at the tthtime step is the semantic vector of etdenoted asvt.
according to the hidden state ht 1at the t thtime step the update gate ztandreset gatertat the tthtime step can be calculated by formula .
zt wzvt uzht rt wrvt urht where is the logistic sigmoid function and wz wr uzand urare network parameters.
then the hidden state htat the tthtime step can be calculated based on ht 1by formula .
ht ztht zt ht ht tanh wvt u rt ht wherewanduare also network parameters and is an element wise multiplication.
in particular before the startup time step i.e.
t no log event comes and thus the hidden state h0is a zero vector.
all network parameters can be learned during training.
attention based mask strategy the output of gru at thetthtime step is mainly decided by the current log event i.e.
vt and the last hidden state ht .
when a log sequence comprises too many log events it could suffer from the long dependency issue which could be harmful to the anomaly detection effectiveness.
moreover the noisy log events in a log sequence can also undermine the anomaly detection effectiveness.
to tackle them plelog further incorporates an attention based mask strategy in our gru neural network.
intuitively a log event that has larger correlation with the anomaly detection result is more important in a log sequence.
that is the larger the correlation between the hidden state hi corresponding to a log event eiat theithtime step and the anomaly detection result the larger the weight of hi indicating that eiis possibly an indicator of anomalies.
in this way important log events can be highlighted by assigning them larger weights while noisy log events can be masked by assigning them smaller weights.
here we assemble the attention vector ha tat thetthtime step by ha t pt i tihi where tiis the learned weight of hiat thetthtime step and tiis calculated by softmax function .
finally plelog takes ha nas the input of gru and leverages a non linear layer formula to predict the final result for a log sequence which employs tanh as the activation function.wnis a weight matrix learned during training.
p normal anomalous tanh wnha n e. usage of plelog after building an anomaly detection model through plelog the model can be deployed to monitor the system in real time.
when there is an incoming log message sequence plelog first conducts log parsing and semantic embedding to obtain semantic vectors and then the model can predict whether it is anomalous or normal.
if an anomaly is detected an alert would be timely produced and sent to the operators in order to start the process of mitigation and diagnosis earlier.
iv.
e valuation in our study we address the following research questions rq1 how does plelog perform in terms of effectiveness?
rq2 does each main component contribute to plelog?
rq3 how does different plelog configurations impact the effectiveness of plelog?
rq4 how does plelog perform in terms of efficiency?
a. datasets in our study we used two most widely used public datasets to evaluate plelog i.e.
hdfs hadoop distributed file system dataset and bgl blue gene l supercomputer dataset which have been widely used in the existing work on log based anomaly detection .
for ease of presentation we call them hdfs andbgl directly.
hdfs contains log messages produced through running hadoop based mapreduce jobs on more than amazon s ec2 nodes for .
hours.
according to block id in its log messages log sequences can be directly extracted.
in total there are log sequences in hdfs among which about .
log sequences indicate system anomalies that were manually labeled by hadoop domain experts.
bgl contains log messages that were produced by the blue gene l supercomputer which consists of 128k processors and was deployed at the lawrence livermore national laboratory with a time span of months.
each log message in bgl was manually labeled to be either anomalous or normal by bgl domain experts.
in total log messages are anomalous.
unlike hdfs bgl does not have obvious tags such as block idto help extract log sequences and thus similar to the existing work we extracted log sequences through the strategy of splitting windows with the size of .
in particular during log sequence extraction we ranked all log messages according to their generation timestamps and considered the node on which each log message was produced.
if a log sequence contains at least one anomalous log message it is regarded as an anomalous log sequence.
finally we obtained normal log sequences and anomalous log sequences in bgl.
similar to the existing work we also split each dataset into a training set a validation set for parameter tuning and a test set with the ratio of to evaluate the performance of a log based anomaly detection approach.
however the splitting methods used in the existing work shuffle all log sequences before splitting 1453which can avoid the occurrence of unstable log data.
different from them we split a dataset in chronological order of log sequences so as to guarantee that all log sequences in the training set are produced before the log sequences in the test set which is much closer to the practical scenario.
also the influence of dataset splitting methods has been investigated in the area of predicting software development practices and it suggests to split a dataset in chronological order to avoid data leakage.
in particular with our splitting method unstable log data indeed occur especially on bgl since its time span of log data is longer.
to evaluate our semisupervised approach plelog we sampled of training data as known normal log sequences and the remaining log sequences in the training data as unlabeled log sequences to simulate the semi supervised scenario.
b. compared approaches existing log based anomaly detection approaches as plelog is a semi supervised approach we compared plelog with the state of the art semi supervised and unsupervised logbased anomaly detection approaches deeplog treats a log sequence as a natural language sequence and adopts a deep neural network lstm to learn normal log patterns from normal log sequences.
during anomaly detection deeplog predicts the next log event and if the real log event is not included in the top prediction results an anomaly is regarded to be detected.
loganomaly is similar to deeplog to learn normal log patterns from normal log sequences.
the main difference is that the former represents a log sequence by considering semantic information instead of one hot representation used in the latter in order to improve prediction effectiveness.
moreover loganomaly also counts log events during representation in order to detect the anomalies reflected by anomalous log event numbers.
logcluster first represents a log sequence by considering the weights of log events and then adopts the agglomerative hierarchical clustering algorithm to cluster log sequences.
in each cluster it selects its centroid as the representative log sequence.
for an incoming log sequence it identifies whether it is normal or anomalous by measuring the distances between the incoming log sequence and all the representative log sequences.
pca principal component analysis is a popular algorithm for dimension reduction .
for its application on logbased anomaly detection it first represents a log sequence by counting log events and then projects log sequences into two spaces i.e.
normal space and anomalous space by considering the first kprincipal components and remaining principal components.
then for an incoming log sequence it can be identified to be normal or anomalous according to the space the log sequence belonging to after projection.
besides plelog tries to incorporate the strength of supervised approaches via probabilistic label estimation and thus it is interesting to investigate how much the gap between plelog and the state of the art supervised log basedanomaly detection approach i.e.
logrobust .
thus we also compared plelog with logrobust which builds a classifier via lstm based on the manually labeled training set after representing a log sequence by considering semantic information.
variants of plelog in rq2 we aim to investigate the contributions of three main components to plelog including making label estimation probabilistic incorporating an attention mechanism and reducing semantic vector dimension.
thus we constructed three variants of plelog accordingly plelog nop removes the component of making label estimation probabilistic from plelog.
that is it directly assigns a deterministic label to each unlabeled log sequence based on the clustered groups.
plelog noa removes the component of incorporating an attention mechanism from plelog.
that is it uses a gru neural network without the attention mechanism to build an anomaly detection model based on the training set with our estimated probabilistic labels.
plelog norremoves the component of reducing semanticvector dimension from plelog.
that is it directly uses the original semantic vectors for the subsequent steps after semantic embedding.
different plelog configurations to answer rq3 we investigated the impact of three main parameters in plelog including the number of gru layers the size of gru hidden states and the number of components in fastica.
regarding the number of gru layers we studied .
regarding the size of gru hidden states we studied .
regarding the number of components in fastica we studied .
c. implementations and environments we implemented plelog based on python .
.
and pytorch .
.
.
we adopted the implementations of hdbscan and fastica provided by hdbscan .
.
and sklearn respectively.
for parameters in plelog we determined them through grid search based on validation sets.
more specifically we set the size of gru hidden states to be the number of gru layers to be learning rate to be .
the number of components in fastica to be the number of epochs to be .
in particular we investigated the impact of parameter settings on plelog in rq3.
also we setmin cluster sizeto be and min samples to be in hdbscan.
as demonstrated by the existing work hdbscan is robust to parameter selection.
regarding the existing log based anomaly detection approaches we adopts their public implementations .
in particular we determined their parameters by first reproducing the results in their corresponding studies and further conducting grid search based on validation sets in order to obtain the best parameter settings for them.
we conducted all the experiments on a linux server with intel r xeon r silver .20ghz cpu 128gb memory rtx2080ti with 11gb gpu memory and operating system 1454version is ubuntu .
.
in particular our tool and experimental data are available in our project homepage1.
d. measurements log based anomaly detection is actually a binary classification problem and thus following the existing work we adopted precision recall and f1 score to measure the effectiveness of log based anomaly detection approaches.
precision is compuited bytp tp fpwhile recall is computed bytp tp fn where tp fp and fnrefer to the number of true positives a log sequence is predicted to beanomalous and its ground truth is also anomalous false positives a log sequence is predicted to be anomalous but its ground truth is normal and false negatives a log sequence is predicted to be normal but its ground truth is anomalous respectively.
f1 score considers both precision and recall which is calculated by2 precision recall precision recall.
besides for logrobust and plelog the effectiveness of their built classifiers could be affected by the threshold used to distinguish the two classes and thus it is also necessary to measure their effectiveness under different thresholds.
therefore we drew the receiver operating characteristic roc curve with thresholds from to with the step of .
and then calculated its area under curve auc value.
the larger the auc value is the better effectiveness the classifier has.
besides a log based anomaly detection approach is deployed to monitor systems in real time and thus the time spent on online predicting a log sequence is also important.
therefore we recorded the time spent on online predicting a log sequence to measure the efficiency of a log based anomaly detection approach.
we also recorded the time spent on offline building an anomaly detection model based on a training set for each approach.
we call the former prediction time and the latter training time .
e. results and analysis effectiveness of plelog table i shows the effectiveness comparison results among all the studied approaches in terms of precision recall and f1 score.
we found that plelog is able to perform well on both hdfs and bgl in terms of all the three metrics.
for example f1 score of plelog is .
and .
on hdfs and bgl respectively.
the results demonstrate the effectiveness of plelog.
comparison with existing semi supervised and unsupervised approaches .
from table i plelog largely outperforms all the compared semi supervised and unsupervised approaches on both hdfs and bgl in terms of all the three metrics except logcluster in precision .
even though logcluster achieves better precision its recall and f1 score are significantly worse than those of plelog.
in particular for anomaly detection recall is a much more important metric than precision since missing to detect anomalies could lead to huge economic loss and other serious consequences.
in terms of f1 score the improvements of plelog over all the i experiment results of studied approaches on hdfs and bgl dataset method precision recall f1 score hdfsdeeplog .
.
.
loganomaly .
.
.
logclustering .
.
.
pca .
.
.
plelog .
.
.
logrobust .
.
.
bgldeeplog .
.
.
loganomaly .
.
.
logclustering .
.
.
pca .
.
.
plelog .
.
.
logrobust .
.
.
compared semi supervised and unsupervised approaches range from .
to .
on hdfs and from .
to .
on bgl.
besides we found that almost all the compared semisupervised and unsupervised approaches perform worse on bgl than hdfs.
for example f1 score of the state of the art semi supervised approach deeplog largely drops from .
on hdfs to .
on bgl.
this is because there are more unstable data in bgl due to its longer time span compared with hdfs.
more specifically about .
log events in the test set of bgl do not appear in its training set while there is no such new log events on hdfs.
as the state ofthe art semi supervised approaches deeplog and loganomaly aim to predict the next log event in a log sequence they can only predict the log events appearing in their training sets and are easy to treat unseen log events as anomalies leading to bad effectiveness.
regarding logcluster since it adopts one hot representation for log sequences and then clusters vectors when coming across unseen log events it directly ignores them during representation.
moreover the log sequences containing unseen log events are usually normal in bgl and thus its effectiveness is better than deeplog and loganomaly.
faced with such unstable log data plelog still performs well due to its semantic embedding and the capability of incorporating the knowledge on historical anomalies.
please note that the effectiveness of these compared semi supervised and unsupervised approaches is worse than that reported in their corresponding studies since our data splitting method avoids data leakage and incurs practical unstable log data.
comparison with the state of the art supervised approach .
even though plelog is a semi supervised approach it tries to incorporate the strength of supervised approaches through probabilistic label estimation and thus it is interesting to explore how much gap between plelog and the state of theart supervised approach logrobust.
from table i although plelog performs worse than logrobust their gap in terms of various metrics is actually quite small indicating the success of incorporating the strength of supervised approaches via probabilistic label estimation.
moreover as logrobust depends on a large amount of manually labeled training data 1455plelog has greater usability in practice.
as presented in section iv d auc is also an important metric for such classifiers of logrobust and plelog and thus we further compared them in terms of this metric.
more specifically auc of plelog is .
on both hdfs and bgl while that of logrobust is .
on both hdfs and bgl.
we found that auc of plelog is very high even competitive with that of logrobust further demonstrating its stable effectiveness.
rq2 contribution of main components in plelog table ii shows the effectiveness comparison results between plelog and its variants.
from this table plelog outperforms plelog nop and plelog noa in terms of effectiveness confirming the contribution of making label estimation probabilistic and incorporating an attention mechanism to the overall effectiveness of plelog.
more specifically the improvement of plelog over plelog nop in terms of f1score is .
on hdfs and .
on bgl while that of plelog over plelog noa is .
on hdfs and .
on bgl demonstrating that the component of making label estimation probabilistic contributes more.
also we found that plelog nopperforms much worse on hdfs .
than bgl .
in terms of f1 score.
this is because the clustering effectiveness on hdfs is worse than that on bgl.
even so the effectiveness of plelog is similar on both hdfs and bgl indicating that our probabilistic label estimation does effectively reduce the influence of noise incurred by clustering no matter how much noise is incurred .
besides even though the clustering effectiveness in plelog may be not very high e.g.
.
f1 score on hdfs its final classification effectiveness can be excellent e.g.
.
f1 score on hdfs further confirming the importance of probabilistic label estimation and also our attention based gru network.
from table ii plelog cannot guarantee to outperform plelog norstably in terms of effectiveness.
for example with dimension reduction f1 score of plelog on bgl is largely improved from .
to .
but that on hdfs is slightly decreased from .
to .
.
on average this component is still helpful to the overall effectiveness of plelog since hdbscan has been demonstrated to be not very effective for high dimensional data .
in particular through dimension reduction the clustering and training efficiency of plelog can be largely improved which is its another important contribution to plelog.
in summary both making label estimation probabilistic and incorporating an attention mechanism contribute to the overall effectiveness of plelog and the component of reducing semantic vector dimension contributes to both effectiveness and efficiency of plelog.
rq3 impact of different plelog configurations figure shows the effectiveness of different plelog configurations introduced in section iv b3 in terms of auc.
we found that although there exists small perturbation under different configurations the overall effectiveness is stable i.e.
the deviation of auc is smaller than .
across all the configurations indicating plelog is insensitive to different configurations and thus robust in practice.table ii experiment results between variants of plelog dataset method precision recall f1 score hdfsplelog .
.
.
plelog nop .
.
.
plelog noa .
.
.
plelog nor .
.
.
bglplelog .
.
.
plelog nop .
.
.
plelog noa .
.
.
plelog nor .
.
.
table iii time cost of studied approaches methodhdfs bgl training testing training testing plelog 43m 42s 24m 10s loganomaly 4h 40m 47m 4h 20m 39m logrobust 1h 20m 49s 30m 15s deeplog 1h 50m 20m 44m 7m logcluster 19m 23s 41s 40s pca 18m 1s 8s 1s rq4 efficiency of plelog table iii presents the training time andprediction time of plelog on hdfs and bgl respectively.
in general all the approaches are efficient with very short prediction time e.g.
taking at most 17ms to predict one log sequence in hdfs .
in particular our gru based classifier is more efficient than the lstmbased classifier of logrobust demonstrating that our gru network is more helpful to support timely anomaly detection than the widely used lstm network.
in terms of training time plelog performs better than the state of the art semisupervised approaches i.e.
deeplog and loganomaly and the state of the art supervised approach i.e.
logrobust but performs worse than logcluster and pca which only utilize simple clustering or projection.
since the training process is offline the training time of plelog i.e.
less than one hour is still acceptable.
to sum up plelog is efficient with very short prediction time and acceptable training time.
v. p ractical evaluation we have successfully applied plelog to two large scale real world distributed online systems from two different organizations i.e.
the network center of our university and one influential motor corporation throughout the world we hide its name due to the company policy .
here we call them a andbrespectively.
since both systems are large scale and complex and in the meanwhile the number of operators is limited it is hard for them to manually analyze and label massive logs.
plelog is designed to conduct automated logbased anomaly detection without manually labeling anomalous logs admirably satisfying their demands.
indeed they largely appreciated the performance of plelog on their systems.
here we report the practical evaluation results based on two datasets from the two real world systems to show the practical effectiveness of plelog.
for the dataset athere are .
.
.0auc valuehdfs bgl a number of ica components .
.
.0auc valuehdfs bgl b number of gru layers .
.
.0auc valuehdfs bgl c size of gru hidden states fig.
auc value of various plelog configurations.
table iv practical evaluation results of studied approaches on real world datasets dataset approach precision recall f1 score adeeplog .
.
.
loganomaly .
.
.
logcluster .
.
.
pca .
.
.
plelog .
.
.
bdeeplog .
.
.
loganomaly .
.
.
logcluster .
.
.
pca .
.
.
plelog .
.
.
log messages among which are anomalous while for the datasetbthere are log messages among which are anomalous.
the number of log events are and foraandbrespectively.
table iv shows the effectiveness of all the studied semisupervised and unsupervised approaches on the two real world datasets.
from this table plelog performs stably well on both datasets in terms of precision recall and f1 score.
for example f1 score of plelog is .
and .
for aand brespectively and in the meanwhile its recall is .
and .
.
the results demonstrate that plelog hardly misses to detect anomalies with very few false positives in the two real world systems.
besides plelog still outperforms all the compared semi supervised and unsupervised approaches on both datasets.
for example the improvements of plelog over all the compared approaches range from .
to .
on aand from .
to .
on bin terms of f1 score.
by combining the results in tables i and iv all the compared approaches perform sensitively on different datasets i.e.
the standard deviation of these compared approaches across all the four datasets ranges from .
to .
in terms of f1 score while plelog performs very stably regardless of the used datasets i.e.
its standard deviation across all the four datasets is only .
in terms of f1 score .
the results further demonstrate the generality and practicability of plelog.
through an informal interview developers indeed confirmed the usefulness of plelog in practice.
vi.
d iscussion a. lessons learned logs evolution .
in practice logs evolve frequently especially for the companies adopting continuous delivery or deployment .
for example as observed in the existingstudy in their studied projects there are log statements evolved throughout their lifetime.
also there are hundreds of new log statements that are added to the source code every month for four systems in google .
log evolution could bring many new log events and log sequences.
therefore handling such unstable log data is a capability essential to an log based anomaly detection approach in practice and plelog indeed possesses it as demonstrated in our study.
anomaly interpretability .
in practice accurate anomaly detection is helpful to mitigate and diagnose anomalies earlier which could reduce the influence of anomalies.
moreover recent studies have presented that making software analytics models interpretable to software practitioners is as important as achieving accurate prediction .
thus to further facilitate the mitigation and diagnosis of anomalies operators expect to receive interpretable anomaly detection results.
otherwise they still have to spend much time to identify root causes of anomalies.
in particular plelog besides effectively and efficiently detecting anomalies also provides interpretable results to some extent through its attention mechanism.
more specifically plelog provides the importance of each log event in an anomalous log sequence which makes operators more clearly identify relevant log events to the detected anomaly so as to provide hints for subsequent root cause diagnosis.
in the future incorporating more advanced interpretability analysis in plelog is necessary.
b. extensions of plelog we plan to further improve plelog from two aspects in the future.
first the anomaly detection effectiveness of plelog relies on the effectiveness of our clustering to some degree.
however our used hdbscan could be costly for highdimensional data and in the meanwhile dimension reduction could lose accuracy to some degree and thus we plan to incorporate a more effective and efficient clustering method for high dimensional data to improve plelog.
second as demonstrated in our study plelog still has a little gap with the state of the art supervised approach i.e.
logrobust in terms of effectiveness.
in the future we can add a feedback process after operators mitigate diagnose an anomaly in order to incrementally update our model with the newly confirmed anomalous log data by operators.
in this way our model could perform stably and well over time to some degree.
c. threats to validity the internal threat to validity mainly lies in the implementations of plelog and compared approaches.
to reduce this 1457threat we implemented plelog based on popular libraries presented in section iv c and two authors have carefully checked the source code.
regarding compared approaches we adopted their open source implementations directly.
the external threat to validity mainly lies in the used datasets.
in our study we used two widely used public datasets i.e.
hdfs and bgl following the existing work .
to further investigate the generality and practicability of plelog we applied plelog to two industrial datasets from the network center of our university and an influential motor corporations respectively.
in the future we will evaluate plelog on more datasets.
vii.
r elated work supervised log based anomaly detection .
with the help oflabeled training data supervised approaches perform relatively well for detecting anomalies .
for example liang et al.
explored four classifiers i.e.
ripper a rule based classifier support vector machines a traditional nearest neighbor and a customized nearest neighbor for predicting failure events via logs.
bodik et al.
used a logistic regression model to identify previously seen performance crises in a datacenter.
to further improve the performance of log based anomaly detection advanced deep learning techniques have been incorporated .
for example besides the state of the art supervised approach logrobust introduced in section iv b1 vinayakumar et al.
proposed a stacked lstm model for accurate log based anomaly detection.
different from them our work proposes a semi supervised log based anomaly detection approach plelog only requiring to know the labels of a set of easy to obtain normal log sequences to get rid of time consuming manual labeling which is the practical limitation of supervised approaches.
in particular the effectiveness of plelog is even close to that of the state of the art supervised approach logrobust as demonstrated in our study.
unsupervised and semi supervised log based anomaly detection .
due to less depending on labeled data for model training unsupervised and semi supervised approaches tend to be more practical .
beside deeplog loganomaly logcluster and pca that have been introduced in section iv b1 lou et al.
proposed invariant mining im that detects anomalies by checking the violations against a set of execution flow invariants mined from previous log sequences.
he et al.
proposed log3c that utilizes the hierarchical agglomerative clustering algorithm to group similar log sequences and identifies the correlations between logs and system kpis key performance indicators .
our proposed approach plelog is also a semi supervised approach only knowing the labels of a part of normal log sequences that are easy to obtain as presented in section i. different from them plelog further incorporates the strength of supervised approaches i.e.
learning the knowledge on historical anomalies via probabilistic label estimation insteadof manual labeling in order to largely improve the effectiveness of unsupervised and semi supervised approaches.
viii.
c onclusion over the years many log based anomaly detection approaches have been proposed but these existing approaches still suffer from practical issues due to either relying on a large amount of manually labeled training data supervised approaches or unsatisfactory performance without learning the knowledge on historical anomalies unsupervised and semisupervised approaches .
in this paper we proposed a practical semi supervised without time consuming manual labeling approach plelog which learns the knowledge on historical anomalies via probabilistic label estimation so that the strength of supervised approaches can be incorporated.
also plelog can stay immune to unstable log data via semantic embedding and detect anomalies efficiently and effectively with an attention based gru neural network.
our experimental results on two most widely used public datasets demonstrate the effectiveness of plelog.
in particular plelog has been applied to two real world systems from our university and an influential motor corporation further demonstrating its practicability.
acknowledgment this work has been partially supported by the national natural science foundation of china and intelligent manufacturing special fund of tianjin and innovation research project of tianjin university under grant no.
2020xzc .