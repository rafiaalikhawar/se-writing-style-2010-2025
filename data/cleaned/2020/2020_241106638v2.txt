model editing for llms4code how far are we?
xiaopeng li shangwen wang shasha lib jun mab jie yu xiaodong liu jing wang bin ji weimin zhang college of computer science national university of defense technology changsha china xiaopengli wangshangwen13 shashali majun yj liuxiaodong wangjing jibin nudt.edu.cn wmzhang104 .com abstract large language models for code llms4code have been found to exhibit outstanding performance in the software engineering domain especially the remarkable performance in coding tasks.
however even the most advanced llms4code can inevitably contain incorrect or outdated code knowledge.
due to the high cost of training llms4code it is impractical to re train the models for fixing these problematic code knowledge.
model editing is a new technical field for effectively and efficiently correcting erroneous knowledge in llms where various model editing techniques and benchmarks have been proposed recently.
despite that a comprehensive study that thoroughly compares and analyzes the performance of the state of the art model editing techniques for adapting the knowledge within llms4code across various code related tasks is notably absent.
to bridge this gap we perform the first systematic study on applying state of the art model editing approaches to repair the inaccuracy of llms4code.
to that end we introduce a benchmark named clmeeval which consists of two datasets i.e.
conala edit cnle with 21k code generation samples and codesearchnet edit csne with 16k code summarization samples.
with the help of clmeeval we evaluate six advanced model editing techniques on three llms4code codellama 7b codeqwen1.
7b and stable code 3b .
our findings include that the external memorization based grace approach achieves the best knowledge editing effectiveness and specificity the editing does not influence untargeted knowledge while generalization whether the editing can generalize to other semantically identical inputs is a universal challenge for existing techniques.
furthermore building on in depth case analysis we introduce an enhanced version of grace called a grace which incorporates contrastive learning to better capture the semantics of the inputs.
results demonstrate that a grace notably enhances generalization while maintaining similar levels of effectiveness and specificity compared to the vanilla grace.
index terms llms4code model editing code generation code summarization i. i ntroduction large language models llms have demonstrated their powerful understanding and generating capabilities and have been applied to areas such as autonomous agents medicine and recommendation system .
llms for code llms4code trained on massive code related datasets also show remarkable performance in coding tasks within these authors contributed equally to this work.
bcorresponding authors.software engineering including code generation and code comment generation .
however even the most advanced llms4code can contain outdated and incorrect code knowledge due to the following reasons .
on one hand the training data for llms4code is limited to a certain period which implies that llms4code cannot learn about the latest software package characteristics from this data as software is continuously changing .
on the other hand the massive training data inevitably contains some noise which ultimately leads llms4code to learn some incorrect code knowledge.
if the aforementioned concerns are not promptly addressed llms4code will continue to produce bugs or vulnerabilities in production environments .
a natural way to fix these issues is retraining but this killing a fly with a cannon approach not only consumes a considerable amount of computational resources but also takes a lot of time.
recently researchers have explored model editing to repair llms outdated and incorrect knowledge whose aim is to efficiently and effectively update targeted knowledge without affecting the other non targeted knowledge of llms .
existing model editing approaches can generally be categorized into three classes which are based on external memorization global optimization and local modification respectively .
external memorization adds an extra module to the original model to store updated knowledge global optimization directly or indirectly updates model weights using fine tuning gradients local modification first identifies critical modules in the model where the target knowledge is stored then uses specific algorithms to update the weights in these key modules.
state of the art model editing approaches have achieved remarkable success for updating the knowledge of llms .
given that researchers have also explored to utilize model editing to fix errors produced during the code generation process .
despite that this initial study has certain limitations due to the following reasons.
first tasks in the software engineering domain like code generation and code summarization often involve sequence generations but the proposed approach by gu et al.
focuses on a single token at each time which is hard to apply to sequence generation and thus misaligns with the application scenarios.
second this existing study mainly focuses on assessing the ability of thearxiv .06638v2 dec 2024edited model to generate correct contents while other essential properties of llms such as the fluency i.e.
measuring the extent to which the edited model can generate natural and non repetitive contents are ignored.
third the existing study focuses on a target model with a maximum of 350m parameters a scale that is relatively restricted compared to the typical size of general llms4code models which commonly comprise several billion parameters .
consequently the literature has limited understanding regarding the strengths and weaknesses of existing model editing techniques within the context of llms4code and it remains an open question as to how effectively these techniques can update the code knowledge embedded in llms4code.
there is thus an urge need for a comprehensive empirical study comparing and analyzing the performance of all the state of the art model editing techniques on llms4code.
such a study is necessary and essential as it can provide answers to fundamental questions such as which types of approaches are the most proficient.
this insight can serve as a guideline for researchers in devising more proficient techniques in the future.
to bridge this gap we perform the first systematic study on applying state of the art model editing approaches to repair the inaccuracy of llms4code.
to that end we first build an evaluation benchmark code large language models editing evaluation clmeeval which consists of two datasets conalaedit cnle and codesearchnet edit csne corresponding to the editing of code knowledge in the context of two widely studied software engineering tasks a natural language to programming language nl2pl code generation task and a programming language to natural language pl2nl code summarization task.
drawing from this benchmark we employ model editing techniques to rectify the inaccuracies produced by llms4code mirroring real world scenarios where updates to code knowledge within a model are necessary.
this could involve situations like changes in required apis for completing specific coding tasks or shifts in the primary functionality of a method due to code changes.
following the common practice in the model editing domain our study evaluates the approaches from four dimensions effectiveness the success rate on editing instances generalization the success rate on tests that are semantically identical to the editing instances specificity the success rate on tests unrelated to the editing instances and fluency the fluency of the contents generated by the model .
we select six state of the art model editing approaches from the three categories mentioned above and three widely used llms4code i.e.
codellama 7b codeqwen .
7b and stablecode 3b as our study subjects.
through an extensive evaluation our study makes the following important findings f1 the external memorization based technique grace can consistently achieve the optimal effectiveness and specificity across different datasets and llms4code.
nonetheless all the existing model editing techniques perform poorly in terms of generalization.
f2 most model editing techniques perform comparativelypoorly on llms4code being far less proficient compared with editing general llms.
f3 model editing techniques are sensitive to the specific tasks with all the editing techniques performing worse in nl2pl editing than in pl2nl editing.
moreover through a case analysis on the best performing editing approach in terms of the effectiveness and specificity i.e.
grace we identify its weakness in the inability to distinguish between the semantics of different inputs.
based on this observation we propose an augmented strategy named asa grace where we introduce an encoder to grace that allows it to better capture the input semantics via contrastive learning and thus improves the generalization of grace.
results show that a grace significantly improves the generalization while achieves similar effectiveness and specificity compared to grace.
for instance on cnle a grace improves the generalization of grace from almost zero to an average of .
in terms of the exact match metric.
to summarise our contributions are as follows benchmark.
we construct clmeeval for llms4code editing which includes a cnle dataset with 21k nl2pl samples and 16k pl2nl samples.
evaluation.
we evaluate six state of the art model editing approaches on three llms4code and find that existing model editing approaches can hardly adapt well to llms4code they usually fail to achieve a good balance among effectiveness generalization and specificity.
strategy.
we propose a refined editing approach agrace and the experiment results show that a grace achieves promising performances on clmeeval.
particularly its generalization is improved by an order of magnitude compared to the vanilla grace.
ii.
t ask definition the code knowledge embedded in llms4code may be outdated or inaccurate.
on one hand software undergoes continuous changes with the addition of new features bug fixes and performance enhancements .
as a result the data used to train llms4code may not accurately capture the most up to date code related knowledge.
an illustrative instance is the challenge llms4code face in selecting the correct library apis during code generation tasks .
on the other hand most llms4code models including codellama are trained on data collected from numerous open source projects.
this inevitably introduces a significant amount of noisy information into the training data which can potentially result in incorrect outputs from llms4code models .
therefore it is necessary to regularly update the code knowledge of the llms4code.
our study investigates the proficiency of model editing approaches on achieving such a target.
in the following we provide the formal definition for this task.
denote an llms4code as fand an editing instance as a tuple m x y c u .
here xandyare the input and label of the editing instance c x ... is the set of inputs where each x cis semantically identical with x u u1 ... is the set of instances unrelated to thetable i selected techniques in this study.
categorization edited layerssingle layer multiple layers global optimization ft l malmen local modification rome memit pmet external memorization grace editing instances where for each u xu f xu u xuis semantically inconsistent with xandf xu is the corresponding output of the llms4code.
the objective of model editing in llms4code is to enable llms4code to accurately identify editing instances and produce corresponding outputs .
in other words the edited model is capable of generating desired outputs for the targeted inputs while not affecting other non targeted inputs or the original capabilities of the model.
formally denote the edited llms4code as fe for each editing instance m feshould satisfy the following conditions fe x y fe x y x c fe xu f xu u u means model editing is capable to generate desired outputs means that the edited model fecan generalize well to semantically identical inputs and means that model editing should not misidentify other non targeted instances thereby avoiding affecting the original knowledge of the model.
iii.
s tudy design a. model editing techniques selection model editing a rapidly developing field with various techniques having been proposed successively can be divided into three categories based on the source of new information and the method of updating the information during the editing process external memorization global optimization and local modification .
for each category we select the most representative and state of the art techniques as our study subjects based on the introduction from a recent survey .
overall our study takes totally six model editing techniques into consideration including one external memorizationbased approach two global optimization based approaches and three local modification based approaches.
table i lists the categorized selected techniques and the following briefly introduce the working mechanism for each of them.
due to the space constraint readers can refer to the original papers for more detailed information.
grace this method is an external memorization approach.
the primary difference between this type of approaches and the other two categories is that these techniques do not directly update model weights but rather add modules to memorize new knowledge.
that is to say this type of method maintains the original weights of the model at the cost of longer inference time.
grace is an advanced approach in this category creating a discrete local codebook of mapping key values in the hidden space of the model and designing adeferral mechanism to enable the codebook to recognize the key value pairs of edited knowledge.
ft l this method is a global optimization approach which fine tunes the model directly on the editing dataset while employing an explicit parameter space l norm constraint to minimize the impact on non targeted data as much as possible.
given that the volume of data for editing is usually small this method fine tunes only a small part of the parameters of the model e.g.
parameters from one layer .
malmen this method employs global optimization.
unlike ft l which directly updates weights based on backward propagation gradients malmen post processes such gradient information through a hyper network to calculate a theoretical offset named as the parameter shift for each parameter and then performs parameter updates.
rome this method falls under the local modification category.
it first uses causal intervention to identify a single key layer where factual knowledge is stored in llms.
rome assumes that the feed forward network ffn of the key layer has a key value mapping capable of recalling factual knowledge models the editing task as a constrained least squares problem and then utilizes the closed form solution of the constrained least squares for rankone model editing i.e.
it only supports editing a single piece of knowledge at a time .
memit this method is also a local modification approach.
similar to rome it views the ffn as a keyvalue mapping and updates the key layers identified by causal intervention.
however unlike rome memit updates the weights of multiple key layers.
pmet the motivation of pmet is that the hidden states of the whole transformer layer used by rome and memit may contain unnecessary or even noisy information unnecessary for the weight update which can lead to imprecise updates and ultimately affect the editing performance.
therefore pmet opts to only focus on the hidden states of the ffn module and exclusively update the weights of ffn for more precise updates.
b. editing subjects we select three popular open source llms4code codellama 7b it is developed by meta which is originated from llama and trained on .
trillion tokens of code data.
codeqwen .
7b it is developed by alibaba which is originated from qwen1.
and trained on trillion tokens of code data.
stablecode 3b it is developed by stability.ai which is pre trained on .
trillion tokens of code data.
these three llms4code are all composed of layer transformer decoders.
c. dataset construction to ease the evaluation of our study we build a benchmark for llms4code editing named clmeeval which considers two types of code related tasks nl2pl the cnle dataset conala mined curated filtering data using llmsfinding nearest neighbor intents for sampled intents using arctic embedgenerating semantically identical rewritten intents using llmsconala edit codesearchnet pythonfiltering data according to codebert generating semantically identical rewritten code using heuristicscodesearchnet editfinding nearest neighbor codes for test set using fine tuned codebertfig.
data construction process of the cnle and csne datasets and pl2nl the csne dataset .
we build the cnle and csne datasets based on the existing conala and codesearchnet datasets respectively.
the conala dataset was crawled from stack overflow with two different versions i.e.
the curated and mined versions containing .88k and 594k pairs of python code snippets and natural language descriptions a.k.a the intents or requirements respectively.
to construct a more comprehensive dataset we rely on the mined version for data construction.
specifically we focus on an officially released version conala mined curated1that further processes the mined version of conala rewriting the intents for the code snippets to improve the clarity.
the codesearchnet dataset was collected from github and includes million code comment pairs across six programming languages.
the overall construction process of the cnle and csne datasets is shown in figure .
conala edit cnle step filtering.
it has been observed that natural language descriptions and code snippets may have semantic discrepancies which may bring noise to our evaluation.
to ensure the reliability of our dataset we decide to further select high quality code and natural language pairs from the dataset.
to that end we use deepseekv2 to evaluate the semantic matches between the code and the description.
specifically we ask deepseek v2 to evaluate the degrees into five levels very irrelevant irrelevant neutral relevant and very relevant.
to ensure the output of deepseek v2 is steady we set the temperature to zero.
to verify the reliability of the scoring of deepseek v2 we manually sample pairs out of the 594k pairs for manual validation confidence level margin of error .
the first two authors label each pair independently categorizing the matching degree into five levels as mentioned above.
after labeling the cohen s kappa coefficient of agreement between the two authors is .
indicating a high degree of consistency.
as for the pairs labeled differently we hold several meetings to resolve the disagreements until an agreement is reached.
after that we calculate the cohen s kappa coefficient of agreement between the manual labelling and the scoring of deepseek v2.
the result is .
also indicating a high degree of consistency and thus the scoring from deepseek v2 is reliable.
the scoring statistics of deepseek v2 are shown ii statistics of the labeling score of deepseek v2.
level very irrelevant irrelevant neutral relevant very relevant numbers in table ii.
in our study we select the high quality samples scored as very relevant for further processing.
we deduplicate the cases whose intents are identical and finally use the left samples for further construction.
step specificity evaluation preparation.
the specificity evaluates to what extent the editing method affects nontargeted knowledge.
that means we need to check if the modification to the model leads to result shifts on a nontargeted input.
in our benchmark we decide to split the dataset into different non overlapped parts i.e.
the targeted data and non targeted data .
the targeted data are used to evaluate the effectiveness of model editing while the non targeted data are used for the specificity assessment.
the rationale of this decision is that with the new advanced lifelong learning or batch editing techniques one might be able to update the model knowledge for a number of inputs simultaneously in the future and our design can safely support the evaluation under such a situation with no overlap data between the targeted and non targeted data.
specifically we randomly sample instances as the targeted data and the remaining ones are treated as non targeted data.
in our evaluation towards specificity we target the most challenging scenario that is if the edited model performs differently on the non targted inputs that are semantically similar to the targeted inputs.
to identify semantically similar intents we employ the advanced embedding model arcticembed to embed the intents of the descriptions.
then for each input description from the targeted data as an editing instance we match it with the five nearest neighbor samples from the non targeted data in terms of their embeddings.
these selected five samples are used to assess the specificity on this editing instance.
specifically we compare if the edited model generates different outputs on such neighbor instances compared with its original output.
step generalization evaluation preparation.
the generalization evaluates to what extent the edited model can success on descriptions that are semantically identical but syntactically different to the editing target.
inspired by the powerful generation ability on natural languages of llms we decide to utilize deepseek v2 to generate such rewritten intents for the 055editing instances.
we enable llms to freely rewrite such intents in our aim to create a diversified dataset rather than relying on pre defined heuristics.
the prompt we used is demonstrated in our online repository.
codesearchnet edit csne step filtering.
we recall that our cnle dataset exclusively contains python code snippets.
to maintain consistency with the code snippet comment pairs in cnle we choose to use the python subset from the codesearchnet.
the code length in the codesearchnet python dataset varies significantly with a number of code snippets exceeding the maximum limit oftable iii dataset statistics of cnle and csne datasets.
dataset cnle csne statistical value mean median num mean median num effectiveness .
.
generalization .
.
specificity .
.
effectiveness generalization target30.
.
specificity target .
.
the state of the art pre trained code models e.g.
tokens for codebert .
considering that we will rely on pretrained code models to capture the semantics of various code snippets in the subsequent steps we have made the decision to filter out code snippets with a token count exceeding .
this filtering step aims to ensure that we obtain precise semantic representations from codebert.
as a result we obtain samples from the training validation set of codesearchnet python and samples from the test set of codesearchnet python and all samples from the test set are used as editing instances.
in other words cases from the test set are used as the targeted data while cases from the training validation set are used as the non targeted data.
step specificity evaluation preparation.
for each sample in the test set we match it with five nearest samples from the training validation set to evaluate specificity.
specifically we use codebert which is fine tuned on the code search task with the codesearchnet python dataset to embed all code snippets in the training validation and test sets and then select five samples with the closest cosine similarity from the training validation set as neighbor samples.
step generalization evaluation preparation.
the input of code summarization task is a code snippet.
therefore to evaluate the generalization we need to create semantic identical but syntactic different code snippets.
a natural way is to utilize llms but the existing study has shown that llms cannot achieve a perfect accuracy when revising the code .
on the contrary we choose to use existing program transformation techniques to ensure the correctness of the generated code.
specifically we reproduce the alert approach where we first identify the variables in the code and replace the variables by the masked prediction from codebert.
we generate a variant for each case from the test set and use these code to evaluate the generalization.
dataset statistics the statistics of the two datasets are shown in table iii.
lines beginning with effectiveness generalization and specificity demonstrate the statistical information of the inputs requirements for the cnle dataset and code snippets for the csne dataset to evaluate the respective capacity of model editing techniques.
while the rest two lines demonstrate the statistical information of the oracle outputs that correspond to different inputs.
note that the oracle outputs for effectiveness and generalization are identical and they are thus merged in the table.
furthermore the oracle output in terms of specificity is the original output of the specific llms4code being considered which can vary when different llms4code models are used.
therefore thetotal number in the last line is calculated as the number of considered llms4code the number of instances to evaluate specificity e.g.
and .
d. evaluation metrics to comprehensively assess the effects of model editing techniques on targeted and non targeted instances we evaluate the performance of editing techniques from four aspects namely effectiveness specificity generalization and fluency.
all of the first three aspects require to compare the output from the model with the oracle the human written content for effectiveness the comparison result represents how well the model works on the targeted inputs for specificity the result represents how well the model can distinguish between targeted and non targeted inputs for generalization the result represents how well the model works on inputs that are semantically identical to the targeted inputs.
given that we utilize the same metrics to evaluate the first three aspects which are exact match em bleu and rouge l .
em evaluates whether the output completely matches the reference.
bleu and rouge l are widely used evaluation metrics in code related tasks .
bleu measures the degree of n gram overlap between a sequence and a set of reference sequences the higher the overlap the higher the bleu score.
rouge l is the most popular variant of rouge which specifically calculates the longest common subsequence between the output and the reference.
fluency evaluates the impact of model editing techniques on the generation ability of the model.
we follow meng et al.
in using p kf k log2f k to assess fluency where f k is the n gram frequency distribution of the n gram kfrom the generated content.
for all the aforementioned metrics a higher value indicates better performance.
e. research questions our study aims to answer the following questions.
rq1.
how proficient are existing model editing approaches at updating llms4code?
we first systematically investigate the performance of model editing techniques selected in section iii a for editing llms4code.
to answer this question we use the clmeeval benchmark to evaluate these model editing techniques across llms4code.
specifically we first use the model editing techniques to adjust the knowledge of llms4code.
after that we test the effectiveness specificity and generalizability of the edited model through our prepared dataset.
finally we use the targeted input and its semanticallyidentical variant the data used to evaluate generalization as model inputs and allow the edited llms4code to freely generate tokens to test fluency.
the final result is the average fluency of these two inputs.
when testing effectiveness specificity and generalizability it is crucial to ensure the consistency of generation of llms4code for which we use a greedy search strategy.
as for testing fluency which assesses the generative capability of theedited model we set top k and use beam search to allow the edited llms4code to generate freely.
rq2.
how efficient are the existing editing techniques for llms on llms4code?
in addition to the proficiency editing efficiency is also a focal point of model editing which may influence the developers adoption in practice.
we address this question by evaluating the editing efficiency of model editing techniques on llms4code.
for each instance edited by each editing techniques we record the time and peak memory costs.
rq3.
how can we improve the model editing techniques for better performances on llms4code?
based on the results of rq1 and rq2 we further analyze the in depth reasons behind the performance of existing editing techniques and attempt to utilize the observations to improve current techniques aiming to enhance the performance of editing techniques on llms4code.
f .
experiment settings all experiments are conducted on a800 gpus.
we use the basic prompt when asking the llms to finish a task.
specifically on the cnle dataset we provide the intent description and ask the llms to generate the code and vice versa on the csne dataset .
for the methods that require training we use leave one out cross validation and divide the targeted data of cnle and csne into a test set and a training set with a split.
the scale of test set in this division is consistent with existing model editing benchmarks .
we then consolidate the outcomes from the two rounds of training and testing as the final results for the complete dataset.
previous works have explored the optimal hyperparameters for the selected model editing techniques on the llama 7b model .
considering that codellama 7b shares the same architecture as llama 7b we follow the previous works when setting hyperparameters.
unless otherwise specified we set the same hyperparameters for the three llms4code models when evaluating a particular model editing method.
additionally unless otherwise noted the inputs and labels for these techniques are the xandyof the edit instance tuples cf.
section ii .
the following details the experiments for each selected model editing technique.
ft l this approach was designed to only target singletoken editing cases whereas both our datasets are multitoken editing cases i.e.
the sequence generation .
therefore we implement ft l with the standard fine tuning schema to adapt it to our datasets i.e ft m in setting the down projection matrix of the 21st layer of llms4code as learnable parameters.
it iterates up to times with a learning rate of .
we set the norm constraint 4to prevent significant changes in model parameters and to protect the original knowledge of the model.
rome memit pmet these techniques first use the causal intervention to locate critical layers storing factual knowledge and then edit those critical layers.
an existing work shows that there is negligible correlation between the location results from causal intervention and model editingperformance.
therefore we do not use causal intervention to select critical layers but directly refer to the existing studies which have already identified the critical layers of the llama 7b model.
specifically rome designed for a single layer editing edits the th layer of the three llms4code while memit and pmet edit the th layers of the three llms4code.
malmen malmen updates the down projection matrices of the last layers of llms4code.
it first trains a hypernetwork on the training set to predict parameter shifts.
specifically the hypernetwork of malmen consists of two consecutive mlp layers with a middle layer dimension of converting the gradients of llms4code on x y x u f xu and keys with regard to x y into parameter shifts.
when training this hypernetwork the learning rate is set to with max steps and the batch size is while clamping the hypernetwork parameters with l2norms greater than .
during editing the hidden states of the editing instance x y at the editing layers are first cached and then the hypernetwork predicts the parameter shifts to complete the weight update.
grace grace inserts an adapter into the down projection matrix of the 27th layer of llms4code.
each key s initial deferral radius is set to .
grace optimizes the values corresponding to the keys using standard fine tuning with a learning rate of and a maximum of iterations.
following the established practice in the field of model editing we adopt a strategy where different model editing techniques are directly applied to modify the llms4code for each targeted case regardless of whether the initial model can produce the correct output.
this decision is based on two reasons firstly as demonstrated in the subsequent section existing llms4code models are only capable of generating correct outputs for a negligible number of inputs.
therefore it is reasonable to assume that llms4code does not possess the required code knowledge in advance.
secondly even if the model already contains the necessary knowledge our evaluation can still assess the potential side effects generated during the model editing process by utilizing metrics such as specificity and fluency.
iv.
s tudy results this section presents details of the study results.
a. rq1 proficiency of editing techniques on llms4code table iv shows the effectiveness of the six model editing techniques on the clmeeval benchmark across three llms4code.
we also list the performances of the original llms4code for convenience.
overall we have the following major findings none of the existing model editing techniques can achieve satisfactory results simultaneously across the effectiveness generalization and specificity.
grace achieves the best overall performance among all the techniques with the highest effectiveness and specificity values.
specifically its rouge l value can reach nearly onetable iv proficiency of each model editing technique on llms4code.
dataset cnle nl2pl csne pl2nl model editor effectiveness generalization specificity fluency effectiveness generalization specificity fluency em bu rl em bu rl em bu rl em bu rl em bu rl em bu rl codellama .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ft l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
memit .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pmet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
malmen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codeqwen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ft l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
memit .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pmet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
malmen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
stablecode .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ft l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rome .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
memit .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pmet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
malmen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table v the effectivenes decreases from csne to cnle.
editor ft l rome memit pmet malmen grace codellama .
.
.
.
.
.
codeqwen .
.
.
.
.
.
stablecode .
.
.
.
.
.
avg.
.
.
.
.
.
.
hundred pencentage i.e.
.
when editing codellama on the nl2pl task.
nonetheless it shows nearly no improvement in generalization compared to the original model.
ft l achieves good effectiveness and generalization in editing codeqwen on the csne dataset but its significant decline in specificity and fluency compared to the original model indicates a substantial impact on the non targeted knowledge and generative capability of the model.
rome exhibits the best performance in editing codellama on the csne dataset but similarly it significantly impacts the original knowledge of the model.
the performances of memit pmet and malmen are generally poor.
on the one hand they do not effectively incorporate the required knowledge into llms4code which is revealed by the comparatively low effectiveness.
on the other hand they damage the original knowledge of the model significantly according to the low specificity.
in terms of fluency ft l produces the overall best results with sometimes even increasing the fluency of the edited model e.g.
when editing codellama on the csne dataset .
most model editing techniques perform poorly on llms4code being far less proficient compared with editing general llms .
for instance ft l achieved effectiveness in terms of the em when editing llama on factual knowledge datasets while its effectiveness on both cnle and csne datasets are generally poor often being less than .
rome memit pmet and malmen achieved nearly effectiveness and over generalization without significantdecreases in specificity and fluency when editing gptj on factual knowledge datasets .
however their effectiveness and generalization on the cnle and csne datasets did not surpass .
additionally there was a notable decrease in both specificity and fluency when utilizing these model editing techniques.
such results indicate that compared with factual knowledge editing code related knowledge is a more challenging task that deserves further investigations in the future.
generalization is a universal challenge for state of theart model editing techniques.
except for four instances where ft l and rome edit llms4code on the csne dataset achieving a generalization rate surpassing the generalization rate in all other scenarios dips below .
most editing techniques exhibit significant performance decreases across two different datasets on the same model with all editing techniques performing worse in nl2pl editing than in pl2nl editing.
this suggests that nl2pl editing is a more challenging task even its length of editing target is smaller cf.
table iii .
this observation suggests that the difficulty of editing is task specific and the presence of more tokens in the editing target does not necessarily indicate a higher level of difficulty.
table v reports the percentage decrease in effectiveness based on the exact match metric from csne to cnle for each editing technique.
we find that except for grace the performance of other editing techniques decreases by more than in average indicating their inability to switch smoothly between different editing tasks.
this suggests that grace exhibits potential for utilization in various software engineering tasks due to its higher applicability compared to other model editing techniques.
our systematic study of the proficiency of existing editing techniques on llms4code reveals that data points24681012141618 euc.
dis.
between oi and gri mean .
std .99euc.
dis.
between oi and sri mean .
std .04envelope plot of euclidean distance euc.
dis.
between oi and gri mean euc.
dis.
between oi and gri envelope euc.
dis.
between oi and sri mean euc.
dis.
between oi and sri envelope fig.
envelope plot of euclidean distance between oi gri and that between oi sri keys generated by grace .
none of the existing model editing techniques can achieve satisfactory results simultaneously across the effectiveness generalization and specificity most model editing techniques perform poorly on llms4code being far less proficient compared with editing general llms generalization is a universal challenge for state of the art model editing techniques most editing techniques exhibit significant performance decreases across two different datasets on the same model with all editing techniques performing worse in nl2pl editing than in pl2nl editing.
in depth analysis.
we note that grace has the best overall performance among all the techniques but it performs unsatisfactorily in generalization.
we conduct an analysis to delve into the reasons behind its poor generalization.
for convenience we select the cnle dataset which is generally more challenging for existing model editing techniques and codellama as the subjects for the analysis.
the study results can naturally be extended to other models and datasets.
the core working mechanism of grace involves storing key value pairs of editing instances in a codebook.
a deferral mechanism is used to measure the euclidean distance between each new input and the keys in the codebook matching whether the key of the new input falls within the deferral radius of existing keys.
if so the stored values will be used to generate output for this input.
here the keys are the last token representations of the inputs and the values are vectors optimized to produce the desired output from the model.
an ideal situation would be that the distance between the key of the edited instance and the key of the semantically identical input is relatively close to ensure a good generalization because they will obtain the identical output .
additionally the distance between the key of the edited instance and the key of the semantically unrelated input should be far apart to ensure a good specificity.
the good specificity of grace indicates that the key of edited instance may be far apart to those of malmen ft l grace rome memit pmet051015cnle time seconds malmen ft l grace rome memit pmet0510152025csne time seconds codellama codeqwen stablecodefig.
average time cost of selected model editing techniques per edit.
malmen ft l grace rome memit pmet01 104cnleaverage peak memory mb malmen ft l grace rome memit pmet01 104csne codellama codeqwen stablecode fig.
average peak memory cost of selected model editing techniques per edit.
unrelated inputs.
however the poor generalization suggests that the key of edited instance may not be close enough to those of semantically identical inputs.
as a result we are motivated to seek for the reasons for the poor generalization of grace by investigating the distances among the keys i.e.
last token representations of different inputs.
specifically we denote the oi original intent as the intent from the cnle dataset gri generalization related intent as the input for testing generalization in cnle and sri specificity related intent as the first specificity case in the cnle dataset.
we input the oi gri and sri of the test samples into codellama and extract the last token representations after which we compute the distances between the representations of oi and gri and those between the oi and sri.
figure indicates that the distances between the representations of oi and gri and those between the oi and sri are severely intertwined making it difficult to distinguish them using a fixed distance radius.
in such circumstances a gri input might be misclassified as an sri input by the codebook.
consequently the edited model may generate incorrect outputs leading to the poor generalization of grace.
to sum up our analysis reveals that utilizing the last token representation of inputs as keys is not effective in distinguishing between gri and sri based on which we design our editing strategy in the following contents.b.
rq2 efficiency of editing techniques on llms4code figure shows the average time cost for each edit using the selected editing techniques.
malmen takes the least amount of time per edit completing each edit in approximately two seconds on average.
pmet and memit have the highest time overhead among these methods but this does not translate into a performance advantage.
in contrast rome ft l and grace take a moderate time per edit and achieve better overall performance than malmen pmet and memit.
this indicates that certain model editing techniques can excel in both performance and efficiency.
additionally the figure shows that the overall editing time for csne is slightly longer than that for cnle.
according to the statistics presented in section iii c3 it is evident that editing longer inputs typically requires more time.
irrespective of dataset and technique variances there are also differences in the time required for editing different llmscode models due to variations in the dimension of the weights.
codeqwen has a larger down projection matrix dimension compared to codellama and stablecode resulting in a longer editing time.
figure shows the average peak memory on gpu for each edit using these editing techniques.
it shows that different models and datasets lead to different peak memory usages corresponding to the aforementioned editing time.
for instance on both datasets codeqwen has the longest per editing time across all editing techniques and it also has the highest average peak memory.
additionally there is no significant difference in peak memory usage among the different editing techniques.
for example when editing codellama on the csne dataset both the most and least efficient approaches i.e.
rome and malmen occupy around 30g gpu memory.
in rare cases malmen take around 40gb of memory when editing codeqwen.
this means that model editing techniques as an efficient knowledge update way are typically able to update the knowledge of a 7b llms4code on a gpu with 48gb of memory.
our systematic study of the efficiency of existing editing techniques on llms4code reveals that malmen is the most efficient editing technique while other techniques balance time efficiency with superior performance codeqwen takes longer and uses more memory compared to codellama and stablecode despite variations model editing techniques can update knowledge on large models like those with billion parameters using gpus within 48gb memory.
c. rq3 improvements of editing techniques on llms4code approach.
to answer this rq we seek for the opportunity to improve grace.
we select grace as the study subject since from the above analysis it achieves the best effectiveness and specificity while maintains high fluency and efficiency.
from the in depth analysis of rq1 we find that using the last token representation of the input as the key cannot help grace effectively distinguish between gri and sri andthus it currently achieves a comparatively poor generalization.
therefore the main objective of this section is to investigate a method to enhance the generalization of grace without compromising its performance in other aspects.
the challenge we face is the inability of the last token representation to accurately distinguish the semantic difference between gri and sri.
we are inspired by some recent works that leverage contrastive learning a semi supervised way to automatically capture the key features of the inputs to boost the performances of diverse software engineering tasks .
as a result we decide to exploit contrastive learning in our approach expecting that the semantic difference between gri and sri could be accurately distinguished.
specifically we propose a grace which introduces an mlp encoder composed of two linear layers.
different from grace the encoder utilizes the mean of all token representations as the key q rdn aiming at capturing the information of the entire sequence from a holistic perspective.
the encoding process of the mlp encoder for the key can be described as q w2 w1q where w1 rdn dn w2 rdn do the output dimension and is the activation function.
we set as relu.
after that we train the mlp encoder using contrastive learning on the training set.
the training target is to differentiate between gri and sri ensuring that the distance between the keys of the oi and the gri is as close as possible while the distance between the keys of the oi and the sri is as far as possible.
to that end we use the hidden states of the oi gri pairs as positive samples and the hidden states of the oi sri pairs as negative samples for training with the following loss function l nnx n 1yd2 y max margin d where nis the number of samples yis the label with for positive samples and for negative samples dis the euclidean distance and margin is a hyperparameter used to control the distance between negative sample pairs.
we set the output dimension of the mlp encoder to do 256to balance encoding performance and efficiency and the margin to .
in cnle and .
in csne to enable agrace to efficiently distinguish between gri and sri.
during the training phase we use a batch size of a learning rate of 1e and a weight decay of 1e for epochs.
we apply an early stopping strategy with a patience of and use the checkpoint with the best performance on the test set as the final mlp encoder weights.
results.
we set the other parameters of a grace to be consistent with those of grace.
the evaluation results on the clmeeval benchmark are shown in table vi.
the results show that a grace consistently outperforms grace overall particularly in terms of the generalization.
for instance when editing codellama on the cnle dataset the em in terms of the generalization is increased from .
of the vanilla grace to .
of a grace.
on the csnetable vi proficiency of the a grace approach and its variants on llms4code.
dataset cnle nl2pl csne pl2nl model editor effectiveness generalization specificity fluency effectiveness generalization specificity fluency em bu rl em bu rl em bu rl em bu rl em bu rl em bu rl codellama .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o cl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o mean .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codeqwen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o cl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o mean .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
stablecode .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a grace .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o cl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w o mean .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
data points0.
.
.
.
.
.
.
.
y .
euc.
dis.
between oi and gri mean .
std .26euc.
dis.
between oi and sri mean .
std .56envelope plot of euclidean distance euc.
dis.
between oi and gri mean euc.
dis.
between oi and gri envelope euc.
dis.
between oi and sri mean euc.
dis.
between oi and sri envelope fig.
envelope plot of euclidean distance between oi gri and that between oi sri keys generated by a grace .
dataset this value is boosted from .
to .
.
additionally in this table w o cl refers to the ablation result of directly using the mean of all token representations as the key without using contrastive learning and w o mean refers to the ablation result of using the mlp encoder trained by contrastive learning to encode the last token representation.
the ablation results indicate that neither directly using the mean of all token representations nor encoding the last token representation with the mlp encoder trained by contrastive learning can improve generalization.
in contrast agrace achieves an order of magnitude improvement in generalization without significant declines in other aspects.
notably on the csne dataset its performance in other aspects is almost identical to that of grace.
this demonstrates the rationale of our approach design that is using a contrastive learningaugmented mlp encoder to encode the mean of all token representations as the key is the optimal choice.
in terms of the efficiency a grace slightly increases the average editing time and memory usage by .
and .
respectively compare to the vanilla grace.
however the substantial performance gains make this trade off acceptable.
case analysis.
we first calculate the distances from the oi to the gri and sri for all samples in the cnle test set as caseid oi return request with url self.login page and callback self.login gri send a request using the url self.login page and execute the callback self.login sri get the value of username from request self a the sampled example.
.
.95pca visualization of representations original intent generalization related intent specificity related intent last t oken representations encoded mean representations b pca of representations.
fig.
the example and pca visualization of case analysis.
shown in figure .
it can be noted that after encoding the distance between oi and gri is significantly smaller than the distance between oi and sri which would ease the distinction between the gri and sri.
to ease the comprehension we draw a dotted line which acts as a clear distinction boundary between the blue and the red lines.
to further demonstrate the effect of the mlp encoder we randomly select an example in figure 6a where we highlight the keywords that lead to the semantic inconsistency between the sri and oi gri.
we input the related intents into codellama to obtain their representations after which we use pca to reduce the dimensionality of the last token representations and the mean representations encoded by the mlp encoder to a two dimensional plane as shown in figure 6b.
in the last token representations the distance between oi and gri is actually greater than the distance between oi and sri indicating that the vanilla grace fails to accurately distinguish between gri and sri in such a case.
in contrast in the mean representations encoded by the mlp encoder the distance between oi and gri is notably smaller than the distance between oi and sri.
this enables a grace to recognize that the gri lies within the deferral radius of the editing target thereby enhancing its generalization capabilities.
our systematic study of the improvement of grace reveals that using the last token representation as the key cannot help grace effectively distinguish between gri and sri by introducing an contrastivelytrained encoder in grace and encoding the mean of all token representations the proposed a grace we successfully enhance the generalization of grace without significantly affecting other aspects.
v. t hreats to validity internal validity.
the data used for testing generalization in the cnle dataset is rewritten by llms based on the original inputs.
it is unknown to what extent llms can generate semantically identical inputs.
to investigate this concern we randomly sample entries from the cnle dataset for manual verification.
the first two authors independently assess whether the original requirements are semantically identical to the llm revised descriptions and any conflict is resolved through a discussion.
the results confirm that out of the entries are semantically identical indicating that the majority of the llm generated data which is used for testing generalization is reliable.
as shown in table iv modern llms achieve less than exact match on the dataset.
there is thus a concern that this could be due to the models not understanding the format of the task and outputting natural language explaining the code alongside the code itself.
to better understand this we randomly selected samples from the cnle dataset where codellama generates different results compared to the oracle and manually checked them.
specifically we investigated whether the answers from codellama contain natural language explanations and results show that the outputs seldom contain such contents with only cases being the exceptions .
we further confirmed that even if such explanations are removed the results of codellama still do not match the oracle.
moreover we checked that out of the cases are indeed semantic misalignments between the codellama s outputs and the oracle.
this indicates that the bias in the evaluation is limited and llms4code are not good at generating completely syntactic identical outputs.
our metrics such as bleu could potentially bring some biases.
however these metrics are commonly used in the software engineering domain .
these metrics enable the large scale evaluation of our study.
moreover they generally keep consistent with the human evaluation as revealed by the recent study .
external validity.
currently we have focused on two widely studied se tasks and the python language.
it is worth while to explore more se tasks and programming languages in the future.
vi.
r elated work model editing is a rapidly evolving field with numerous editing techniques being proposed in recent years for llms .
additionally many benchmarks for llms editing have been introduced to evaluate the performance of these techniques .
these works primarily focus on model editing techniques on general llms.
the most relevant studies to our work are ment and codeupdatearena .
ment repairs next token errors in code generation by patching specific few neurons in llms4code.
codeupdatearena is a benchmark that focuses on api function changes for llms4code editing expecting to address the issue of api function updates through model editing.
our work differs from them in several ways.
firstly we are the first to systematically study the performance of state ofthe art model editing techniques on llms4code.
secondly our work focuses on sequence generation tasks e.g.
nl2pl and pl2nl in llms4code rather than next token prediction as sequence generation tasks have broader application scenarios in the software engineering domain.
lastly based on our evaluation results we propose a grace a new model editing approach for llms4code.
vii.
c onlusion this paper systematically investigates the performance of six state of the art model editing techniques on llms4code using our proposed clmeeval benchmark.
through evaluations based on effectiveness generalization specificity and fluency our study analyzes the strengths and weaknesses of existing editing methods for editing llms4code.
based on the best performing model editing technique grace we further propose its augmented version a grace whose generalization is improved to a large extent.
a grace builds a strong baseline for future llms4code editing techniques.
source code.
we have open sourced all code for this study at viii.
a cknowledgments this work was partly supported by the hunan provincial natural science foundation projects no.2022jj30668 and no.
2022jj30046 .