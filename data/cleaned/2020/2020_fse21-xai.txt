xaitools in the publicsector a casestudy onpredicting combined seweroverflows nicholas maltbie maltbind mail.uc.edu universityof cincinnati cincinnati ohio usananniu universityof cincinnati cincinnati ohio usa nan.niu uc.edu matthewvandoren metropolitansewerdistrict of greatercincinnati cincinnati ohio usa matthew.vandoren cincinnati oh.govreese johnson metropolitansewerdistrict of greatercincinnati cincinnati ohio usa reese.johnson cincinnati oh.gov abstract artificial intelligence and deep learning are becoming increasingly prevalentincontemporarysoftwaresolutions.explainableartificial intelligence xai tools attempt to address the black box nature of thedeeplearningmodelsandmakethemmoreunderstandableto humans.
in this work we applythree state of the art xai tools in a real world case study.
our study focuses on predicting combined sewer overflow events for a municipal wastewater treatment organization.throughadatadriveninquiry wecollectbothqualitative information via stakeholder interviews and quantitative measures.
these help us assess the predictive accuracy of the xai tools as well as the simplicity soundness and insightfulness of the produced explanations.
our results not only show the varying degrees thatthexaitoolsmeettherequirements butalsohighlightthat domain experts can draw new insights from complex explanations that maydifferfrom theirprevious expectations.
ccs concepts computingmethodologies machinelearning neuralnetworks software andits engineering process validation .
keywords explainability ai casestudy goal question metric gqm acmreference format nicholas maltbie nan niu matthew van doren and reese johnson.
.
xaitoolsinthepublicsector acasestudyonpredictingcombinedsewer overflows.in proceedingsofthe29thacmjointeuropeansoftwareengineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse august 23 28 athens greece.
acm new york ny usa 13pages.
corresponding author.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse august 23 28 athens greece associationfor computing machinery.
acm isbn ... .
introduction artificial intelligence ai has become so ubiquitous that many decisions nowadays in our daily life are shaped by it e.g.
news feed suggestions and shopping item recommendations.to putthe flourish of ai into perspective adadi and berrada highlight the reportsforecastingthatfrom2017to2021 theglobalinvestment on ai will increase from billion us dollars to .
billion and the revenues from the ai enabled industries worldwide is expected to growfrom billionto .
trillion.
theunstoppablepenetrationofaialsoreaches intothepublic sector.forexample theeuropeancommissionenvisionsthatai could be used to serve citizens in faster more agile and more accessibleways .however somepublicaiserviceshavealready shown harmful consequences.
in the u.s. for instance ai was used to allocate caregiver hours for people with disabilities but dramaticallyloweredthenumberofhoursinmultiplecaseswithout anyexplanationormeaningfulopportunitytocontestthedecisions madebythe proprietary algorithm .
althoughaimistakesareinevitable thelackof explainability raisessignificantconcernsfromthecitizensandpublicorganizations about ai based decision making s accountability fairness responsibility and transparency.
explainable artificial intelligence xai addresses these concerns by aiming to make ai more understandable to users soas to increasethe users trustandrelianceon the ai system.
consequently many xaitoolsare builttoautomaticallygenerate explanations from deep learning models.
deep learning models havebeenabletoachievenear humanaccuracylevelsinvarious typesofclassificationandpredictiontasksincludingimages text speech andvideodata .thesedeeplearningmodelsareoften opaqueintheirnatureandhencereferredtoas blackboxes dueto thedifficultyinunderstandinghowtheyoperate .aninfluential xaitoolislime localinterpretablemodel agnosticexplanations proposedbyribeiroandhiscolleagues .limecanapproximate a black box model locally in the neighborhood of any prediction of interest.
anillustration given byribeiro et al.
is that once a model predicts a patient has the flu lime shows with relative weightsthat sneeze and headache contributeto this particular predictionwhereas nofatigue isevidenceagainstit.identifyinga fewweightedfeaturesasinlimeisonlyonewaytoproduceexplanations.othersextractrules visualizesaliencemaps orimplement othermethodologies .
1032esec fse august 23 28 athens greece nicholas maltbie nan niu matthewvandoren andreese johnson milleretal.
arguethatmostxairesearchersarecurrently building tools for themselves rather than for the intended user.
even the seminal work on lime scored the lowest on miller et al.
s datadriven criteria becauseribeiro etal.
constructedtheir ownunderstandingofhowpeoplemightevaluateexplanationsand recruited human subjects on amazon mechanical turk to perform theexperiments.behavioralexperimentsconductedwithlaypersonsaresimplificationsof andthereforecannotreplace puttingthe explanationintothereal worldapplicationandlettingtheactual end user typicallyadomainexpert test it .
to gain insights into application grounded xai evaluations we conducted a case study on how public services might exploit deep learningtopredictcombinedseweroverflows csos .combined sewer systems transport various sources of water from residential industrial and commercial customers as well as storm runoff.
a problem with these systems is handling cso events when the system is overwhelmed by surges of water and the combined sewer system is forced to discharge untreated water into the local environment.
with infrastructures like sensor networks collecting real timewaterflowdata alongwiththeavailabilityofcontributing sources like the rainfall data public sewer services show keen interestsindeeplearningtechniquesthatarecapableofoffering high degrees ofpredictive accuracyas well as explainability.
thispapermakestwomaincontributions.weperformagoalquestion metric analysis of explainability to quantitatively measurethreestate of the artxaitools andweinterviewtwodomain experts to qualitatively assess the xai results on data driven cso predictions.
our study not only updates some commonly held beliefs about explainability but also emphasizes the engineering considerations of incorporating explainability intothe entire deep learning sdevelopmentworkflow.inwhatfollows wepresentbackgroundinformationinsection detailourcasedesigninsection analyze the results in section discuss our work s implications in section5 anddraw someconcluding remarks insection .
background .
explainable artificial intelligence xai andtools inai thehighlevelofdifficultyforthesystemtoprovideasuitable explanation for how it arrived at an answer is referred to as the black box problem .
this difficulty is particularly prominent for deep learning models because a deep neural network trained endto end can be as complex as an accurate explanation of why the modelworks .thecomplexitycanbeillustratedbyresnet which incorporates about 107learned parameters and executes about 1010floating point operations to classify a single image.
xai triestodemystifytheblackboxesastheybeginmakingdecisions previouslyentrustedtohumans.thus explainability theability tointerprettheinnerworkingsorthelogicofreasoningbehindthe decision making helps to achieve an ai system s accountability justifyingthe decisions andactions fairness havingimpartialtreatment andbehavior 1by data driven we mean explicitly referencing articles on explanation in social science and testing if the produced explanations are appropriate for the intended users .
responsibility answering for one s decisions and identifyingerrorsorunexpectedresults and transparency describing inspecting and reproducing the mechanismsthroughwhichthe decisions are made.
adadiandberrada identified17xaitechniquesbysurveying papers published between and .
according to the survey most recent work done in the xai field offers a post hoc local explanation.becauseonlyafewmodels suchaslinearregression or decision trees are inherently interpretable generating post hoc explanations is necessary for complex models like deep neural networks.
post hoc xai tool can therefore be applied to any classifier orregressorthatisappropriatefortheapplicationdomain even those that are yet to be proposed .
local explanations justify why asingleprediction was made whichare incontrastto global explanations tryingto understandthe entire reasoningleading to allpossible outcomes.
what the xai tools do can be classified by how they emulate the processing of the data to draw connections between the inputs and outputs.
gilpin et al.
s taxonomy organizes xai tools by their function to extract rules to summarize decisions createasaliencemaptohighlightasmallportionofthecomputation which is most relevant and employ a simplified proxy that behaves similarly to the original model.
for instance ben tez et al.
transformeddeepneuralnetworkstofuzzyrulesthroughan equivalence by approximation process simonyan et al.
producedasaliencemapbydirectlycomputingtheinputgradient and ribeiroet al.
used a local linear model in lime as a simplified proxyfor the full model.
with the increased usage of xai techniques evaluating their efficacybecomesimportanttoinformpractitionersabouttooladoptions.miller etal.
ssurveyof23xaipapers showsthatrigorous humanbehavioralexperimentsarenotcurrentlybeingundertaken.
as the verb to explain is a three place predicate someone explainssomethingtosomeone millerandhiscolleagues argue that most xai tools explain things e.g.
feature or neuron importance to the ai researchers but notto the intended users.
doshi velez and kim further argue thatthe best way to show how an xai technique works is to evaluate the tool by consulting domain expert grounded in the exact application task.
although costly the application grounded evaluations provide direct and strongevidence orlackthereof ofxai sfulfillmentoftherequirements.
.
explainability as anon functional requirement nfr in software engineering functional requirements describe what the system does whereas non functional requirements nfrs focus on how well the system does it .
making classifications recommendations and predictions are among the common functional requirements of an ai system and so in an explainable wayis often regardedas a non functional concern .
thus researchersconsiderexplainabilityto be an nfr.
in a survey study with participants from brazil and fromgermany chazetteandschneider elicitedtheparticipants expectations from an explanation.
chazette and schneider s online 1033xaitoolsin the public sector a casestudyonpredictingcombinedseweroverflows esec fse august 23 28 athens greece questionnaireusedahypotheticalscenariowherethesurveyparticipantswoulduseavehicle sai basednavigationsystemwhile driving on a route they had traveled before however ai suggested a different route than usual.
of the codes analyzed from all theresponses expresseddesirein knowing whatspecific pieceofinformationsupportedandinfluencedthesuggestion wanted toknowthe howofthealgorithm sinnerreasoning and expressedwillingness to understand whysomething happened e.g.
why the route is not being suggested and benefitsofthe newroute when comparedto the usual .
thesurveyresultsclearlyshowthatpeople sexplainabilityrequirementsaredifferent.chazetteandschneider furtherpointed outthatelicitingexplainabilityshouldalsoconsiderlawsandnorms culturalandcorporatevalues domainaspects andpracticalproject constraintssuchastimeandbudget.theeuropeanunion forinstance debated about a general right to explanation which is partly enshrined in certain regulations .
such policies along with thegloballyemergingethicsguidelines are makingai especiallyai incitizenservices more auditable.
nfrs may interact the attempts to achieve one nfr can hurt or help the achievement of another .
for example generating a post hocexplanationimposesadditionalcomputationaloverhead possiblyhurtinganaiservice sresponsiveness.meanwhile of thecodesin correspondedtotheresponsesinwhichtheusers perceived explanations as a way to reduce obscurity due to the moreinformationabouttheaisystemanditsoutcomes.yet15 cautioned that too technical or lengthy explanations might add more obscurity.
recognizing the trade offs between explainability andothernfrsisthereforeimportantforprioritizingrequirements andmakingdesignchoices.
insummary therequirementsengineeringliteraturesuggests that explainability is an nfr or a softgoal whose satisfaction is a matter of degree without a clear cut criterion .
because explainabilityisnotatechnicalconceptbuttightlycoupledtohuman understanding examiningwhatxaitoolsactuallydoandwhatthey shoulddomustbecarriedoutwithrespecttotherelevantaspectin relevant contexts.
understanding the degree to which existing xai tools satisfy the explainability softgoal in an application grounded taskisprecisely the focusofour research.
case studydesign .
problemcontext nearly860citiesandtownsacrosstheu.s.havecombinedsewer systems whichmanagestormwateraswellaswastewater creating what the u.s. environmental protection agency epa considers to be the largest unaddressed risk to human health from the water infrastructure .
according to an epa report about billion gallons of untreated wastewater is discharged into waterways annually in the u.s. the excess water from storms carries dust trash and debris from developed regions and washes them into the combined sewer system.
when these combined sewer systems areoverwhelmed theywilldischargeuntreatedwastewaterinto nearby waterways at an outfall site.
this is defined as a combined sewer overflow cso event.
figure 1shows a simplified view of the causes ofcsoevents.
domestic wastewaterstormwater runoff industrial wastewater pipemaximum conveyance capacity wastewater treatmentriverfigure illustration of a cso combined sewer overflow siteandhow overflow can lead into nearby water sources.
in the u.s. the over cso outfall sites account for approximately5 000infectionsannually damageshabitatsforanimalsin wetlands killingfishinrivers andclosuresofrecreationalwaterways and beaches .
thisproblem is not unique to the u.s. but occurs all around the world.
for example an average of million tons of untreated wastewater is dumped into the river thames in london uk annually due to the cso events .
even moderncitieswithcombinedsewersystemssuchasshenzhen china have to handle mitigation of pollution into rivers from the cso events .
with increasing levels of urbanization and changes inweatherpatternsduetoclimate change theseproblemsare expected to become more severe and require new solutions to handle theminthe future .
weworkedwithawastewatertreatmentorganization metropolitansewerdistrictofgreatercincinnati msdgc thatservicesan operatingareaofabout300squaremiles over850 000customers andover3 000milesofcombinedsewers.msdgchassetupalarge scalesensornetworktocollectdataandremotelyoperatetheirsystem.someof the olderoutflowsites intheir systemcanonly hold a limited amount of water before they will overflow and cause a csoevent.thecurrentpracticeofmsdgcis toreferenceweather forecast then alert citizens if a cso event may occur within the nextday.
since msdgc is a public service they need to be able to justify their reasoningfor their decisions especiallywhen their decisionsaffect thesafetyofcustomers.this needfor transparencyis why their current system of mostly relying on weather forecasts is preferred.theycanjustifytheirdecisionseasily quicklyidentify mistakes andutilizethisinformationforfuturewarning.ideally alertingcustomersearlybeforeacsoeventoccurscanhelpkeep their customers safe.
when using weather forecasts a warning maybesenteverytimealargestorm isexpected.however many of the alerts sent are false positives leading to customers simply ignoring them.
reducing the high false positives inpredicting the csoevents predictingcsos forshort isthemainreasonwhy msdgcisexploringdeep learningsolutions.
drawingfrompriorexperience wedesignedanexploratory case study to investigate the use of deep learning and xai tools to predict the csos within the real life context of msdgc.
inparticular weworkedwithtwodomainexpertsfrommsdgc 1034esec fse august 23 28 athens greece nicholas maltbie nan niu matthewvandoren andreese johnson table sample of rainfall data which is sampled every minutefromasensorandreturnsthedepthofrainfallmeasured inan area upstreamofthecsooutfall site.
timestamp rainfall inches oct12 .
oct12 .
oct12 .
oct12 .
oct12 .
table sample of level velocity and flow data from the manholesiteupstreamoftheoutflowsensor.thisdatawas sampled at arate ofonceevery minutes.
timestamp levelvelocity flow aug .
.
.
aug .
.
.
aug .
.
.
aug .
.
.
aug .
.
.
a hydrologist and an operational manager.
they were points of contactforourcasestudyandweretheemployeesthatmanaged thedata.theyarerepresentativeofourstakeholdersduetotheir organizationalrolesandworkingexperiences.wecommunicated viaemailsaswellasonlinemeetingsanddiscussionsthroughout the course of the case study due to restrictions relating to covid19.
these meetings were conducted in an informal interview style where we co designed the goal question metric framework2with thedomainexpertsandwepresentedresultsandmaderevisions as needed.
when presenting results of the xai tools to the domain experts weusedthevisualsandinteractiveelementsprovideddirectly from the selected xai tools to observe how well the tool could provide insight to domain experts more familiar with the data butnot familiarwith ai research.our workseeks to provide some examples of successes and problems of conducting further researchintohowdomainexpertscanusexaitoolstobetterapply andutilizedeeplearningmodels.
the data collected by our wastewater treatment organization wastakenfromvarioussensorsatacsooutflowsite amanhole approximately ft upstream of the outflow site and a rainfall sensorforthearea.thesiteisconsideredtobe overflowing wheneverthelevelofwateratthecsositeexceedsthesite scapacity.
eachof thesesitescollectsdata independentlyfrom eachotherat different rates.
the slowest sampling rate is one sample for every minutes while the fastest is every minute.
in order to handle the inconsistencyandvariationsinrealdata weusedlinearinterpolation to handle variations in sampling time to synchronize the samples from each of our sources.
as illustrations with fictitious data table 1shows a sample of the rainfall data table 2shows samples from sensors in a manhole a few minutes upstream in a pipe upstream and table 3shows a sample of the synchronized andinterpolateddataset.
2the frameworkwill befurther discussed in section .
.table from our dataset we collected three features flow level velocity from the manhole upstream of the outflow site onefeature outfall fromtheoutfallsiteitself andone feature rainfall from the rainfall sensor.
this is a sample of the synchronized and interpolated data points from our dataset.
timestamp flowlevelvelocity rainfall outfall aug .
.
.
.
.
aug .
.
.
.
.
aug .
.
.
.
.
aug .
.
.
.
.
figure figure from showing an abstraction of how limeformsalocal lineardecisionboundaryfromthemore complex decision space.
.
deep learning solution andxaitools weareusingadeeplearningmodeltotakeadvantageoftheyear ofcontinuousdatacollectedbyourwastewatertreatmentorganization and the smart network they have developed.
since our data issequentialinnature weareusingalongshorttermmemory lstm cellstructureinourmodel whichisinlinewithourrecent workondeeplearningbasedcsopredictions .inorderto simplify the problem we first check when all of the cso events occur thenpassthelstmthefeaturesfromtable 3for12hours ofdata.we utilizedan adamoptimizer andtensorflow to create andtrainour model3.
simply giving our deep learning model to the wastewater treatment organization is not sufficient to meet their needs for transparency and justification.
therefore we applied various xai tools to our deep learning solution.
there are quite a few tools that provideexplanationsforlstm basedmodels .inordertoselectand compare tools for our work the tools we used must be available to the engineers at the wastewater treatment organization even without our direct input so they can continue to use and expand on our work.
given this constraint the tool should be open source compatible withour solution andeasyto use.
in addition to the above selection criteria we wanted to use the xaitoolstoexplainhowthelstmhasmadeadecisionofoverflow for the cso site.
ideally this can be used to help inform future decisionsforourstakeholdersatthemsdgc.thesetoolsshould 3oursource codeand resultsareshared at 1035xaitoolsin the public sector a casestudyonpredictingcombinedseweroverflows esec fse august 23 28 athens greece a lime b shap c rulematrix figure illustrating the explanations generated from the xai tools a lime s explanation displays the most influential features supporting or opposing a prediction decision b shap s explanation visualizes how much input features affect the csopredictions and c rulematrix s explanationgenerates ahierarchyofrulesby usingthe input features.
increasetransparencyandaccountabilityofthedeeplearningmodel bybetterunderstanding howitoperates.
fromtheserequirements weselectedthexaitoolsoflime shap basedondeeplift andrulematrix .eachof thesestate of the arttoolscantakeanlstm basedmodelanda givensamplefromourdataset andthenproduceanexplanation forhowthedeeplearningmodelmadeadecision.thetoolshave differentassumptions andmake differentexplanations.
limecreates a local approximation of the deep learning model soutputspacebysamplingvariousinputsfromour dataset.
lime then uses this approximation of the output space to determine which features in the input space are the most significant to determine the model s prediction.
figure2shows a representation of how a linear boundary iscreatedforagivensampleofinterest.thisidentifiesthe most significant features and which classes these features support.
shapusesbackpropagationandcomputesshapelyvalues todeterminehow muchinfluencetheinputsofeachlayer have on the next layer.
through backpropogation these values are progressed from each layer starting with the output backtowardstheinputofthedeeplearningmodel.thisis then used to create a significance map of how much each individualinputinfluencedthefinalprediction.thesevalues ofinfluencecan range dynamically onagradient.
rulematrix creates a global approximation of the deep learning model s decisions.
this is done through a set of rulesorganized hierarchicallywhere eachruleinsequence divides the dataset based on a threshold for a given input feature.
these rules though only approximations are inherently explainableto humans.
figure3providessampleexplanationsillustratingtheoutputs from our chosen xai tools.
some key differences of these tools can be seen through how they represent their explanations.
for 1036esec fse august 23 28 athens greece nicholas maltbie nan niu matthewvandoren andreese johnson goal question metricaccuracy w.r.t.
predictions explainability w.r.t.
explanations predicting all csos?
predicting only csos?
simple?
disruptive?
sound?
recall precision entropy stakeholder interviewsprediction change figure4 thegoal question metric gqm frameworkguidesourcasestudy.eacharrowrepresentsdefiningaconceptinmore detail.the diagram progresses fromhigh levelgoalsinto more concrete questions then finally to the measurable metrics.
example betweenlimeandshap shapgivesagradientofvalues toeachfeatureastohowmuchithelpedorhurtaprediction.lime attributes influence to different features instead of a gradient for eachfeaturetotheprediction.shapusesbackpropagationtoassign influencefromthepredictiontotheinputspacewhilelimesamples otherdatapointsinthelocaloutputspacetogenerateexplanations.
rulematrix operates in a completely different manner as it creates awholenewmodelmakingitdifficulttodirectlycomparetoshap and lime.
these differences motivate us to establish a coherent framework for evaluatingthe xaitools.
.
research questions wefollowthegoal question metric gqm approach tocritically understand the three chosen xai tools in the context of deep learning based cso prediction.
the analysis is drawn from our gqm analysis of visual requirments analytics tools .
comparedwithmanyxaistudiesthatfocusedonevaluatingthe produced explanations with lay persons or ai researchers we collected the feedback directly from two domain experts i.e.
a hydrologistandanoperationalmanageratthemsdgc.wewere in constant email communications with the two domain experts.
furthermore we heldthreeone hourvirtualmeetingswiththese two experts to understand the data shared with the research team toelicittheirexplainabilityrelatedconcerns andtointerviewthem whilepresenting the explanation results from the xaitools.
the structure of our gqm analysis is presented in figure where two general goals of accuracy and explainability of cso prediction are addressed.
relevant questions are used to refine the goals.
while we measure accuracy by well known metrics of recall and precision the questions of explainability are explicitly built on human behavioral studies making our case study directly data driven accordingtomiller etal.
.inparticular weconsidertwo studiesonexplainabilityfromcognitivepsychologyandbehavioral sciences.
lombrozo conducted human subject experiments to decide whatcausedagiveneventfromasetofpossiblechoicesthenjustify these decisions and showed that people disproportionately preferred simpler explanations over more likely ones indicating sometrade offbetweenthesimplicityandsoundnessofexplanations.moreover thagard indevelopinghiswell knownecho model to characterize the cognitive processes responsible for selecting between competing explanatory hypotheses reported that peoplepreferredtheexplanationsconsistentwiththeirpriorknowledge.
therefore we also investigate how disruptive the xai tools explanations are compared to the domain experts existing cso understandings.
as shown in figure our case study addresses five researchquestions rqs rq1 how complete does the xai enabled deep learning solution predict csos?
we measured this through recallwhich is the number of correctly identified cso events out of all cso events in the dataset.
lime and shap make post hoc predictions directly fromthelstm baseddeeplearningmodel.consequently lime and shap have the same recall value as the lstm.
rulematrix on the other hand requires a re computation of recallaccording to the generatedrules.
rq2 how much noise is there as the xai enabled deep learningsolution predicts csoevents?
wemeasuredthisthrough precision thenumberofcorrectly identifiedcsoeventsoutofallpredictedeventsbythedeep learning model.
just as with recall lime and shap make post hoc predictions and thus have the same precision as the original lstm whereas this metric will need to be recomputedfor the rule basedmodelcreatedbyrulematrix.
rq3 howsimplearetheexplanationsgeneratedbythexai tools?
in order to evaluate the simplicity of these tools we used bothnumericmetricsaswellasinterviews.foraquantitative metric we computed the entropyof the explanations produced by the xai tools.
entropy measures the uncertainty ordisorder ofadistribution whichcanbeused to approximate how much unique information and variabilityisintheexplanation.inaddition weinterviewedthetwo expertsandshowedthemexplanationsfromthevariousxai tools.
rq4 howsoundaretheexplanationsgeneratedbythexai tools?
soundnessofanexplanationcanbedifficulttoinvestigate sinceitdependsonthebackground ofastakeholder asdiscussed by gilpin et al.
.
however xai tools such as deeplift shap and layer wise relevance propagation all attempt to assess the correctness of the generatedexplanationsbymaskingthemostsignificantdata valuesidentifiedbyanxaitoolfromasampletoexamine 1037xaitoolsin the public sector a casestudyonpredictingcombinedseweroverflows esec fse august 23 28 athens greece the corresponding prediction changes of a given deep learning model.
thus we applied this prediction change metric asaquantitativemeasureofthesoundnessofanexplanation.
wealso interviewedthetwodomain expertsto assesstheir confidence in the plausibility of the xai tools explanations.
rq5 how much new insight if any do the xai tools explanationsoffer?
to our stakeholders from the msdgc performance is an important aspectas anaisystem must bothperformbetter and be as interpretable as their existing system to justify its use.
it is clear to us that our stakeholders want to know if deeplearningcanprovideanewperspectiveontheproblem.
however havingamodeldeviatetoomuchfromtheirexpectations may make it difficult to trust.
through interviewing thestakeholders weassessedhowdeeplearningequipped with the explanations can help provide new insights toward identifyingthecsoevents therebypotentiallydisrupting someaspectsofthe msdgc s practice.
results and analysis as mentioned in section we designed an lstm deep learning solutiontopredictcsosforthemsdgc.weanalyzedtheresults fromthisdeeplearningmodelaswellasthexaitoolsdescribed in section .
.
these results are from an lstm that predicts if a cso event will occur within the next hour after being given the previous hoursofdata.
.
predictiveaccuracy recall andprecision themostimportantmetricformodelperformancetothemsdgcis accurately identifyingevents.ifthesolution failsto identifyevents i.e.
recall is low it will not be able to warn the citizens in the serving area.
if there are too many falsepositives i.e.
precision is low citizenswilllikely ignore the warnings.
as discussed in section .
lime and shap had the same recall andprecisionvaluesasthelstmmodel.rulematrix srecalland precisionneededtobere computedoncetheresultingruleswere generated.tocalculatetheaccuracymeasuresofbothlstmand rulematrix we used a month long test subset of the dataset and thenevaluatedthepredictionsbasedonthegivenlabelsfromthe wastewater treatment organization elevated means cso events occurred normal means otherwise.
therecallandprecisionresultsareplottedinfigure .thefigure showstheresultsofpredictingwhetheracsoeventwillhappen within the next hour for every minute interval over a month duration.
the recall and precision of the deep learning model and hencelimeandshap areapproximately80 and45 respectively.
therecallandprecisionforrulematrixareonlyabout40 and20 respectively representing a recall drop and a precision drop.
while disappointed in the rulematrix s accuracy levels the two domain experts believed lstm s cso predictions were encouraging and agreed with our suggestions of improving the model performances by incorporatingdata from more cso sites.
during the interviews the experts were also interested in how the lstm s performances would comparetothemsdgc s currentpracticeofrecall elevated precision elevatedrecall normal precision normal020406080100 performance metrics for predicting cso events lstm rulematrix figure answering rq 1and rq .
recall of predicting csos elevated class lime shap lstm .
rulematrix .
.
precision of predicting csos lime shap lstm .
rulematrix .
.
recall of predicting non cso events normal class lime shap lstm .
rulematrix .
.
precision of predicting non cso events lime shap lstm .
rulematrix .
.
relying onweatherforecastto informthecitizens aboutpotential csoevents.
toinvestigatethis wefurthercollectedrainfalldatafromnoaa diver for the area around the cso site for the same date and time range that our deep learning model was tested on.
when usingaconstantrainfallthresholdforagivenday i.e.
.5inches of rainfall per day a recall of and a precision of were obtained.
the low precision level helped illustrate the exploration ofthedeeplearningsolutions.althoughlstm s41.
doubledthe csopredictions precision itisimportanttonotethatthenoaa dataset can only collect rainfall data for each day whereas the lstm based deeplearning model makes predictions continuously for a time range in the future.
this continuous prediction of the deep learning model could lead to lower recall as a prediction one hour before an event may be correct but half an hour before an eventmaybeincorrect.lstmdoesnotpredicteveryintervalbefore thecsoeventcorrectlybutitdoesidentify78.
ofthe5minute intervals an hour before a cso event from figure .
the deep learning model is more precise than onlyconsideringrainfall data andcanbesubstantiallyimprovedinthefuture.thevalueadded by thedeep learning modelredefines theproblem and withsome improvementtothedeeplearningmodel couldgivethewastewater treatmentorganizationtoproactivelyaddresscsoeventsbefore they occur.the above analysessuggest finding1 while rulematrix s recallandprecision are low lstm andhence limeandshap achieves about80 recallandperforms more precisely than the currentpractice.in addition lstm providesnew predictive capabilities for every five minutesbefore a csoeventas opposedto daily weather forecasts.
1038esec fse august 23 28 athens greece nicholas maltbie nan niu matthewvandoren andreese johnson explanation size0123456explanation entropy analysis of explanation complexity xai tool shap lime random rulematrix figure6 explanationcomplexitymeasuredbyentropy.random is sampled from a uniform distribution representing a baseline entropy.
entropy is computed from the scikitlearnlibrary withalog2scalefortheseresults.greater entropy valuesindicatemore complex explanations rq .
.
explanationsimplicity andsoundness ourrq 3andrq4investigatetowhatextent simplerexplanations favored over more likely ones holds in our case study.
as illustrated in table our lstm model had a total of input parametersconsistingoffivefeaturessampledatarateofevery5 minutesfor12hours.toquantitativelyevaluateallthreexaitools we computed shap for all input parameters and experimented lime for default and all parameters.
rulematrix hadafixednumber ofdecisions sowe usedthe rulesthat were produced.
in order to quantify simplicity we computed the entropy to measurethevariationanduniquenessoftheexplanationsproduced byeachtool.theexplanationofeachinstancewasamatrixsharing the same shape as the input dataset.
the influence of each element onthefinalpredictionwasstoredinthismatrix.rulematrixcreated asetofrules sotoapproximatethislevelofinformationwesimply used a string of ones and zeros where a one indicates a rule was usedandazeroindicatesthatarulewasskipped.whencomputing entropy a set of numbers of the same value has an entropy of zero whileasetofcompletelyuniqueorrandomnumberswouldhave much greater entropy.
we repeated a random sampling times toidentifyanaccuraterepresentationofrandomentropy.thisis limited to a small sampling due to the time it takes to create an explanation.
results in figure 6show an increase in entropy with increase in explanation size i.e.
the number of parameters in an explanation .
this trend is expected as more unique values are being added to the explanations.
as far as the xai tools are concerned lime had complexity almost equivalent to a random sampling and it could also be filtered to the most significant results.
shap had much less entropy for the parameters that it explained and10 top k features removed0.
.
.
.
.
.
.
.14absolute change in prediction distribution of change when removing prominent features xai tool shap lime figure7 explanationsoundnessgeneratedbyremovingthe most significant kfeatures identified by shap and lime from a sample and then evaluating how much the removal changed the prediction.
greater change in prediction implies more soundness ofthe explanations rq .
hadawidervariancethanarandomsampling.wespeculatethat this is due to the majority of the features have little significance potentiallyresultingfrombackpropagationandhowtheshaptool parsesthelstmstructure.rulematrix sentropywasonparwith a random sample of that size but the variation was wide.
note that rulematrix produced what is commonly believed to be inherently explainableandsimpleintermsofcitingasmalllistofcausesfor eachprediction.
explanation soundness rq evaluates how correct an explanation is at determining the decision made by the black box lstm.
we quantifiedhow much thedeep learning model s prediction would change when removing the most significant features at specifictimestampswithinthedataset.thismeasuregivesusan experimental verification of the importance of the identified featuresintheexplanations.sincerulematrixdidnotdistinguishin theexplanationswhichelementswerethemostsignificantforeach individualprediction wecouldnotdirectlyevaluaterulematrixby using the prediction change procedure.
figure7shows the soundness results when removing the top kfeatures for shap and lime.
for the top features shap andlimehaveachangeinpredictionofabout0.0001and0.
respectively.
when increasing kto shap has a much wider range centeredat0.0217whilelimehasa0.0028average.when kincreasesfurtherto200 shapandlime sdistributionscluster around .
and .
respectively.
figure s results suggest that lime has the slightly more sound results for a smaller k but a largerkseems to manifestshap ssoundnessbetter.
theinterviewswiththetwodomainexpertsconfirmedourobservationsandprovidednewviews.althoughrulematrixisinherently more explainable than lime and shap the hydrologist and the operational manager at the msdgc did not find the rule hierarchy capturedtherelevantknowledgeinthecsodomain.bothlime andshapwerewellreceivedbytheexperts withmorepreferences shiftingtoshapasthisxaitoolexhibitsmoresoundnesswhenall thefeaturesareconsideredtogether.despitebeingsoundatsmaller kandbeingflexibleintermsofhavingacustomizablenumberof 1039xaitoolsin the public sector a casestudyonpredictingcombinedseweroverflows esec fse august 23 28 athens greece shap lime lime lime xai tool020406080100significance percentage significance by feature for explanations flow level velocity rainfall outfall level figure8 theaverageinfluenceeachfeaturehadontheoverall prediction.
observing where the influence comes from helpstoestablishbothsoundnessofresults rq andtoexplore new insights rq .
parametersinanexplanation limecouldbeconfusingwhenincorporatingmanyfeatures.surprisingly fromfigure sentropy perspective shapissimplerthanlimewhenallthefeaturesare taken into account.
based on the analyses of rq 3and rq we summarize finding2 explanation s simplicity does notalways come at the costofsoundness.domainexpertswould clearlyfavorsoundnessover simplicity andinour casestudy ofcsopredictions shap smore sound explanations withmanyparametersturnoutto be alsosimpler.
.
newinsights from theexplanations ourstakeholderinterviewsincludedaninteractivesessionwhere the two domain experts could explore the xai tools beyond the results that the research team had prepared.
we highlight some concrete insights gained from the interactive session which focusedmoreonlimeandshap duetorulematrix slowrecalland precision levels.
figure8shows the general results as to which features from the datasetarethemostinfluentialtothedeeplearningmodelacrossall predictions.
shap distributedinfluencefairlyevenlybetween the featureswhilelimeheavilyfavoredrainfall.aslimeincreasedto includemorefeaturesintheexplanation thedistributionevened outmore.inadditiontothis avisualizationoftheinfluencebytime isshowninfigure .shapheavilyfavoredthetimerightbefore the cso event whilelime favoredthe start of the sample.similar tofigure theinfluenceoflimeinfigure 9evenedoutasmore features were includedinthe explanation.
theexpertsstatedthatlimeresultswereusefulinidentifying themostsignificant features when lookingatthetop10elements.
meanwhile theywereable to interpretnewinsights from the significance plots generated by shap such as figure b and the summaryplotoffigure theyfoundapatterninthecorrelation betweenvelocity flow andlevelandsuggestedwhytheseattributes mightbemoresignificantinafewsampleevents.atfirst theyhad not assumed the feature of velocity to be useful for predictions but2 hour of input sequence0.
.
.
.
.
.
.
.35significance percentagedataset analysis charts significance by hour for explanations shap lime10 lime100 lime720 figure9 theaverageinfluenceeachhouroftimebeforethe predictedeventhadontheoverallprediction.thisdistribution helps to identify the new insight about what specific featuresto focuson and whento focuson them rq .
realized and discussed how correlations between velocity and flow could help predict future cso events.
this provided a new insight tohelpguidefuture developmentandresourcesfortheiranalysis that challengedtheirinitialview.
another major insight was the prominence of rainfall in the decisionmakingprocessofthemodel asshowninfigure .this confirmed our stakeholders expectations as excess storm water is the main cause of cso events and helped them to trust the results.
however the dominant influence of rainfall leveled off not only when lime s explanations involved more features but also when shap was applied.
given that lime and shap achieved the samerecallandprecisionlevels thepatternsrevealedinfigure offers remarkable insights into whento focus on which features.
while weather forecast s rainfall could still be dependent upon in alerting csos to the relevant citizens hours ahead of the time paying additional and equal attention to sensor network data likevelocitycouldpotentiallyleave2 4hoursforthemsdgcto dispatchengineersonsitetoprevent alleviate orotherwisemanage the csoevents.basedonthe interviews we conclude finding3 xaitoolsof our casestudy especially limeandshap have the potentialto disrupt stakeholder expectationsof the influentialfactors ofcsos as well as to take justifiableactions to amelioratethe csosituation.
.
threatsto validity ourinquiryisanexploratorycasestudy aimedatinvestigating the contemporary cso phenomenon in depth and within its real life context.
we discuss some of the most important aspects that must be considered when interpreting our case study s results.
athreattoconstructvalidityisourchoiceofthethreexaitools.
asmentionedinsection .
ourtoolselectionswereguidedbythe considerations of being open source being compatible with our lstm solution and being easy to use for the stakeholders in the 1040esec fse august 23 28 athens greece nicholas maltbie nan niu matthewvandoren andreese johnson accuracy explainability maximize recallmaximize precisionfoster simplicityenhance soundnessembrace disruptiveness lime shap rulematrix figure10 softgoalinterdependencegraph sig forthexai tools.
the undirected lines represent goal decompositions informed by our gqm analysis cf.
figure .
the arrows representsoftgoalcontributions meansbreaks meanshurts meanshelps and meansmakes.
wastewater treatment organization even without our assistance.
to those ends we limited our scope to evaluate the xai tools as is without any further adjustment or customization.
another construct validity relates to our use of entropy.
insights provided by thexaitoolsprovideaddedvaluetostakeholdersandcanguidefuture development and improvements for cso prediction.although entropy directly measures variance and uniqueness and thus impliesmoreoninformationdensity wefoundittobeusefulwhen quantitativelymeasuring howcomplex aresult was.
athreattointernalvalidityconcernsthesizeofthereal world datasetsharedwithusbyourstakeholderorganization.ourdataset was limited in scale e.g.
the dataset was heavily biased towards thenegativeclass normalflow .wethereforeaugmenteditwith oversamplingtoeffectivelytrainthedeeplearningmodel.theaugmentationsmighthavehadunintendedconsequencesontheresults ofthexaitoolsasthiswasnotfullyexploredinourwork.more historicaldataofthecsositewouldhelpevaluateourassumptions ofour augmentations mitigating the threatto internal validity.
we believe our study s conclusion validity is high.
first and foremost we set out to overcome the data driven weakness of currentxaistudies .referringexplicitlytotherelevantliterature allowed our inquires to stay focused and our conclusions to be well grounded.
furthermore bias is mitigated by investigating xai tools not developed by the research team.
although the two domain experts are only a small sample in the field theyhaverealstakeinthepotentialchangestobeintroduced by deep learning.
last but not least we share our source code at andexpansion ofour results.
discussion .
satisficingexplainability explainability as an nfr discussed in section .
is satisficed inamatterofdegrees.basedonourgqmanalysisfromfigure andthe quantitative and qualitative results presentedin section webuildasoftgoalinterdependencegraph sig infigure .inthe sig eachofthexaitoolscontributeseitherpositivelyornegatively tothesoftgoals.fromfigure wenotethatnoneofthexaitoolsthat we investigated makes all positive contributions indicating thetoolsarealllimitedinsomeaspects.atoolcannothelpmeet somesoftgoalwithouthurtingsomeothers suggestingthetradeoffs among the softgoals.
interestingly figure 10does not reveal the well known trade off between recall and precision.
instead thelstm andhencelimeandshap achieved higherrecall and higher precision than rulematrix showing that rulematrix is less fit for making cso predictions.
however as of now lime and shap do not perform accurately enough to be adopted by our stakeholders.
onemightarguethekeytoimprovingaccuracyiswiththeunderlyingdeeplearningmodel namely lstminour case making explainabilityless ofa concern.
we argueconsideringexplainability even when accuracy levels are not ideal is still valuable.
for example aninterestingcontrastbetweenlimeandshapiswhere theinfluencefor cso predictionscomes from.although different theexplanationsprovidedbylimeandshaparebothvalidand showhowthesexaimethodologiesdivergeinoperation.thedeep learning model predicts events one hour into the future.
however limeprioritizesinfluenceofrainfallfrommanyhoursbeforeacso event whereas shap prioritizes influence from all features evenly immediately preceding a cso event.
deciphering the black box deeplearningmodels thoughwithvaryingdegreesofsatisficing explainability is of vital importance for ensuring public sector s transparency andaccountability.
.
data drivenexplainability as we drive our inquiry by explicitly referencing the explainability findingsfrom wecastourcasestudy sresultsinlightof therelevantliterature.lombrozo showedthatpeopledisproportionately prefer simpler explanations over more likely ones however lombrozo sworkwascarriedoutwithstudentsubjects who were not domain experts in the field.
through our case study we have seen that the hydrologist and the operational manager greatly favoredthe soundness of the explanations.it isalso worth mentioning that simplicity does not necessarily correlate with size i.e.
the number of features an explanation has .
by computing entropytomeasuretherandomnessoftheinformationcontained intheexplanations cf.figure weobservethatsoundnessand simplicity co exist inshap sexplanations.
thagard reportedthatpeopleprefertheexplanationsthat are consistent with their prior knowledge.
our domain experts conformedlargelytothagard sconjecture.theyconfirmedthexai tools outputs generally cohered with what they expected.
in some occasions we noticed that the xai tools explanations refuted our domainexperts expectations.theexpertskeptanopenmindset andwereabletofindnewinsightsfromthedataset.specifically the resultsofshapgavemoreinfluencetovelocitythantheexperts initially expected cf.
figure .
since shap mostly drew influence from right before the cso event cf.
figure they were able to reasonthatthisexplanationdrewuponinformationthattheymight have overlooked.
because of these observations lime or shap alonemight not be able to uncover the new insights.
in the sig of figure therefore it is the synergyof lime and shap that contributespositivelytowardthe embracedisruptiveness softgoal.
1041xaitoolsin the public sector a casestudyonpredictingcombinedseweroverflows esec fse august 23 28 athens greece requirements gatheringdata preparationmodel developmentmodel evaluationrevision and feedback figure simplified view of the software engineering process forai based systemsproposed by amershi et al.
.
.
softwareengineeringforai based systems animportantlessonlearnedfromourcasestudyisthatoneshall nottreatexplainabilityassomethingtoaddafteradeeplearning modelisbuilt.ourexperienceadvocatesstronglyforexplainability to be engineered throughout the deep learning project.
amershi et al.
breaksthesoftwareengineeringprocessformachinelearning into nine stages.
a simplified view of this process into four phases is shown in figure .
explainability is so broadly scoped that it influencedourdecisionsforeachphaseofthesoftwareengineering process.
requirementsgathering .
we interviewed our stakeholders to identify what needs to be explained and why.
the critical requirement of warning citizens about the cso risks helped us to better understand the role that xai might play inaccountability andtobetterbuildthedeeplearningmodel inmakingcsopredictions.
data preparation .wemadeafewassumptionsaboutthe dataandapplieddatacleaningandaugmentationbyinterpolatingdata points to synchronize the variousdata sources cf.tables 1 3 .understandingthecompositionofdatafrom each of the sensors and their preparation helped us better contextualizethe results ofthe xaitools explanations.
model development .
integrating xai into the deep learning model not only required extra effort but also led to performance decrements duetotheresources requiredtogenerateandvisualizeexplanations.fortoolslikerulematrix anadditionalstepwasrequiredtopredictcsosaccording to the generatedrules.
modelevaluation .explanationscanbeconsumedbymore thanjustairesearchersorlaypersons fromourwork we have found that domain experts cancontextualizetheseexplanations or use them to gain new insights into the task at hand.
the revision and feedback of figure 11may involve exploring different numbers of top features from explanationsprovidedbylime.additionalfeedbackcouldbelinked tootherphases.forexample theinsightgainedfromshap s resultsdiscussedinsection .3helpedelicitanewrequirement of using deep learning to inform engineer dispatch decisions 2 4hoursprior to alikely csoevent.
xaitoolscanbeintegratedintoallphasesoffigure 11asshown throughourcase study.they helped informdecisions throughout software development and can be integrated into pipelines as a method of verifying model performance or to diagnose issues and their causes.
as noted by zhang et al.
there is a need for identifying how and why deep learning models make decisions tosatisfyotherbroadlyscopedrequirements suchasfairness privacy androbustness.webelievetheseconcernsmustbeincorporated into all the phases of machine learning development and our case studyhasdemonstratedthefeasibilityofengineeringexplainability withstate of the artxaitools.
concludingremarks research on explainability of xai tools with domain experts using real worlddataissignificanttoensuringthetool sabilitytobeused with new topics.
our work is a proof of concept for how a gqm analysis might be used to assess various xai tools.
despite not developingan immediately applicable product fordomain experts we believe that this research can serve as a foundation to assist development of xai tools that are more useful for a broader set of stakeholdersbyprovidingaframeworkbasedongqmforpotential analysis.
inourexploratorycasestudy weemployedqualitativeandquantitativemethodstoevaluatethepredictions accuracyandtheexplanations simplicity soundness anddisruptiveness.throughcomparing thenumeric metrics and reviewing the resultsof thestakeholder interviews we are able to build upon existing psychological results and contextualize them with respect to the modern xai tools.
domain experts welcome new insights and more complex explanations with multiple causes.
our findings do not directly refute the work of lombrozo and thagard but rather build upon their work in noting that the different levels of complexity maybeappropriatefordifferentstakeholdersdependingontheir background .
to further expand upon this work explanations from more xai toolscanbeinvestigatedfornewinsightsandforsupportingdifferentsoftwareengineeringtasks e.g.
.futurework can also explore how to effectively efficiently dynamically and continuously present value added explanations of deep learning modeltostakeholders .additionally wewanttousemoredata to expand our case study and investigate how seasonal rainfall differences affect the xai results.
last but not least we seek to employ diverse empirical methods in our future work such as case studiesco designwithdomainexperts andtheoreticalreplications .
investigating explainability as a non functional requirementofai basedsystemsisanopenareaofresearch.theindividualmetricsand interviewquestions ofourstudyareonly first steps to spark adiscussionof howthey can be improvedfurther.