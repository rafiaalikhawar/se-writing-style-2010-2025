exploiting input sanitization for regex denial of service efe barlas purdue university west lafayette indiana usa ebarlas purdue.eduxin du purdue university west lafayette indiana usa du201 purdue.edujames c. davis purdue university west lafayette indiana usa davisjam purdue.edu abstract web services use server side input sanitization to guard against harmful input.
some web services publish their sanitization logic to make their client interface more usable e.g.
allowing clients to debug invalid requests locally.
however this usability practice poses a security risk.
specifically services may share the regexes they use to sanitize input strings and regex based denial of service redos is an emerging threat.
although prominent service outages caused by redos have spurred interest in this topic we know little about the degree to which live web services are vulnerable to redos.
in this paper we conduct the first black box study measuring the extent of redos vulnerabilities in live web services.
we apply theconsistent sanitization assumption that client side sanitization logic including regexes is consistent with the sanitization logic on the server side.
we identify a service s regex based input sanitization in its html forms or its api find vulnerable regexes among these regexes craft redos probes and pinpoint vulnerabilities.
we analyzed the html forms of services and the apis of services.
of these services publish regexes services publish unsafe regexes and services are vulnerable to redos through their apis domains subdomains .
both microsoft and amazon web services patched their web services as a result of our disclosure.
since these vulnerabilities were from api specifications not html forms we proposed a redos defense for a popular api validation library and our patch has been merged.
to summarize in client visible sanitization logic some web services advertise redos vulnerabilities in plain sight.
our results motivate short term patches and long term fundamental solutions.
make measurable what cannot be measured.
galileo galilei ccs concepts security and privacy denial of service attacks web application security general and reference empirical studies measurement validation.
keywords empirical software engineering regular expressions redos web security denial of service algorithmic complexity attacks both authors contributed equally to this research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
reference format efe barlas xin du and james c. davis.
.
exploiting input sanitization for regex denial of service.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction internet based web services play a major role in modern society.
by their nature web services are accessible through an interface and so they must handle input from users both legitimate and adversarial.
web services interpret string based inputs into appropriate types such as email addresses phone numbers and credit card information.
a common first line of defense is therefore to filter for reasonable looking input.
if this input sanitization is flawed the health of the web service can be compromised .
unfortunately a common input sanitization strategy exposes web services to a denial of service attack called regular expression denial of service redos .
many software systems rely on regular expressions regexes for input sanitization .
some of these regexes are problematically ambiguous and may require super linear time in the input length to evaluate in an unsafe regex engine .
at present most regex engines are unsafe in this regard .
the cost of regex processing combined with the use of regexes across the system stack can affect the availability of web services leading to regex based denial of service redos .
the redos problem has been considered from several perspectives.
theoretically the properties of problematic regexes under different search models have been established including both kleeneregular semantics and extended semantics .
in terms of the supply chain davis et al.
showed that up to of the regexes in open source modules are problematically ambiguous .
with respect to live services w stholz et al.
showed that problematic regexes in many java applications are exploitable and staicu pradel showed that of the node.js based web services they examined were vulnerable to redos .
however these approaches relied on implementation knowledge they could not be applied to an arbitrary web service.
prior researchers have not studied whether attackers can identify redos vulnerabilities in a black box manner.
if so the engineering community should prioritize adopting redos mitigations .
in this paper we describe the first black box measurement methodology for redos vulnerabilities .
we exploit software engineering practice examining a previously unstudied source of redos information the regexes that web services provide for use in clientside sanitization.
in the first step of our method we collect the regexes provided in html forms sampling popular websites and in apis using a directory of services with openapi specifications .
then we analyze them locally for problematic worst case behavior in a typical unsafe regex engine.
finally we ethically probe web services for redos vulnerabilities.
ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa efe barlas xin du and james c. davis a sanitization in html form.
regexes can be applied using an input field s pattern attribute and via javascript event handlers.
b sanitization in openapi based web api.
regexes are supported to encode string constraints in popular web api schema definition languages including openapi raml and api blueprint .
figure the use of regexes for client side sanitization in a web form and a web api.
our findings indicate that emerging software engineering practices on the web expose web service providers to redos .
based on a sample of n popular websites we report that web service providers do not reveal redos vulnerabilities through their traditional html forms.
in contrast web service providers reveal redos vulnerabilities through api specification documents.
in our study of a live web services with openapi specifications there were web domains of them document their input sanitization regexes and of these distinct subdomains are vulnerable to redos.
web service providers publish much more information about their sanitization practices in their api specifications than in their traditional html forms.
to summarize our contributions we use the consistent sanitization assumption to design the first black box redos measurement scheme for web services .
we identify redos vulnerabilities in several live web services .
comparing traditional html forms with the emerging approach of api specification we report that current api specification practices expose web service providers to redos.
we describe the responses of engineering practitioners to the vulnerabilities we identified .
.
we contribute to the engineering practitioner community through a pull request to a major web api input sanitization library.
the pull request has been merged and released.
we share a dataset of web sanitization regexes to complement existing regex datasets mined from other sources.
significance three aspects of our research contributions are significant.
first we establish the first black box measurement methodology for redos vulnerabilities in live web services.
second we use this methodology to identify insecure software engineering practices that affect a growing area of the web apis.
third we offer an anti redos patch that will benefit millions of dependent modules in the openapi ecosystem.
measurements drive change.
1see and background .
web services and web interfaces a web service is a software component server with which a user client can communicate over the internet via a uniform resource identifier .
common examples include the web services offered by youtube and amazon.
clients interact with a web service using its interface which is commonly defined in two ways a browserbased interface and an application programming interface api .
most web services offer browser based interfaces within their websites.
websites are built using technologies such as html css and javascript and displayed to clients through a web browser.
to make websites responsive web engineers provide interfaces such as search boxes and login forms to websites users.
those interfaces allow users to send data to websites servers to process the data and respond if needed.
to build such user interfaces engineers often use html forms as depicted in figure 1a.
while browser based interfaces target the general public some web services support an application programming interface api for automated interactions.
apis are software interfaces that describe how different pieces of software should communicate with each other.
through an api a web service provider can give a more formal description of how to interact with the service.
this description enables engineers to develop software that interacts with the service programmatically.
apis can be described with an informal text based document or with a schema definition language such as openapi raml or api blueprint cf.
figure 1b .
api semantics may be explicit or implied e.g.
using the conventional meaning of rest verbs to develop a rest ful api .
by their nature web services interact with untrusted clients.
it is therefore standard engineering practice to sanitize any client input whether it comes via a browser based interface or an api .
since the client controls this input sanitization ought always to be performed as part of the server s logic.
however some web services also publish input sanitization logic to their clients to reduce network traffic and give clients feedback about invalid requests.
884exploiting input sanitization for regex denial of service icse may pittsburgh pa usa figure depicts common forms of sanitization published to clients.
the html form in figure 1a illustrates the two ways to perform client side sanitization for html forms html based and javascript based .
using html based form validation engineers can enforce attributes on various html tags in html forms.
the attribute of interest in this work is the pattern attribute which lets an engineer specify the language of legitimate input.
for more sophisticated checks javascript based validation supports custom client side validation logic applied on a relevant event such as an attempted form submission.
just like html based form validation regexes can also be used in javascript based validation to check the validity of an input string.
meanwhile figure 1b depicts an openapi style api definition.
similar to html forms api schema documents may constrain request headers payload structure and field types and valid values.
these constraints may indicate enumerations numeric ranges string lengths and of interest in our study regexes prescribing string input languages.
client side sanitization can help legitimate users debug their requests e.g.
via feedback from the web browser or an automatically generated client api driver.
however malicious clients can bypass client side sanitization and send unsanitized content to the web service so services must sanitize again on the server side .
.
regexes and regex based denial of service our work measures a form of denial of service that web services risk as a result of sharing input sanitization regexes with their clients.
denial of service attacks a denial of service attack consumes the resources of a service so that legitimate access to the service is delayed or prevented .
there are many types of denial of service attacks varying in the resource exhausted and the exhaustion mechanism.
for example attacks might exhaust network resources e.g.
distributed denial of service or computational resources e.g.
algorithmic complexity attacks .
regex based denial of service redos is a denial of service attack that exhausts computational resources by exploiting the worst case time complexity of an unsafe regex engine.
unsafe regex engines a regex describes a language a set of strings .
to determine whether a string matches a regex membership testing is conducted by a system component called a regex engine .
most programming languages embed a custom regex engine for efficient interactions with the programming language s string encoding.
these regex engines support diverse features with complex semantics .
to reduce implementation and maintenance costs some regex engine developers chose designs that favor simplicity over safety they use a predictive parsing algorithm with backtracking .
the emphasis on simplicity comes at a cost this algorithm has high time complexity polynomial or exponential in the worst case.
the high time complexity of the standard regex engine algorithm is triggered by a problematic combination of a regex and an input string.
these regexes are super linear there are input strings w that incur time complexity super linear in the length of w. most regexes in this class are problematically ambiguous .
because they are ambiguous these regexes can match a string in multiple ways.
the typical regex engine s backtracking search algorithm will explore all potential matching paths before returninga mismatch.
when the number of paths or the cost of each path depends on the length w the result can be super linear time complexity.
figure illustrates an example of exponential behavior.
many researchers have proposed tools to identify regex input pairs with worst case polynomial or exponential behavior .
a ab figure this non deterministic finite automaton nfa corresponds to the regex a a b .
using the typical spencer algorithm viz.
a predictive parse with backtracking the search space can be exponential in the length of the input.
for example consider the behavior on input a...a length k .
the regex does not match this string the backtracking algorithm explores all 2kfailing paths.
regex based denial of service regex based denial of service redos exhausts computational resources by exploiting the algorithmic complexity of regex engines .
when client controlled input can trigger worst case regex behavior figure it can be harmful.
for example in stack overflow had a system wide outage due to redos and in cloudflare had a redos outage that affected thousands of its customers .
while redos vulnerabilities directly impact compute resources depending on the system design they may impact higher order resources.
for example many web services multiplex between clients e.g.
event handler threads and in these designs a redos attack will be more effective.
however even with near perfect client isolation e.g.
via aws lambda algorithmic complexity attacks like redos provide attackers with an asymmetric attack defense cost ratio to inflict economic damage .
slow reachable server side regexes are a security risk.
more formally a redos attack requires four redos conditions of a victim web service it accepts attacker controlled input it uses a server side super linear regex on this input it uses an unsafe regex engine and it has insufficient mitigations to insulate other clients from slow server side regex matches e.g.
timeouts .
although mitigations may reduce the service s redos risk they do not eliminate it .
therefore if a service meets conditions we consider it vulnerable to redos.
for example suppose a web service is built with the node.js framework.
if it has a reachable super linear regex then conditions and are met.
the regex may be evaluated on node.js s unsafe default regex engine condition .
its slow performance would then affect other clients due to client multiplexing on the node.js event loop condition .
.
prior empirical studies on redos the two previous empirical measurements of the extent of redos in practice have used a strong threat model that the attacker controls the input and also has server side implementation knowledge.
885icse may pittsburgh pa usa efe barlas xin du and james c. davis under that model w stholz et al.
identified redos vulnerabilities in open source java applications using program reachability analysis reporting that many java applications used reachable super linear regexes .
staicu pradel exploited knowledge of the open source javascript software supply chain to predict redos vulnerabilities in web services that use express the node.js server side framework of the services they probed had redos vulnerabilities .
although these studies document the risks of redos in practice their methodologies depend on knowledge of web service internals and are unsuitable to larger scale probing of web services in a black box manner.
finally studies by davis et al.
measured the extent of super linear regexes within the software module supply chain and do not shed light on web service vulnerabilities.
attack and research questions threat model our primary interest is to answer the question to what extent do redos vulnerabilities exist in live web services?
as discussed in prior work has measured the possibility of redos through module analysis and the presence of redos through white box analysis .
thus far we lack a methodology to measure the risk that redos poses to general black box web services.
we therefore assume the weakest reasonable threat model.
first we suppose the attacker controls only the input condition .
second we suppose the attacker does nothave access to the web service s server side logic.
under this threat model the primary difficulty is in identifying a reachable super linear regex to satisfy condition .2once such a regex is identified the attacker can tailor their input to the regex then use probes to experimentally determine whether conditions and are met.
when engineering teams evaluate their own services this threat model may be needlessly restrictive.
but it may imitate the perspective of engineers assessing the risks of incorporating a third party service or component into their product or that of adversaries penetration testers and security scanning as a service vendors.
sanitization based redos attacks given this constraint we propose sanitization based redos attacks.
as noted in while web services do not typically publish their server side implementations some of them do publish client side input sanitization logic.
we adopt the consistent sanitization assumption figure following engineering conventions the client side sanitization logic that a web service publishes is a subset of its server side sanitization logic.
this assumption implies that a super linear regex used in clientside sanitization logic will fulfill redos conditions and this regex will be applied to attacker controlled input on the server side.
if true redos vulnerabilities can be discovered by finding superlinear sanitization regexes in client side sanitization logic and then probing web services to test the remaining conditions.
research questions we conduct the first black box web measurement study of the extent of redos vulnerable web services in practice.
our operationalized research questions are rq1 how common is regex based client side input sanitization?
redos condition 2in the 2000s there were many cves for web services that allowed users to specify a regex to be evaluated on the server side condition and they used an unsafe regex engine.
such cves are now rare so we suppose a weaker threat model.
figure web service model.
for redos the attacker identifies a web service with a vulnerable sanitization regex then transmits data that a passes preceding constraints then b triggers the worst case behavior of the regex engine.
rq2 what proportion of these regexes would be super linear in an unsafe regex engine?
redos condition rq3 to what extent do these regexes exhibit super linear behavior in live web services?
redos condition rq4 how does the web service community mitigate these problematic super linear regexes?
redos condition research ethics constrain us from fully characterizing the extent of redos vulnerabilities with this method.
using a black box approach we can measure whether a service meets the first three redos conditions it evaluates untrusted input on a problematic regex with a slow regex engine.
however we cannot comment on the mitigations the service has in place whether architectural or runtime.
web services are opaque.
their mitigations are difficult to assess without launching a denial of service attack.
in addition because our method relies only on publicly accessible service information we cannot comment on the existence of hidden redos vulnerabilities such as those identified by staicu pradel via implementation inference .
with these caveats our method lets us measure black box web services for a new perspective on the risks of redos in the wild.
methodology our methodology is shown in figure .
given a web interface first we analyze its client side input sanitization logic to determine the input fields and the regexes applied to them .
.
next we identify any super linear regexes .
.
then we ethically probe the web service for redos vulnerabilities .
.
finally we interact with web service engineers to understand their perspectives .
.
.
web service selection as discussed in web services commonly offer two kinds of interfaces html forms and apis.
we measure redos vulnerabilities through both kinds of interfaces.
html form interfaces for html form interfaces we examine the forms within a random sample of domains from the top million domains according to the tranco top 1m ranking .
the tranco directory is a website popularity ranking designed to address shortcomings of the alexa list .
apis api based interfaces are less common than html form interfaces.
to obtain sufficient data we focused on the popular openapi schema description language for rest ful apis.
we determined popularity using web searches and github stars 886exploiting input sanitization for regex denial of service icse may pittsburgh pa usa figure overview of the study design.
openapi has 23k stargazers about three times more than the nextmost common api language api blueprint .
following prior work on specification measurement we used the apis.guru directory to obtain openapi documents.
.
rq1 input sanitization regexes in this part of the study our goal is to measure how frequently web services include regexes as part of their client side input sanitization.
to do this we build a list of live web services and the regexes they use in their client visible input sanitization.
as depicted in figure software engineers can impose similar input sanitization using the two interface types.
the mechanism for specifying this sanitization differs by interface type.
html form interfaces for html forms regexes may occur in two places .
first in the relevant form field the string language of valid input can be described by a javascript dialect regex using the html5 pattern attribute.
second javascript logic can be applied to form fields e.g.
on data entry or button press events.
this logic can impose constraints including regex tests.
table regex based sanitization in browser interfaces .
figure 1a also shows both types of html form regexes.
type functions html attribute pattern attribute js string string.
match matchall search js regexp regexp.
test exec the html forms that comprise a web service s browser interface may occur on any page.
we used the apify web crawler to crawl each website from its homepage and identify forms.
to balance our desire for detailed crawls with the need to not take resourcesfrom real users we used a maximum crawl depth of and fully crawled of the crawled web sites.
after identifying each form we determine the regexes it uses in its client side sanitization.
we define this set as any regex that is applied to any form field prior to sending form content to the server .
we statically extract the regexes given as form attributes.
we use a simple dynamic taint analysis to identify regex constraints in javascript logic.
first we monkey patch the client side javascript regex functions to log each regex string pair.
this is done by modifying those functions definitions in the browser so that each time the functions are called we have access to their input parameters.
then we drive a web browser via openwpm which is a software that can control browsers programmatically to populate form fields with unique values and simulate a button press.
the forms and buttons are detected by parsing the html code of each webpage.
we use a proxy to discard the resulting form traffic so that we do not spam the web service.
then inspecting the monkey patch traces we identify the regexes applied to each form field.
this may be a subset of the desired set our approach omits any regexes that are applied to the substrings of form fields e.g.
logic that splits an email and checks a property on the username.
unsatisfactory form field values may lead the browser to reject our form before our program instrumentation is triggered.
to reduce these cases we solve the constraints encoded within html form attributes e.g.
integer constraints directly and regex constraints using z3 although javascript based constraints may still fail.
apis for api based interfaces we have the same goal to identify the set of regexes that constrain client input.
for such an interface client input can appear in http headers endpoints query strings and request bodies.
in typical api schema definition languages including openapi engineers can set regex based constraints on string inputs.
we parse a schema and identify the regex es that constrain any string inputs referred to by at least one request schema.
.
rq2 super linear sanitization regexes our next goal is to measure the proportion of client side input sanitization regexes that present a potential redos vector.
we lack knowledge of a web service s server side implementation including its choice of regex engine.
a regex is a potential redos vector if it has super linear worst case behavior in some regex engine.
to identify a super linear regex we apply the ensemble of stateof the art super linear regex analyses supported by davis et al.
s tool vuln regex detector .
these analyses vary in their soundness and completeness so we dynamically test any potentially super linear regex in a representative unsafe regex engine.
davis et al.
found the java javascript and python regex engines were in the most unsafe class of engines .
although the java and javascript v8 regex engines have recently been optimized the python regex engine has not.
we therefore tested regexes in the python regex engine python v3.
.
.
we define a regex as super linear using the definition from algorithmic complexity theory when it exhibits a more than linear increase in match time in the input length w as we increase the number of pumps of the attack input strings.
we further distinguished the degree as high complexity and low complexity 887icse may pittsburgh pa usa efe barlas xin du and james c. davis figure measurement process for redos condition .
we begin with a web service interface a client input field and a super linear regex applied to that input on the client side.
as a reachability check we seek a valid status code from a proxy and then from the live web service.
after a probe sequence of treatment and control requests a decision tree follows.
depending on the number of pumps necessary to yield substantial matching times similar to davis et al.
.
.
rq3 use of unsafe regex engine by now we have identified interfaces in live web services that publish a super linear regex used on client input.
our next goal is to understand the proportion of these regexes that are actual redos vectors i.e.
testing whether the server uses these regexes in an unsafe regex engine.
.
.
measurement algorithm.
as depicted in figure our measurement algorithm attempts to reach the relevant logic in the interface s server side implementation and then identify linear vs. super linear regex behavior on the server side while avoiding actually conducting a denial of service attack.
we assume that these live web services follow standard http semantics in particular that if a web service responds to a client request with a success return code 2xx then the request was legitimate.
we assume such a request has passed all server side sanitization.
if the consistent sanitization assumption holds then this means that any client side regexes were also applied to the relevant input field s on the server side.
requests can then be sent to determine whether these regexes exhibit super linear behavior.
it is possible that the target regex can be reached even without a successful baseline request but it depends on the cause of the failure.
for example the target regex constraint might be applied before the failing condition.
thus if no successful request can be crafted super linear behavior may still be observed but if we observe lineartime behavior the results are inconclusive.
if we cannot identify a valid request we use an invalid one.
.
.
crafting a valid client request html form interfaces for html form interfaces we identified and satisfied the constraints embedded in html form attributes as part of rq1 .
.
to test constraint validity for these interfaces if a request reaches our http proxy then we conclude that it passed the client side sanitization.
we send requests to the web service using the python requests module.
apis for apis the first problem is reaching the target endpoint.
for example reaching an endpoint like home user id photos requires dynamic information a user id obtained from the web service.
after this we must satisfy the constraints associated with the fields of the endpoint in question among other limitations restler s fuzzing strategy may not satisfy the regex constraint that is present for these endpoints.
for reachability we implemented our api analysis as a checker plug in within microsoft s stateof the art rest api fuzzer restler version v7.
.
.
restler uses api conventions to determine dependency relationships between endpoints with a simple fuzzing dictionary to attempt to satisfy each endpoint s constraints.
once restler reaches the target endpoint our plug in is called to populate the request fields.
we usejson schema faker for this purpose and then populate the redos relevant field in the endpoint of interest.
to test constraint satisfaction for apis using restler with our plug in we use the prism mock server tool v4.
.
to validate requests and generate mock responses according to the openapi specification of interest.
prism returns codes in the 4xxrange if any constraints are missing.
we treat other codes as an indicator of satisfied constraints.
.
.
ethical redos probing.
we send probe requests by injectingprobe strings into a previously sent valid request or an invalid request if no valid status codes were obtained during the probing experiment.
these probe strings are assembled from templates produced by the regex analysis component.
the templates contain three strings a prefix suffix and a pump.
a probe string is a concatenation of the prefix one or more repetitions of the pump and the suffix and triggers the worst case behavior of a regex during a mismatch.
each additional pump increases the match time super linearly in an unsafe regex engine.
we devise a five stage probing experiment based on that of staicu pradel .
our overall goal is to identify treatment input strings that yield a 1second increase in response time relative to a comparable control string without causing substantial slowdowns for normal clients.
to that end we identify an initial set of treatment input strings with a range of matching times 200ms to 3s using the performance of the maximally unsafe python regex engine on our workstation.
these input strings should yield a small but measurable time difference in an unsafe regex engine.
we send preferably valid warm up requests to address response time noise caused by first time operations such as cache filling.
these use a valid request if we identified one else an invalid one.
for each timing configuration we send an experiment sequence of requests with the vulnerable field populated with a probe string and a control sequence of requests with that field populated with randomly generated strings of the same lengths these run in linear time .
both groups of requests are expected to fail at the same 888exploiting input sanitization for regex denial of service icse may pittsburgh pa usa stage of validation viz.
the regex constraint.
if the median roundtrip response time in the treatment group is substantially larger than in the control group we conclude that the web service being probed uses an unsafe regex engine.
specifically we look for a second increase in the median round trip time for the treatment group.
if the service is using an unsafe regex engine its server hardware or runtime timeouts may affect the actual response time relative to our predication.
if we observe a 5xxresponse code or a response time greater than seconds we halt the experiment to avoid harm and consider the regex engine unsafe.
conversely if the treatment group exhibits deviations but below the second threshold we manually explore a longer probe sequence.
we identified three known mitigations that can mask unsafe regex engine behavior under this protocol.
first server side rate limiting could delay our probes regardless of their content.
although rate limiting would presumably not cause the treatment control deviations that we measure we sent no more than request per second to account for this possibility.
second caching either of the validation outcome or of end to end results could cause only the initial query at each probe size to be slow.
we manually observed one case of this form.
third a recent approach can identify the signatures of anomalously slow regex input although we are not aware of applications of this technique in practice.
.
rq4 redos mitigation in this part of the study our goal was to understand the perspective of the web service engineering community on the use of superlinear regexes in server side input validation.
we assessed this constructively proposed mitigation as well as in a responsibly destructive manner vulnerability identification and disclosure .
constructively we assessed the state of redos mitigations in openapi based automatic client sanitization libraries.
we found an absence of mitigations proposed one and report on our findings.
destructively we contacted the owners of live web services for which redos conditions held cases where our experiments identified super linear regex performance in live web services.
since redos is a security problem we disclosed such issues to web service engineers using their documented route e.g.
thesecurity domain.com email for major companies.
we informed them of a super linear regex in their client side input sanitization presented the attack format and gave a minimal example.
we asked whether they considered this a security vulnerability in their service and what mitigations they had in place.
.
automating rq1 rq3 we automated most parts of this measurement process using existing tools as indicated.
we manually intervened when this automation failed.
this was particularly notable for the apis these services vary in the accuracy of the semantics that they encode in the openapi schema.
we intervened to repair schema syntax authenticate and supply values restler could not obtain e.g.
some resource id values or under documented constraints .
some interventions were guided by a service s error messages.
for one web service with a particularly complex api we used the official client sdk documentation and browser interface to craft valid requests to endpoints with super linear regexes.
results and analysis for security measurement purposes we are interested in understanding the extent to which a given web service is potentially vulnerable to redos attacks.
the attack surfaces in question are clear individual html forms and api endpoints.
however services may employ the same sanitization policy across multiple surfaces.
we present results aggregated by web domain as well as aggregated by subdomains where appropriate.
.
rq1 published sanitization information finding web services frequently use regexes to sanitize input on the client side.
of the reachable html form domains do so as do of the studied api domains.
table use of regexes in client side input sanitization.
domains and sub entities for html forms we report the number of web domains and web pages that apply client side regexes to any form fields.
for apis we report by domain and subdomain.
interface type domains sub entities html form .
.
api .
.
html forms we crawled domains sampled randomly from the tranco top 1m list.
through web crawling we found at least one web page for of those web sites.
our crawler failed on the remainder e.g.
blocked by the service and we omit them from the following statistics.
among the crawled domains the median number of pages per domain was and the median number of forms per domain was .
apis we obtained documents from apis.guru.
documents contained at least one operation with a regex validation constraint.
these documents corresponded to web services with unique domain names out of web services.
the median number of documents per domain and per subdomain are .
analysis we observed substantial variation in the number of unique regexes amongst the input sanitization regexes.
for html form s pattern attribute regexes there were total regex uses but only unique regexes.
substantial regex re use across websites is consistent with the findings of hodov n et al.
who examined the regexes parsed during browsing sessions.
this repetition may be the result of client side library or framework re use with the regexes originating in web frameworks or javascript libraries rather than independently authored by many engineers.
in marked contrast to the duplication of regexes in html forms in the api documents there were total regex uses and unique regexes.
for html forms we also note that most regexes were employed in javascript logic which was used by domains.
only of domains used the html5 pattern attribute in any form.
889icse may pittsburgh pa usa efe barlas xin du and james c. davis figure among the web services with super linear clientside regexes the percent of web services with any low and any high complexity regexes.
n refers to the total number of web services indicated by the lines pointing to the columns.
.
rq2 super linear regexes finding super linear regex usage varies widely by interface type.
among the domains with regexes in their html forms only use a super linear regex.
meanwhile among the regex using api domains domains use a super linear regex in at least one constraint.
html forms we identified super linear regexes on distinct web services.
each vulnerable regex appears on exactly one service.
three web services had a super linear regex on one page each and the other used a super linear regex on distinct pages.
apis we found super linear regexes on documents associated with domains spanning subdomains.
analysis since high complexity regexes are more severe than lowcomplexity regexes these service providers are exposed to different degrees of risk.
figure shows the distribution of super linear regexes by time complexity grouped by interface type.
.
rq3 live unsafe regex engines finding the presence of redos vulnerabilities varies widely by interface type.
our black box methodology did not identify any redos vulnerabilities from our analysis of html forms.
from the api analysis we identified domains 15subdomains that meet redos conditions they apply untrusted input to a super linear regex in an unsafe regex engine on the server side.
despite our automation the probing experiments required substantial manual intervention.
we chose to consider two kinds of equivalence classes domains and subdomains.
following the algorithm from figure once we reached a conclusive result in one of these equivalence classes we did not attempt other possibilities within the class.
we did this in two distinct phases once at the level of domains and once at the level of subdomains.
we probed each domain and subdomain aiming to reach a conclusive result for some super linear regex in their interface.
we began with candidate domains subdomains .
in two domains with unsafe regexes in their html forms we weren t able to obtain enough information for running probing experiments.
we identified zero redos vulnerable domains via html form analysis and six redos vulnerable domains through api analysis.
table summarizes our findings for the apis with super linear client side regexes.
we reached a conclusive outcome safe or unsafe regex engine on at least one probing experiment from domains subdomains .
the remaining five domains either did not respond to requests or required a paid subscription.
on domains subdomains at least one of our probe experiments were inconclusive.
we measured response time deviations i.e.
redos vulnerabilities for at least one super linear regex in domains subdomains .
notably of these domains are on the tranco top list and are major tech companies.
following our algorithm we concluded that there were safe services by subdomain these subdomains have at least one superlinear client side regex but we did not measure response time deviations on the server side.
however of these subdomains belong to domain a which also has subdomains which had measurable response time deviations.
a large company could have distinct policies at the organizational level which may manifest by subdomain as we observed.
length based mitigation davis et al.
reported that input length checks are a common safety measure for low complexity regexes .
we observed this during our experiments.
the adomain had many subdomains whose only super linear regexes were low complexity.
of the conclusively safe subdomains of ain table used only low complexity regexes.
the error messages from these subdomains indicated that our probe strings were too long.
table findings from our study of the apis with super linear regexes in their client visible sanitization logic.
columns represent anonymized domains.
service provider ahas subdomains with varying properties.
we measured response time deviations in domains subdomains .
we did not attempt to probe web services which had response time deviations indicating redos caused by high time complexity regexes.
sl super linear.
n a domain does not have a regex of this type.
metric a b c d e f g number of subdomains subdomains with sl behavior subdomains with high complexity sl behavior n a failed experiment subdomains with low complexity sl behavior did not attempt n a n a n a conclusively safe subdomains 890exploiting input sanitization for regex denial of service icse may pittsburgh pa usa .
rq4 redos mitigation finding the maintainers of openapi middleware tools are concerned about redos and interested in eliminating this possibility.
it is unclear whether individual web service providers consider the vulnerability a threat.
.
.
redos mitigation for openapi.
all of the redos vulnerabilities we identified through our black box methodology came from apis not web forms.
we therefore investigated a mitigation for the openapi ecosystem.
one benefit of api specifications is that client and server side code can be generated automatically.
in openapi two popular code generation tools are swagger codegen and openapi generator in use by dozens of companies .
among other features these tools can generate server stubs with input validation code followed by a fill in the blank for the business logic.
their generated code which includes regex checks can expose their dependents to redos.
table popular code generators and input validation tools for openapi based apis.
data as of february .
gh stars the number of github stars.
name gh stars contributors swagger codegen 14k 1k openapi generator .2k 2k these tools do not address the risk of redos for their dependents.
their documentation does not discuss how the code for regex patterns is generated.
according to our tests these tools generate code in the target programming language and use the default often unsafe regex engine in that language.
one tool s documentation mentions the risk of redos but places the burden on the specification engineer to avoid or mitigate such regexes.
they do not allow users to tune this logic e.g.
to choose a safe regex engine.
to eliminate this risk we proposed a patch to the ajvtool.
ajv is used by openapi generator to validate requests and has several million dependent packages according to github.
our goal was to allow software engineers to choose the safe regex engine re2 instead of the built in programming language regex engine.
ajvis sometimes used in client side contexts so the engineering team prioritizes a small binary.
adding the node.js bindings for re2inajv s dependencies would more than double ajv s unpacked binary size from .02mb to .31mb.
our patch therefore used the factory pattern allowing users to inject another regex engine as a dependency at runtime.
the ajvengineering team reviewed our patch and included it in release v8.
along with documentation outlining how the patch should be used to eliminate risk of redos caused by input sanitization.
our patch will allow the millions of ajvdependents to eliminate this form of redos in their applications.
.
.
responses to redos vulnerability disclosures.
as described in .
we disclosed possible redos vulnerabilities to live web service providers who met redos conditions .
to summarize the responses four service providers did not respond one major technology company acknowledged andrepaired the disclosed vulnerability one major technology company initially told us that they did not perceive a vulnerability but after several months have informed us that they have patched the unsafe regexes.
ultimately both microsoft and amazon web services made changes to repair their unsafe regexes.
overall the sample size is too small for comment.
discussion should web service providers prioritize redos mitigations?
several research communities have investigated the redos problem including from empirical software engineering systems cybersecurity and theory .
this research investment has somewhat shaky motivation crosby s proposal of redos case studies of regex induced service outages and three empirical measurement studies .
our study provides a new perspective large scale black box measurements of redos risks using a weak threat model .
.
.
our findings establish the first systematic and empirical evaluation of redos risks of web services without an assumption of their frameworks.
our measurements indicate that many web services are safe from redos under this threat model.
in particular for web services whose interfaces are traditional html forms few sanitization regexes are revealed on the client side and these regexes are not superlinear .
.
.
in contrast web services that publish apis face more risk of redos.
in publishing api specifications web services are choosing to reveal more about their server side sanitization logic.
however the cause is unclear.
we conjecture two explanations for further examination.
first the choice may be deliberate.
software engineers may be providing a fuller definition of their input validation constructs in their api specifications so that code generation tools can be used to automatically handle input validation.
alternatively it may be accidental.
software engineers may be using tools which generate api specifications from code similar to model extraction rather than writing specifications first and then generating code from them.
this process may be inadvertently exposing internally used regexes which could explain the greater regex variety and greater incidence of redos among api regexes.
a visibility security tradeoff our measurements indicate a tradeoff between visibility and security .
.
web service providers who promote usability by specifying the nature of valid input may expose themselves to redos.
although this class of attacks could be mitigated by hiding the sanitization rules software engineers should not seek security through obscurity .
indeed describing the characteristics of valid input is necessary to enable communication.
software engineers should not need to choose between visibility and security.
rather than obscuring input formats the software engineering community would benefit from principled solutions to redos.
davis et al.
described several sound solutions and they concluded that making regex engines safe seemed like the most natural mitigation .
further research into adopting safe regex engines or retrofitting existing unsafe engines will improve the safety of software engineering practice.
middleware for input sanitization can use a level of indirection to select a safe regex engine.
in our mitigation study .
we found that middleware providers were 891icse may pittsburgh pa usa efe barlas xin du and james c. davis happy to accept such a change.
however they were concerned with introducing an external dependency on a safe regex engine indicating that improving the safety of programming language regex engines should be an area of focus for a long term solution.
using api specifications has many advantages from a software engineering standpoint.
specific to redos the standardization they provide allows engineers to use specification compatible middleware tools.
hence any security patches to these tools can protect many web services at once.
we took advantage of this centralization to provide redos mitigations to the numerous dependents of such middleware tools.
the same property that lends itself to easy exploitation also lends itself to a centralized repair.
mismatch between openapi sdl and needs in practice ideally an interface specification should describe everything necessary for successful communication.
however during our measurements we observed that most openapi documents were underspecified.
our requests which passed the specified validation constraints according to the prism tool were still invalid according to the server.
we identified several causes of these underspecifications.
one of these causes is well known the openapi syntax cannot indicate dependencies between requests.
although restler attempts to infer these dependencies using heuristics it cannot handle all cases.
we encountered several more causes during our experiments.
some api documents indicate the full set of possible payload variables but actually accept subsets of those variables.
some parameters are interdependent coupled e.g.
including one optional parameter requires including others .
some parameters of string type are actually type aliased within the web service e.g.
strings that represent a comma separated numeric sequence.
these various missing semantics are of practical utility and the maintainers of api specification languages should consider supporting them.
related work our work descends from two lines of research web service vulnerability scanning and regular expression engineering.
we employed a black box probing methodology to scan web services for a specific class of security vulnerability.
other researchers have employed grey box and white box methodologies for this vulnerability .
researchers and commercial tools offer blackbox and grey box scanning for diverse vulnerabilities including algorithmic complexity vulnerabilities service crashes cross site scripting xss and sql injection .
notably some researchers have pursued the opposite of our consistent sanitization assumption to identify cases where backend sanitization appears problematically inconsistent .
while our study focuses on regex cybersecurity researchers have considered other aspects of the regex engineering lifecycle.
michael et al.
reported that many software engineers find regex engineering difficult .
to assist the engineering community in this domain researchers have recently described regex engineering practices related to composition comprehension and testing identified common regex bug patterns and taxonomies and proposed tools to support regex comprehension testing and repair .
there has also been a longstanding effort to automatically compose regexes with diverse approaches including formal methods evolutionary algorithms optimization crowdsourcing natural language translation and human in theloop interactive development .
threats to validity this paper describes a substantial web measurement study covering two distinct interface types.
we acknowledge a variety of threats to the validity of our findings and note mitigating factors.
construct validity the primary threat here is in our definition of a browser based web interface via html forms.
while html forms appeared in of the web services whose html form interfaces we crawled trends such as single page applications process user input without form submissions.
this is also a threat to external validity as we cannot comment on the risk of redos for such web services.
beyond this construct we relied on definitions of super linear regexes and regex based denial of service.
these concepts are well established in the research literature and we measured them with state of the art tools and probing methodologies.
internal validity our methodology does not let us measure the degree of a redos vulnerability.
we identified super linear regexes that are applied to user input in an unsafe regex engine on the server side.
however we cannot assess redos condition redos mitigations without either having server side knowledge or launching a full scale denial of service attack.
to shed light on this threat in .
we discussed perspectives from the web service engineering community.
there are three potential sources of under reporting in our probing methodology.
first as noted in .
some web services cache end to end results and this caching will mask worst case behavior when we use identical worst case probe strings.
second we conservatively chose input lengths for our probes based on the slowdowns observed on a workstation grade machine.
if a web service provider processes input on a server class machine the response time deviation induced by our probes may not be observable.
finally our decision tree yielded inconclusive results in cases figure .
external validity our goal was to measure the extent of redos vulnerabilities in live web services.
the populations we used may have biased our results.
we probed web services that were listed in directories of live web services from the tranco top 1m directory for html form interfaces and from apis.guru for openapi interfaces.
for html forms we randomly sampled from the top million web domains for html forms.
we expect these results to generalize to other popular websites popularity may be correlated with a certain caliber of engineering in order to service the client load and so our results may not generalize to less popular websites.
a related bias is that .
of html form services rejected our connections outright because we were using vms from google cloud platform for our experiments.
for apis we considered all api specifications from apis.guru .
this directory only contains openapi specifications from distinct domains distinct subdomains .
different results may emerge from studying other api specification directories e.g.
swaggerhub or by mining github.
we performed a preliminary analysis on swaggerhub specifications and found that often they are simpler and are not associated with a live web service url.
echoing wittern et al.
we suggest that 892exploiting input sanitization for regex denial of service icse may pittsburgh pa usa the results we obtained from apis.guru may be more representative ofengineered openapi specifications.
beyond limitations in our sampling generalizability is threatened by our black box methodology.
our approach depends on the consistent sanitization assumption that web services are consistent in their input sanitization i.e.
that client visible input sanitization is also applied on the back end.
this assumption permits a scalable black box approach.
however without full knowledge of serverside logic we may omit server side regex evaluations that are not exposed to clients.
web services may apply additional or alternative sanitization on the back end.
for example the redos vulnerabilities identified by staicu pradel would likely not have been discovered using our methodology since they targeted regexes that would only be used server side in http header processing.
conversely client side sanitization gives us insight into business logic regexes that might not be visible through examination only of the open source software supply chain as davis et al.
and staicu pradel did.
our findings thus complement the prior empirical studies of the risks of redos in practice.
future work transferring redos vulnerabilities software engineers solve similar problems in similar ways .
a general version of the consistent sanitization assumption is possible that web services validate similar content in similar ways so sanitization logic revealed by one service may be transferred to another.
for example suppose two web services use an accessibility feature like aria labels to label a form field as an email.
if one service provides client side sanitization logic similar logic might be in use by the other.
why are api practices more dangerous?
in answering each research question there were marked differences between redos risks in traditional html forms as compared to the emerging approach of api specification.
we conjectured two causes providing detailed api specifications to ease the development of input validation logic and inadvertent exposure resulting from api extraction from server side code.
we believe this finding bears further investigation.
improved tooling although we chose restler to help us reach endpoints in complex apis we eventually performed manual intervention for most of the apis we probed.
in practice api specifications underspecify valid interactions.
when we intervened we consulted api documentation as well as the service error messages.
incorporating nlp techniques into automated api interactions is a natural direction for improved black box web service testing .
regex dataset previous researchers have collected regex datasets from open source software repositories with applications including improved regex usability tools and safer regex engines .
to complement this effort we contribute a dataset of web input sanitization regexes.
this dataset contains the unique regexes identified during our experiments.
conclusions regex based denial of service redos has received much recent attention.
web service providers are curious about the degree to which redos threatens them and regex engine maintainers wonderwhether they should prioritize optimizations to ameliorate redos.
in light of this interest we report the results of the first black box measurement study of redos vulnerabilities on live web services.
our method is based in the observation that server side input sanitization may be mirrored on the client side as part of usability engineering.
we therefore examined the extent to which super linear regexes on the client side can be exploited as redos vulnerabilities on the server side.
we compared two common interface types html forms n 000domains and apis n 475domains .
we report that although client visible regexes are common in both types of interfaces super linear regexes are only common in apis.
we identified redos vulnerabilities in the apis of domains subdomains including in services operated by major technology companies.
our findings add weight to the concerns of researchers about the risks of redos in practice.
specifically we show that the movement toward api specification development provides leverage for redos attacks.