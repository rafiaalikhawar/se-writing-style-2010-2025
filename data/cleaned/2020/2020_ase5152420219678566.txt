efficient state synchronisation in model based testing through reinforcement learning 1sturaz cengiz t urker the school of computing and mathematical sciences the university of leicester leicester uk u.c.turker leicester.ac.uk2ndrobert m. hierons department of computer science the university of sheffield sheffield uk r.hierons sheffield.ac.uk3rdmohammad reza mousavi department of informatics king s college london london uk mohammad.mousavi kcl.ac.uk4thivan y .
tyukin the school of computing and mathematical sciences the university of leicester leicester uk i.tyukin leicester.ac.uk abstract model based testing is a structured method to test complex systems.
scaling up model based testing to large systems requires improving the efficiency of various steps involved in testcase generation and more importantly in test execution.
one of the most costly steps of model based testing is to bring the system to a known state best achieved through synchronising sequences.
a synchronising sequence is an input sequence that brings a given system to a predetermined state regardless of system s initial state.
depending on the structure the system might be complete i.e.
all inputs are applicable at every state of the system.
however some systems are partial and in this case not all inputs are usable at every state.
derivation of synchronising sequences from complete or partial systems is a challenging task.
in this paper we introduce a novel q learning algorithm that can derive synchronising sequences from systems with complete or partial structures.
the proposed algorithm is faster and can process larger systems than the fastest sequential algorithm that derives synchronising sequences from complete systems.
moreover the proposed method is also faster and can process larger systems than the most recent massively parallel algorithm that derives synchronising sequences from partial systems.
furthermore the proposed algorithm generates shorter synchronising sequences.
index terms model based testing synchronising sequence reinforcement learning q learning i. i ntroduction model based testing is a rigorous method for validation and verification.
the application of model based testing to areas such as autonomous reactive systems has been further facilitated in recent years by advances in model learning techniques and their extensions to adaptive and evolving systems .
the industrial scale application of model based testing in these challenging areas requires the scaling up of various parts of the underlying techniques.
many systems are state based they have an internal state that affects behaviour and is updated by events and operations.
when testing a state based system one typically needs to apply a number of test sequences or adaptive test cases from the initial state of the system under test sut .
as a result the state of the sut is normally reset to this initial state between test sequences.
sometimes it is straightforward to reset the state of the sut by for example turning the system off and then on again or issuing the reset signal provided by the manufacturer developer.
however the process of resettingthe sut between test cases can be one of the most timeconsuming parts of test execution and there may be no simple approach to resetting the sut there may be no reset input or the use of a reset input may be infeasible.
in such circumstances the reset operation is implemented through the use of synchronising sequences sss .
sss are used to bring the underlying system to a specific state to resume testing with a new test case .
in such cases the length of sss affects the cost of test execution.
this motivates the work in this paper which utilises reinforcement learning in the generation of short sss.
the development of efficient techniques that produce short sss will contribute to the agenda of producing systematic mbt techniques that scale to large models.
in addition to model based testing synchronising sequences are also heavily used in motion planing of robotic controllers such as orienting parts on assembly lines .
moreover sss have also been studied in automata theory and genomics and form an active area of research .
a. problem statement related work the underlying transition function of a reactive system test model can be complete orpartial.
in a complete model all inputs of the system are applicable in each of its states.
the development of efficient methods to generate short sss from such a model is very appealing as the use of shorter sss reduces the cost and time of test execution.
however the problem of generating a minimal ss from a complete model isnp hard .
even though there are several methods for generating an ss the initial step of all these methods is the construction of a product automaton a data structure that requires quadratic space with respect to the number of states of the model.
all existing methods use this data structure as the basis of different approaches heuristics to derive short sss.
amongst these the fastest known algorithm requires o n3 kn2 time where ndenotes the number of states and kdenotes the number of inputs of the model.
to address the computational cost recent work introduced many core and multi core approaches for deriving short synchronising sequences from complete systems.
however 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee it is relatively difficult to implement these methods as they require i proficiency in parallel programming and programming general purpose graphics processing units gpgpus and ii sophisticated graphics cards and cpus.
in a partial model not all inputs are applicable in all of the states of the model.
for such models the problem of deciding whether there is an ss is pspace complete .
as a result the current focus of research is merely related to the derivation of sss.
in this line of research the first published algorithm was based on constructing a breadth first search tree which requires exponential space and time and hence is not efficient .
the only known efficient algorithm checks all input sequences that are not longer than some upper bound.
experimental results show that a combination of a relatively powerful gpu card and cpu can process systems with 000states and 10inputs.
however the performance of the algorithm degrades as the number of inputs increases.
moreover this method also requires i solid expertise in programming general purpose graphics processing units and ii a powerful graphics card.
finally in the authors introduce an algorithm that uses answer set programming to generate sss from complete systems.
as the method requires a conjunctive normal form formulation of the system and the ss generation problem it is not scaleable to large systems with many states.
given recent breakthroughs in the reinforcement learning rl spectra the objective of the work presented in this paper is to use rl as the basis for novel algorithms that efficiently find short synchronisation sequences.
rl defines a family of machine learning algorithms and is becoming a major tool in computational intelligence .
in rl computers agents make their own choices take actions in a given environment without having prior information or labelled data .
recently the use of rl in test generation has received significant attention and we now briefly review a few examples.
rl has formed the basis of an algorithm to automatically generate test cases for android applications with the aim of improving code coverage and has also been used in mutation testing to predict whether a mutant will be killed by a given test suite without incurring the cost of executing the mutant .
rl has also been used in graphical user interface gui testing .
in online testing test inputs are derived during test execution.
it has been shown that rl can be used to address the associated problem of optimizing the choice of test actions to reduce test costs .
rl has been used to learn a behaviour model of the system under test to aid risk based testing .
three rl algorithms have been proposed and embedded in evosuitefit to support hyperheuristic search based test generation algorithms .
rl has also been used in security testing.
for example researchers have developed an rl based testing algorithm that trains dishonest agents to reveal dangerous behaviours of autonomous cyber physical systems .
an rl based test generation technique has also been devised with the aim of increasing hardware trojan detection accuracy .this paper usesq learning which defines a family of rl methods in which the actions taken by an agent are decided according to quality values q values .
q learning has become the basis of many rl algorithms because unlike other methods q learning is simple and exhibits excellent learning facilities .q learning techniques have recently attracted significant attention with the introduction of deep qnetworks dqns .
although q learning algorithms are promising it is still challenging to use them in our setting.
the initial qlearning algorithm has good convergence properties however it requires tables or matrices to hold q values.
when the agent interacts with the environment a single q value on the table is updated.
so this simple setting introduces at least two problems i in circumstances where the environment is complicated critical states might not be experienced learned generalisation problem and ii because the underlying data structure is a table it leads to a large amount of storage space being required.
therefore for complex learning problems with large complex environments it is difficult to achieve effective learning by using the tabular qlearning algorithm.
function approximators have been used to overcome generalisation problem as an alternative to keeping qvalues .
b. research questions in light of these findings the research questions studied in this paper are summarised as follows rq is our proposed q learning algorithm more efficient and scalable than the state of the art algorithms?
we further refine this research question into the following sub questions pertaining to the execution time scalability and memory usage respectively rq .
is the growth of the execution time more modest in the q learning algorithm?
rq .
how scalable can a q learning algorithm be?
by fixing the amount of memory and execution time can a qlearning algorithm generate sss from specifications that cannot be processed by the state of the art ss generation algorithms?
rq .
how does the memory usage behaviour of the qlearning algorithm compare to state of the art ss generation algorithms?
rq is our proposed q learning algorithm more effective in generating shorter synchronising sequences for a larger class of system models?
we also refine this research question into the following sub questions pertaining to the length of the generated synchronising sequence and the types of systems handled by the algorithm respectively rq .
does the q learning algorithm generate shorter sss relative to execution time and memory consumption than the state of the art ss generation algorithms?
rq .
can theq learning algorithm be extended to mitigate the above mentioned generalisation problem?
c. contributions in this paper we report what is to the best our knowledge the first application of reinforcement learning to deriving 369state synchronisation sequences in order to make model based testing more efficient.
in particular we propose a sequential q learning reinforcement learning algorithm that derives synchronising sequences from partial and complete systems.
regarding deriving sss using machine learning apart from the work surveyed before we are aware of only one closely related publication .
in this work the authors report a deep learning method to predict the length of the ss without generating it.
we however aim to generate the ss from a given specification.
regarding our novel q graph formalism we are aware of two related pieces of work that used different types of data structures in theq learning setting to ease the generalisation problem.
in one approach the markov decision process mdp search space has been represented as a kdimensional tree which makes it possible to keep unrelated part of the environment in a database.
this reduces the memory cost of the rl algorithm.
a second piece of work shows how the steps taken by an agent can be represented as a graph.
this graph is then used to aid exploration exploitation dilemma using a shortest path algorithm.
none of these methods however employ a similar technique to ease the generalisation problem as the presented method.
ourq learning framework unlike the tabular setting does not require prior knowledge of the entire search space.
instead it explores and constructs the search space as the agent interacts with the environment.
therefore with this framework we are allowed to apply the q learning method to problems that possess a very large search space using a limited memory space.
for example the classical tabular q learning method would require 2nrows and in total x 2ncells for a given system with nstates andxinputs.
through empirical evaluation we show that the proposed algorithm is superior to the state of the art sequential and parallel algorithms the proposed algorithm can generate shorter sss and times faster and can process times larger specifications than the state of the art sequential parallel algorithm.
to ensure the scalability of our approach we retrieved the specification of an industrial scale system to manage the engine status of oc e printers and copiers a subsidiary of canon .
the specification has states and 77inputs it has been developed in an industrial context and has been formally cross checked against the design documents of the system from which the implementation code is automatically generated .
our method could derive an ss from this specification in less than a second where other state of the art ss generation methods would not terminate in an hour.
our proposed algorithm serves as a basis for future efficient algorithms for large state space exploration that cannot be explored exhaustively.
d. organisation of the paper the paper is organised as follows.
in the next section we provide the terminology and notation regarding reactive systems andq learning used throughout the paper.
this is then followed by a section in which we explain the proposedalgorithm.
in section iv we present the experimental subjects conducted experiments and their results.
later in section v we discuss the results.
finally section vi draws conclusions and discusses some future research directions.
ii.
p reliminaries a. automata we use the standard notation a hs hito denote an automaton.
here sis a set of states is a finite input alphabet andh s sis the set of transitions.
the set of all input sequences is represented by ?.
we letjs0jdenote the number of elements in a given set of states.
we let dom a denote the set of pairs s x 2s such that there exists a transition leaving state swith inputxand we refer it as x is defined in state s .ais said to be completelyspecified ifdom a s and otherwise a is partial.
a transition of ais represented by tuple s x s0 2h wheresis the starting state s0is the ending state and xis the input label of the transition .
an inputxis said to be defined at statesif and only if s x 2dom a. otherwise xisundefined at states.
awalk of an automaton ais a sequence s1 x1 s2 s2 x2 s3 si xi sj sj xj sk of consecutive transitions ofa.
the input sequence of this walk !
x1x2 x ixjis called its trace we use to denote the empty sequence.
we can extend the notion of a defined input to defined input sequences as follows.
an input sequence !isdefined at a given set of statess0 sif one of the following is true i !is an empty sequence or ii for any prefix !1of!
with!
!
!
!
is defined at s0and!2is defined at the states reached from s0using!
.
let 2s ?
!2sbe a map specifying the set of states which can be reached using an input sequence from a given set of states.
let !
x!0be an input sequence.
if for a given s2s0 !is not defined then s0 !
is also not defined.
otherwise the delta function can be defined in a recursive way s0 s0 s0 x s2s0 s x and s0 !
s0 x !
.
if for some pair of states s s02 s0 s !
s0 !
then!is said to be a merging input sequence mis .
if8si sj2s si !
sj !
then!is called a synchronising sequence ss .
in figure we provided the specification for the ceiling speed monitoring with service brake intervention sbi from .
note that this specifications is partial.
this can be completed by adding self loop transitions that label missing input and this completion method is well known in this field .
note that an input sequence ss1 ?c3?c5?c6?c7 resets the automaton to state normal i.e.
s ?c3?c5?c6?c7 fnormalg.
while testing a given system we need to apply a number of test sequences all of which start from the reset state normal in this case and there is a need to reset the system under test between the execution of these test sequences.
thus the length 1we did not draw the added transitions in figure .
370of the ss has an impact on test execution time and shorter sss are preferable.
the automaton asbi given in figure has another ss ss2 ?c3?c0?c7 that also resets the system to state normal .
sincess2is25 shorter using ss2is preferable for testing.
for example consider the classical test generation method thewmethod .
asymptotically the wmethod can generate a test suite with jsj jsj j j elements.
if we use ss2instead ofss1then we would save jsj jsj j j inputs during testing an implementation of automaton asbi.
normal overspeedsbrake warningebrake?c0?c1 ?c2 ?c3?c0?c5?c0?c6 ?c7?c1 fig.
automaton asbi withs fnormal warning overspeed s brake e brakeg f?c0 ?c1 ?c2 ?c3 ?c5 ?c6 ?c7g.
b.q learning environment q learning algorithms can operate on a markov decision process mdp .
an mdp is defined with a tuple p s a p r where sis a set of states ais a set of actions p is the probability function in the form of p s0js a that is for each actiona2a and states2s it defines the probability of reaching states02sandris the immediate reward function in the form ofr s a s0 i.e.
it returns the reward received after transitioning from state stos0with action a. theq learning is a value based reinforcement learning method which is used to find the optimal policy when state transition probabilities pareunknown for a given mdp p s a p r .
instead of estimating these unknown probabilities the method uses a value function q .
the value functionqis recursively defined as q s a q s a fr s a arg max a0q s0 a0 q s a g ifs current state and an actionais executed no change otherwise wheres2 s a2a andq s a is the value of applying actionaat states r s a is the immediate reward received after applying action aat states is the learning rate and is the future reward discount factor.
and are values within the range .
theq learning algorithm asymptotically reconstructs the true expected discounted reward and as a result workstowards recovering the optimal policy.
in this respect policy selection based on q learning can be viewed as an off policy temporal difference control algorithm which asymptotically approximates the optimal policy .
iii.
t he proposed algorithm a. fromq tables toq graphs and problem formulation the naive implementation of the q learning algorithm relies on aq table which holds the q values for each of the states of the underlying environment i.e.
mdp p .
this is a drastic improvement over cases where complete knowledge of the transition probabilities pis required.
the use of a q table is straightforward if the agent wants to learn the q value of a states2sit just reads the information from the table after it computes its index.
however consider a scenario in which the agent does not require the whole q table while learning the environment.
in such a case we may not have to store the q values using a preset table but instead we can use a directed graph.
using such an approach we let the graph grow as the agent discovers states on the fly.
this method will reduce the memory requirements of the algorithm in situations in which the agent can find an optimum policy based on only a portion of the state space which is the case in our application domain.
aq graph has a finite set of nodes n q nodes such that each noden2nis associated with a state of the environment say mdp state s ns and aq value q s a for each action a of p. intuitively in the set of admissible edges in aq graph there exists an edge labelled with action afrom q nodentoq noden0if and only if one can reach ns0from nsusing an action a. finally we represent the ss construction problem as an mdpp.
an element s0of the power set of s s02pow s corresponds to a state s02s of the mdp pand each input x2xofais an action a2a ofp.
we use one to one and onto functions to denote corresponding inputs and set of states i i maps an input x2 of the automatonato the corresponding action a2a of the mdp pand vice versa and ii st maps a set of states s02p s of the automaton ato the corresponding state s02sof the mdppand vice versa.
for eachsanda we let p s0js a if and only if st s i a st s0 otherwise that is inpthere exists a transition from a given state sto another states0labelled with action aif and only if st s i a st s0 .
the immediate rewards r s a are computed as follows lets0be the next mdp state such that st s i a st s0 .
r s a ifjst s j jst s0 j nan ifi a is undefined for st s jst s j else jst s j jst s0 j .
the above formulation introduces a heuristic that helps to break ties when we have a set of merging inputs.
the heuristic 371step considers the number of merged states and promotes the input that is causing more states to merge.
note that when js0j is equal tojsj the reward is this is a step to prevent the agent from introducing redundant inputs to ss.
moreover with immediate reward nan the proposed algorithm can derive sss from partial systems without worrying about constructing sss with undefined inputs.
however the algorithm may generate longer sss when the underlying system is partial.
this is due to the fact that the length of a shortest ss for a complete system is bounded by o n2 and for a partial system the bound iso n2 4n .
finally note that the for a given ss construction problem instance the constructed mdp is finite.
we formally state this property in the corollary below corollary .
letabe an automaton the mdp pconstructed fromafor constructing an ss is a finite mdp .
b. the algorithm before going into the details of the algorithm we will introduce some basic concepts that the algorithm uses.
the proposed algorithm uses an greedy approach .
using this approach we addressed the exploration and the exploitation trade off.
with probability the agent chooses inputs randomly.
otherwise it selects inputs according to the q values which are computed using the standard q value function given in formula where the next state s0 is computed using the function i.e.
s0 st s i a .
the learning rate and future reward discount values were both set to i.e.
and .
the algorithm receives an automaton aand an upper bound on the number of episodes3 e as its inputs.
then it constructs theq graph.
the construction of the q graph is done by introducing a single q node a node that is associated with the set of states ns sofawith randomq values.
lines of algorithm .
the graph then gradually grows by introducing new nodes while the agent explores the environment.
once the initial node has been created the algorithm initialises the graph with an empty input sequence and a pointer to the initial node initialnode that will be used when the algorithm wants to reach the first node of the q graph i.e.
when it picks an undefined input.
the algorithm also sets the episode counter to0 line of algorithm .
afterwards it enters a loop that ends when i it finds an ss i.e.
it reaches a node n0such thatjn0 sj or ii the maximum number of episodes has been reached.
note that in a finite mdp theq learning algorithm converges after a number of episodes i.e.
runs .
however not all automata possess an ss.
when there is no ss the agent will repeatedly compute !and never succeed.
as a result of this observation the algorithm requires an upper bound on the number of episodes as input e .
2the and values were manually set based on controlled experiments during which we measure the total rewards gained.
this is then followed by the experiments.
that is hyperparameters and were constant throughout the experiments.
3throughout the experiments ewas set to episodes.input automatona s h such thatjsj e output an ss!fora begin initiate aq nodensuch thatns st s foreachx2 do assign a random value to q ns i x ofn.
end 4initialnode n n0 n n n n !
e .
whilee e do foreachx2 do ifn00withn00 s st n0 s x does not exist in n then introducen00tonsuch that n00 s st n0 s x having random q values.
introduce an edge from n0ton00labelled with i x .
end end 10e e pick a random value rin the range .
ifr then select a random input x updaten0 andq values.
end else selectxusingq ns0 i x update n0 and q values.
end ifr n0 sji x nan then n0 initialnode !
.
end else !
!
x. ifjn0 sj then return!.
end end end return .
end algorithm theq synch algorithm.
at each iteration the algorithm picks the current node n0 extracts the set of states ns0and it checks if all the adjacent nodes ofn0are inni.e.
for each input x2 it checks if an adjacent node n00is included in the q graph.
if not then it introduces the missing q nodes with random q values and introduces the edge information lines of algorithm .
after this the algorithm increments e updates the input symbol x the current q node n0 andqvalues according to the greedy method lines of algorithm .
if the immediate reward is nan then the algorithm clears !
sets the current node as the initial node and repeats the process.
otherwise it appends the input to !and checks whether the current node is associated with a singleton set or not.
if so the algorithm returns !as the ss lines of algorithm .
otherwise it repeats the above mentioned process.
if the upper bound on the number of episodes is reached and no ss has been found then the algorithm terminates and returns an empty !.
the above algorithm can generate an ss from a given aif 372and only ifahas an ss and a suitably large episode value e is provided.
proposition .
theq synch algorithm can construct an ss from an automaton aif and only if ahas one and eis sufficiently large.
proof.!!
!this part follows from the corollary and the fact thatq learning algorithm finds the optimal policy in finite mdps .
now assume that the q synch algorithm returns a non empty input sequence !but this is not a synchronising sequence.
we consider two cases i there exists s s02s such that s !
s0 !
and ii there exist s2ssuch that s !
is not defined.
since!is non empty the algorithm must return when a singleton set is reached.
therefore j s !
j 1and i cannot be true.
next assume that !is in the form of !0x!00where !0and!00are sequences.
let us assume that s !
s0but s0 x is not defined.
we need to consider two sub cases a !0is an ss for a and b !0is not an ss for a. note that the algorithm returns as soon as it reaches an ss so a cannot be true.
if b happens then the algorithm should clear !and cannot return a non empty sequence.
therefore ii cannot be true.
hence the result follows.
iv.
e xperiments in this section we provide the details of the controlled experiments conducted to answer our research questions.
we first recall the research questions and the evaluation criteria focusing on which aspects of the algorithms were compared.
this is then followed by a description of the experimental subjects.
next we outline the benchmark algorithms used as a baseline and the experiment environment used.
finally we provide the results of the experiments.
a. research questions and evaluation criteria the following research questions were posed at the outset in section rq is our proposed q learning algorithm more efficient and scalable than the state of the art algorithms?
rq .
is the growth of the execution time more modest in theq learning algorithm?
rq .
how scalable can a q learning algorithm be?
by fixing the amount of memory and execution time can aq learning algorithm generate sss from specifications that cannot be processed by the state of the art ss generation algorithms?
rq .
how does the memory usage behaviour of the qlearning algorithm compare to state of the art ss generation algorithms?
rq is our proposed q learning algorithm more effective in generating shorter synchronising sequences for a larger class of system models?
rq .
does theq learning algorithm generate shorter sss relative to execution time and memory consumption than the state of the art ss generation algorithms?
rq .
can theq learning algorithm be extended to mitigate the above mentioned generalisation problem?to answer these questions we consider a number of evaluation criteria.
the first one is the time an algorithm requires to generate an ss with a faster algorithm being better.
the second criterion is related to the length of the sss generated by the algorithms.
since there is a cost associated with the application of an ss we say that an algorithm is better than others if it generates a shorter sequence for a given automaton.
the last criterion relates to the scalability of the algorithms.
by scalability we refer to two different aspects of the algorithms.
first we consider the maximum number of model states that the algorithm can process in a given time.
second we considered the memory used by the algorithm while constructing sss.
so a scalable algorithm is the one that can process larger automata and requires less memory than others.
b. experiment subjects we used two sets of automata in the experiments.
the sets s 1ands2 contained randomly constructed synthetic automata where s1had completely specified automata and s2contained partially specified automata.
synthetic automata were constructed using the following procedure.
letnbe the number of states and pbe the number of inputs.
to generate a completely specified automaton with nstates and pinputs we first generated a graph with nnodes states and for each node state we randomly generate padjacent nodes.
then we checked whether the resultant automaton has an ss.
if so we kept it we discarded it otherwise.
if the underlying automaton is to be partial then we again generated a graph with nnodes but this time for each node we pickedkadjacent nodes from the graph where kwas picked randomly in the range .
if the underlying automaton had an ss then we stored the automaton.
we used each n p pair in whichn2f32 131072g andp2f10 22g.
for each such n p pair we randomly generated automata fors1and another 100automata for s2 resulting in a total of automata.
in order to complement the experiments we also used a specification of a real software engine status manager esm .
an esm is a piece of control software that is used to manage the status of the engine in oc e printers and copiers a subsidiary of canon .
this example is chosen because its structure and behaviour is representative of embedded control software .
moreover the esm model was not retrieved from its designers developers but it was learned by another piece of software learnlib and rigorous verification has confirmed that the behavioural model is indicative of the actual system.
the esm model is partial and has inputs and states.
c. benchmark algorithms and experiment environment fors1 we compared the q synch algorithm with the fastest sequential algorithm algorithm called the the greedy method .
there are other more recent algorithms such as fastsynchro and synchrop that can find shorter sss than the greedy approach however the computational complexities of 373these algorithms are much worse than the greedy algorithm and areo n4j j ando n5j j respectively .
an algorithm based on a sat solver is provided in however this algorithm is slow and cannot process large automata.
for s2 we compared theq synch algorithm against the only existing algorithm reported in the literature the parallel brute force parallel bf ss generation algorithm .
the sequential algorithms were implemented in c and compiled using microsoft visual studio edition .
the parallel bf algorithm was implemented in cuda .
using compute capability .
the source code the constructed sss the automata in sets s1 s2 and the esm are publicly available4.
the computer used in the experiments had an intel i7 3630qm cpu at 0ghz with 8gb ram equipped with an nvidia geforce 610m with 2gb memory.
the operating system used was bit windows .
d. results in order to evaluate the relative performance of the algorithms for each automaton a we separately computed sss using the proposed algorithm and the benchmark algorithms.
we recorded the generated ss the execution time and memory required by the algorithms.
throughout the experiments we set the execution time limit to one hour and set the memory limit to 1gb of ram.
we ran the greedy algorithm with automata from set s1.
the greedy algorithm could generate ss when n .
whenn the memory requirement exceeded the given limit.
we ran the parallel bf algorithm with automata from sets2 and the parallel bf algorithm could not compute sss whenn 64within the given time limit.
in the rest of this section we discuss the results in greater detail.
we used r for statistical tests and to generate graphs .
time comparison we provided the time comparison results conducted on s1in figure 2a.
the y axis values give the mean ratio of the time taken by the q synch algorithm to the time taken by the greedy method.
the results are promising and show that the proposed q synch algorithm is 500times faster than the greedy algorithm on average.
we also observe that as the number of states increases the difference increases.
when n theq synch algorithm is about times faster than the greedy method.
to investigate the results further we conducted a statistical effect size analysis through computing cohen s distance d using the r tool .
the results regarding to the effect sizes for execution time are given in table i. the statistical analyses indicate that the effect size between the population s is large.
we provide the time comparison results conducted on s2 in figure 3a.
again we observe that the results are promising.
4code and data are anonymously available at software codeanddata 5please note that in the authors used a tesla k40 gpu to conduct the experiments which is about times faster than the gpu card used in these experiments.
6the r source codes and the data are anonymously published in https figshare.com s cbf85c54a1ff11674374the proposedq synch algorithm was times faster than the parallel bf algorithm on average and when n the proposed algorithm was times faster on average.
the result of cohens danalysis is given in table iv.
results again indicate that the effect size between the populations is large.
length of constructed sss the length comparison results for the sss generated for s1are given in figure 2b.
similar to before the yaxis denotes the ratio of the lengths length of an ss constructed by the greedy method length of an ss constructed by the q synch algorithm therefore the higher the value the better the q synch algorithm.
the results indicate that the q synch algorithm constructs shorter sss shorter on average than the greedy method.
the difference appears to plateau at around and does not change with the number of states and inputs.
the result of effect size analysis is given in table ii.
the results also suggest that the populations effect size is slightly different.
regardless of the number of inputs and the number of states the median for the length of sss generated by the greedy method is slightly higher than the median of the length of the sss generated by the q synch algorithm.
the results conducted on s2are given in figure 3b.
this time theyaxis denotes the averages of the ratio of the lengths of sss constructed by the parallel bf method to the lengths of sss constructed by the q synch algorithm.
since the parallel bf algorithm is a brute force algorithm it finds one of the shortest ss for each automaton from set s2.
the sss generated by the q synch algorithm are longer than the sss generated by the parallel bf algorithm on average.
one promising observation is that the difference seems to plateau at around and does not grow with the number of states or inputs.
the cohen s danalysis also indicates that the median of the lengths of the sss constructed by the parallel bf algorithm is smaller the median of the lengths of the sss constructed by the q synch algorithm table v .
memory requirements the memory requirements for automata in s1are given in figure 2c.
the y axis provides the ratio of the memory required by the greedy algorithm to the memory required by the qsynch algorithm.
we used workingsetsize property of process memory counters of windows api to get the memory information.
the figure indicates that the greedy algorithm requested more memory than the q synch algorithm times more memory on average and the difference increases with the number of states.
when n the greedy algorithm requires 76times more memory than the proposed algorithm on average and when n the greedy algorithm failed to generate sss within the given memory limit.
moreover considering the effect size analysis in table iii we see the results of effect size analysis and that the results are conclusive the effect size between the two populations is large.
the memory consumption comparison for the parallel bf method andq synch algorithm is given in figure 3c.
again theyaxis gives the ratio of the amount of memory required by the parallel bf algorithm to the memory required by the states inputstime e q a states inputslength e q b states inputsmemory e q c fig.
comparison of the performances of the greedy and the q synch algorithms on dataset s1.
figure 2a summarises the ratio of the time required to construct sss figure 2b summaries the ratio of the lengths of the sss and figure 2c summarises the ratio of memory required to construct sss.
states inputstime p q a .
.
.
.
states inputslength p q b states inputsmemory p e c fig.
comparison of the performances of the parallel bf and the q synch algorithms on dataset s2.
figure 3a summarises the ratio of the time required to construct sss figure 3b summaries the ratio of the lengths of the sss and figure 3c summarises the ratio of memory required to construct sss.
q synch algorithm.
the results are similar with the proposed algorithm requiring less memory on average than the parallel bf algorithm.
similar to before we can observed that the ratio increases as the number of states increases.
we again complement the analysis using the cohen s dmetric table vi .
results suggest that the effect size is large meaning that the populations are different.
np cohens d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table i cohens d analysis on the amount of time required by the algorithms on s1.np cohens d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table ii cohens d analysis on the lengths of sss constructed by the algorithms on s1.np cohens d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iii cohens d analysis on the memory requirements of the algorithms on s1.
results on the benchmark automaton recall that the esm specification is partially specified.
to conduct the experiments we generated two versions of the esm specification v1 v2 .v1was generated by completing the missing transi np cohens d .
.
.
.
.
.
table iv cohens d analysis on the amount of time required by the algorithms on s2.np cohens d .
.
.
.
.
.
table v cohens d analysis on the lengths of sss constructed by the algorithms on s2.np cohens d .
.
.
.
.
.
table vi cohens d analysis on the memory requirements of the algorithms on s2.
tions to obtain a completely specified model.
completing a partial model is a well known approach in mbt .
to complete the missing transitions we first introduced an error state and for each state sand for each missing transition labeled by an input x we introduced a transition from sto the error state with inputx.
the second version of esm was the original partial version.
we then investigated the model and discovered that the esm model does not possess an ss.
this is because state pairs s524 s s70 s s304 s s344 s and s1080 s are absorbing pairs.
an absorbing pair is a pair of states si sj such that for each inputx2 we have that si x sjand sj x si.
that is no transitions leave these pairs.
in order to be able to use the automaton in the experiments for each of these states we modified transitions labelled with a common input 375symboli21 such that they all merge at state s524.
after the above modifications were made we provided v1 to the greedy and the q synch algorithms and provided v2 to the parallel bf and the q synch algorithms as input.
in the experiments the greedy and the parallel bf algorithms could not generate sss.
as in the case of randomly generated automata the greedy algorithm failed to construct an ss due to memory problem and similarly the parallel bf algorithm could not finish computing an ss within one hour.
however theq synch algorithm did generate an ss for these specifications.
the lengths of the sss for v1andv2were and respectively and they reset the system to state s524 as expected.
theq synch algorithm was able to generate the sequences in 351milliseconds and used 326mbs of ram on average.
scalability as indicated earlier we investigated the scalability of the algorithms with respect to i the maximum number of states and ii amount of memory required to construct sss.
in figure we provided the average times required to construct sss from s1ands2using theq synch algorithm.
the proposed algorithm is able to construct sss from specifications with states and 22inputs.
since the greedy and the parallel bf algorithms could only generate sss whenn 512andn respectively with respect to number of states the proposed algorithm is times more scalable than the greedy algorithm.
moreover the q synch algorithm is times more scalable than the parallel bf algorithm.
in figure we present the results for s1.
here each value is the mean memory required for the greedy algorithm figure 5a to process the given size of automata in s1.
similarly figure 5b gives the mean memory usage by the implementation of the parallel bf algorithm for s2.
moreover in figure we give the mean memory required in mb by the proposed algorithm to construct sss s 1in figure 6a and s2in figure 6b .
the memory requirement of the proposed algorithm does not grow as fast as that of the other algorithms.
to investigate this as theq learning algorithm uses q nodes we checked the mean number of q nodes created while computing sss.
in figure we provided these values mean number of qnodes constructed by the proposed algorithm figure 7a for s1 and figure 7b for s2.
we observe that the q synch algorithm is very economic in the sense that it generates qnodes tentatively.
this is important and implies that the qlearning algorithm selects its inputs wisely.
if this was not the case in each episode the agent would select different inputs and introduce new nodes to the q graph.
v. d iscussions in this section in light of the conducted experiments we discuss the answers to the research questions.
this is then followed by an analysis of some threats to validity.a.
answers to the research questions rq is our proposed q learning algorithm more efficient and scalable than the state of the art algorithms?
answer the results of the experiments suggest that our algorithm is more efficient and scalable than the most scalable algorithms reported in the literature.
however we also noted that the number of inputs and states have negative impact on the scalability of the proposed method.
rq .
is the growth of the execution time more modest in the q learning algorithm?
answer .
the results indicate that the time requirement of the proposed algorithm is modest and its growth rate is much slower than the state of the art ss generation algorithms.
results indicate that the proposed algorithm is times faster than the fastest sequential parallel algorithm on average.
rq .
how scalable can a q learning algorithm be?
by fixing the amount of memory and execution time can a q learning algorithm generate sss from specifications that cannot be processed by the state of the art ss generation algorithms?
answer .
the proposed algorithm can quickly generate sss while using less memory.
the experimental results show that the proposed method can process 256times larger specifications than the state of the art sequential algorithm.
what is more the proposed algorithm is times more scalable than the state of the art gpu based massively parallel algorithm.
finally the experimental results suggest that when there are limited computation resources the proposed algorithm can generate sss from real specifications where other algorithms cannot.
rq .
how does the memory usage behaviour of the q learning algorithm compare to state of the art ss generation algorithms?
answer .
the proposed algorithm requires neither a product automaton nor a preset q table to be built therefore the memory requirement of the proposed method grows much slower than the state of the art sequential ss generation method.
experimental study indicates that the greedy algorithm requires 30times more memory than the proposed algorithm on average.
the parallel bf ss generation algorithm on the other hand requires 18times more memory than the proposed algorithm.
rq is our proposed q learning algorithm more effective in generating shorter synchronising sequences for a larger class of system models?
answer the result of the experiments indicate that the proposed algorithm is more effective in generating short sss.
we compared the results with the greedy algorithm and the results suggested that the proposed algorithm finds sss that are30 shorter on average.
states inputstime msecs a states inputstime msecs b fig.
the average amount of time spent to generate sss from s1 figure 4a and from s2by using theq synch algorithm figure 4b .
states inputsmemory mbs a states inputsmemory mbs b fig.
the averages of memory requirement to generate sss from s1using the greedy method figure 5a and from s2using the parallel bf algorithm figure 5b .
rq .
does the q learning algorithm generate shorter sss relative to execution time and memory consumption than the state of the art ss generation algorithms?
answer .
the proposed algorithm can generate shorter sss and consumes much less memory and time than the fastest sequential ss generation algorithm does.
moreover the proposed algorithm can compute sss that are comparable in length with the parallel bf ss generation algorithm.
rq .
can the q learning algorithm be extended to mitigate the above mentioned generalisation problem?
answer .
we introduced a new q learning framework in which we abandoned the idea of keeping the entire search space in a preset form.
instead we employ a method where the search space grows as the agent interacts with the environment.
this allows us to represent the search space using a qgraph.
we used this formalism in the classical q learning algorithm.
to our knowledge this is new and experimental studies showed that it allows agents to learn q values from large environments.
experimental evaluation indicates that using the q graph formalism we can solve problems using a search space whose size is a fraction of the size of the search space needed in the classicalq table setting.
for example as presented in figure 7a and figure 7b our method needs 70q nodes to derive an ss from an automaton that has jsj statesandj j inputs on average.
if we were using a tabular setting we would generate a table having pow s j j 22cells which would not be possible.
b. threats to validity there are number of threats to the validity of the experiments.
the first threat is to generalisability which originates from the fact that the experimental subjects may not be representative of real systems.
the use of randomly generated automata clearly introduces such a threat and we addressed this by creating two versions of the specification of engine status manager software which has states and 77inputs.
importantly the experimental results obtained with these realworld models are similar to those obtained with randomly generated automata.
there are also threats to internal validity and the possibility that one or more of the implementations were incorrect.
to reduce this threat we applied unit testing in the development cycle.
moreover when a sequence !
was generated by an algorithm we checked that the generated sequence was an ss.
to achieve this when an ss !was computed for an automaton a we randomly selected an initial state s of aand starting froms we applied !to find the reset state s0.
clearlys0 should be the state that the areaches regardless of the initial state from which !is applied.
to confirm this for every state s00ofa we checked that the application of !ins00tookato s0.
throughout the experiments we did not encounter a case where the underlying automaton failed to reach the reset state.
finally there is potential to misinterpret the results obtained from the experiments.
to address this threat we validated our results by conducting cohen s deffect size analysis.
vi.
c onclusion model based testing mbt is an increasingly important type of software testing.
most mbt techniques require some method that brings the system under test sut to a specific initial state in order for a test sequence to be applied to the sut.
this requirement can be fulfilled by a synchronising sequence ss .
the length of an ss used affects the cost of test execution and so there has been long standing interest in the problem of finding a short ss .
however the problem of generating short sss is known to be np hard.
the other motivation for the work described in this paper comes from the fact that previous work has developed a variety states inputsmemory mbs a states inputsmemory mbs b fig.
the averages of the memory requirement of the q synch algorithm when generating sss from s1 figure 6a and from s2 figure 6b .
states inputsnumber of q nodes a states inputsnumber of q nodes b fig.
the averages of the number of q nodes generated while constructing sss from s1 a ands2 b by using theq synch algorithm.
of successful automated test generation methods based on reinforcement learning rl .
in this paper we proposed a newq learning algorithm to derive synchronising sequences.
the proposed method introduces the notion of qgraph which instead of holding the entire search space in memory allows the search space to be expanded on the fly.
experimental results indicate that the proposed method is more efficient and effective in generating sss than the state of the art ss generation methods.
there are a number of lines of future work.
first we will investigate the implications of the introduced q learning framework on other state exploration problems in model based testing and beyond.
instead of holding the entire search space in a table ourq learning framework allows the search space to be represented as a graph and therefore uses less memory space allowing learning from very large search spaces.
this study might lead to new rl algorithms.
besides there may be scope to investigate other rl approaches for deriving sss.
further potential directions include extensions of the framework to probabilistic automata with unknown transition probabilities.
moreover it would be interesting to study the effect of shorter ss on testing.
since the impact will depend on the test technique used a systematic evaluation of different test methods should be carried out.
finally the experimental results suggest that as the number of states and inputs of the automata grow the time and memory requirements of the method increase.
although this is unsur prising we plan to explore approaches such as parallelisation that might allow the technique to scale further.