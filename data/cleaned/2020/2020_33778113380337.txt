reludiff differential verification of deep neural networks brandon paulsen university of southern california los angeles california usajingbo wang university of southern california los angeles california usachao wang university of southern california los angeles california usa abstract as deep neural networks are increasingly being deployed in practice their efficiency has become an important issue.
while there are compression techniques for reducing the network s size energy consumption and computational requirement they only demonstrate empirically that there is no loss of accuracy but lack formal guarantees of the compressed network e.g.
in the presence of adversarial examples.
existing verification techniques such as reluplex reluval and deeppoly provide formal guarantees but they are designed for analyzing a single network instead of the relationship between two networks.
to fill the gap we develop a new method for differential verification of two closely related networks.
our method consists of a fast but approximate forward interval analysis pass followed by a backward pass that iteratively refines the approximation until the desired property is verified.
we have two main innovations.
during the forward pass we exploit structural and behavioral similarities of the two networks to more accurately bound the difference between the output neurons of the two networks.
then in the backward pass we leverage the gradient differences to more accurately compute the most beneficial refinement.
our experiments show that compared to state of theart verification tools our method can achieve orders of magnitude speedup and prove many more properties than existing tools.
introduction as deep neural networks dnns make their way into safety critical systems such as aircraft collision avoidance and autonomous driving where errors may lead to catastrophes there is a growing need for formal verification.
the situation is further exacerbated byadversarial examples which are security exploits created specifically to cause erroneous classifications .
there is also a growing need for reducing the size of the neural networks deployed on energy and computation constrained devices.
consequently compression techniques have emerged to prune unnecessary edges quantize the weights of remaining edges and retrain the networks but they do not provide any formal guarantee typically the accuracy of a compressed network is only demonstrated empirically.
while empirical evidence or statistical analysis may increase our confidence that a network behaves as expected for most of the inputs they cannot prove that it does so for all inputs.
similarly permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .. .
.
.
n1 n1 n 1n n 2n n 1n3 1n2 n2 2n0 n0 2f f figure differential verification of deep neural networks.
while heuristic search and dynamic analysis techniques including testing and fuzzing may quickly discover adversarial examples they cannot prove the absence of such examples.
at the same time while state of the art verification techniques including reluplex reluval anddeeppoly can provide formal proofs they are designed for analyzing a single network as opposed to the relationship between two networks.
in this work we focus on differential verification of two closely related networks.
in this problem domain we assume that fand f are two neural networks trained for the same task that is they accept the same input xand are expected to produce the same output.
they are also structurally the same while differing only in the numerical values of edge weights which allows us to analyze compression techniques such as quantization and edge pruning .
in this context differential verification is concerned with proving x x. f x f x where xis an input region of interest and is some reasonably small bound.
this problem has not received adequate attention and as we will show in this work existing tools are ill suited for solving this problem.
the key limitation of existing tools is that since they are designed to analyze the behavior of a single network they do not have the ability to exploit the structural similarities of two closely related networks.
they also have difficulty handling the constraint that the inputs to both fand f are identical.
typically these tools work by computing the conservative value ranges of all neurons from input to output in a layer by layer style.
in the early layers they may be able to maintain relationships between the inputs but as the functions become increasingly non linear in subsequent layers approximations must be made.
this eager approximation means relationships between the inputs of fandf are mostly lost causing extremely large over approximations in the output layer.
ieee acm 42nd international conference on software engineering icse .
.
.x1 x2 y1y2.
.
.x1 x2 y 1y 2parameters x1 x1 x2 x2 forwar d interval analysis checking safe verifie dsampling input spacebackward refinementx1 x2 violatedno violation countere xamplednn dnn interval violation figure the differential verification flow of reludiff.
in fact state of the art verification tools that we have investigated reluval and deeppoly struggle to verify that two identical networks are the same.
to carry out this litmus test without drastically altering these tools we construct a combined network as shown in figure where fandf are actually the same network i.e.
same structure and edge weights .
since they share the same input x we expect f x f x to be regardless of the input region forx.
while our method can easily prove that f x f x for an arbitrarily small in less than a second none of the existing tools are able to do so.
in fact deeppoly cannot verify it no matter how much time is given it is not a complete method and reluval times out after several hours.
figure shows the overall flow of our method reludiff whose input consists of the two networks dnn1 and dnn2 an input region x1 x1andx2 x2 and a bound on the output difference.
there are three possible outcomes verified meaning that the output difference is proved to be less than falsified meaning a counterexample is found or unknown meaning that verification remains inconclusive due to bounds on the computing resources.
internally reludiff iterates through two steps a forward pass and a backward pass.
the forward pass computes over approximated value differences of corresponding neurons in the two networks and propagates them layer by layer from the input to the output.
if the output difference is within the region the property is verified.
otherwise reludiff samples a fixed number of concrete examples from the input space and tests if they violate the property.
if a violation is found the property is falsified otherwise reludiff enters the refinement phase.
the goal of refinement is to identify an input region that should be divided into subregions.
by using these subregions to perform the forward pass again some of the forced over approximations may be avoided thus leading to significant accuracy increase.
to identify the right input region for refinement the backward pass computes the difference of the gradients of the two networks and uses it to find input regions that once divided into subregions are more likely to result in accuracy increase.
while iterative interval analysis has been used in verifying neural networks before the focus has always been on a singlenetwork.
in this work we show that by focusing on both networks simultaneously we can be more efficient and accurate compared to analyzing each network in isolation.
note that in differential verification the two networks have identical structures and similar behaviors therefore we can easily develop a correspondence between neurons in fand f thus allowing a lock step style verification.
lock step verification allows us to directly compute the differences in values of neurons and propagate these differences through edges.
it also allows symbolic intervals to be used to avoid some of the approximations.
since error caused by approximation grows quickly sometimes exponentially as it is propagated through edges and neurons this can significantly increase accuracy.
when approximation must be made e.g.
due to non linearity of relu we can handle them better by focusing on the value differences instead of the absolute values.
for example in reluval if a symbolic expression that represents the relu input may be both positive and negative the symbolic expression must be replaced by an interval with concrete upper and lower bounds which introduces additional error.
in contrast we can be more accurate even if the input value of a neuron may be both positive and negative in many cases we still can avoid introducing error into the difference.
we have implemented reludiff in a tool and evaluated it on a number of feed forward neural network benchmarks including acas xu for aircraft collision avoidance mnist for handwritten digit recognition and har for human activity recognition .
we also experimentally compared reludiff with stateof the art tools including reluval and deeppoly .
our experimental results show that in almost all cases reludiff outperforms these existing tools in both speed and accuracy.
in total we evaluate on properties over our benchmark networks.
reludiff was often one to two orders of magnitude faster and was able to prove out of the properties whereas none of the other tools can prove more than properties.
to summarize we make the following contributions we propose the first iterative symbolic interval analysis for differential verification of two neural networks.
we develop a forward pass algorithm for more accurately computing the value differences for corresponding neurons.
we develop a backward pass algorithm based on gradient difference for computing the refinement.
we implement the method and demonstrate its advantages over existing tools in terms of both speed and accuracy.
the remainder of the paper is organized as follows.
first we use examples to motivate our method in section .
then we review the basics of neural networks and interval analysis in section .
next we present our method for the forward pass in section followed by our method for the backward pass in section .
we present our experimental results in section .
we review the related work in section .
finally we give our conclusions in section .
motivation we illustrate the problems of existing verification tools using examples and then highlight our main contributions.
.
.
.
.
.
.
.
.
.
.0n0 n0 n1 2n1 n2 n2 2n3 1x1 x2y f x1 x2 figure a neural network with two inputs and one output.
.
differential verification figure shows a feed forward neural network with one input layer two hidden layers and one output layer.
the input layer has two nodes n0 1andn0 corresponding to the two input variables x1 andx2.
each hidden layer consists of two neurons n1 n1 2in one layer and n2 n2 2in the other layer.
each of these neurons has two computation steps the affine transformation and the relu activation.
for example inside n1 the affine transformation is .9x1 .1x2and the relu activation is max .9x1 .1x2 .
the output layer has one node representing the value of y f x1 x2 .
in general fis a non linear function over x1andx2.
in differential verification we are concerned with the relationship between f x1 x2 and another network f x1 x2 .
for the sake of example we focus on a network compression technique called quantiziation in which the edge weights of fare rounded to the nearest whole number to obtain f .
however we note that our method can be used on anytwo networks with similar structures e.g.
when f is created using other techniques including edge pruning andnetwork retraining .
these techniques in general raise the concern on how they affect the network s behavior.
in particular we would like to verify that the new network produces outputs within some bound relative to the original network.
formally let f x ybe the second network and f x ybe the first network.
we would like to verify that f x f x for all x x where x xis some region of importance in the input domain x. .
existing approaches existing tools for verifying neural networks target only a single network at a time and are often geared toward proving the absence ofadversarial examples.
that is given an input region of interest they decide if the output stays in a desired region.
for the network in figure in particular the input region may be x1 and x2 and the desired output may be f x1 x2 .
however these tools are not designed for verifying the relationship between two networks.
while we could try and re use them for our purpose they lack the ability to exploit the similarities of the two networks.
for example we could use the existing tool reluval on both fandf to compare the concrete output intervals it computes for an input region of interest e.g.
and f low f up .
in order to conservatively estimate the difference between fand f we must assume the maximum difference falls in the interval f low fup f up flow .
in figure the interval difference would be which is too large to be useful.
even though reluval could tighten the interval by refining the input intervals this naive approach cannot even verify that1.
.
.
.
.
.
.
.
.
.
1n0 n0 2n1 n1 2n2 n2 2n3 n n 2n n 2n 1 1x1 x2f f figure naive differential verification of the two networks.
two identical networks always produce the same output since the output intervals do not capture that the corresponding inputs to f and f i.e.
values of x1andx2 are always the same.
to compensate we could encode the constraint that values of the corresponding inputs are always the same by composing fand f into a single feed forward network equivalent to f f as shown in figure .
in theory a sound and complete technique would be able to verify eventually that the output difference is bounded by an arbitrarily small but with a caveat.
that is to maintain the relationships between the input variables and the difference in the outputs of the two networks each neuron must remain in a linear state across the entire input region otherwise approximation must be made to maintain the soundness of the interval analysis.
however approximation inevitably loses some of the relationships between the inputs and the outputs.
indeed we constructed some merged networks in the same way as in figure and then fed them to existing tools.
unfortunately they all exhibit the worst case value range blowup in the output.
the key reason is that existing tools such as reluval are forced to approximate relu activations by concretizing which is then followed by interval subtractions thus causing error introduced by these approximations to be quickly amplified.
the forward pass over fcomputes an output interval of and for f it computes .
although the equations are symbolic the difference computed conservatively by reluval is still too large to be useful.
.
our method existing tools cannot exploit structural and behavioral similarities of the two networks in differential verification.
our insight is to leverage such similarities to drastically improve both the efficiency and the accuracy of the verification tool.
specifically in this work we pair neurons and edges of the first network with those of the second network and then perform a lockstepverification.
this allows us to focus on the value differences of the corresponding neurons as opposed to their absolute values.
the benefit is that so results in both fewer and tighter approximations and more error reduction due to the use of symbolic 716n0 2n0 n1 2n1 n2 n2 2n3 .
.
.
.
.
.
.
.
.
.0s n1 s n2 s n1 s n2 x1 x2 x1 x2 n1 n1 n2 n2 n3 figure forward interval analysis of a neural network.
intervals.
we also perform better refinement by focusing on inputs that have the greatest influence on the output difference rather than the absolute output values.
while focusing on the difference as opposed to absolute values seems to be a straightforward idea there are many technical challenges.
for example there will be significantly more complex relu activation patterns to consider since we have to handle both networks simultaneously instead of one network at a time.
approximating symbolic intervals when considering the output difference of two relu activations i.e.
relu x relu x has yet to be studied and is non trivial.
furthermore how to determine which input neuron to refine when the goal is to reduce error in the output difference between two networks has not been considered either.
in this work we develop solutions to overcome these challenges.
during forward interval analysis we carefully consider the relu activation patterns and propose a technique for handling each pattern soundly while minimizing the approximation error.
during the refinement we compute the difference between gradients of the two networks and use it to identify the input neuron most likely to increase the accuracy of the differential verification result.
as a result our method can solve the differential verification problems much more efficiently.
consider the litmus test of verifying the equivalence of two identical networks.
our method can obtain a formal proof that f f after performing the forward interval analysis once in contrast all other existing tools have failed to do so.
for the example in figure we can prove the output difference n3 is bounded by after only the first pass.
it also outperforms existing tools on other verification problems where f is obtained from fthrough quantization details of the experimental comparisons are in section .
preliminaries first we review the basics of interval analysis for neural networks.
.
neural networks we consider a neural network as a non linear function that takes some value in rnas input and returns some value in rmas output where nis the number of input variables and mis the number of output variables.
let the network fbe denoted f x y where x rnis the input domain and y rmis the output domain.
in image recognition applications for instance xmay be a vector of pixels representing an image and ymay be a vector of probabilitiesfor class labels.
in aircraft collision detection on the other hand x may be sensor data and ymay be a set of actions to take.
in this work we consider fully connected feed forward networks with rectified linear unit relu activations which are the most popular in practical hardware software implementations.
thus y f x is a series of affine transformations e.g.
x w1 ixiw1 i followed by point wise relu e.g.
relu x w1 max x w1 .
letwk where k l be the weight matrix associated with the k th layer and lbe the number of layers the affine transformation in the k th layer is a standard matrix multiplication followed by the point wise application of relu.
formally f fl fl ...f2 f1 x w1 w2 ... wl where each fk k l is a point wise relu.
for the network in figure in particular the input is a vector x x1 x2 the weight matrix w3 .
.
t and x w1 .9x1 .1x2 .1x1 .0x2 .
for ease of presentation we denote the weight of the edge from thei th neuron of layer k 1to the j th neuron of layer kaswk .
we also denote the j th neuron of layer kasnk j. .
interval analysis to ensure that our analysis is over approximated we use interval analysis which can be viewed as a specific instantiation of the general abstract interpretation framework.
interval analysis is well suited for analyzing relu neural networks as it has welldefined transformers over addition subtraction and scaling i.e.
multiplication by a constant .
interval addition as denoted does not lead to loss of accuracy.
scaling as denoted c when c or c when c does not lead to loss of accuracy either.
interval subtraction as denoted however may lead to accuracy loss.
to illustrate consider f x .1xand f x 2x and say we want to approximate their difference for the input region x .
using interval arithmetic we would compute f f .
clearly this is far from the exact interval of f x f x .1x 2x .1xover x which is .
the reason for such loss of accuracy is that during interval arithmetic the relationship between values of2.1xand2x i.e.
they are for the same value of x is lost.
.
symbolic interval one way to overcome the accuracy loss is using symbolic intervals which can encode the constraint that inputs to fand f are actually related.
with this technique we would use the symbol xwith the constraint x to initialize the input intervals.
then the computation becomes f f .
finally we would compute the upper and lower bounds for x and return the precise interval .
unfortunately symbolic intervals depend on fand f being linear in the entire input region in order to be sound.
indeed if we add relu to the functions i.e.
relu f x max .1x and relu f x max 2x where x then the lower and upper bounds are no longer precise nor sound.
the reason is because max .1x is non linear in x .
thus we have to approximate using the concrete interval .
similarly max 2x is approximated using .
thus .
.
.
.nk nk j nk iwk wk layer k layer k figure diagram of weight notations.
.
refinement to improve the accuracy of the symbolic interval analysis we need to divide the input region into subregions.
the intuition is that within a smaller subregion the relu is less likely to exhibit nonlinear behavior and force the analyzer to over approximate.
consider relu .1x max .1x where x .
after the input region is divided into subregions x we have max .1x forx andmax .1x forx .
in both cases the intervals are precise there is no approximation at all.
when we only have one input variable we do not have a choice on which variable to refine.
however neural networks have many inputs and refining some of them will not always yield benefit.
thus we have to identify the right input to split.
consider f x1 x2 relu 5x1 relu 2x2 relu x2 where x1 andx2 .
the initial analysis is not very accurate due to approximations caused by the relu relu relu relu relu relu relu .
if we split x1 into x1 and perform interval analysis for both subregions the output would be which does not improve over the initial result.
in contrast if we split x2 into x2 the accuracy would improve significantly.
since the relu is always activated for x2 relu 2x2 andrelu x2 can be represented by and respectively and relu 2x2 relu x2 .
since the relu is always de activated forx2 we have relu 2x2 relu x2 .
thus the combined output is more accurate than the initial approximation .
while how to analyze non linear activation functions such as relu has been studied in prior work none of the existing techniques touch upon the complex scenarios arising from differential verification of two closely related networks.
our work fills the gap.
specifically we propose a more accurate forward pass for the interval analysis section and a more accurate backward pass for the refinement section .
forward interval analysis in this section we describe our forward pass for computing the value differences between neurons in the two networks.
recall that network fhasllayers and weight matrices wk k l and nk jis the j th node in the k th layer.
furthermore wk is the weight of the edge from nk itonk j. we illustrate these notations in figure .
similarly network f has weight matrices w k nodesinput network f network f input region x result difference nl j for output initialize s n0 j s n j to input region xand n0 j to0 forkin1..nlayerdo affine transformer forjin1..layersize do sin nk j is nk i wk sin n k j is n k i w k in nk j i s nk i w k nk i w k end ifk nlayerthen return in nk j end relu transformer forjin1..layersize do s nk j s n k j nk j relutransform sin nk j sin n k j in nk j end end algorithm forward symbolic interval analysis.
n k j and weights w k .
let w k be the weight difference i.e w k w k wk .
we now define notations for the interval values of neurons.
since each neuron nk jhas an affine transformation multiplying by the incoming weights and a relu we denote the input interval to the neuron after applying the affine transform as sin nk j and we denote the output interval of the neuron after applying the relu ass nk j .
we denote the interval bound on the difference between the inputs to n k jandnk jas in nk j and we denote the interval difference between the outputs as nk j .
finally we denote the symbolic upper and lower bound of any value using the notation ub andlb .
for example ub s nk j andlb s nk j denote the symbolic upper and lower bound for the output of neuron nk j. with this notation our forward pass is shown in algorithms and .
the input consists of the two networks fand f and the input region of interest x which defines an interval for each input neuron.
after initializing the input intervals the algorithm iteratively computes each s nk j s n k j and nk j of the subsequent layer by applying the affine transformation followed by the relu transformation.
the algorithm iterates until the output layer is reached.
in addition it computes the gradient masks for the neurons of fand f denoted as rand r which record the state of each neuron in the forward pass is inactive is active and is both .
these are used in the refinement phase section to determine which input neuron to refine.
we omit discussion of computing sin nk j ands nk j because it has been studied in previous work .
we focus on computing in nk j and nk j in the following section.
.
the affine transformer computing in nk j involves two steps.
first we compute wk for each incoming edge to nk j. here wk is the difference in values produced by the edges from nk itonk jand from n k i ton k j. second we sum them to obtain in nk j .
in the first step there are two components to consider when computing wk .
first there is the new quantity introduced by the difference in edge weights which formally is s nk i w k .
in english this is the interval of neuron nk iin the previous layer 718multiplied by the edge weight difference in the current layer.
second there is the old quantity accumulated in previous layers being scaled by the edge weight in the current layer.
formally this is nk i w k .
below we write out the formal derivation wk w k s n k i wk s nk i w k s n k i wk s nk i w k s nk i w k s nk i w k s n k i w k s nk i w k s nk i wk s nk i nk i w k s nk i w k .
in the second step we sum together each incoming wk term to obtain in nk j which is the difference of the values sin nk j andsin n k j .
that is in nk j i wk .
we demonstrate the computation on the example in figure .
first we compute w1 .
and w1 .
.
then we compute w1 and w1 .
next we compute in n1 w1 w1 and in n1 w1 w1 .
.
the relu transformer next we apply the relu activation to in nk j to obtain nk j .
we consider nine cases based on whether the relus of nk jandn k j arealways activated always deactivated or non linear.
in the remainder of the section we discuss how to soundly over approximate.
algorithm shows the details.
first we consider the three cases when the relu in nk jisalways deactivated.
if the relu in n k jis also always deactivated the outputs of both relus are and the difference is .
if the relu in n k jisalways activated the output difference will be sin n k j sin n k j .
note that we can maintain symbolic equations here.
if the relu in n k jisnon linear the difference will be ub sin n k j .
while ub sin n k j is the symbolic upper bound of sin n k j ub sin n k j is the concrete upper bound.
note that since n k jis non linear we must concretize to be sound.
next we consider the three cases when the relu in nk jis always activated.
if the relu in n k jisalways deactivated the difference is sin nk j .
again we can soundly maintain symbolic equations.
if the relu in n k jisalways activated then the difference is the same as in nk j .
if the relu in n k jisnonlinear the difference is ub sin n k j sin nk j which is the same as ub sin nk j ub sin n k j lb sin nk j .
again we concretize to ensure soundness.
third we consider the three cases where the relu in nk jisnonlinear.
if the relu in n k jisalways deactivated the difference is .
if the relu in n k jisalways activated the difference is sin n k j which is the sameinput value sin nk j value sin n k j difference in nk j result value s nk j value s n k j difference nk j ifub sin nk j 0then r s nk j ifub sin n k j 0then r s n k j nk j else if lb sin n k j 0then r s n k j sin n k j nk j sin n k j else r s n k j ub sin n k j n k j ub sin n k j else if lb sin nk j 0then r s nk j sin nk j ifub sin n k j 0then r s n k j nk j sin nk j else if lb sin n k j 0then r s n k j sin n k j nk j in nk j else r s n k j ub sin n k j nk j ub sin nk j ub sin n k j lb sin nk j opt.
nk j max ub sin nk j lb in nk j max lb sin nk j ub in nk j else r s nk j ifub sin n k j 0then r s n k j nk j else if lb sin n k j 0then r s n k j sin n k j nk j lb sin n k j ub sin nk j ub sin n k j opt.
nk j min lb sin n k j lb in nk j min ub in nk j ub sin n k j else r s n k j ub sin n k j nk j ub sin nk j ub sin n k j opt.
iflb in nk j 0then nk j else if ub in nk j 0then nk j else nk j max lb in nk j ub sin nk j ub sin n k j algorithm over approximating relu activation function.
as lb sin n k j ub sin nk j ub sin n k j .
if the relu 719inn k jis also non linear then the difference is ub sin n k j which is ub sin nk j ub sin n k j .
.
optimization the most important optimization we make in the forward pass when computing nk j is in shifting from bounding the equation relu sin n k j relu sin nk j to bounding one the following equivalent equations relu sin nk j in nk j relu sin nk j relu sin n k j relu sin n k j in nk j equation says that for any concrete nk j sin nk j the most n k jcan change is bounded by in nk j and similarly for equation .
as shown in algorithm we have identified three optimization opportunities marked as opt.
.
we note that even though we widen the difference interval in some of these cases the interval is almost always tighter than if we subtract the bounds of n k jand nk j even when they are symbolic.
below we give formal proofs for most of the bounds and the remaining proofs can be found in the appendix of our arxiv paper .
.
.
opt.
nk jis active n k jis non linear.
using equation we can potentially tighten the lower bound.
to reduce the notation complexity we rewrite equation as the function n d relu n d relu n relu n d n where n sin nk j andd in nk j and we can simplify from equation to because nk jis active.
now computing nk j amounts to finding the upper and lower bounds on n d .
observe that if n d 0then n d dbecause relu n d simplifies to n d and the like terms cancel.
otherwise n d n because relu n d .
observing that n d d n this means n d is equivalent to n d d d n n d n max n d max is well defined for intervals.
specifically for two intervals we have max .
now plugging in the the bounds of nanddwe get n d max max ub sin nk j lb in nk j max lb sin nk j ub in nk j .
.
opt.
nk jis non linear n k jis active.
using equation we can tighten the upper bound.
we first rewrite equation as n d n relu n d .
just like equation equation can be broken into two cases based on the inequality n d n d which gives us the piece wise equation n d d n d n n d min n d for two intervals we have min .
replacing n and dwith the proper bounds gives us the min function in algorithm .
.
.
opt.
both nk jandn k jare non linear.
we consider three cases.
first let lb in nk j .
this means that n k j nk j before applying relu and then we can derive as a lower bound as follows n k j nk j relu n k j relu nk j relu n k j relu nk j in addition ub in nk j can be derived as an upper bound1from equation .
this is the case in our motivating example so n1 .
second we consider ub in nk j .
this means n k j nk jbefore relu which allows us to derive an upper bound of in a symmetric manner to the first case.
the lower bound shown in algorithm can be derived from equation .
in the third case where lb in nk j ub in nk j the lower bound and the upper bound shown in algorithm can be derived from equations and respectively also see footnote .
.
on the correctness the operations of the affine transformation are soundly defined for intervals as described in section .
.
for the relu transformation we give formal explanations to show that they over approximate see also for proofs .
since composing over approximations also results in an over approximation the forward analysis is itself a sound over approximation.
gradient based refinement after performing the forward pass the computed difference may not be tight enough to prove the desired property.
in this section we discuss how we can improve the analysis result.
.
splitting input intervals as mentioned in section a common way to improve the result of interval analysis is dividing an input interval into disjoint subintervals and then performing interval analysis on the sub intervals.
after unioning the output intervals the result will be at least as good as the original result .
prior work also shows a nice property of relu networks after a finite number of such splits the result of the interval analysis can be arbitrarily accurate.
however determining the optimal order of refinement is difficult and so far the best algorithms are all heuristic based.
for example the method used in reluval chooses to split the input interval that hasthe most influence on the output value.
the intuition is that 1in fact the tighter upper bound min ub in nk j ub sin n k j can be derived however we had not yet proved this at the time of submission.
720input network f mask matrix r result gradient initialize to edge weights in the output layer ub lb fork l ..2do n ew .
.
.
forj ..layersize do perform relu for node nk j ifr then lb j ub j else if r then lb j min lb j ub j max ub j multiply by weights of incoming edges to node nk j fori ..layersize do ifwk 0then ub n ew i wk ub j lb n ew i wk lb j else ub n ew i wk lb j lb n ew i wk ub j end end n ew end return algorithm computing the gradient of a network.
splitting such an input interval reduces the approximation error of the output interval.
however the approach is not suitable in our case because we focus on the difference between the two networks the input interval with the most influence on the absolute value of the output may not have the most influence on the output difference.
to account for this difference we develop a method for determining which input interval to split.
.
the refinement algorithm our idea is to compute the difference of the gradients for the two networks denoted .
toward this end we compute the gradient of the first network and the gradient of the second network .
then we use them to compute the difference .
formally f x1 .
.
.
f xn is a vector whose i th element i f xi is the partial derivative of the output fwith respect to the input xi.
similarly f x1 .
.
.
f xn .
the difference is f f x1 .
.
.
f f xn .
that is the derivative of a difference of functions is the difference of their derivatives.
during interval analysis the accurate gradient is difficult to compute.
therefore we compute the approximated gradient where each element iis represented by a concrete interval.
algorithm shows our gradient computation procedure.
in addition to the network which may be either forf it also takes the mask matrix ras input.
recall that both r andr have been computed by algorithm during the forward pass.
r may be or indicating if the relu in nk jisalways de activated always activated or non linear respectively.
it can be understood as the gradient interval of the relu.
the gradient computation is performed backwardly beginning at the output layer and then moving through the previous layers.
in each layer the computation has two steps.
first we apply relu to the current gradient and update the upper and lower bounds of1.
.
1x1 x2 n1 n2n31.
.
figure example for backward refinement.
the gradient if needed.
then we scale the gradient interval by the weights of the previous layer.
after computing and by invoking algorithm on fand f respectively we compute the gradient difference .
then we use the gradient difference to determine which input has the most influence on the output difference.
note that the gradient itself is not sufficient to act as an indicator of influence.
for example while an input s gradient may be large but the width of its input interval is small splitting it will not have much impact on the output interval.
thus we split the input interval with the maximum smear value .
the smear value of an input xiis defined as the width of its input interval ub xi lb xi scaled by the upper bound of its corresponding gradient difference ub i. .
an example we now walk through the gradient computation in algorithm for the example in figure where blue weights are for network f and green weights are for network f .
we focus on the gradient of f first.
after performing the forward pass we know that n1is in a linear state i.e.
r and n2is in a non linear state i.e.
r .
we initialize the gradient to the weights of the final layer that is ub lb 1andub lb .
next we apply relu.
since n1is in the always activated mode we leave its gradient unchanged.
however n2is in the non linear mode meaning the gradient could be and hence we must ensure that is in the gradient interval.
we update lb .
then we scale the gradient interval by weights of the incoming edges which gives us the gradient intervals for input variables forx1 and forx2.
here we point out a problem with reluval s refinement method.
it would compute the smear value of x1andx2to be .
.
.65and .
.
.
.
respectively which means it would split on x1.
however this is not appropriate for differential verification since the two networks differ the most in the weights of the outgoing edge of x2.
our method instead would compute the gradient difference .
therefore we have forx1and forx2.
based on the new smear values we would choose to split the input interval of x2.
experiments we have implemented reludiff and compared it experimentally with state of the art neural network verification tools.
like reluval reludiff is written in c using openblas as the library for matrix multiplications.
we also note that we implement outward rounding to soundly handle floating point arithmetic.
symbolic interval arithmetic is implemented using matrix multiplication.
721reludiff takes two networks fand f together with a small and input region xas input and then decides whether x x. f x f x for the target label s value.
since reludiff is the only tool currently available for differential verification of neural networks to facilitate the experimental comparison with existing tools we developed a tool to merge fand f into a combined network f as shown in figure before feeding f to these existing tools as input.
.
benchmarks our benchmarks are feed forward neural networks from three applications aircraft collision detection image recognition and human activity recognition.
we produce f by truncating each network s weights from bit floats to bit floats.
.
.
acas xu .acas xu is a set of neural networks commonly used in evaluating neural network verification tools.
they are designed to be used in an aircraft to advise the pilot of what action to take in the presence of an intruder aircraft.
they each take five inputs distance between self and the intruder angle of self relative to the intruder angle of intruder relative to self speed of self and speed of intruder.
they output a score in the range for five different actions clear of conflict weak left weak right strong left and strong right.
the action with the minimum score is the action advised.
in addition to the input and output layers each network has hidden layers of neurons each for a total of neurons.
for differential verification we use the same input ranges as in as our regions of interest.
.
.
mnist .mnist is a data set of labeled images of handwritten digits that are often used as a benchmark to test image classifiers.
the images are 28x28 pixels and each pixel has a grayscale value in the range .
neural networks trained on this data set take in inputs one per pixel each in the range and output scores typically in the range of for our networks for each of the digits.
the digit with the highest score is the chosen classification.
we use three neural networks trained on the mnist data set with architectures of 3x100 2x512 and 4x1024 that is the networks have and hidden layers with layer size and neurons respectively.
thus in addition to the input and output layers these networks have and hidden neurons respectively.
empirical analysis shows that each network has accuracy on hold out test data.
.
.
human activity recognition har .har is a labeled data set used to train models to recognize specific human activities based on input from a smartphone s accelerometer and gyroscope.
input examples in this data set are labeled with one of six activities walking walking upstairs walking downstairs sitting standing and laying down.
the input data for the model are statistics computed from a smartphone s accelerometer and gyroscope sensor such as mean median min max etc.
in total input statistics are computed from these two sensors.
inputs to the network are normalized to be in the range of .
we use a network trained on this data set with an architecture of 1x500 meaning there is a hidden layer with neurons.
the network takes the inputs and produces a score in the range of for each of the outputs one per activity.
the output with the maximum score is the classification.table statistics of the benchmarks the total number of verification problems is .
name in each network input out nn s in out hidden neurons region acas 1 2 .
acas 3 .
acas 4 .
acas 5 13 .
acas 14 .
acas 15 .
mnist 3x100 mnist 2x512 mnist 4x1024 har 1x500 .
table shows the statistics of these benchmarks including the number of input neurons the number of output neurons the number of hidden layers as well as the total number of neurons in these hidden layers.
the last two columns list the experimental parameters we used namely the number of regions of interest in the verification problems and the output we attempt to verify.
.
experimental evaluation we want to answer the following research questions isreludiff more efficient than existing methods in differential verification of neural networks in that it can both verify properties faster and verify more properties in general?
isreludiff more accurate than existing methods in the forward pass?
toward this end we directly compared reludiff to two state ofthe art verification tools reluval and deeppoly .
both are designed to formally verify the absence of adversarial examples.
a comparison with reluplex was not possible since it does not support affine hidden layers which are necessary for analyzing the combined network f x as shown in figure however we note that reluval previously has been shown to significantly outperform reluplex on all acas xu benchmarks .deeppoly a followup of ai2 has also been shown to outperform ai2.
we ran all experiments on a linux server running ubuntu .
an intel xeon cpu e5 and gb memory.
timeout is set at minutes for each verification problem.
when available we enable parallelization in all tools and configure them to allow up to threads at a time.
.
results to evaluate efficiency and accuracy we truncate each network s weights from bit floats to bit floats and attempt to verify the shown in table .
we measure the number of properties verified and the execution time to verify each property.
.
.
acas xu.
the results for acas xu are shown in tables and .
in table columns and show the input property used and the number of networks we verified the property on which are taken from .
columns show the number of neural networks for which the property was verified and undetermined for each tool.
undetermined means that either the tool reported it could not verify the problem due to over approximation or the timeout of minutes was reached.
722table accuracy of comparison of the three tools on acas.
benchmark verif.
reludiff new reluval deeppoly problems proved undet.
proved undet.
proved undet.
acas 1 2 acas 3 acas 4 acas 5 acas 6 acas 7 acas 8 acas 9 acas 10 acas 11 acas 12 acas 13 acas 14 acas 15 total table efficiency of reludiff vs.reluval on acas xu.
benchmark verif.
total time s problems reludiff new reluval avg.
speedup acas 1 .
.
.
acas 3 .
.
acas 4 .
.
.
acas 5 .
.
.
acas 6 .
.
.
acas 7 .
.
.
acas 8 .
.
.
acas 9 .
.
.
acas 10 .
.
.
acas 11 .
.
.
acas 12 .
.
.
acas 13 .
.
.
acas 14 .
.
.
acas 15 .
.
.
in table columns show the time taken by reludiff and reluval for all problems verified.
the last column shows the time ofreluval divided by the time of reludiff.
for timeouts we add minutes to the total which is why we display that the speedup is greater than or equal to xfor some properties.
we omit the timing data for deeppoly since it cannot verify most properties.
these results emphasize the improvement that reludiff can obtain in both speed and accuracy.
it achieves orders of magnitude speedups over reluval.
for example reludiff finishes the networks for 4in .
seconds whereas reluval takes at least more than hours.
overall reludiff verifies more problems for which reluval times out and more problems for which deeppoly is too inaccurate to verify.
to understand why reludiff performs better we plot the distribution of the depth at which each sub interval was finally able to be verified for 4 shown in figure .
we can see that reludiff consistently verifies sub intervals at much shallower split depths.
we point out that the number of sub problems grows exponentially as the split depth increases.
indeed even though the difference between the average depths does not seem large about for reludiff and for reluval reluval had to verify million sub intervals for 4 whereas reludiff only had to verify 66k.
.
.
mnist.
while in acas xu the input region to verify is defined by the property for mnist we must generate the input figure 4max depth distribution.
figure interval on mnist 4x1024.
table accuracy comparison of the three tools on mnist.
benchmark verif.
reludiff new reluval deeppol y problems proved undet.
proved undet.
proved undet.
3x100 global 2x512 global 4x1024 global 3x100 pixel 2x512 pixel 4x1024 pixel table efficiency comparison of the three tools on mnist.
benchmark verif.
total time s problems reludiff new reluval deeppol y 3x100 global .
.
.
2x512 global .
.
.
4x1024 global .
.
.
3x100 pixel .
.
.
2x512 pixel .
.
.
4x1024 pixel .
.
.
region ourselves.
we generate input regions for mnist using two methods.
the first method is based on global perturbation .
we take test images and for each one we allow each of the pixels to be perturbed by gray scale units.
the second method is based on targeted pixel perturbation .
we take the same test images and for each one we set the range of random pixels to while the remaining remain fixed.
we can again see in tables and that reludiff is significantly more accurate and efficient than both reluval anddeeppoly.
both competing techniques struggle to handle global perturbations even on the small 3x100 network let alone the larger 2x512 and 4x1024 networks.
on the other hand reludiff can easily handle both the 3x100 and 2x512 networks achieving at least orders of magnitude speedup on these networks.
we also see a three orders of magnitude speedup on the two largest networks for our targeted pixel perturbation experiments.
even though reludiff begins to reach its limit in the global perturbation experiment on the largest 4x1024 network we point out that reludiff is significantly outperforming both deeppoly andreluval in the accuracy of their forward passes.
figure compares the output bound verified on the first single forward pass of each technique.
the comparison is presented as a scatter plot where the x axis is the bound verified by reludiff and the y axis is that of the competing technique.
723table accuracy comparison of the three tools on har.
benchmark verif.
reludiff new reluval deeppol y problems proved undet.
proved undet.
proved undet.
1x500 table efficiency comparison of the three tools on har.
benchmark verif.
total time s problems reludiff new reluval deeppoly 1x500 .
.
.
the graph shows that reludiff is nearly two orders of magnitude more accurate than reluval and one order of magnitude more than deeppoly.
the improvement over deeppoly especially emphasizes the promise of reludiff s approach.
this is because reludiff is already outperforming deeppoly yet it uses a simpler concretization approach during the forward pass whereas deeppoly uses a more sophisticated linear relaxation.
we believe that reludiff can be extended to use more accurate techniques such as linear relaxation which would further improve the accuracy however we leave this as future work.
.
.
har.
for har we also created our verification problems using input perturbation.
we take concrete test inputs and for each one we allow a global perturbation of .
.
the results are summarized in tables and .
again the experimental comparison shows that reludiff is significantly more accurate and efficient.
.
threats to validity our method is designed for verifying neural networks typically found in control applications where the number of input signals is not large.
in this context dividing the input region turns out to be a very effective way of increasing the accuracy of interval analysis.
however neural networks in different application domains may have different characteristics.
therefore it remains an open problem whether bi section of individual input intervals is always an effective way of performing refinement.
our method is designed for feed forward relu networks.
although there is no significant technical hurdle for it to be extended to convolutional neural networks or other activation functions such as sigmoid tanh and max pool as shown recently by singh et al.
we have not evaluated the effectiveness.
specifically linear relaxation can be used to handle these features when it comes to approximating non linear behavior.
while we use concretization inreludiff extending it with linear relaxation is possible .
however we leave these extensions for future work.
related work while there is a large and growing body of work on detecting adversarial examples for neural networks they are typically based on heuristic search or other dynamic analysis techniques such as testing .
although they are effective in finding security vulnerabilities and violations of other critical properties we consider them as being orthogonal to formal verification.
the reason is because these techniques are geared toward finding violations as opposed to proving the absence of violations.early work on formal verification of deep neural networks relies on using smt solvers or smt solving algorithms designed for efficiently reasoning about constraints from the relu activation function.
along this line a state of the art tool is reluplex .
in theory these smt solver based techniques can solve the neural network verification problem in a sound and complete fashion i.e.
returning a proof if and only if the network satisfies the property.
in practice however their scalability is often limited and they may run out of time for larger networks.
another line of work on verification of deep neural networks is based on interval analysis which can be more scalable than smt solver based techniques .
they compute conservative bounds on the value ranges of the neurons and output signals for an input region of interest.
they also exploit the fact that neural networks are lipschitz continuous to ensure that the interval analysis results are sound.
reluval and deeppoly are two representatives among other similar tools .
in addition to formal verification there are techniques for evaluating and certifying the robustness of neural networks or certified defense against adversarial examples .
however neither they nor the existing verification techniques were designed fordifferential verification of two closely related neural networks which is the focus of this paper.
as shown by the examples in section and the experimental results in section directly applying these techniques to differential verification is often extremely inefficient.
in contrast our method is designed specifically for solving the differential verification problem efficiently.
at a higher level our method relies on symbolic interval analysis which can be viewed as a specific form of abstract interpretation .
while the abstract interpretation framework allows approximations to be performed in a more general way e.g.
using relational abstract domains such as the octagon and polyhedral domains so far it has not be adequately explored.
we plan to explore the use of these abstract domains as part of the future work.
finally the term differential verification has been used in the context of verifying a new version of a program with respect to a previous version which is treated as an oracle .
in a sense the truncated network is a new version of the original network and the original network can be thought of as an oracle.
conclusion we have presented a new method named reludiff for differential verification of two closely related neural networks.
it is capable of formally proving the accuracy of a compressed network with respect to the original network.
internally reludiff relies on symbolic interval analysis to more accurately compute and propagate differences in the values of neurons of the two networks from the input to the output and then relies on the gradient difference to more accurately compute the refinement.
our experimental comparison ofreludiff with state of the art formal verification techniques shows that it can often achieve two orders of magnitude speedup and produce many more proofs.