naturallanguage to code how fararewe?
shangwen wang wangshangwen13 nudt.edu.cn collegeof computer national universityof defensetechnology changsha chinamingyanggeng gengmingyang13 nudt.edu.cn collegeof computer national universityof defensetechnology changsha chinabolin linbo19 nudt.edu.cn collegeof computer national universityof defensetechnology changsha china zhensu sun zhensuuu gmail.com school of computing and information systems singaporemanagement university singaporeming wen mwenaa hust.edu.cn schoolof cyberscienceand engineering huazhonguniversityof scienceand technology wuhan chinayepangliu liuyp1 sustech.edu.cn department of computer science and engineering southernuniversityof scienceand technology shenzhen china li li lilicoding ieee.org schoolof software beihang university beijing chinategawend f.bissyand tegawende.bissyande uni.lu universityof luxembourg luxembourgxiaoguangmao xgmao nudt.edu.cn collegeof computer national universityof defensetechnology changsha china abstract alongstandingdreaminsoftwareengineeringresearchistodevise effectiveapproachesforautomatingdevelopmenttasksbasedon developers informally speci f ied intentions.
such intentions are generally in the form of natural language descriptions.
in recent literature a number of approaches have been proposed to automate tasks such as code search and even code generation based on naturallanguageinputs.whiletheseapproachesvaryintermsof technical designs their objective is the same transforming a developer sintentionintosourcecode.theliterature however lacksa comprehensive understandingtowardsthe effectivenessofexisting techniques as well as their complementarity to each other.
we proposeto f illthisgapthroughalarge scaleempiricalstudywhere we systematically evaluate natural language to code techniques.
speci f ically weconsidersixstate of the arttechniquestargeting codesearch andfourtargetingcodegeneration.throughextensive evaluationsonadatasetof22k naturallanguagequeries ourstudy reveals the following major f indings code search techniques based on model pre training are so far the most effective while code generation techniques can also provide promising results complementaritywidelyexistsamongtheexistingtechniques and combining the ten techniques together can enhance the shangwenwang bolin andxiaoguangmaoarealsowiththekeylaboratoryof softwareengineering for complexsystems ming wen and yepang liu arethe corresponding authors.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
for compared with the most effective standalone technique.finally weproposeapost processingstrategytoautomaticallyintegratedifferenttechniquesbasedontheirgenerated code.experimentalresultsshowthatour devisedstrategy isboth effective andextensible.
ccs concepts software and its engineering reusability automatic programming .
keywords code search code generation pre training technique acmreference format shangwen wang mingyang geng bo lin zhensu sun ming wen yepang liu li li tegawend f. bissyand and xiaoguang mao.
.
natural language to code how far are we?
.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
.
.
introduction recommender systems are widely studied in software engineering research as they are concrete building blocks for improving developers productivity .ahighly soughtachievementinthis domain is to effectively transform developers intentions which aregenerallyspeci f iedinnaturallanguage intopiecesofcode .
addressing such a challenge will alleviate some software development burdens and facilitate critical designs and implementation choices such as selecting the appropriate programming interfaces touse .indeed developersinalllevelsofprogrammingpro f iciencyfrequentlyaskquestionsofvaryingcomplexityabouthowto implement speci f ic functionalities .
for example it is typical to seedevelopershavingtheintentionto removeaspeci f icitemfrom esec fse december3 san francisco ca usa wang geng lin sun wen liu li bissyand mao.
anarray lookforrelatedcodeinq aforums.1recentadvances in deep learning have enabled the development of promising techniquesintwolinesofresearchtowardstransformingdevelopers informally speci f iedintentions a.k.a queriesthataregenerallyin theformofnaturallanguagedescriptions intosourcecode code searchaims at retrieving a relevant piece of code within a large codebase while code generation aims to synthesize codefromscratch .inpractice toobtainthedesiredcode adeveloper mayleveragecodegeneration techniquesto generate codedirectly orretrieverelevantcodesnippets froma large scale codebasesuchasstackover f low.asaresult theapplicationscenarios of these two types of techniques could overlap with each other to a certain degree .
we refer to all such relevant literature techniques as natural language to code nl2code techniques.
nl2code techniques differ in terms of various design aspects.
first atahighlevel thetheoreticalworkingmechanismsofcode search and code generation are different code search focuses on mappingthesemanticrelevancebetweenthequeryandanexisting code snippet and directly returns the code with the highest relevance score code generation in contrast constructs a piece of code fromscratch.
second theycanbe differentiatedaccording to whether a pre trained model is used.
those that build on pretrainingtechniques suchasgraphcodebert andcodet5 adopt a pre training and f ine tuning pipeline where deep learningmodelsare f irstpre trainedonalarge scalecorpusaimingat capturing the semantic relation between natural language and programminglanguage andthen f ine tunedonspeci f icdownstream tasks.
in contrast those that do not rely on pre trained models referredtoas non pre trainingtechniques suchasmman and tranx traintheirmodelsfromscratchonrelativelysmall scale labelleddatasets.thesetrainingmethodscansigni f icantlyaffect theeffectivenessof nl2codetechniques.
for instance zeng et al.
observedthatpre trainingtechniquesoutperformnon pre training techniques on a number of code intelligence tasks such as code clone detection .
furthermore according to the intrinsic difference between code search and code generation the former may produce high quality results if there exists a code snippet that is similar to the intended functionality on the contrary the latter maygenerate more reasonable results if there isno code snippet for reference whose semantic issimilar to the intention.
although enormous efforts have been made towards advancing the nl2code techniques the effectiveness of existing techniques has not been systematically studied and compared.
particularly intheliterature codegenerationandcodesearchtechniques are often evaluated separately which means that code generation techniques are compared to each other without considering code search techniques and vice versa.
as a result little is known about their complementarity with each other i.e.
can the query ineffectively handled by one technique be addressed wellbyanother?thereisthusanurgentneedforacomprehensive empiricalstudycomparingandanalyzingtheeffectivenessofthe state of the art nl2code techniques basedon a large number of nldescriptions.suchastudyisnecessaryandessential whichcan help us f ind the answers to important questions when designing nl2code techniques in the future.
for instance which is the most 1a related question nl2code technique so far and what is the common weakness of existing nl2code techniques?
moreover to what extent do existing nl2code techniques complement each other and whether the integration of them can enhance the performance?
answering such questions can provide practical guidance for studies within this f ield.
driven by this xu et al.
took the f irst step in this direction buttheiruserstudyislimitedinitsscaleduetotheextensivehumanintervention.speci f ically only14functionalitiesand twonl2code techniques were investigated.
inthis paper we aimat f illthe gapby performing the f irstlargescale empirical study that evaluates the effectiveness of both code generation and code search techniques collectively under a controlledexperimentsetting.speci f ically ourstudycoverstenstate ofthe art nl2code techniques including six code search techniques andfourcodegenerationtechniques onacomprehensivebenchmarkcontaining22k naturallanguagequeries.ourexperiment settingisuser oriented.first weaimtoevaluatehowsimilarthe code returned by an nl2code technique is to the oracle and we usethecodebleumetric tocomparethesimilaritybetween thereturnedresultandthe oraclecodewithrespecttothetokens syntacticstructures anddata f lows.second tomimicreal world scenarios we remove the oracle code i.e.
the code snippet that correspondstoeachquery fromthesearchspaceofcodesearch techniques.
the rationale behind this is that in practice developers can hardly f ind exactly what they want from the codebase .third weevaluatetheeffectivenessofcurrenttechniquestogeneratemethod levelcodesnippets whichisthemost desirable granularity for developers compared with other granularities e.g.
variables andstatements .
our study makesthe following important f indings f1 theeffectivenessofcodegenerationtechniquesispromising and exceeds that of the non pre training code search techniques.
however the state of the art pre training based codesearchtechnique isstillthemosteffective oneamong the nl2code techniques.
f2 accuratelygeneratingprogramidenti f iersisauniversalchallengeforboth codesearch andcodegeneration techniques sincetheygenerallyachieverelativelylowtokensimilarities to the oracle.
f3 existingnl2codetechniquescomplementeachotherwell if we combine all the ten selected techniques we can enhance theperformanceofover35 withrespecttotop compared to the mosteffective standalone technique.
f4 combining code search with code generation or different code searchtechniques demonstratespromisingresults.
moreover we design a post processing strategy that re ranks the results from different techniques based on the number of overlapped tokens with the query.
our results show that such a combinationstrategyiseffective bycombiningthemosteffectivecode searchandcodegenerationtechniques wecangainaneffectiveness improvement of and with respect to the top results comparedwitheachstandalonetechnique.furthermore itisalso extensible further effectiveness enhancement could be achieved byinvolving more techniques for combination.
376natural languageto code howfarare we?
esec fse december3 san francisco ca usa backgroundand related works .
codesearch code search cs aims at helping developers retrieve some implementationsthatcanserveasreferencesfortheirdevelopment activities .
given a natural language nl query from the developer cs searches for the relevant code snippets from a large scale code corpus.
traditionally this process is mainly done by theinformation retrieval technique such askeyword matching .
however these techniques are known to be suboptimal atcapturingthesemanticrelationsbetweencodeandnaturallanguage queries .
later on researchers have proposed various deep learning basedapproachestobridgethesemanticgaps.for instance gu etal.
proposedeepcswhichjointlyembedscode snippetsandnaturallanguagedescriptionsintoahigh dimensional vector space where codesnippetsandqueries canbe matchedaccordingtotheirsimilarities.wan etal.
designamulti modal attentionnetworkthataggregatesinformationfromthetokensequence abstractsyntaxtree ast andcontrol f lowgraph cfg for representing programs.
.
codegeneration code generation aims at directly generating source code according to software requirements .
traditional approaches leverage formalmethodstoautomaticallygeneratesourcecode butthe formalspeci f ications are hardto create .with the advancesin deeplearning researchersproposetoautomaticallylearnthetransformationsfromtherequirementstosourcecode.speci f ically ling etal.
treatcodegenerationassequence to sequencetranslation andbuildaneural networkthattargetsgeneral purposeprogramming languages like python.
dong et al.
explore the idea of usingtwodecodersinthecodegenerationtask wherethe f irstone aims at predicting a rough program sketchand the second one f ills inthe details.
.
pre training techniques training a deep learning model from scratch usually needs a large amount of task speci f ic annotated data which is hard to collect in practice.
to overcome this limitation pre training techniques have been proposed in recent years.
the core idea is to pre train a modelon oneormore self supervised taskswhere largeamounts oftrainingdataarereadilyavailablesothatthenetworkweights can encode some commonsense knowledge compared with randomlyinitialized.afterthat withasmallamountoftask speci f ic data the pre trained models can be f ine tuned in the traditional supervised manner.
recently researchers have build several pretrained models for programming language pl byusing the large amountofbimodalinstancesofnl plpairs i.e.
thesourcecode anditscorrespondingcomments .arecentstudy investigatedtheexistingpre trainedmodelsforplonstandalone downstreamtasks i.e.
codesearchandcodegenerationareseparately evaluated .
in contrast our study includes both pre training andnon pre trainingtechniquesandinvestigatestheirstrengths andweaknessesinacontrolled naturallanguagetocode experiment setting i.e.
using the identical queries and the oracle code is removedfrom the searchspacefor code searchtechniques .table1 selectedtechniques inthisstudy.
code search code generation w opre trainingself attention tree lstm ggnn multi modal tranx w pre trainingcodebert graphcodebert codet5 natgen spt code studydesign .
selected techniques over the years a large number of code search and code generation techniques have been proposed .
therefore it requires tremendousengineeringeffortstoevaluateallofthem.inthisstudy weselectrepresentativetechniquesandweleavetheexplorationof moretechniques asourfuture work.totally we usetennl2code techniques including six code search techniques and four code generation techniques.
those techniques can be classi f ied into two typesaccording to whether they use pre trainingand table 1lists thecategorization.theselectedtechniqueshaveservedasthebaselinesforanumberofstudies andachievedpromising results in recent replication studies and thus they can represent the state of the art well.
for instance through a comprehensive comparison among existing pre training techniques e.g.
plbart andcodegpt zengetal.
foundthat graphcodebertandcodet5achievethebestperformanceoncode search and code generation respectively.
the following brie f ly introduces eachofthe selectedtechniques.
.
.
code search.
typically a code search technique should embedboththecodesnippetandthequeryintovectors afterwhich therelevancebetweenthecodeandthequerycanbecalculated.for theselectedfournon pre trainingtechniques theapproachusedto embedqueriesisidentical weuseanencoderwithsixtransformer blocks to deal with the natural language token sequence plus with byte pair encoding bpe to split tokens.
in the following fourparagraphs we introduce howto embedthe code snippets.
self a t tention.
this is a baseline proposed by husain et al.
.
it treats code as token sequences and uses an encoder of the transformer architecture to embed such sequences.
this approach mimics the work f low of the well known deepcs i.e.
both approachestreatprogramsascodetokens butisexpectedtoestablish amoreadvancedeffectivenessbaseline asthetransformerarchitecture is known to perform well on the long term dependency problemfacedbytherecurrentneuralnetworks rnn which isusedbydeepcs.
tree lstm.
tree lstmisanapproachthatgeneralizesthelong short termmemory lstm network to tree structuredtopologies.
initially it targeted at capturing the syntactic properties of natural languages anditwas f irstlyappliedtotheastsofprograms bywanet al.
.
ggnn.zenget al.
proposeto constructthe variable based f lowgraphthatdepictsdataandcontrol f lowsintheprogram.such graphs are constructed by transforming the programs into their intermediate representations irs extracting the identi f iers in each ir instruction as nodes and building dependencies among nodes.afterthat agatedgraphneuralnetwork ggnn isused 377esec fse december3 san francisco ca usa wang geng lin sun wen liu li bissyand mao.
togeneratetheembeddingforthegraph whichisalsotherepresentationofthe code.
multi modal.
withtheintuitionthataggregatinginformation frommultiplemodalitiesofsourcecodecanenrichitsrepresentation wan etal.
proposemmanthatutilizesthetokensequence the ast as well as the graph information of a program.
in this paper werebuildthemulti modallearningmodelviafusingthethree aforementioned approaches self attention tree lstm ggnn .
itshouldbenotedthattherebuiltmulti modallearningmodelis supposed to perform better than the vanilla mman since mman onlyinvolvescontrol f low informationwhile the ggnn approach involves both data andcontrol f lowinformation.
codebert.
codebert is a transformer based pre trained model for programming languages like python and java.
it has two tasks in the pre training stage which are masked language modelingandreplacedtokendetection.toapplythepre trained modelonthecodesearchtask therepresentationofaspecialtoken the beginning token of the input is used to measure the semantic relevancebetween the code snippetandquery.
graphcodebert.
guoet al.
take data f low information into consideration in the pre training stage.
in addition to masked language modeling there are two newly proposed structure aware tasks inthe pre training i.e.
edge predictionand nodealignment.
then the work f low of applying the pre trained model to code searchisidenticalto that ofcodebert.
.
.
code generation.
tranx.tranx predicts a sequence of actions to construct an ast based on which the source code is generated.it f irstde f inesanabstractsyntaxdescriptionlanguage framework whichisagrammaticalformalismofasts.basedon that threetypesofactionsarepredictedateachtimesteptoexpand the tree until the whole tree is constructed.
note that a number of follow upstudiesrelyonthegrammarrulesintroducedbytranx to construct asts .
therefore we select tranx as the representative technique.
codet5.codet5 followsthet5architecture withthe input being the sequence of code and text tokens and the output also in a sequential format.
one specially designed pre training taskisnl pldualgenerationinwhichthemodellearnstogenerate codefromtextsandgeneratetextsforcodesimultaneously.after pre training codet5canbenaturallyadaptedtocodegeneration due to its encoder decoder framework.
a number of follow up studies rely on the pre trained parameter values of codet5 so we select codet5 as the representative pre training based code generation approach.
the authors of codet5 provide two versions ofthismodel whichhavedifferentparametersizes.weuse codet5baseinthis study since itismore effective .
natgen.
natgen is designed based on codet5 and incorporates an extra pre training task code naturalizing which is designedtoteachthemodelhowtotransformunnaturalcodeintoa morenatural human writtenform.thisadditionaltaskisintended to encourage the model to better understand the underlying semantics of the code and thus enhance the model s capability on generatingcode that closelyresembles human written ones.
spt code.
spt code is another state of the art pre trained model with the encoder decoder framework.
the input to the spt code model during the pre training stage differs from thatofcodet5intwoways.first itsinputincludestheabstractsyntax tree ast of the code which enablesit to leverage syntactic information.
second to eliminate the need for a bilingual corpus i.e.
acodesnippetpairedwithacorrespondingcomment spt code leverages the method name and the names of the methods that are called within that method as a natural language description of the sourcecode being analyzed.
exclusion.
abranchofstudyfocusesonutilizingtheretrieval results to guide code generation .
we exclude them from this study since the retrieval and edit approach assumes thatthe input and outputof the method isalready knownand the methodispartiallywritten whichisunfairtoourstudysubjects i.e.
wedonotrequirepriorknowledge and recode isbuilt ontopofasetofsuboptimalgrammarrules whichisnotasgeneral as tranx.
we also exclude a recently proposed code generation approach pycodegpt since it only supports generating code thatreusesthethird partylibraries pandasandnumpy whichis not generalenough.
.
dataset we select the widely used codesearchnet dataset as our evaluationbenchmark whichisminedfrompopulargithubprojects interms of the number of stars and forks .
in our study we focus onthe bimodaldata i.e.
the codesnippetanditsassociateddocumentation by treating thedocumentation as thenatural language queryandthecodesnippetasthegroundtruth.codeinthisdataset are all method level snippets and our study thus focuses on the effectiveness of existing techniques at the method level.
to keep in high quality thisdatasethasalreadybeenpre processedbyseveral steps.forinstance anydocumentationshorterthanthreetokens is removed since it might not be informative and any code snippet shorterthanthreelinesisalsoremovedsinceitislikelytobegetters and setters.
we explicitly focus on the python language in this study since python is the most widely targeted general purpose programming language in the code generation domain andourselectedtranxonlysupportsthepythonlanguagesofar.
the dataset has already been split into the training validation test sets by the authors of codesearchnet and the training set has beenusedforpre training whichmeansthecross validationonthe wholedatasetisinappropriate techniques on the f ixed test set following existing studies .
in the end our dataset contains code query pairsfor training validation testing respectively.
.
research questions rq1 how effective are existing techniques on transformingnaturallanguagedescriptionsintosourcecode?
we f irst systematically investigate the effectiveness of each individual techniqueassummarizedintable 1ongeneratingmethod levelcode snippetsbasedonnaturallanguages.beyondthetraditionalsetting where the oracle code snippets are within the search space of code search techniques we design a new experiment setting in this rq where we assume the oracle does not exist in the search space and performthesearchontheleft22 175codesnippetsinthetestset.
378natural languageto code howfarare we?
esec fse december3 san francisco ca usa previous studies have shown that developers usually need to modify the retrieved code snippets to adapt them to the local contexts whichmeansthat inarealisticscenario thedesired code snippets can rarely be directly retrieved.
speci f ically gabel andsu investigatedthesyntacticuniquenessofsourcecodeand foundthatredundancyusuallyexistsonlyatthelinelevel whileat themethod level whichisourinvestigated granularity near total uniqueness was observed.
more recently in the user study performed byxu et al.
users modi f ied tokens of the retrieved code chunks several lines of code on average and such a number is expected to increase when it comes to the method level.
consequently our oracle excluded setting mimics the real application scenariowheretheuserssearchfortheresultsfromalarge scale corpusthatdoesnotcontaintheexactlydesiredcodesnippetand see how useful the retrieved results could be.
evaluations under such a user oriented setting can help us better understand the usefulnessofcodesearchtechniquesinreal worldscenarios.thisis thusthebasicsettingofthisstudyandourfollow upinvestigations are basedonthe results obtainedfrom this setting.
rq2 dodifferent techniquescomplementeachother?
in thisrq weaimtoinvestigateifdifferenttechniquestendtoperform similarly on thesame queries or if they exhibit performance differencesoncertainqueries.aswehaveintroduced existingtechniquescanbecharacterizedindifferentaspects suchasusingeither searchorgenerationstrategy withorwithoutpre training.this questioninvestigateswhethersuchdifferencesinthedesignspaces lead to certain complementarities with respect to their effectiveness.theanswerisessentialtoourfurtherinvestigations wecould be able to design a combinational strategy to integrate different techniques only if they demonstratecertaincomplementarities.
rq3.can we gobeyond thestate oftheart by combining existingtechniques?
basedontheexperimentaloutputsobtained from the previous rqs we further seek to investigate whether combining different techniques can achieve better performance.
we propose to investigate twosub questions rq3.1what is the best performance achievable by combining differenttechniques?
we f irstaimtoinvestigatethebestperformance achievableviacombiningdifferenttechniques whoseresultswill pave the wayfor the following question rq3.2canweautomaticallycombinedifferenttechniques?
reachingthebestperformancerequiresmanuallyinspectinganumber ofresults whichwouldbetime consuming.wefurtherseekto design a novel strategy that is able to combine different techniques automatically.
.
effectiveness assessment tojointlyevaluatecodegenerationandcodesearch wefocuson assessingthesimilaritybetweenthepredictedresultandtheoracle code.speci f ically wedecidetousefourmetricsfollowingtheexistingstudy including tokenmatch tm whichiscalculated by the standard bleu and aims to re f lect the similarity betweenthetokensequencesofthepredictedandoraclecode syntax match sm which aims to evaluate code quality from the natural tree structure of programminglanguage i.e.
the ast data f low match dm which aims to evaluate the semantic information of code through the dependency relations among variables andtable2 effectiveness of eachselectedtechnique in .
techniquestop top tm sm dm cb tm sm dm cb tranx .
.
.
.
.
.
.
.
codet5 .
.
.
.
.
.
.
.
natgen .
.
.
.
.
.
.
.
spt code .
.
.
.
.
.
.
.
self attention .
.
.
.
.
.
.
.
tree lstm .
.
.
.
.
.
.
.
ggnn .
.
.
.
.
.
.
.
multi modal .
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
self attentionw ooracle .
.
.
.
.
.
.
.
tree lstm w ooracle .
.
.
.
.
.
.
.
ggnn w ooracle .
.
.
.
.
.
.
.
multi modal w ooracle .
.
.
.
.
.
.
.
codebertw o oracle .
.
.
.
.
.
.
.
graphcodebertw o oracle .
.
.
.
.
.
.
.
theboldnamemeansthetechniquerequirespre training.
thegreencell denotes theoracle is excluded fromthesearchspace ofcodesearchtechniques.
theoptimum performances ofgeneration searchtechniquesare in bold.
codebleu cb whichisacombinationoftheabovethreemetrics andprovidesaholisticperspectivetothequalityofgeneratedcode.
readers can refer to for more details aboutthesemetrics.
in this study we calculate the similarity for the top results of each technique as well as the maximum values from the top5 results.
the rationale is that as suggested by the prior study developers only inspect a few results returned by recommendation tools .
.
experiment setting all our experiments were performed on a server which possesses nvidiateslav100with32gbmemory.notethattoalleviatepotentialreproduciblebias theselectednon pre trainingtechniques aretrainedfromscratchandthepre trainingonesare f ine tunedby ourselves.sinceallofourselectedtechniquesopensourcedtheir artifactsongithub wereusedtheoriginalimplementationsaswell asthevaluesofthehyper parametersselectedfor f ine tuning to avoid potential bugs in our implementation as well as enhance the reliability of our results.
note that initiallyggnn targeted c language.toapplyittopython weusethe dismodule2togenerate theirs afterwhichthegraphcanbegeneratedbasedonthescripts releasedbytheauthors.spt codewasnotevaluatedonthecode generation task in the original study .
to f ine tune it on this task wereusethehyperparametersusedto f ine tunethismodelon thecodesummarizationtask whichcanbe considered as another generationtask.
studyresults .
rq1 effectiveness ofexistingtechniques for each technique we calculate the metrics of the code produced by them for each query and the mean values on the whole test set are shown in table .
the mean value is one of the most representativestatisticsandithasbeenwidelyused byexistingstudies toassesstheperformancesofdifferenttechniques .
fromtheresults we f irstnotethatcomparedwithnon pre training techniques pre trainingtechniquesgenerallyachievebetterperformances.forinstance thecbof codet5withrespecttothetop 379esec fse december3 san francisco ca usa wang geng lin sun wen liu li bissyand mao.
resultis22.
whichexceedsthatof tranx i.e.
.
by75 .similarly thecbof codebert withrespecttothetop 1result whenthe oracleisexcludedfromthesearchspace is25.
whichexceeds that ofmulti modal i.e.
.
by around .
furthermore we notefromtheresultsthatthemosteffectivenl2codetechniqueso farisgraphcodebert withthecbscoreof25.
.thisiswithin ourexpectationconsideringthatpre trainingtechniquesrequire much more data e.g.
graphcodebert andcodet5are pre trained ondatafromsixprogramminglanguages andthuscanpreserve more domain knowledge .
we also note that compared with theothertwopre training basedcodegenerator i.e.
codet5and natgen spt codeachievescomparativelylowperformances.one possibleexplanation asintroducedinsection .
isthatthepretraining phase of spt code relies on method names which may besyntacticallyincomplete toapproximatethenaturallanguage description of the code.
this setting could potentially reduce its abilitytoalignnaturallanguageandprogramminglanguageand generatecode from naturallanguageinputs.
finding thepre trainingtechniquesachievebetterperformancesthannon pre trainingtechniquesforbothcodegeneration andcodesearch.themosteffectivenl2codetechniquesofaris graphcodebertwiththecodebleu of25.
.
wealso f indthatwithremovingtheoraclefromthesearchspace the effectiveness of code search techniques decreases signi f icantly.
speci f ically thecbofthestate of the artcodesearchtechnique i.e.
graphcodebert with respect to the top results when the oraclecodeisinvolved excludedis68.
.
respectively and the other f ive search techniques undergo the similar decreases when the oracle is excluded.
by further investigating its search results we f ind that with oracle involved it can rank the oracle code at top for queries which account for nearly of the total queries .
that is why it can achieve a highcbwiththeoracleinvolved.amongthenon pre trainingcode searchtechniques multi modal achievesthebestperformancewhen the oracle is involved with the cb of .
.
however these four techniquesachievenearlyidenticaleffectivenesswhentheoracleis excluded with respect to both top and top results.
speci f ically their cbs with respect to the top and top results are all around .
and23.
respectively.suchresultsmayindicatethatthere isa gap between the current evaluationof code search techniques and their real usefulness in practice.
indeed the current evaluation alwaysassumestheexistenceoftheexactlymatchedcodeinthe search space which ampli f ies the usefulness of code searchtechniques.wethuscallforauser orientedevaluationfor future studies that is to investigate to what extent the retrieved results could help developers when what they exactly need may not be retrieved.
finding if the oracle does notexistin thesearch space the effectivenessofcode search techniques decreases signi f icantly.
another phenomenon we observe is that comparing with focusingonlyonthetop 1results theeffectivenessofthecodesearch techniques increases signi f icantly if the top results are considered while that of the codegenerationtechniques nearly remains figure the cbs of the syntactically correct incorrect top code snippets generatedby codet5.
unchanged.
speci f ically without the oracle the cbs of graphcodebertwhen considering top top results are .
.
respectively an increase of when all the top results are considered.
in contrast those of codet5are .
and .
respectively with only slightenhancement.
such resultssuggest that themost quali f ied candidate code snippets are sometimes not ranked at the top positions for code search techniques while the top generated codesnippetsusuallyreachtheoptimum.
therefore recommendingmoreresultsfromthereturnedliststotheuserscouldbeuseful for code search but such usefulness would not be signi f icant for codegeneration.despitethat we f indthe top 1resultsfrom code generation techniques are already promising they can be more similar to the oracle code compared with the retrieved results of certain code search techniques.
for instance the cb of codet5 with respect to the top results is .
while those of the four non pre training code search techniques are around .
.
finding unlikecodesearchtechniquesthatsometimesdo notrankthebestcandidatesatthetop 1positions codegeneration techniquesusually predict the optimumresultsatthe top positions and their effectiveness can exceed that of non pre training code search techniques.
wefurtherinvestigatethepromisingresultsachievedby codet5 the best performing code generation technique.
since it directly generatesthetokensequencewithoutanygrammaticalguideline unliketranx onecriticalconcernisthatthegeneratedcodemight be syntactically incorrect.
ourinvestigation shows thatthegeneratedtop 1codesnippetsaresyntacticallycorrectfor19 340queries accounting for of the queries in the test set.
we also dissect thecbsofthesesyntacticallycorrect incorrectcodesnippetsand demonstratetheresultsinfigure .we f indthatthemediansofthe cbsofsyntacticallycorrect incorrectcodeare verysimilar .
vs. .
andthe differencesbetween thetwogroupsare not statistically signi f icant i.e.
with the p value .
in a one sided mann whitney u test .
this indicates that being syntactically correctornotdoesnotnecessarilyaffectthemetricvalueofthegeneratedcode.italsoindicatesthatmoremetricsareneededtobetter re f lect the syntax differences of different code.
we perform further analysis towards such incorrect cases and f ind that codet5often generatesablockofcoderecurrently.theincorrectnesshappens when the token sequence exceeds a pre de f ined length determined by the hyper parameter which means codet5stops generating more tokens while the current line is not f inished.
therefore such incorrect code still ful f ills certain functionalities and thus can have high cbs.
we give an example inour onlinerepository.
finding beingsyntacticallycorrect ornotdoesnotnecessarilyaffectthecbsofcode generated bycodet5.
380natural languageto code howfarare we?
esec fse december3 san francisco ca usa wealsocarefullycheckourexperimentresults.we f indthatour results are generally consistent with those reported in previous studies.
for instance for four non pre training code search techniques ourresultsshowthatiftheoracleisinvolved combining informationfrommultiplemodalitiescanachievethebestperformance and the token sequence information is the most rewarding single modality i.e.
self attention achieves higher cb than treelstmandggnn .
this is identical to the phenomenon reported bywanetal.
.we notethe tmof tranx .
issigni f icantly lower than the value reported in the previous study which is .
.
after further investigation we f ind that their dataset is fromcontest programsinwhichtheidenti f iersare usuallysimple but meaningless like i j and k. on the contrary our dataset is fromreal worldopen sourceprojectsinwhicheachidenti f ierisexpected to express rich semantic information and thus may be more complex e.g.
camelcasesandunderscorenamingconventions .
sincetm thestandardbleu focusesontheidenti f iermatching relations we consider our result as reasonable itre f lects that currentlysemantic meaningfulidenti f iersinreal worldprojectsare difficult to predict.
indeed our results illustrate that for all the involved techniques their tms are signi f icantly lower than their sms and dms indicatingthattheinabilitytoaccuratelygenerateidenti f iersisauniversal weaknessoftheexistingcodegeneration searchtechniques please note the identi f ier nameis ignored when calculating smand dm .
thiscanbeexplainedbythefactthatprogramidenti f iersusually demonstrate uniqueness.
for instance nguyen et al.
found thatmorethan60 ofthemethodnamesoccuronlyonceamong 14k projects.
suppose an identi f ier in the oracle code is unique theretrieved resultswill notmatchwith it andsimilarly thegeneration techniques seem unlikely to generate it since it may not beinvolvedinthevocabularyfromwhichthe outputispredicted.
a concrete example is shown in listing .
in this case the query expressestheintentiontoremovequotesfromastring.we f indthat thesemanticsofthecodegeneratedby codet5isnearlyidenticalto thatoftheoraclecode exceptthatitfailstocheckifthestringstarts andendswithdoublequotes inpython astringcanbewrapped with either single or double quotes .
therefore the sm and dm of codet5are extremely high both exceeding70 .
however codet5 fails to accurately predict the names of the identi f iers.
for instance ituses storepresenttheinputstringparameterwhileintheoracle this identi f ier is named as istr.
since this identi f ier occurs for many times in the code the tm of codet5is thus only .
a relatively low value.
this case also reveals that relying solely on thebleuvaluetoevaluatethegeneratedcodeispotentiallybiased demonstratingtherationaleofamorecomprehensivemetriclike codebleu.
finding producingaccurateprogramidenti f iersisauniversal challengeforboth generation and search techniques.
.
rq2 complementarityofexisting techniques toinvestigatethecomplementarityofexistingnl2codetechniques for each technique pair we compute the pearson correlation u1d45f with respectto their cbs achievedon each query we focus on cb1 code generated by codet5 2def remove quotes s 3ifs ands returns 5else returns oracle code 9defunquote ends istr 10ifnotistr returnistr 12if istr andistr or istr andistr returnistr 15else returnistr code retrieved by graphcodebert 19defstrip email quotes text 20lines text.splitlines 21matches set 22forlineinlines prefix re.match r s line ifprefix matches.add prefix.group listing the code generated by codet5 the oracle code and the coderetrievedby graphcodebert forthequery removeasingle pair of quotes fromtheendpointsof a string .
heresinceitrepresentstheoveralleffectiveness .pearsoncorrelationisawidelyusedmetrictoassessthecorrelationdegreebetween two sets ofdata .
theoretically a high pearsoncorrelation coefficient suggests that the two sets of data follow a similar trend.
in our context it means two techniquesmay have similar cbs for a speci f ic query.
in contrast if two techniques have a relatively lowpearsonvalue itsuggeststhatthereislittleornocorrelation betweentheircbs.thisindicatesthepotentialexistenceofqueries on which the two techniques achieve rather different cbs.
in such cases they could be considered as complementary to each other.
forinstance iftwotechniquesexhibitidenticalperformanceson eachquery theirpearsonvaluewouldreachthemaximumvalueof .however theymaynotcomplementeachotherwellbecausethey sharesimilareffectivenesstowardsthesameinputs.ourinterpretation of u1d45fis based on the previous study negligible correlation u1d45f .
lowcorrelation .
u1d45f .
moderatecorrelation .
u1d45f .
high correlation .
u1d45f .
and very high correlation .
u1d45f .
results are shown in figure .
we observe that according to the pearsoncorrelationvalues theselectedtechniquescangenerally be classi f ied into three clusters as highlighted the code generation techniques thenon pre trainingcodesearchtechniques andthe pre trainingcodesearchtechniques.fortechniquesineachcluster they have a relatively high correlation with each other and a relativelylowcorrelationwiththosefromotherclusters.speci f ically in figure 2a tranxandcodet5have moderate pearson correlation between them i.e.
.
.
similarly codebert andgraphcodebert havehighpearsoncorrelation i.e.
.
.asforthefournon pretraining code search techniques they all have moderate pearson correlation between each other e.g.
the value between tree lstm andggnnis .
.
in contrast the pearson correlation between cross cluster techniques is usually low or negligible e.g.
the value betweenmulti modal andgraphcodebert is .
demonstrating the effectiveness of such techniques is weakly correlated.
we also 381esec fse december3 san francisco ca usa wang geng lin sun wen liu li bissyand mao.
a pearsoncorrelationfor cbsof the top results b pearsoncorrelationfor cbsof the top results figure2 pearson correlation results for theselectedtechniques.
notethatthe highest pearson value i.e.
.
between graphcodebertandcodebert is still lower than .
the threshold of the very high correlation degree .
this indicates that the effectiveness of two highly correlated techniques may still differ to a certain degree on speci f ic queries.
we also observe the similar trend in figure2b.suchresultsindicatethatcomplementaritieswidelyexist among existing techniques.
figure3illustrates the relationship among three representative techniques of each cluster the ones with the highest effectiveness ineachcluster i.e.
graphcodebert codet5 andmulti modal viaa scatterplot.thexandyvaluesofeachscatterdenotethecodebleu values of two different techniques achieved on a speci f ic query.
we investigate both top and top results and f ind similar trends so weonlyshowthetop 1resultshere.foreaseofcomparison wealso drawtheline y xinthe f igure.scattersabovethislinerepresent that the technique denoted by the vertical axis outperforms the technique denoted by the horizontal axis on those queries and vice versa.weobservethatnotechniquecanconsistentlyoutperform the other competitor even the most effective one graphcodebert can still perform worse on speci f ic queries compared with codet5 ormulti modal .
this further shows the complementarity of the existing techniques.
finding existingnl2codetechniquesarecomplementary since theycangenerallybeclassi f iedintothreeclusterswith high intra cluster pearson correlations and low inter cluster pearsoncorrelations and notechniquecanconsistentlyoutperform theothers onall thequeries.
caseanalysis.
todemonstratethecomplementarityofexisting techniques weanalyzetwocaseshere.the f irstisshowninlisting where we also list the retrieved result from graphcodebert for the same query.
due to space limitation we only show the f irst severallines.werecallthat codet5achievesahighcbonthisquery i.e.
higher than .
we note that both the syntactic structure andthetokensofthecodereturnedby graphcodebert arevery dissimilartotheoracle.forinstance theoraclecodeusesanif else structure while the code returned by graphcodebert contains a loop structure.
therefore the cb of graphcodebert on this query isonly whichismuchlower thanthat of codet5.
code generated by codet5 2defmigrate self target kwargs 3if commit mode not inkwargs kwargs self.commit mode 5if commit mode not inkwargs kwargs self.commit mode 7returnself.
migrate target kwargs oracle code 10defmigrate self target follow true kwargs 11if id not inselfor notself raiseexception no source dataset id found.
13ifisinstance target dataset target id target.id 15else target id target 17migration datasetmigration.create source id self target id target id kwargs 19returnmigration code retrieved by graphcodebert 22defmigrate self target follow true kwargs 23ifisinstance target dataset target id target.id 25else target id target 27limit kwargs.pop limit none 28params self.
build query limit limit 29migration datasetmigration.create source id self.
dataset id target id target id source params params kwargs 31returnmigration listing the code generated by codet5 the oracle code and the coderetrievedby graphcodebert forthequery migratethedata fromthisdataset to a targetdataset .
anotherexampleisshowninlisting .theintendedfunctionality is to migrate data from one dataset to another.theoracle code ful f ills thisby checking if theid ofthesource dataset is provided obtainingtheidofthetargetdataset and f inallyperformingthe migration.thecoderetrievedby graphcodebert isonlyslightly differentfromtheoraclecodesinceitinitializesavariablewhich is not used by the oracle code during migration i.e.
params .
the code generated by codet5differs signi f icantly to the oracle code since it does not perform the sanity check it generates a block of code recurrently as we have mentioned before and it does not rely on the datasetmigration package to perform the migration.
consequently the cb of graphcodebert on this query ismuchhigher thanthat of codet5 .
vs. .
.
382natural languageto code howfarare we?
esec fse december3 san francisco ca usa a graphcodebert vs. codet5 b graphcodebert vs. multi modal c codet5 vs. multi modal figure scatter plots of codebleus of three representative techniques with respect to the top results.
we also draw the line y xfor comparison.
table the highest codebleu values achievable by combining differentstrategies in .
combinations cb top cb top codet5 .
.
multi modal .
.
graphcodebert .
.
codet5 tranx .
.
multi modal self attention .
.
graphcodebert codebert .
.
graphcodebert codet5 .
.
graphcodebert multi modal .
.
codet5 multi modal .
.
graphcodebert codet5 multi modal .
.
graphcodebert codet5 codebert .
.
all 10techniques .
.
these two cases demonstrate that different techniques perform well ondifferentqueriesandthus complementeachother.
.
rq3 combination ofexistingtechniques .
.
rq3.
whatisthebestperformanceachievablebycombining different techniques?
to investigate this rq for each query we supposealltheresultsfromasetoftechniquescanbeinspectedby thedeveloperandthemostquali f iedcode theonewiththehighest cb score can be identi f ied and used as the f inal result of such a technique combination.
we calculate the overall performance on the whole test set obtained in such a manner and results are shownintable .we f irstobservethatseveraltechniquestogether can work better than standalone techniques which shows that combinations of different techniques are promising.
speci f ically ifwetakeallthetentechniquesintoconsideration thecbofthe top 1resultscanreach35.
outperformingthebestsearchand generation techniques i.e.
graphcodebert andcodet5 by and58 respectively.
finding combiningthetentechniquescangainatleast35 effectiveness enhancement compared with standalone techniques.
obtaining the results of all the eight techniques however requiresmuchcomputationresource whichmaynotbeaffordable in practice.
therefore we also investigate the effectiveness of combining a pair of techniques.
speci f ically we combine techniques fromthesameclusters e.g.
codet5 tranxshowninthesecond part and techniques across different clusters e.g.
graphcodebert codet5shown in the third part .
surprisingly we f ind that although the latter is more effective than the former in general e.g.
combining codet5withmulti modal works better than combining itwithtranx combining graphcodebert withcodebert isthe most effective way for search search intra combinations such a strategycanachievehighercbsthancombining graphcodebert withmulti modal anditscbwithrespecttothetop 5resultsnearly equalstothatof graphcodebert codet5 .
vs37.
.this couldbeexplainedthroughfigure 3wherewenotethatforthesub f igurecomparing graphcodebert andmulti modal themajority of the scatters are below the line y x which means the latter only outperformstheformeronalimitedsetofqueries.asaresult combiningthesetwotechniquesmaynotboosttheeffectivenesstoa large extent although they have relatively low pearson correlation.
similarly wealsotrytocombinethreerepresentativetechniques fromdifferentclusters shownas graphcodebert codet5 multimodal but this is still outperformedby replacing multi modal with codebert .consequently ifweareabletouseonlytwotechniques under a resource constrained situation search generation intercombination of graphcodebert withcodet5and search search intra combination of graphcodebert withcodebert canprovide promisingresults.
finding search generation inter combination of graphcodebertwithcodet5andsearch searchintra combinationof graphcodebertwithcodebertshow promisingresults.
.
.
rq3.
can we automatically combine different techniques?
to achieve an automatic combination we design a post processing strategy where we re rank results obtained from different techniquestogeneratethe f inaloutputs inspiredbyarecentstudy .
intuition.
toachieve our target we needapredictortoassess thequalityofeachgeneratedcodesnippet.recallthatoneofour observationsisthatexistingtechniquesusuallyhaverelativelypoor performance towards tm cf.
table .
that is to say if a generated code snippet has a high tm value it is likely to achieve good overallperformance i.e.
cb .inspiredbypreviousstudieswhich point out that query tokens may represent key concepts in the requirements we postulate that a code snippet with more overlappedtokenswiththequerymaycontainmoremeaningful identi f ier names and thus has higher value towards tm so as cb .
forinstance thecodetoimplementthefunctionalityrequiredby the query convert string to int needs to include the api int anditthuscontainstheoverlappedtoken int.givenaquery we denoteitsnumberoftokensas u1d441 u1d448 u1d440 u1d461andthenumberofitstokens contained in a generated code snippet as u1d441 u1d448 u1d440 u1d45c.
we propose to 383esec fse december3 san francisco ca usa wang geng lin sun wen liu li bissyand mao.
table4 thecodebleuvaluesachievedbydifferentcombinations usingourstrategy in .
combinations cb top cb top graphcodebert codebert .
.
.
.
graphcodebert codet5 .
.
.
.
graphcodebert codet5 codebert .
.
.
.
rely on the overlap degree which is calculated as u1d441 u1d448 u1d440 u1d45c u1d441 u1d448 u1d440 u1d461 to help assess the quality of the generated code snippet a code snippet with a higher overlap degree is considered to be more quali f ied.
speci f ically toperformsuchanalysis thecodeandqueryaretokenized by the nltk package and program identi f iers are further split into multiple tokens based on the camel cases and underscore naming conventions.
hypothesisvalidation.
tovalidateourintuition wesplitthe overlap degree into f ive different intervals and calculate the cbs of the top code snippets returned by different techniques whose overlap degrees fall ineachinterval.
results are shown in figure .
we note that code snippets with higheroverlapdegreesaregenerallymoresimilartotheoraclecode withhighercbs forallthreerepresentativetechniques.speci f ically whentheoverlapdegreeisinthe interval themedian valueofthecbsofthetop 1resultsreturnedby graphcodebert isaround40 nearlyastwiceasthatofthecodesnippetswhose overlap degree is in the .
interval which is only around .
wealsoperformtheone sidedmann whitneyu test toanalyze the statistical signi f icance of the cb differences for code snippets from adjacent intervals.
our null hypothesis is that h0 code with higher overlap degrees to the query will not achieve signi f icantlyhighercbs andthealternativehypothesisis h1 codewithhigheroverlapdegreestothequerywillachieve signi f icantlyhighercbs .resultsrevealthatthedifferencesare statisticallysigni f icant i.e.
p value .
under all the cases indicating that h0can be rejected with a con f idence level of over .
.suchresultsindicatethattheoverlapdegreewiththequery could be a competent indicator to re assess the quality of the code snippetsreturnedbyexisting techniques.
strategy.
motivated by our validation we design a combination strategytointegratetheresultsfromdifferenttechniqueswhose overallprocessisstraightforward.givenanaturallanguagedescription i.e.
the query different techniques are executed and their resultsarestoredintoacandidatecodesnippetpool.afterthat we use a heuristic that assesses the overlap degree between the query and each candidate code snippet tore rank those candidates code snippets possessing high overlaps with the query are ranked at thetoppositions.consequently resultsfromdifferenttechniques are re ranked together and integrated into one list at this step and the output is the f inal combination result.
in this study to keep reasonable trade offs between the effectiveness and efficiency we combine the top results ofeachselectedtechnique.
evaluation results.
to investigate the effectiveness of our proposedcombinationstrategy weselectthethreerepresentative techniques i.e.
graphcodebert codebert andcodet5 identi f ied through our analysis in section .
and evaluate the performances after combining two or all of them.
results are shown in table 4where the data in the format x y denotes the effectiveness obtainedbyourstrategy thebestperformanceachievablebydifferent combinations.
we f ind thatour combinationstrategy isgenerally effective all thecombinationscannearlyreachtheirmaximumpotential.for instance if for each query the maximum codebleu value from graphcodebert andcodet5is achieved then the average codebleu value of the top results is .
.
by using our strategy such a combination can have a codebleu of .
with respect to the top results.
moreover given the data in table such an automatic combination can outperform each standalone technique by16 .
vs. .
and35 .
vs. .
respectively.we also note that search generation inter combination works more effectively than search search intra combination the combination ofgraphcodebert codet5achieves higher codebleus than the combination of graphcodebert codebert with respect to both top and top results especially when we only focus on the top 1results .
vs. .
.thisindicatesthatinaresourceconstrainedscenariowherewecanonlyexecuteafewtechniques e.g.
two combiningcodesearchandcodegenerationtechniquesisrecommended.furthermore our strategyisalso extensible theeffectiveness of the combination keeps increasing when involving more techniques.speci f ically thecodebleuvalueofthetop 1results increases by nearly two percentage points when all three representativetechniquesarecombined comparedwithonlyconsidering two of them .
vs. .
.
as a result further effectiveness enhancement isexpectedwhen involving more techniques.
finding asimpleheuristic basedpost processingstrategy canleadtosigni f icant effectivenessenhancementcomparedwith each standalone technique.
discussion .
implications ittakes two to tango our investigation shows that code search and code generation techniques share certain complementarities a query that is not handled effectively by one technique may be addressed well by the other.
therefore developers may consider using both of them in their development activities toboosttheir productivity.ourstudy proposesapost processingapproachforcombiningthesetwotypes of techniques.
in fact we also explore a pre processing way for combinationwherewetrainamodeltopredictwhetherasearchor generationtechniqueshouldbeusedforagivenquery.speci f ically weuseapre trainedbertmodeltoembedthequeryandtraina fully connected layer to predict if graphcodebert orcodet5is to be used asapreliminaryexploration we focusoncombiningthe most effective search and generation techniques but the accuracy is only on our dataset.
therefore for researchers effortscould be devoted to devise more effective way for combination in the future.
.
comparisonwith chatgpt chatgptisahotchatbotthatcaninteractwithhumansinaconversational way.3to compare it with the study subjects in this 384natural languageto code howfarare we?
esec fse december3 san francisco ca usa a graphcodebert b codebert c codet5 figure4 the performances of threerepresentativetechniquesunder differentoverlap degrees.
paper we also investigateitscode generationperformance onour test set.
to perform this experiment we leverage the chatgpt api accessed on may with the prompts to the model in the formof assumethatyouareapythonprogrammer.pleasewrite a python function that ... followed by the query contents.
the f irstsentenceistopreparechatgptforthecodegenerationtask whilethesecondonedescribesthedetailedrequirement.wesetthe temperature parametertobe0 whichensuresthatchatgptalways return the code with the highest probability.
results show that on average thecodebleu scoreof thecode returned bychatgpt is .
which is slightly lower than that of the state of the art code generationtechniquessuchascodet5.suchresultsindicatethat although chatgpt can provide detailed instructions to developers its performance may not exceed those of the state of the art nl2code techniquesifweonly focus onthe generatedcode.one possible explanation could be that the state of the art code generationtechniqueshavebeenadequately f ine tunedforthistask whereas chatgpt is optimized for arti f icial general intelligence agi andnotspeci f icallyoptimizedforthetaskofcodegeneration.
.
the existenceofcodesimilar to theoracle in this study to mimic the real scenario of applying code search techniques weremovetheoraclecodefromthesearchspacefor eachquery.onefollowingquestionisthatisthereanycodesnippet in the search space similar to the oracle one?
to investigate such a question we utilize a state of the art code clone detector nil toidentifycodeclonepairsamongourtestset.werecallthatnilis atoken basedclonedetectorsinceitidenti f iescodeclonesbasedon then gramrepresentationandthelongestcommonsubsequence of code token sequences.
that is to say the detected clones of the oracle code can be transformed to the oracle through minor modi f icationsontheircodetokens.resultsshowthatmorethan75 i.e.
of the code snippets have the corresponding clones in the search space.
this indicates that for most queries codesnippetsthatcanmatchthequerywithminormodi f ications existinthesearchspaceandaquali f iedcodesearchtechniqueis supposed to rank such code snippets at top positions.
by analyzing the search logs of developers the previous study concludes that developers sometimes get nothing from their searches.
this observation suggests that in a realistic setting it is not always possible for all the queries to have code snippets that are similar to the oracle as otherwise developers could always obtain a solution by making minormodi f icationsto theoracle s clones.
oursetting is aligned with this assumption and is well suited for practical scenarios.
.
threatsto validity external threats.
code search and code generation are active research f ieldswithanumberofapproachesbeingproposedduring the last years.
it is thus quite hard to involve all of them in this study.
the selected approaches in this paper are state of the art and have served as baselines for many studies and thus can be consideredas representative ones safely.
internal threats.
in our study we use the code comment as thequery whichiswidelyadoptedbyexistingstudies .
the rationale is that the comment usually summarises the main functionality of the code making the code comment pair close to actual use scenarios.
existing studies have shown that common queries from developers are similar to the comments i.e.
eitherbeingidenticaltothecommentorbyslightlyprependingthe comment with howto howdo i .
we rely on the codebleu score to serve as a proxy of code quality following existing studies .
the previous study has demonstrated that codebleu is strongly related withhumanevaluations whichmeanscodewithhighercodebleu scores is more quali f ied to ful f ill the intended functionality as judged by humans.
our case analysis also shows that code with higher codebleu scores is more semantically similar to the oracle code.asaresult weleaveassessingtheusefulnessofthegenerated code from the developers perspective as our future work.
conclusion in this paper we evaluate the effectiveness of ten representative nl2code techniques on a large scale dataset.
through in depth analysis of their correlation degrees and case analysis we show thatexistingnl2codetechniquescomplementeachotherwell.we also investigate the theoretical upper bound effectiveness which can be achieved by combining different techniques and f ind that it outperforms those of standalone techniques to a large extent.
therefore future studies could be undertaken to further utilize the complementarityofnl2codetechniques.moreover wedesigna strategytoautomaticallycombineresultsfromdifferenttechniques andachievepromisingresults.allcodeanddatainthisstudyare publiclyavailable online .