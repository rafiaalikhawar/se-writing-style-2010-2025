resource guided configuration space reduction for deep learning models yanjie gao1 yonghao zhu1 hongyu zhang2 haoxiang lin1 mao yang1 1microsoft research beijing china 2the university of newcastle nsw australia email fyanjga v yonghz haoxlin maoyang g microsoft.com hongyu.zhang newcastle.edu.au abstract deep learning models like traditional software systems provide a large number of configuration options.
a deep learning model can be configured with different hyperparameters and neural architectures.
recently automl automated machine learning has been widely adopted to automate model training by systematically exploring diverse configurations.
however current automl approaches do not take into consideration the computational constraints imposed by various resources such as available memory computing power of devices or execution time.
the training with non conforming configurations could lead to many failed automl trial jobs or inappropriate models which cause significant resource waste and severely slow down development productivity.
in this paper we propose dnnsat a resource guided automl approach for deep learning models to help existing automl tools efficiently reduce the configuration space ahead of time.
dnnsat can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials.
we formulate the resource guided configuration space reduction as a constraint satisfaction problem.
dnnsat includes a unified analytic cost model to construct common constraints with respect to the model weight size number of floating point operations model inference time and gpu memory consumption.
it then utilizes an smt solver to obtain the satisfiable configurations of hyperparameters and neural architectures.
our evaluation results demonstrate the effectiveness of dnnsat in accelerating stateof the art automl methods hyperparameter optimization and neural architecture search with an average speedup from .19x to .95x on public benchmarks.
we believe that dnnsat can make automl more practical in a real world environment with constrained resources.
index terms configurable systems deep learning automl constraint solving i. i ntroduction many traditional software systems such as compilers and web servers are highly configurable.
they provide many configuration options and different combinations of the options could lead to different system quality attributes.
in recent years deep learning dl models have become an integral part of many modern software intensive systems such as speech recognition systems chatbots and games.
like traditional software systems dlmodels also provide many configuration options for developers to tune.
these configuration options can be largely classified into two categories hyperparameter such as the batch size and learning rate and neural architecture such corresponding author.as the number of layers and layer type .
the numerous options provided by a dlmodel result in an exponential number of possible configurations making manual tuning extremely tedious and time consuming.
recently automated machine learning automl techniques have been widely adopted to automate and accelerate dl model tuning.
automl tools such as nni neural network intelligence usually apply hyperparameter optimization hpo and neural architecture search nas algorithms and launch hundreds or even thousands of automl trial jobs or trials for short to systematically explore diverse configurations of hyperparameters and neural architectures.
it has been found that automl significantly boosts the productivity of dl development .
however current automl approaches do not take into consideration the computational constraints imposed by various resources such as available memory computing power of devices or execution time because the resources required by a dlmodel often remain unknown to developers before job execution.
non conforming model configurations could lead to many failed automl trials or inappropriate models which not only waste significant shared resources such as gpu cpu storage and network i o but also severely slow down development productivity.
a typical resource is gpu memory which is critical yet limited for training.
if developers do not size the model carefully enough trials will trigger oom out of memory exceptions and fail.
for instance one pytorch resnet trial with an overlarge batch size of causes an oom when being scheduled on the nvidia tesla p100 gpu it requires gb of gpu memory but p100 has only gb in total .
even worse since there can be more than ten tunable hyperparameters for the resnet model other hundreds of trials with the same batch size could also experience oom and crash.
according to our recent empirical study on failed dljobs in microsoft .
of the job failures were caused by gpu memory exhaustion which accounts for the largest category in all deep learning specific failures.
therefore it is necessary for automl tools to enforce a constraint that a dlmodel cannot consume more gpu memory than the capacity when exploring model configurations.
another useful constraint is that the size of a model s weights cannot exceed a certain upper bound.
otherwise the resulting dlapplication may be too large for efficient management and execution due to insufficient computing power of the target ieee acm 43rd international conference on software engineering icse .
ieee resource restricted devices.
if the unsatisfiable automl trials can be excluded ahead of time resources will be saved to perform more trials thus a larger configuration space could be explored.
a simple workaround is to run and profile trials for a while to estimate their resource consumption.
such a resourceconsuming method is unaffordable in the scenario of automl where there exist a large number of possible hyperparameter combinations and neural architectures.
some research work incorporates certain resource quantification into the loss function for a global optimization with the model learning performance e.g.
accuracy .
however their purpose is to reduce the resource consumption of the final model as much as possible while achieving an expected model learning performance instead of excluding unsatisfiable trials in advance to improve the search efficiency.
therefore such work could also cause failed trials inappropriate models and resource waste.
besides they are limited to nas algorithms only and cannot be applied to hpo algorithms.
in this paper we propose dnnsat a resource guided automl approach for deep learning models which can help existing automl tools efficiently reduce the configuration space ahead of time.
we formulate such a resource guided space reduction problem as a constraint satisfaction problem csp .
dnnsat includes a unified analytic cost model to construct common constraints with respect to the model weight size number of floating point operations flops model inference time and gpu memory consumption.
it then utilizes an smt satisfiability modulo theories solver e.g.
microsoft z3 to obtain the satisfiable model configurations before trial execution.
according to the characteristics of constraints e.g.
monotonicity we also apply some special optimizations to accelerate the solving.
we have implemented dnnsat and evaluated it extensively on public automl benchmarks hpobench and nas bench with various search methods random search regularized evolution hyperband and reinforcement learning and representative dlmodels vgg and lstm based seq2seq .
the experimental results show that dnnsat achieves an average speedup from .19x to .95x on public benchmarks and noticeably reduces the configuration space.
in summary this paper makes the following contributions we propose a resource guided automl approach for deep learning models to efficiently reduce the configuration space ahead of time.
we build a unified analytic cost model to construct common constraints and utilize an smt solver to obtain the satisfiable configurations of hyperparameters and neural architectures.
we implement a tool named dnnsat and demonstrate its practical effectiveness.
the rest of the paper is organized as follows.
section ii introduces the background.
section iii presents our analytic approach.
section iv details the design and implementation ofdnnsat .
section v shows experimental results.
section vi1from tensorflow.keras importlayers models ... 3model models.sequential 4model.add layers .conv2d filters kernel size activation relu input shape !
5model.add layers .averagepooling2d pool size padding same !
6model.add layers .flatten 7model.add layers .dense units activation relu 8model.compile optimizer adam loss tf.keras.losses.meansquarederror !
9model.fit train images train labels batch size epochs a training program using keras api.
b computation graph for model inference.
c tensors with shapes on which conv2d operates.
fig.
a sample tensorflow model sequentially constructed by the framework built inconv2d 2d convolution avgpool2d 2d average pooling flatten collapsing the input into one dimension without affecting the batch size and dense fully connected layer operators.
discusses extensibility and possible threats.
we survey related work in section vii and conclude this paper in section viii.
ii.
b ackground deep learning dl is a subfield of machine learning to learn layered data representations known as models.
a dlmodel is formalized as a tensor oriented computation graph by frameworks such as tensorflow tf pytorch and mxnet which is a directed acyclic graph dag .
the inputs and outputs of such a computation graph and its nodes aretensors multi dimensional arrays of numerical values .
the shape of a tensor is the element number in each dimension plus the element data type.
each node represents the invocation of a mathematical operation called an operator e.g.
elementwise matrix addition .
an edge delivers an output tensor and specifies the execution dependency.
in this paper we use the terms operator and node interchangeably since a node is completely determined by its invoked operator.
fig.
1a shows a simple tensorflow training program using the keras api which sets up a sequential model with the framework built in conv2d 2d convolution with a kernel size avgpool2d 2d average pooling with the same padding setting1 flatten collapsing the input into one dimension without affecting the batch size and dense fully connected layer with units operators lines .
the above filter size padding and number of units are hyperparameters which are parameters to control the training process.
fig.
1b demonstrates the corresponding computation graph for model 1see layers average pooling2d for an explanation of the padding argument.
search space.json batch size type choice value kernel size type choice value filters type choice value unit size type choice value lr type choice value config.yml ... 12searchspacepath search space .json 13tuner 14builtintunername random 15trial 16command python3 mnist .py 17gpunum 18cpunum 19memorymb ... fig.
settings of an mnist training program supported by nni with hyperparameter optimization.
inference.
fig.
1c illustrates the input weight and output tensors with shapes on which conv2d operates.
to choose better configurations of hyperparameters and neural architectures developers often adopt a trial and error strategy submitting hundreds or even thousands of trial jobs with each being assigned a different model configuration.
this strategy is very inefficient because of the overlarge configuration space.
recently many automl tools such as nni neural network intelligence auto keras and autosklearn are proposed to automate the exploration of model configurations.
they help developers find a hyperparameter combination hyperparameter optimization hpo or design an elaborate neural network neural architecture search nas which can both minimize the loss and maximize the model learning performance e.g.
accuracy .
suppose that a dlmodel hasmhyperparameters whose domains are b1 b2 bm.
a model configuration is an instance of such a model with concrete hyperparameter values.
all model configurations constitute the configuration space .
hpo applies some search method e.g.
random search evolutionary strategies or bayesian optimization in the enumeration of the configuration space to find a candidate with the optimal hyperparameter vector combination 2b1 b2 bm which best meets the training objective.
fig.
shows the settings of an mnist training program supported by the automl tool nni with hpo.
the upper part lines defines the possible values of tunable hyperparameters batch size learning rate etc.
the bottom part lines specifies the search method and runtime resource requirements gpu count main memory size etc.
.
the controller process of nni performs a random search line and may fork over a hundred trials executing the command on line .
trials send timely feedback such as the mean squared error or accuracy to the controller for the decision of early stopping.
similarly nas automates the architecture engineering of a dlmodel.
the configuration space consists of various automatically generated syntactically legal neural network structures such as chained or multi branch networks.
nas also appliestable i common hyperparameters of dl models .
hyperparameter domain hyperparameter domain batch size n kernel size n output channels n stride n padding n of rnn layers n of units n sequence length n operator type n edge b learning rate r dropout different search methods including random search gradientbased algorithms and reinforcement learning rl for the space exploration.
iii.
p roposed approach a. problem formulation formally a dlmodelxis represented as a directed acyclic graph dag x hfuign i f ui uj gi6 j fpkgm k 1i each nodeuiis an operator i.e.
a mathematical operation such as convolution and pooling .
a directed edge ui uj pointing from an output of node uito an input of ujdelivers the output tensor and specifies the execution dependency.
each pkis a hyperparameter e.g.
input tensor shape and batch size whose domain is denoted as bk.
table i lists some commonly used hyperparameters with their domains.
forbk2bkwhere k m we use x b1 b2 bm to represent one model configuration of x. then the configuration space of model x denoted by x is defined as follows x fx b1 b2 bm jbk2bkfork2 g in an automl experiment there may exist a series of dl modelsx1 x2 xn e.g.
models searched by nas .
for such an experiment its configuration space is defined as the union set of all models configuration spaces sn i xi we formulate the resource guided configuration space reduction for an automl experiment with nmodels as the following constraint satisfaction problem hv d ci v fv1 v2 vng d fd1 d2 dnjdi xifori2 g c fcj lbj fj vi ubjg vis a set of model configuration variables drepresents the respective variable domains and cis the constraint set.
each variablevi2 can take on the model configurations of xi i.e.
the domain of viis xi .
for a constraint cj fjis a non negative restriction function which denotes the demand for a certain resource lbjandubjare the lower and upper bounds of that resource respectively.
if one is more interested in the upper bound we simply assume that lbj .
hyperparameters 177aredecision variables of the constraints which are quantities controlled by the decision maker to define the search space for optimization.
as mentioned before an example fjcalculates the gpu memory consumption for training a dlmodel and ubjis the memory capacity of the allocated gpu device e.g.
gb for nvidia tesla p100 .
b. the proposed analytic approach dnnsat adopts an analytic approach to construct the restriction functions on resources.
we observe that the algorithmic execution of a dlmodel can be represented as iterative forward and backward propagation on its computation graph.2therefore computing the resource consumption for one iteration is then reduced to the calculation of the resource required by each operator on the computation graph in accordance with a certain graph traversal order.
currently a dlmodel is required to be deterministic without control flow operators e.g.
loops and conditional branches thus we assume that the execution flow and resource consumption across different iterations are identical.
we define an auxiliary function gon the operator set which represents the current resource consumption when the operator under visiting has just finished execution in the iteration.
let s hui1 ui2 uinibe a topological linear order extended from the edge order of the model such thatuij suik uik uij 2x.sis called an operator schedule representing the actual runtime execution order of operators.
dnnsat pre generates sby referring to the framework implementations .
suppose that ris the operator resource cost function itercnt is the iteration count andrinit rfini are the resource consumption of the one off initialization and clean up performed by dlframeworks which are assumed to be 0if not specifically mentioned.
we define gand the restriction function fas follows g ui1 r ui1 g uij h fhui1 g ui1 i huij g uij ig r uij f x rinit rfini t itercnt fg uij gn j so long as the above handtare functions guniquely exists by the transfinite recursion theorem schema since sis a well order andfalso exists.
the formalization of gindicates that it could compute the current resource consumption by referring to extra information of previous consumption and visited operators.
examples of gandfcan be found in section iv b .dnnsat includes a unified analytic cost model to define operators resource cost functions and common constraints.
table ii lists the constraints that we have implemented.
the objective of dnnsat is to reduce the configuration space by eliminating those xi bi1 bi2 bim which violate the enforced constraints before submitting automl trials.
a naive method is to compute the values of the restriction functions for each model configuration in and then check whether such values fall within the allowed bounds.
dnnsat adopts a much more efficient approach it specifies those 2this also applies to model inference which has a simplified representation with single pass forward propagation and no backward propagation.table ii common constraints imposed by thecomputational resources .
restriction category resour ce upper bound scenario model weight size allowed binary size inference number of floating point device flops inference operations flops sla of inference latency model inference time sla of inference latency inference gpu memory consumption device memory capacity training inference flops denotes floating point operations per second and sla stands for service level agreement.
fig.
workflow of dnnsat.
constraints in the smt lib satisfiability modulo theories library syntax and uses the microsoft z3 solver to obtain all the satisfiable model configurations.
such an approach is feasible because the restriction functions of the common constraints can be composed by smt solvers built in functions e.g.
multiplication and division and the constraints are simple numerical inequalities.
we also apply some special optimizations to accelerate the solving based on the constraint characteristics e.g.
monotonicity which are described in section iv e. iv.
d esign and implementation a. workflow fig.
shows the workflow of dnnsat .
it accepts a source dl model configuration settings and constraint settings as input.
the model is parsed by a front end parser and reconstructed to the corresponding computation graph.
some dlframeworks such as pytorch employ the define by run approach such that a saved model records only an execution trace instead of the full computation graph.
dnnsat currently relies on users to supply multiple models in case the graph may dynamically change in the future it may be possible to try extracting the full computation graph from a dlprogram by static code analysis .
configuration settings include the hyperparameters to be tuned and their domain definitions.
constraint settings contain the built in constraints that need to be satisfied and their allowed bounds as well as constraint related runtime constants of the dlframework e.g.
type and version and target device e.g.
flops .
dnnsat has defined four common constraints section iv b and a set of analytic and framework independent resource cost functions for dloperators section iv c .
it traverses the computation graph in accordance with a predefined operator schedule i.e.
operator execution order to automatically generate the constraint specifications in smt lib for later solving section iv d .
there are two working modes for 178integrating dnnsat with existing automl tools.
one is the interactive mode in which automl tools work as usual but each configuration will be sent to dnnsat for solving via an api call before launching a trial.
such a mode is simple non intrusive and requires less effort.
we have run dnnsat with hpobench and nas bench interactively to evaluate the effectiveness in speeding up automl methods section v a and similar integration work could be done in automl tools such as nni and auto keras.
the other is the pruning mode in which dnnsat eliminates the unsatisfiable model configurations in advance and feedbacks a reduced configuration space to automl tools during their initialization.
to solve the constraints the microsoft z3 solver is invoked with some optimizations section iv e .dnnsat is extensible to support user defined constraints by permitting users to specify their own constraint specifications in smt lib with those defined hyperparameters.
b. the constraints dl models are both compute intensive and memoryintensive making them difficult to be trained or deployed on systems and platforms with limited resources.
in this paper we consider four representative computational constraints with respect to the model namely weight size number of floating point operations inference time and gpu memory consumption.
the meaning of the notations and symbols can be found in section iii.
model weight size.
weights including biases are the numerical learnable parameters of operators being saved in the model file and taking up most of the space.
the size of weights is important especially on resource restricted devices such as mobile phones.
an overlarge dl model causes inefficient model application management expends unaffordable energy or even cannot fit in the target devices main memory.
the total model weight size is calculated by accumulating the weight size of each operator.
assuming that wt is the restriction function and wtmin wtmax are the lower and upper bounds in bytes the constraint is then defined as follows g uij g uij r uij wt x maxfg uij gn j pn j 1r uij wtmin wt x wtmax number of floating point operations flops .
flops is considered as a stronger predictor of energy usage and inference time .
the total flops for inference is calculated by accumulating the flops of each operator in accordance with the operator schedule.
assuming that fis the restriction function and fmin fmax are the minimal and maximal values allowed the constraint is then defined as follows g uij g uij r uij itercnt f x itercnt maxfg uij gn j pn j 1r uij fmin f x fmaxmodel inference time.
this is a critical runtime performance indicator for dlapplications.
the total time is calculated by accumulating the execution time of each operator in accordance with the operator schedule.
assuming that tis the restriction function and tmin tmax are the lower and upper bounds the constraint is then defined as follows g uij g uij r uij itercnt t x itercnt maxfg uij gn j pn j 1r uij tmin t x tmax gpu memory consumption.
as mentioned before gpu oom accounts for the largest dlfailure category therefore controlling the gpu memory consumption is critical to reduce oom exceptions and save shared resources.
however the calculation is rather complicated since there are many hidden framework factors observably affecting the final gpu memory consumption .
we adopt a simplified yet common approach for both inference and data parallel training which accumulates the gpu memory required by each operator under forward propagation in accordance with the operator schedule.
assuming that mis the restriction function and mmin mmax are the minimal and maximal gpu memory consumption allowed in bytes e.g.
taking the memory capacity as the maximal value the constraint is then defined as follows g uij g uij r uij m x rinit maxfg uij gn j rinit pn j 1r uij mmin m x mmax ifuis an operator under backward propagation we let r u .rinit represents the gpu memory consumed by the cuda context and initial input tensors during the framework initialization.
the cuda context contains managing information to control and use gpu devices which is assumed to be a constant obtained by profiling.
c. resource cost functions of operators operators are mathematical functions on various types of tensors.
dnnsat defines analytic and framework independent resource cost functions for operators.
such a solution is technically feasible because frequently used operators are well defined with clear syntax and semantics dl frameworks implement them similarly e.g.
calling nvidia cuda cudnn or cublas apis .
in this section we take theconv2d operator in fig.
1b as an example to illustrate the four resource cost functions with respect to the studied constraints.
the following symbols are used to denote the hyperparameters and tensor shapes.
sfis the size of input data type e.g.
bytes for float32 data .nrepresents batch size.
hk andwkare kernel filter height and width they are usually equal.fltiandsiare filter size and stride size at index i.flt represents the number of filters.
padding padi controls the amount of implicit zero paddings on both sides for padding number of points for each dimension and dilation di controls the spacing between the kernel points .
hin win and 179cinare input height width and channels respectively.
ho wo andcoare output height width and channels which have the following relations with other symbols ho hin pad0 d0 hk s0 wo win pad1 d1 wk s1 co flt then the resource cost functions with respect to the four constraints model weight size wt flops f model inference time t and gpu memory consumption m are defined as follows wt conv2d sf cin hk wk co co f conv2d co hk wk cin n ho wo mit n cin hin win mwt wt conv2d mot sf n co ho wo t conv2d mit mwt mot bd f conv2d flops m conv2d mwt mot mit mwt andmoutdenote the gpu memory occupied by the input weight and output tensors respectively.
bdand flopsare memory bandwidth and floating point operations per second flops of the target device which can be assumed to be constants.
the first item of t conv2d represents the data access time from and to gpu memory .
we do not count mitin the gpu memory consumption because an operator s input tensors reuse the gpu memory of either the initial inputs or predecessors outputs which have been calculated in the initialization cost rinit and predecessor operators cost functions.
more details about the estimation of flops and model inference time can be found at and .
currently dnnsat supports operators.
it is also extensible and can support new operators which are discussed in section vi.
d. constraint specification in this section we describe how to automatically generate a constraint specification in smt lib.
the dlmodel in fig.
is used as an example and the enforced constraint is the model weight size must be less than or equal to mb .
a snippet of the constraint setting is shown as follows constraint weight size max min fig.
lists the illustrated smt lib code.
first dnnsat specifies the constraint bounds read from the constraint setting file lines .
it then declares the hyperparameters of the size of input data type sf number of input channels cin kernel size hkandwk filter size flt and unit size u of thedense operator lines and writes their domains lines according to the contents of the configuration setting file.
as the batch size n does not contribute to the model weight size we simply ignore it.
next dnnsat traverses the computation graph from conv2d todense one after another.
for each operator type we prepare smt lib templates in python via z3 apis for1 set logic qf ufnia non linear integer arithmetic logic constraint bounds declare const wt min int lower bound mb declare const wt max int upper bound mb assert and wt min0 wt max10485760 hyperparameters declare const sf int size of input data type declare const cin int conv2d input channels declare const hk int conv2d kernel height declare const wk int conv2d kernel weight declare const flt int conv2d number of filters declare const u int dense unit size hyperparameter domains assert and and sf4 cin3 and hk wk u !
assert or or hk3 hk5 or hk7 hk11 assert or or flt64 flt128 flt512 compute the weight size of conv2d declare const wt conv2d int conv2d weight size declare const co int conv2d output channels assert flt co assert sf hk wk cin co co wt conv2d compute the weight size of dense declare const wt dense int assert sf u1 wt dense compute the model weight size declare const wt int assert wt conv2d wt dense wt specify the constraint assert and wt min wt wt wt max check sat exit fig.
illustrated constraint specification in smt lib which enforces that the weight size of the model in fig.
must be within the interval .
the resource cost functions.
when an operator is visited dnnsat locates the matched template and generates the python wrapper of smt lib code using those declared hyperparameter symbols.
for example lines correspond to calculating the weight size of conv2d .
sinceavgpool2d uses the same padding setting line in fig.
1a its output tensor shape keeps unchanged with that of conv2d .
bothavgpool2d andflatten do not have weights so we also ignore them.
dense has a weight tensor of a element array plus a bias of data element lines .
therefore the total model weight size is equal to the sum of the weight sizes of both conv2d anddense lines .
finally dnnsat asserts that the result must fall between the lower and upper bounds line .
note that although the example uses only integer variables dnnsat can easily replace the variable statements to support real valued hyperparameters because the underlying smt solver z3 supports real values and real functions.
e. constraint solving the resource guided configuration space reduction is formulated as a constraint satisfaction problem csp .
dnnsat chooses microsoft z3 to solve the constraints because the smt solver is very efficient to handle higher order and nonlinear functions.
however the solving may slow down significantly when dealing with an overlarge configuration space or a very complicated restriction function.
we summarize below our major optimization techniques for accelerating the solving speed parallel and distributed solving .dnnsat partitions the full configuration space into multiple smaller subspaces and solves them in parallel.
the process is shown schematically in fig.
.
for the parallel solving each worker thread is assigned a standalone z3 context.
the distributed solving is built on top of spark which handles configuration space partitioning distributed task deployment via the mappartitions api scheduling and fault tolerance.
tiny subspaces .
proper partitioning of the configuration space is very important for tackling the skew problem in the parallel and distributed solving.
dnnsat adopts the idea of tiny tasks and divides the original space into numerous tiny subspaces e.g.
each containing less than configurations .
our approach has two advantages it will not result in observable computation skew across subspaces the z3 solver cannot return all the satisfiable model configurations at once like what allsat does hence dnnsat has to iteratively call z3 by providing the conjunction of the negation of each existing solution to derive the next one.
a tiny subspace does not make the conjunction long and complicated thus the solving efficiency is significantly improved.
dnnsat currently implements a simple work queue to manage the tiny subspaces.
we will consider dynamic partitioning and work stealing for better load balancing in the future.
interval filtering .
the restriction function of a constraint may be monotonic with regard to w.r.t.
for short some hyperparameters.
for instance the model weight size function is monotonically increasing w.r.t.
kernel size filter size and unit size mentioned in fig.
.
another example is that the flops function is monotonically increasing w.r.t.
batch size but not to kernel size.
this is because the output height and width of the conv2d operator decrease with kernel size increasing which may reduce the flops of subsequent operators.
with such monotonicity information we can apply the interval filtering technique which safely discards specific value intervals of the hyperparameters.
suppose that the restriction function is monotonically increasing w.r.t.
the hyperparameter p1whose domain is .
if the function value of a configuration hp1 v1 p2 vp2 pm vpmiexceeds the upper bound any configurations hp12 v1 vmax p2 vp2 pm vpmiwill not satisfy the constraint either if it is smaller than the lower bound any configurations hp12 vmin v1 p2 vp2 pm vpmiwill also violate the constraint.
furthermore if two configurations hp12fv1 v2g p2 vp2 pm vpmiare satisfied andv1 v2 any configurations hp12 v1 v2 p2 vp2 pm vpmiwill satisfy the constraint as well.
v. e valuation to evaluate the proposed dnnsat we experiment with different representative automl benchmarks and dlmodels.
fig.
parallel and distributed solving by partitioning the configuration space.
we aim to answer the following research questions rqs rq1 how effective is dnnsat in speeding up automl methods?
rq2 how effective is dnnsat in reducing the configuration space?
rq3 how efficient is dnnsat in constraint solving?
our experiments are conducted on an azure standard nd12s virtual machine with intel xeon e5 vcpus gb main memory and nvidia tesla p40 gb gddr5x memory gpus running ubuntu .
.
a. rq1 how effective is dnnsat in speeding up automl methods?
in this section we consider the model weight size constraint and evaluate the effectiveness of dnnsat on the following two benchmarks hpobench is for hpo methods and consists of a large grid of hyperparameter configurations of feedforward neural networks for regression .
the model has two tunable dense operators followed by a non tunable dense on top.
there are nine hyperparameters e.g.
batch size unit size and initial learning rate and model configurations in total.
we use the hpo benchprotein dataset.
the maximal model weight size is kb.
nas bench is for nas methods and constructs a compact yet expressive search space exploiting graph isomorphisms to identify 423k unique convolutional architectures .
the dataset is cifar10 .
the maximal model weight size is about .
mb.
both benchmarks generated dlmodels and collected a rich set of runtime statistics final training validation test error and accuracy total training time etc.
from the trained models.
these statistics can be used to simulate the execution of automl trials and evaluate different configuration search methods.
to evaluate the learning performance of a model configuration resulted from an automl trial following the existing work we use the mean squared error mse for hpobench and regret i.e.
accuracy fornas bench as the test measurement.
the smaller the value the better the model.
we choose the model weight wall clock time 104seconds testmeasurement mse rs rs dnnsa t .04x speedup a random search rs wall clock time 104seconds testmeasurement mse re re dnnsa t .19x speedup b regularized evolution re wall clock time 105seconds testmeasurement mse hb hb dnnsa t .95x speedup c hyperband hb fig.
test measurement curves and speedups of the hpobench experiments on trials using different search methods with an kb upper bound of the model weight size.
wall clock time 105seconds testmeasurement regret rs rs dnnsa t .23x speedup a random search rs wall clock time 105seconds testmeasurement regret re re dnnsa t .19x speedup b regularized evolution re wall clock time 105seconds testmeasurement regret rl rl dnnsa t .52x speedup c reinforcement learning rl fig.
test measurement curves and speedups of the nas bench experiments on trials using different search methods with a mb upper bound of the model weight size.
wall clock time 104seconds testmeasurement regret rs rs dnnsa t .15x speedup a random search rs kb00 wall clock time 104seconds testmeasurement regret rs rs dnnsa t .21x speedup b random search rs kb fig.
test measurement curves and speedups of the hpobench experiments on trials using random search with three upper bounds kb kb and kb of the model weight size.
size constraint because it is one of the most representative constraints for dlapplications and for performing model inference on resource restricted devices such as mobile phones.
on each benchmark we perform two sets of experiments baseline experiments and dnnsat guided experiments.
we choose the following commonly used search methods inautoml as baselines random search rs regularized evolution re hyperband hb hpobench only and reinforcement learning rl nas bench only .
the two sets of experiments perform the same number of trials for hpobench and for nas bench .
we then measure the speedup achieved by dnnsat over each baseline method on each benchmark.
the speedup is calculated astbase tdnnsat wheretbase is the estimated time of a baseline i.e.
rs re and hb reaching the lowest test measurement i.e.
the best learning performance and tdnnsat is the time of a dnnsat guided method reaching the same or lower test measurement.
we repeat each experiment times and compute the average speedup value.
dnnsat runs in the interactive mode because of the simplicity of the integration effort.
to solve the first model configuration dnnsat needs to build the constraint from scratch i.e.
warm up which spends more time than solving later individual configurations of the same model.
for hpobench the solving time is .1s warm up .5s on average.
for nas bench it is .6s warm up .0s on average.
fig.
demonstrates the test measurement mse curves of hpobench on evaluated trials.
the upper bound of the 182model weight size is set to a small value of kb which is set for deploying kb sized dlmodels to resource restricted iot devices .
the x axis is the wall clock time and the y axis denotes the test measurement value.
overall dnnsat achieves a speedup of .04x rs .19x re and .95x hb on hpobench for obtaining the same optimal model learning performance that can be found by the baseline.
from the experiment results we also find that dnnsat helps the curves go down faster and reach smaller test measurement values which means that better model configurations are found.
the reason is that dnnsat reduces the configuration space so that hpobench can perform a much more efficient search than before.
we also notice that the experiment time has been observably shortened when dnnsat is enabled since a smaller model size implies fewer flops and thus less training time.
in fig.
6c time reduction is particularly significant because of the mechanism of hyperband the more resources saved by dnnsat the more training budget allocated to hyperband for high efficiency search.
fig.
demonstrates the test measurement regret curves of nas bench on evaluated trials with the upper bound of the model weight size set to mb.
dnnsat achieves a speedup of .23x rs .19x re and .52x rl .
to understand the generality of our approach under different constraint bounds we additionally choose two upper bounds kb and kb and conduct hpobench experiments using random search.
the results of fig.
and fig.
6a indicate that dnnsat is generally effective and achieves a speedup of .15x kb .04x kb and .21x kb .
the results also show that the stricter the constraint the greater the improvement achieved by dnnsat.
b. rq2 how effective is dnnsat in reducing the configuration space?
in this section we evaluate the reduction effectiveness of dnnsat on real world dlmodels.
we choose two representative models as our experimental subjects namely vgg vgg model with layers and lstm based seq2seq .
for vgg we select batch size interval kernel size and and unit size and as the hyperparameters hence there are model configurations in total.
for seq2seq we consider batch size interval and hidden size interval in which the configuration space consists of model configurations.
we apply all the four discussed constraints separately and collectively to both dlmodels.
each constraint is set with several upper bounds.
the bound choices are based on actual conditions such as the model scale device capability and application sla.
for example we choose and gb forvgg under the gpu memory consumption constraint because they correspond to three typical memory capacities of nvidia gpus.
since the two models differ a lot we cannot use the same bound value for both.
note that we use batched inference time that is the total inference time of a batch of data items.
specific upper bound values can be found in table iii.table iii configuration space reduction on real world dl m odels .
model max weight max gpu max bi max flops all name size mb memory gb time s gflops constraints vgg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
seq2seq .
lstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
note bitime stands for batched inference time which calculates the total inference time of a batch of data items.
the two values in a cell represent the upper bound and space ratio respectively.
the space ratio sr is used to assess the reduction effectiveness of dnnsat .
suppose that and dnnsat are the original and reduced configuration spaces respectively.
then sr j dnnsatj j j .
a smaller sr means a stronger reduction effect.
the promising experimental results in table iii demonstrate that dnnsat is effective in configuration space reduction.
for example the sr of vgg under the gpu memory consumption constraint ranges from .
to .
.
meanwhile the sr of seq2seq under the batched inference bi time constraint ranges from .
to .
.
tighter bounds or a combination of multiple constraints will lead to a more significant reduction.
the results can also give hints to developers and help them choose optimal settings of neural architectures hyperparameters and computational resources.
for instance the sr of vgg equals when the upper bound of the model weight size is set to mb which indicates that such a model may not be further dwindled by simply adjusting the hyperparameters.
therefore developers need to look for advanced dltechniques e.g.
model compression to embed it into a resource restricted application.
after applying four constraints collectively the sr value further decreases and is below the minimum of the one constraint sr values.
the results demonstrate the stronger reduction effect when adopting multiple constraints collectively.
dnnsat runs in both interactive and pruning modes with threads tiny subspaces containing model configurations and interval filtering being off.
we show the runtime performance of dnnsat under one constraint and four constraints as follows interactive mode.
for vgg the solving time per configuration is .10s warm up .13s on average under one constraint and .13s warm up .18s under four constraints.
for seq2seq the corresponding time is .05s warm up .40s on average and .12s warm up .44s respectively.
pruning mode.
for vgg dnnsat spends 226s on average under one constraint and 283s under four constraints.
for seq2seq the corresponding time is 806s on average and 1301s respectively.
183c.
rq3 how efficient is dnnsat in constraint solving?
in this section we evaluate the solving efficiency of the optimization techniques proposed in section iv e .
we choose thelstm based seq2seq model with batch size interval and hidden size interval as the hyperparameters.
the configuration space then consists of model configurations in total.
to increase the solving difficulty we use a loose flops constraint under which every configuration satisfies.
a series of experiments are conducted with different thread numbers and subspace sizes and equipartition and interval filtering settings on and off .
we do not create more threads since the experimental machine has only vcpus.
equipartition means that we divide the original configuration space equally by the number of threads and do not further split it into tiny subspaces.
for example in the case of threads each thread independently solves a subspace of configurations.
table iv shows the end to end execution time in seconds and speedup relative to the baseline experiment thread equipartition and interval filtering being off .
the speedup ranges from .6x to .1x confirming the strong effectiveness of our optimizations.
simply increasing the number of threads achieves an ultra linear speedup from .3x to .9x because a smaller configuration space notably reduces the overhead of allsat solving section iv e .
tiny subspaces achieve a speedup from .6x to .0x in the experiments that turn off interval filtering with the same reason as the parallel solving.
nevertheless as the subspace size getting smaller the speedup grows slowly and then drops from the size of because the overhead of allsat solving is no longer noticeable while the management cost of tiny subspaces increases.
interval filtering demonstrates dramatic improvements in the equipartition experiments and achieves a maximal speedup of .1x.
the reason is that dnnsat divides the original configuration space on hidden size to keep as long a continuous interval of batch size as possible since flops is monotonically increasing with regard to batch size.
therefore dnnsat solves only a small number of configurations to reach the conclusion that the entire subspace satisfies the constraint.
however if there exist many short intervals of batch size e.g.
the domain of batch size is small or tiny subspaces are used the effect of interval filtering will not be so significant.
vi.
d iscussion a. extensibility of dnnsat currently dnnsat supports commonly used operators.
dnnsat is extensible and users can incorporate new operators and constraints.
to add a new operator users formulate the analytic resource cost functions based on its semantics and then implement the smt lib templates.
to support a new constraint users formulate the analytic restriction function using defined hyperparameters and carry out the above operator adding steps for each of the operators under consideration.
besides userstable iv runtime performance of dnnsat under the flop sconstraint withoptimization techniques of parallel solving tiny subspaces and interval filtering .
subspace number of threads size inter val filtering off inter val filtering on equipartition .
.
.
.
.
.
.
.
.0s .2s .1s .2s .1s .11s .12s .27s .
.
.
.
.
.
.
.
.4s .0s .2s .3s .0s .9s .5s .0s .
.
.
.
.
.
.
.
.3s .0s .2s .6s .4s .4s .8s .7s .
.
.
.
.
.
.
.
.5s .9s .4s .1s .9s .1s .3s .5s .
.
.
.
.
.
.
.
.6s .4s .0s .9s .0s .2s .6s .9s .
.
.
.
.
.
.
.
.6s .8s .0s .4s .9s .4s .9s .7s note the two values in a cell represent the speedup and execution time in seconds .
may need to reimplement the graph traversal to compute more accurate current resource consumption by employing additional information including visited operators edges and previously calculated resource consumption.
b. threats to validity we discuss the following threats to the validity of our work resource cost functions .
we examine the source code of frameworks to extract the resource cost functions ofdloperators for inferring resource usage.
however the implementation of operators can call proprietary nvidia cuda cudnn or cublas apis which may introduce some fluctuations in the cost functions.
for example cudnnconvolutionforward could use temporary gpu memory called workspace to improve the runtime performance.
nevertheless the workspace size is convolution algorithm dependent and thus unpredictable.
we mitigated this threat by refining the resource cost functions after carefully referring to the nvidia development documentation dynamically profiling the apis using nvidia nvprof and analyzing the framework runtime logs.
hidden factors .
there are a number of hidden framework factors that can observably affect the gpu memory consumption and inference time of a dlmodel.
for example the gpu memory consumption has complicated dependencies on the allocation policy e.g.
tensor fragmentation alignment garbage collection and reservation internal usage e.g.
cuda context operator scheduling etc.to mitigate this threat we referred to the framework source code carefully to identify hidden factors.
however it is very challenging to directly formulate all such factors e.g.
garbage collection and reservation analytically.
hence dnnsat conservatively calculates the resource usage to reduce the influence of hidden factors.
for instance dnnsat adopts a simplified yet common approach to accumulate the gpu memory required by each operator under only forward propagation section iv b which computes a smaller value than the actual gpu memory consumption.
if the computed value a conservative value already exceeds 184the gpu memory upper bound the corresponding model configuration indeed does not satisfy the constraint.
therefore valid satisfiable model configurations will not be discarded.
however some invalid unsatisfiable model configurations may not be eliminated correctly due to the inaccuracy in the calculation of hidden factors.
in the experiment on vgg section v b we notice that on average .
of model configurations are actually invalid under four constraints yet passed dnnsat but no valid model configurations are discarded.
in the future we will identify more hidden factors and design more accurate restriction functions to obtain more precise results.
smt solving .
the simple and non intrusive integration with existing automl tools is to run dnnsat in the interactive mode.
according to the experimental results in section v the cost of solving one model configuration is very low.
in the pruning mode it takes a longer time for constraint solving because of the large configuration space.
for example table iv in section v c shows that dnnsat spends seconds about .
hours to solve the configuration space of seq2seq .
we currently propose some effective optimization techniques in section iv e to increase the scalability of constraint solving dnnsat supports parallel and distributed solving via spark in which the full configuration space can be partitioned into any number of independent subspaces and solved concurrently by different threads and machines the use of tiny subspaces reduces the complexity of solving individual subspaces removes the computation skew across subspaces balances the workload among threads and thus avoids stragglers i.e.
threads that take an unusually long time to finish interval filtering can further reduce the solving complexity significantly if the restriction function of a constraint is monotonic with regard to some hyperparameters.
the experimental results in table iv confirm the strong effectiveness of our optimizations.
however as the number of hyperparameters and their domains increase the configuration space can enlarge exponentially thus the computational complexity may still exceed the capabilities of an smt solver.
in addition to advancing the solvers it is possible to tackle this problem by trying larger scale distributed solving with better load balancing.
vii.
r elated work many software systems are highly configurable by providing a rich set of configuration options.
configuration options are also considered as features in the software product line context.
however it is very time consuming and error prone for manual configuration tuning due to a large number of option combinations.
software engineering researchers have proposed various approaches to predicting the performance of configurable systems checking the consistency of configurations and understanding how configuration options and their interactions influence systemperformance .
like traditional software systems dlmodels are also highly configurable.
in this work we analyze dlmodels and propose to optimize the configuration exploration automl through resource guided space reduction.
google vizier and microsoft hyperdrive are representative automl systems which concentrate more on the system design and operation.
hyperband is an hpo search method and focuses on speeding up random search through adaptive resource allocation and early stopping.
enas uses a controller to discover various neural architectures and search for the optimal one.
these methods and systems are not aware of the constraints imposed by computational resources which can cause an expensive waste of shared resources.
our work can help them efficiently reduce the configuration space ahead of time and accelerate training.
the authors of analyzed the resource budget constraint for hpo.
however they encoded the budget into the algorithm instead of formulating explicit resource constraints and thus their method cannot be applied to other automl methods directly.
hern andez lobato et al.
considered constraints for bayesian optimization but the work is for a specific algorithm.
gordon et al.
proposed an approach to automate the design of neural structures via a resource weighted sparsifying regularizer.
ams generated the automl search space from an unordered set of api components.
our work formulates common constraints imposed by resources and uses a unified analytic approach to eliminate the unsatisfiable model configurations in advance.
there have been many program analysis methods to determine the quantitative resource usage e.g.
memory and heap space of computer programs.
for example hoffmann et al.
used the automatic amortized resource analysis aara technique in analyzing the worst case resource consumption of arbitrary multivariate polynomial functions.
jost et al.
employed a type based approach by exploiting linearity and focusing on the reference number to an object.
however such work usually targets higher order functional programs and cannot be directly applied to dlmodels because of the wide differences in the representation structures.
our work proposes an analytic and framework independent cost model to infer the resource consumption of a dlmodel and utilizes an smt solver to obtain all the satisfiable model configurations.
viii.
c onclusion in this paper we have presented dnnsat a resource guided automl approach for deep learning models to efficiently reduce the configuration space under computational constraints.
powered by dnnsat we demonstrate that commonly used automl methods can efficiently prune unsatisfiable model configurations ahead of time to avoid unnecessary training costs and achieve significant speedups.
we believe that dnnsat can make automl more practical in a real world environment with constrained resources.