neuri diversifyingdnngenerationviainductiveruleinference jiawei liu universityof illinois urbana champaign usa jiawei6 illinois.edujinjun peng columbia university new york usa jinjun.peng columbia.eduyuyaowang nanjing university nanjing china yuyao6 outlook.comlingming zhang universityof illinois urbana champaign usa lingming illinois.edu abstract deep learning dl is prevalently used in various industries to improve decision making and automateprocesses driven by the ever evolving dl libraries and compilers.
the correctness of dl systems is crucialfor trust in dl applications.
as such the recent waveofresearchhasbeenstudyingtheautomatedsynthesisoftestcases i.e.
dnnmodelsandtheirinputs forfuzzingdlsystems.
however existingmodelgeneratorsonlysubsumealimitednumber ofoperators forlackingtheabilitytopervasivelymodeloperator constraints.toaddressthischallenge wepropose neuri afully automated approach for generating valid and diverse dl models composed of hundreds of types of operators.
neuriadopts a threestepprocess i collectingvalidandinvalidapitracesfromvarious sources ii applying inductive programsynthesisoverthetraces toinfertheconstraintsforconstructingvalidmodels and iii using hybrid model generation which incorporates both symbolic and concrete operators.
our evaluation shows that neuriimproves branchcoverage oftensorflow andpytorchby and15 over the state of the art model level fuzzers.
neuri f inds newbugs for pytorch and tensorflow in four months with already f ixed orcon f irmed.ofthese 9bugsarelabelledas highpriority orsecurity vulnerability constituting10 ofallhigh prioritybugsoftheperiod.
open source developers regard error inducing tests reported by us as high quality and common inpractice .
ccs concepts softwareanditsengineering softwaretestinganddebugging computingmethodologies neuralnetworks .
keywords fuzzing compilertesting deeplearningcompilers acmreference format jiaweiliu jinjunpeng yuyaowang andlingmingzhang.
.
neuri diversifyingdnngenerationviainductiveruleinference.in proceedingsof the31stacmjointeuropeansoftwareengineeringconferenceandsymposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.https the workwas performed during a remote internshipat universityof illinois.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
introduction theriseofdeep learning dl librariesandcompilershasenabled emerging ai applications such as ai chatbots art generators and autonomous driving powering hundreds of millions ofusers.thesecomplexsystemshavebecomeincreasinglyadopted and everevolving.
forexample pytorch and tensorflow themostpopulardlsystemswith62kand171kgithubstarsrespectively aremovingtowardtheirnextmajorversion i.e.
pytorch and tensorflow aiming at better model compilation support.
however taking pytorch s new compiler as an example sincebirth i.e.
17months itisinsufficientlytestedbyatest suiteineightthousandloc.consequently itiscrucialtoharness the correctness of dl systems via extensive and automated testing.
thetest casegenerationproblem fordlsystemsistosynthesize adnnmodelanditscomputationalinputs.additionally generating diverseandvalidmodels is essential for making high quality tests.
modeldiversity effectivedlsystemtestingasksformodeldiversitycomingfromthevarietyofapis aswellasthewaythey are composed.
additionally to test the complicated dl compilers itis importanttogenerate models withmultiple operators ofvarioustypes for practicing the compilerpasses .
validity dnn models are programs for well formedness theyneed tocomplywithvalidity constraints.
arbitrarilyconstructing and composing operators such as creating pooling operators with negative kernel sizes or connecting operators withunwantedtensorshapes oftentimesviolatetheconstraints for constructing a well de f ined model.
as a result argument errors for dl libraries or parser errors for dl compilers are raisedbefore deeper systembehavioursare tested.
single apistrongly constrained weaklyconstrainedmanualoperatorrules.this paper auto.
rule inference!our goal operator diversity modeldiversity figure test case diversity.motivation.
the model diversity primarilydependsonthecomprehensiveness of operators which arethebuildingblockstoamodel.
prior work on single api testing can generate a large body of api invocations includingbothoperatorandutilityapis viamutationorgenerationwhich complywithhigh leveltypeconstraints or the plausible value sets.
can we directly apply such high level information to generate valid dnn models?
unfortunately it is impractical.
because constructing a validapiinvocation further requires satisfying f inegrained constraints between operator attributes i.e.
non tensor argumentssuchaskernelsizesandstrides andinputtensortypes1 1followingpriorwork atensortypeis a tuple of its shapeand data type.
esec fse december3 san francisco ca usa jiaweiliu jinjunpeng yuyao wang andlingming zhang particularly shapes .
for example while single api testers may understandthat conv2dacceptsanimageandaweighttensorof f loating points i.e.
typeconstraints their newlycreatedconv2d invocationsarenotguaranteedtohavethechanneldimensionof theimagematchingthatoftheweight i.e.
shapeconstraints .consequently suchattributesviolatethevaliditypropertiesrequired byconv2dand lead to invocation failures.
without understanding such f ine grainedconstraints itisunlikelytocorrectlycompose variousapisfor constructing well formedanddiversemodels.intuitively in figure 1prior single api testers can achieve ideal apidiversitywhenapisbeingvalidlyconstructed.however the diversity hardly extends at model wise which requires multiple apis to be constructedand connected correctlysimultaneously.
meanwhile there are two categories of proposals for constructingvalidmodels.
weaklyconstrainedmodelgeneration limits the use of apis to those with simple and straight forward constraints.
for example lemon only uses shape preserving operatorsthathavenoinputconstraints.consequently suchoperatorscanbearbitrarilyconstructedandaddedtobuildamodel.
morerecentwork additionallyinserts reshaping layers suchthatreshapedoutputtensorscanstayincompatibleshapes.
however it still may not construct operators with valid attributes non tensor arguments .
even worse using such layer wrappers compromisesthestructuraldiversityofthemodels whichcanoverlookcompilerpassesactivatedbyspeci f icpatterns.tosupportmore diverse apis correctly nnsmith as astrongly constrained approach de f inesaspeci f icationfordescribinginputconstraints and shape propagation elaborated in .
nonetheless it requires manual effortsfor specifyingthose rules.
for example while a dl framework e.g.
pytorch can de f ine over two thousand apis only about sixty are supported by nnsmith after its f irst year development.hence itcantakeyearsfor nnsmith tocompletely support aframework i.e.
fromto whichisunscalable.
insight.canwescalethediversityofmodelgenerationbyenabling more operators e.g.
by hundreds fully automatically ?
we start to answerthisquestionfromtwoinsights i empiricallyweobserved that most operator rules are simple e.g.
consisting of arithmetic expressionsforshapecomputationandif elsebranchesforhandling conditions incurred by some attributes.
as a result it is feasible to search a program that functions as operator rules given the size of the problem is acceptable.
speci f ically by instrumenting dl api invocations we can obtain a set of input output examples with whichtheinferenceofoperatorrulescanberegardedasainductive program synthesis problem .
ii can an operator still be usedfor modelgenerationevenifitsoperatorrule is not available?
we f ind it feasibleby inserting a concrete operator initialized by recorded invocation traces.
to make use of both symbolically andconcretelyobtainedoperators wecanapplya concolicmodel generationapproach to constructmodels withboth sources.
summary.
this work makesthe following contributions inthiswork wepresenttheurgencyforimprovingapidiversity of model generation and formally introduce the essential properties for generating valid dnns operator rules .
furthermore we open the f irst proposal of automatically inferring operator rules for diversifying andscalingvalid modelgeneration.
figure the symbolicview of avg pool2d .
webuild neuri neuralnetworksynthesisvia ruleinference a fuzzerfortestingdlsystemswiththreesteps i aninstrumenter thatcollectsandaugmentsapiinvocationsfromvarioussources ii an optimized rule synthesizer that efficiently infers operator rules with inductive program synthesis and iii a hybrid model generator that compiles both symbolic and concrete information for producing valid anddiversednns.
we extensively and rigorously evaluated neuri.
within four months neuri f inds newbugs for pytorch and tensorflow with f ixed or con f irmed.
of the pytorch bugs are labelled as highpriority orsecurityvulnerability constitutingaround10 of all high priority bugs in pytorch s bug tracker of the period.
by evaluating branch coverage neuriimproves the state of the art model level fuzzerby15 pytorch tensorflow .
operator rules the functionalityofa deep learningmodel i.e.
dnn canberepresented as a list of operations each of which transforms one or multiple input tensors i.e.
multi dimensional arrays to output tensors.
accordingly a test case indl systems constructs adnn and is evaluated over some computational inputs expecting the modelcan be successfully executedandproduce correctresults.
for generating effective test cases automatically it is crucial togenerateanddiversifyvaliddnnmodels.state of the art nnsmith constructsvaliddnnswithoperatorconstraintsand shape propagation rules.
with smt solvers such rules can help staticallyconstructanoperatorwhichcanbesafelyinsertedtoa given model.
because in dl frameworks such operator rules are implicitde f inedandcannotbeexporteddirectly theyaremanually speci f ied in nnsmith .
however crafting them from scratch is unscalable.forexample inthe f irst yeardevelopmentof nnsmith onlyaroundsixtyoperatorsareimplementedwithrules despitethe factthatmanyrulesareevenrepetitive.asaresult fordiversifying operatorsbeingusedandsavingmanualeffortofdomainexperts we aim at inferring those operator rules automatically .
we now formalize andelaboratethe operator rules symbolizing operators.
as is shown in figure an operator is a function which takes input tensors e.g.
input and con f igurations e.g.
kh as arguments.
the con f igurations also known as operator attributes describe high level semantics for performing an operation and can impact the operator rules.
for example kwandkh de f ine the size for applying the avg f ilter over the input image whichmustbenosmallerthanthekernelsize assumenopadding .
forbeingevaluated statically operatorrulesonlyleverageandsymbolize an operator s compile time information i operator type e.g.
avg pool2d ii u1d43cwhichisalistofinputshape vectors and iii u1d434asthesetofoperatorattributes.runtimeinformationsuch as the detailed element values inside the input tensors is too costly to bemodelled.meanwhile we use u1d442to denotetheoutputshapes producedbythe shape propagationrule.
rule input constraints.
the input constraints of an operator are a set of predicate functions c u1d4501 u1d4502 over u1d434 u1d43c.
for 658neuri diversifyingdnn generationviainductive ruleinference esec fse december3 san francisco ca usa example constraints in avg pool2d require the kernel size to be nolarger thanthe paddedimagesize e.g.
u1d456 uni210e u1d44epadh namely u1d450 u1d458 parenleftbig u1d434 u1d44ekh u1d44epadh u1d43c parenrightbig u1d44ekh u1d456 uni210e u1d44epadh theargumentsof u1d450 u1d458consistoftheattributesto avg pool2d and theshapelistwiththeshapeoftheonlyinputtensor i.e.
u1d43c .it is worth noting that for clarity we assume the input is a non batch image with only three dimensions i.e.
the channel height and width however inpractice 2d poolingalsoacceptsbatchedinputs withan extra batchdimension.meanwhile someoperatorscould takea variablelength ofinputs e.g.
concatenate oroutputs e.g.
split .
for being general a predicate may not assume the tensor signature i.e.
of input output tensors and their ranks to be f ixed.
thus operatorruleswithsuchpatternsmayneedtobedescribed with conditional branches e.g.
the syntax of pytea .
we later in .2introduce how to leverage partial operators to simplify such branches whichare difficult to handle inruleinference.
rule shape propagation.
because evaluating rule requires knowingthedimensionsofoperatorinputs whichareoutputsof other operators fromthe dnn underconstruction output shapes of operators also need to be evaluated.
the shape propagation rule for anoperatorcanbedescribedby afunction pover u1d434 u1d43c which returnsalistofpropagatedoutputshapesas u1d442.forinstance the shape propagationfor avg pool2d can be describedas p parenleftbig u1d434 u1d44ekh u1d44epadh u1d43c parenrightbig where u1d45c u1d450 u1d456 u1d450 u1d45c uni210e floorleftbig u1d456 uni210e u1d44epadh u1d44ekh u1d44estride floorrightbig u1d45c u1d464 floorleftbig u1d456 u1d464 u1d44epadw u1d44ekw u1d44estride floorrightbig for example given inputshapeof we can tellthe correspondingoutputshapeforoperator infigure assuming u1d44estride is1 is withoutinvokingit.
approach figure3showsthe overviewof neuri swork f low.
neuriimprovesthesearchspaceofmodelgenerationbymaking useofconcreteinvocation.tocollectthose desiredinvocations of tensor apis we instrument various sources such as developer tests.
next we f ilter out invocations that do not meet properties suchasdeterminism tofacilitateruleinferenceandbugdetection in later phases.
for the convenience of rule inference we summarizetheinvocationrecordstoasimpli f iedstructureand further augment data diversityviamutation .
.
these records are discrete data points with which we can inductivelysynthesizearithmeticexpressionsintheircorresponding operatorrules.theinductiveprogramsynthesisproblemis however an np hard problem whose complexity rests with the grammar under enumeration.
for affordability we split a complete operator rule into multiple sub rules in order to be describable by a simple arithmetic grammar.
furthermore we prune the enumeration space over equivalence andrarity and as a shortcut reuse rules when possible.
additionally redundant inputconstraintsare removedfor runtimeefficiency.
next we apply hybrid dnn generation which performs dnn generationover i symbolicoperators i.e.
thosewithoperator rules and ii concreteoperators i.e.
thosewhoserulesarenot inferred but with concrete invocation records.
to achieve this we perform concolic operator insertion where symbolic operators areinsertedwithsmtsolvingwhileconcreteonesareinserted by searching a compatible tensor type i.e.
shape and data type .
lastlythegeneratedmodel aftermaterialization iscross checked between the interpreter andcompilerviaoracles in .
.
.
instrumentation invocation collection.
following prior work we instrument the desired apis to store successful invocations locally.
in contrast tofreefuzzwhichcomprehensivelyinstrumentsapisfromboth tensorandhighlevels wefocuson tensorapis sincehigh levelapis canbedecomposedtoaseriesoftensoroperations.additionally thesetensorapismustbedeterministicandvalue independent detailedin .weperforminstrumentationatpythonlevel e.g.
over the python test suite since python is commonly used as the frontendofdlframeworks.meanwhile wesimplifythedisorganized and super f luous raw invocation figure by dropping concrete tensorvaluesandtherebyonlypreservethetensortypes functor and other arguments.
for instance of avg pool2d figure2 the layoutofits simpli f iedrecord2isillustratedinfigure .
data augmentation via mutation.
the robustness of inferred rulesdependsonthequantityandqualityofrecords.unfortunately by only using instrumented records each rule on average only shares5 7records whichareinsufficient.moreimportantly allcollected records are passing examples however counter examples are alsorequiredforinferringinputconstraints .
.consequently wefurtherdiversifytherecordsbymutatingexistingrecords where validmutantsareusedas passingexamples denotedbyr andinvalidonesareusedas counterexamples denotedbyr .speci f ically weperformthreephasesofmutationsoverinputshapedimensions andattributes ofrecords i.e.
u1d434 u1d43c offset based the goal of offset based mutation is to quickly build a large set of preferably passing examples.
to achieve this we enumerate subsets over u1d434 u1d43c for each of which we increment the elements by until a desired number of records e.g.
in our experiments or time budget runs out.
the hypothesis behind is validity locality oftentimes validity is preservedafter alight weightmutation i.e.
increment by1 .
swapping exchangingtwovaluesfrom u1d434 u1d43ccanquicklyverify simple inequalities.
for example assuming u1d44e u1d44fholds in all collectedpassingexamples weinvalidatetheinequalityifthe recordisstillvalid after exchangingthe valuesin u1d44eand u1d44f.
specialvalues lastly werandomlyassignattributeswithspecial values e.g.
inorder to test attributes negativity.
meanwhile ifno counter examples are produced after sufficient mutation itmeansprobabilisticallyithasno orextremelyweak inputconstraints.consequently forcounter example freeoperators we directly assign an empty set as its input constraints i.e.
no need to infer input constraints with inductive synthesis in .
.
2forclarity weuse record torepresent simpli f iedinvocationrecords fromnowon.
659esec fse december3 san francisco ca usa jiaweiliu jinjunpeng yuyao wang andlingming zhang rule synthesisunit tests model hub... filter simplify augmentinvocationstrace tensor apis .
instrumentation summarized recordsexpr prune rule reuse deduplicate .
rule inferenceshape propagationinput constraintsrules invocations forwardbackward .
hybrid dnn generation concolic op insertion graphir modelsinterpreter compilerinconsistent results runtime error sanitizer error .
oraclesbug reports figure overview of neuri figure layoutofrecordsbefore andafter simpli f ication.
.
ruleinference wenowexplainhowtoperformruleinferenceviainductiveprogram synthesis and make it affordable with a line of optimizations.
inductive synthesis of operator rules.
input constraints and shape propagation rules can be described by functions over u1d434 u1d43c whose bodies are oftentimes arithmetic expressions .
speci f ically we can de f inesuch an arithmetic grammar gas follows expr op expr expr item op min max mod item symbol constant symbol symbolsfrom u1d43cand u1d434 constant constantintegers withthegrammar wecaninferanoperatorruleviainductive program synthesis i.e.
by enumeratinggto f ind an expression thatmatches inputs outputsoftherecords incertain timebudget and program size.
because expressions in operator rules tend to be short weperform bottom up enumerativesearch which f irst constructssmallterms e.g.
item andcomposethemgraduallyfor generatinglargerones.forclaritywedenotethesetofenumerated expressions to be e. meanwhile there are a few hypotheses to consider e.g.
u1d434 u1d43c and u1d442 areassumedbe f ixed.wewilldetail themlaterinthe partialoperator paragraph.formula2describes a shape propagation rule with a set of expressionsforcomputingcorrespondingoutputdimensions.toinfer thepropagationexpressionforthei thoutputdimension i.e.
u1d45c u1d456 we enumerateeuntilexpr u1d458 eis found to match all records formula3 .otherwise wesay therules arenotinferredandwillnot be usedfor inserting symbolicoperators duringmodelgeneration.
p simequal u1d45c1 expr1 u1d43c u1d434 u1d45c u1d45b expr u1d45b u1d43c u1d434 expr u1d456 e expr e u1d434 u1d43c u1d442 r expr u1d45c u1d456 similarly input constraints care predicates of equalities and inequalities which can be normalized to exprand0 expr respectively.algorithm 1illustrateshowc startingwithanempty set line2 isinferred.byenumeratingpredicatesorientedfrom e withinthetimelimit line we f indpredicatesthataresatis f ied byallpassingexamples line .speci f ically ifanypassingexample does not match the predicate under enumeration i.e.
u1d450 line6 u1d450 isthenundesired andconsequentlywerestarttheloopforthenext predicate line .otherwise weinclude u1d450inc line8 .meanwhile forsoundnessinputconstraintsshouldrejectinvalidinputs ifany .
consequently cshouldrejectall ifany counterexamples line .
otherwise the ruleisnot inferred line .
algorithm1 inference of inputconstraints c 1function inferinputconstraints e r r 2c 3label for u1d450 expr expr expr e do iftimeoutthen break for u1d434 u1d43c r do ifevaluate u1d450 is falsethen continue label go to next u1d450at line 8c c u1d450 9for u1d434 u1d43c r do ifevaluate logicalandtext.1c is truethen raiseinference failure 12returnc partial operator.
innnsmith operator rules are directly written inpython whosegrammarismuchmorecomplicatedthan g.runninginductiveprogramsynthesisoversuchacomplexgrammar 660neuri diversifyingdnn generationviainductive ruleinference esec fse december3 san francisco ca usa figure5 examplesofsimilarbutdistinctpartialoperators.
f32 stands for a f loat32 tensor variable whose rank is .
though being more capable is impractical.
to preserve a grammar as simple asg we split an operator into multiple partial operators by f ixing components whose variation incurs a more complicatedgrammar.wecanempiricallysummarizesuchcomponents for de f ining partial operators.
for example branches in operator rulesareusedforhandlingvariablelengthsofinputsorinputranks.
additionally rulesofoperatorswithdimension sensitiveattributes e.g.
diminmax x dim often requires array operations.
with figure5 in addition to api names e.g.
1and2 we identify a partialoperator withthe following properties tensorsignature recall 2thatanoperatorcouldtakeandreturn avariable length oftensorsin variousranks.
because incorporating such variability is costly we let each partial operator havea f ixedtensorsignature andthusa f ixedformof u1d43cand u1d442 .
forexample 2and3havethesameapinamebutaredifferent partial operators for having different input output ranks i.e.
3versus4 .itisalsoworthnotingthatwedonotdistinguish partial operator over the data types of input output tensors whichare often orthogonalto the operator rules.
symbolicattributes besidesinputtensors weregardotherargumentsthatcanbesymbolizedto symbolicintegers assymbolic attributes which are the free variables in operator rules i.e.
u1d434 in .
therefore invocations with different sets of symbolic attributes are associatedwithdifferentpartialoperators.
other arguments we further classify the rest of arguments i.e.
non tensor and non symbolic integer into two categories i rule orthogonalarguments e.g.
f loat pointscalarssuchas bias and ii likely rule dependentarguments e.g.
image layoutin nchw or nhwc .only ii willbeusedfor identifying partial operators for its potential impact on operator rules.ingeneral thesub categoryofan otherargument isdetermined by its type and value.
for instance in figure the ceilargument as a boolean falls into the ii category which makes 3and4differentpartialoperators.infact ceilimpacts the shape propagation rule of avg pool2d where being true makestheoutputheightandwidthroundedby ceilinsteadof f loor see formula .
further details willbe elaboratedin .
pruning.ecanbetoolargetoenumerate.forefficiency weprune semantic equivalentduplicates i.e.
equivalence aswellasthose thatareuncommoninoperatorrules i.e.
rarity .speci f ically we listthe pruning methodsintheirorder ofbeing applied bound without constraints eis in f inite.
therefore we bound ebylimitingthenumberof u1d45c u1d45d andthesetofconstantliterals.for example in our default experimental setting the maximum numberof u1d45c u1d45d beingusedis5andweuse u1d450 u1d45c u1d45b u1d460 u1d461 u1d44e u1d45b u1d461 .
additionally expressions describing inequality are further limited to have at most one u1d45c u1d45d because i inequalities are oftentimes simple and ii a larger upper limit will lead to many false positives as inequalities are more f lexiblethanequalities.
rarity weempiricallypruneexpressionswiththesamesymbolsoccurringmorethanonce whichareuncommon.those withconstantsub expression i.e.
u1d45c u1d45d u1d450 u1d45c u1d45b u1d460 u1d461 u1d44e u1d45b u1d461 u1d450 u1d45c u1d45b u1d460 u1d461 u1d44e u1d45b u1d461 are alsoprunedfor being constant foldable.
equivalence we f indsemantically equivalentexpressionsin e and only keep the simplest one.
speci f ically we leverage a twopass approach inspired by ruler first for the expressions withthesamefreevariables we quicklyevaluatethemovera number of randomly generated assignments and group them according to the outputs i.e.
often known as characteristic vectorsor f inger prints .foreachgroup wethenrigorously f ind equivalentsbyapplyingan smtprover.
emayvaryfordifferent u1d434 u1d43c.whilepruningeforeachpartial operator is costly we make it one time effort by i pruning e with holes i.e.
symbol placeholders and ii extending e toe byreplacingtheholeswithactualsymbolsofeachpartialoperators.
for example assume that e is the pruned set of expressions with holes of i.e.
symbol .
to infer an operator rule with u1d434 u1d43c u1d4601 u1d4604 we get the actual eby extendinge bymappings u1d4601 u1d4604 to invarious ways.morespeci f ically foreach exprwith uni210eholes symbols we select uni210esymbols from u1d434 u1d43cto f ill the holes i.e.
substitution .
because of the one time occurrence hypothesis each symbol in u1d434 u1d43cwill not be selected to f ill multiple holes i.e.
injective .
in addition themappingfromtheselectedsymbols in u1d434 u1d43c toholesis determined according to the relative order of indices.
for example for by selecting u1d4601 u1d4602 we only get u1d4601 u1d4602.
why not consider permutation over the mappings?
consider if we allow u1d4602 u1d4601as an extension when extending indices swapped we get the same duplicated expression.
as a result for each expr with uni210eholes we can extend parenleftbig u1d434 u1d43c uni210e parenrightbigexpressions.
meanwhile when u1d434 u1d43c is smaller than the maximum number of holes we only considerextending exprwhere uni210e u1d434 u1d43c .
rule reusing.
partial operators can share equivalent rules.
before running inference from scratch we can f irst test if the records can bematchedbyalreadyinferredrules iftheysharethesameformof u1d434 u1d43cand u1d442 .ifanexistingrulecanbematched wecansimply copy and paste it for the new partial operator as a short cut otherwise we can still run inference from scratch.
furthermore since this optimizationisorthogonaltothegrammar wecanalsoreusethose expert craftedrulesfrom nnsmith python grammar .
algorithm2 predicate deduplicationfor c 1function deduplicate c 2repeat for u1d450 cdo ifprove logicalandtext.
logicalandtext.
then c c u1d450 6untilcunchanged 661esec fse december3 san francisco ca usa jiaweiliu jinjunpeng yuyao wang andlingming zhang deduplication.
theinferredinputconstraintscouldhavemanyredundant predicates which slows down smt solving when fuzzing online and makes it less readable.
therefore algorithm 2deduplicates the predicates obtained from algorithm .
we each time remove a predicate u1d450ifcis equivalent to that without u1d450 line4 .
we run thealgorithm until a f ixed point whennopredicatesfrom care removedafter an iteration.
.
hybriddnn generation following the algorithm in nnsmith the generation of a dnn can be regarded as a problem of inserting a validoperator correctlytoanalready validdnnmodel.in nnsmith sincealloperatorshastheircorrespondingrulesimplementedbydomainexperts a dnnmodelis synthesized symbolically i.e.
allshapedimensions and attributes are viewed as symbols during construction and later materialized by a set of assignments offered by the smt solver.
for neuri we adopt a concolic style of dnn generation in order incorporateboth symbolic andconcrete operators3.
speci f ically the hybrid dnn generator inserts operators concolicly such that each operator after insertion is immediately concretizedbythemodelfromthesolver insteadofdeferringitwhena fulldnnisbuilt.therefore thednnunderconstructionisalways concrete i.e.
all shape dimensions data types and attributes are concreteatconstructiontime whichmakesitapplicabletoinserta concreteoperator.thereareafewbene f itswithaconcretednn i the insertion of concrete operators can be efficiently implemented by looking up compatible tensor types between traced records and themodelunderconstruction and ii theoreticallysmtsolving isincurredlessintensively thusfaster withlesssymbols.in nnsmith an operator can be inserted in two directions forward insertionthat inserts anoperator which consumesexisting values and backward insertion that lets an operator be a producer by occupying existing placeholders.
next we elaborate how symbolic andconcreteoperatorsare forwardinsertedandforclarityomitthe detailsforbackwardinsertion whichcanberegardedasareversed versionover the placeholders instead ofarbitrary tensors .
inserting a symbolic operator.
both the manually written operatorrules i.e.
nnsmith andinferredones i.e.
neuri canbe used to insert symbolic operators.
operators in both groups are selected separately in equal amount of probability.
each time to insert a selected operator u1d719 we f irst enumerate arity sized combinations of tensor variables as the input candidates to u1d719 where each of them must respect the data type and rank requirements of u1d719.
taking the batched 2d convolution as an example whose input tensormusthavefourdimensions anyothertensorswhoserank is not four will not be taken into the enumeration.
next each of theinputcandidatetuplesischeckedbytheinputconstraints c u1d719 until one satis f iesc u1d719and thus becomes the tensor inputs ito u1d719.
furthermore weaskthesmtsolvertoprovideamodelfromthe inputconstraintsandusetheassignmentsofoperatorattributes u1d434 toinitialize u1d719 u1d719 .wetheninsert u1d719 takingiasinputs tothe dnnunderconstruction.
we alsopropagateitsoutputtensortypeswiththeshapepropagationrule formula formaking futureinsertionfeasible .ofcourse ifnoneofthecandidates 3asymbolic operator is an operator with inferred rules while a concrete operator does not haverulessuccessfullyinferred butstill has its corresponding validated records.canmaketherulesatis f iable the insertionof u1d719willbediscarded andthe algorithm willre try anotheroperator.
inserting a concrete operator.
the feasibility of inserting a concreteoperatorisassimpleas f indinganintersectionoftensortypes between inputs i.e.
u1d43c in records and visible variables in the workingdnn.forexample givena avg pool whichhasinputtensor type offloat32 in the records if there is a tensor variable with a shape of and a data type of float32 in the dnn under construction we can safely insert it to thetargetplace.however becauseofthelargevolumeofrecords checkingthesatis f iabilityoperatorbyoperatorandrecordbyrecord isinefficient.instead wecanbuildamappingfromtensortype i.e.
shapeplusdatatype toasetofpartialoperators anyofwhichhas aninputtensorofsuchatype.then byaccumulatingthesetofpartialoperatorsmappedfromtensortypesavailableintheworking dnn wegetareducedsetofoperatorcandidateswhichexclude thosewithunsatis f iableinputtypes.consequently weonlyneed toenumeraterecordsofreducedsetsofpartialoperators.oncea recordiffound tobematchable weinitialize thepartialoperators withthe recordandinsert itto the dnn underconstruction.
.
testoracle in this section we listthree test oracles for manifesting bugs.
resultinconsistency.
inadditiontorunningthednns eagerly with the pre compiled library functions i.e.
interpreter tensorflow and pytorch can further optimize the models via compilation for better performance.
hence we cross check the results obtained by running the same model and inputs from the interpreter and compiler where asubtle f loating pointerrorisallowed.
runtimeerror.
weidentifyaruntime errorbugifthecompilation orexecutionofamodelabortsunexpectedly.thecorresponding symptoms include a crash or an unexpected python exceptions not incurred by incompatibility e.g.
not implemented error .
furthermore interpreter exceptions are not considered as bugs as itcould be causedthe use of an incorrectoperator rule.
sanitizer error.
we also enrich the test oracles with sanitizers such as asan memory error ubsan use of compiler s unde f inedbehavior andcsan cudaerror .sanitizererrors are reported by sanitizer injected checkers at runtime.
without sanitizers bugsmaynotmanifestthemselvesviaacrash e.g.
buffer over f low or occur at a late stage making debugging challenging.
implementation neuriimplementsthreecomponentsincludinganinstrumentation tool a rule synthesizer and a fuzzer via a total effort of .9k loc.
apiinstrumentation.
the instrumentationtool is implemented with .8k loc in python.
the instrumentation is performed by insertinganapi hijackingcodesnippetinthe init f ilesofdl framework packages.
speci f ically we run the instrumentation over thedevelopertestsfromtheopen sourcerepositoriesofpytorch and tensorflow.
as soon as the dl packages are imported the api hijacking code adds a function wrapper to all functions within the package.
during execution i.e.
running the regression tests ofdlframeworks theinvocationsnapshotstodesiredapisare serialized for reproduction.
in post processing a f ilter is applied to removeinvocationsthatdonotcomplywith determinism andvalue 662neuri diversifyingdnn generationviainductive ruleinference esec fse december3 san francisco ca usa independence .todetect determinism eachinvocationisreplayed forthreetimesforcheckingoutputconsistency.fortesting value independence i.e.
the operator rules are independent to the values elementsintheinputtensor eachapiistestedbythreegroups of randominputs initialized from 106to106 andisexpected to output the same tensor types without runtime failure.
this componentalsoincludesutilitiesforparsingandcomposing replayinga dl api inorder to constructnewinvocation.
rulesynthesizer.
weimplementedthe synthesizer in1.5k loc in python.
before the actual rule synthesis we f irst apply data augmentationforenrichingtherecordsondemand i.e.
until100 records for each partial operator .
in .2it is found that not all argumentsimpacttherules consequentlyweidentify rule orthogonal argumentsinapartialoperatorthroughtheargumenttype fora f loating pointargument e.g.
bias weassumeitdoesnotimpact operator rules.
furthermore the arithmetic expressions are represented as binary trees and for memory efficiency smaller trees arere usedtocomposelargertreesviapointersinthebottom up enumerativesearch.weconstrainthemaximumnumberof op to i.e.
symbols atmost .
asa result as a one time effortwe f irst enumerategbyregardingthesymbolsas6 holes andpruneiton the f ly to gete .
meanwhile we also leverage commutativity of min max toskiptheenumerationof operandswappingand associativityover min max inordertoacceleratethe equivalence based pruning.
with e obtained as a one time effort for anynewoperatorruleunderinference we canquicklyextend e toeby f illingitsactualsymbolsintothe holes asdiscussed in .
.
speci f ically for each partial operator we set a timeout of secondsto inferthe shape propagationorinputconstraints.
fuzzer.the fuzzing engine of neuriis built by extending the nnsmith prototype with .6k new loc and removing .9k oldloc .majoreffortsarespenttoimprovetheextensibilityand debuggibility of the original nnsmith for bene f iting algorithm prototyping and bug f inding.
previously nnsmith uses directed multi graphsin networkx fordescribingdlmodelsinternally.
however the graph data structure is not suitable for manipulating model structures and being translated to real world model formats.
additionally dl models are fundamentally programs which is notnecessarilyalwayspuredata f lowgraphs.forexample in place operators for reproducibility require a total order during execution whereas traversinga graph cannot guarantee.
asa result we buildanssa basedintermediaterepresentation namelygraphir to describe dnn structures.
following the llvm interface style dnnmanipulationismadesafeandconvenientviathreefundamentalapisof insert remove unused andreplace alluse .
thankstotheextensibility f ivegraphgenerationstrategiesused in this paper including three neurivariants and two nnsmith variants are implementedinmerely1.1kloc.
evaluation we evaluate neuribyasking following researchquestions rq1 .
howdoes neuricompareagainststateoftheartin dl compilerfuzzinginterms ofcode coverage?
rq2 .
how many apis partial operators and records arecollectedandeventuallyinferredwithoperatorrules?howefficient and effective is our rule synthesizer compared with general purpose program synthesis toolssuch as rosette ?
rq3 .
how effective is neuriwhen detecting previously unknownbugsfor real world dl compilers?
.
experimentalsetup systemsundertest.
wetesttheemerging compilers ofthemost popular dl frameworks i.e.
tensorflow and pytorch whichfor clarity are denotedby tf and pt respectively.
tensorflow xla compiler converts a tensorflow model e.g.
savedmodel toitsgraph levelir i.e.
hlo forrunningvarious optimizationpasses.tensorflowde f inesover1500operators.
of these around are supported by xla.
this is because dl compilers often focus on a small set of primitive operators from whichotherhigh level operators can be composed.
pytorch jit the pytorch s equivalent of xla supports around apis including alias e.g.
torch.max a anda.max outofatotalofover pytorch operators.
metrics.
we evaluate neuriover various metrics.
speci f ically we explainthe mostimportanttwohere anddefer the others.
found bugs we count the bugs at the basis of bug reports whichareclassi f iedtofourstatuses f ixed apatchhasbeen effectivelyappliedto f ixthebug con f irmed inadditionto f ixed bugs we conservatively i.e.
lowerbound identifyacon f irmed bugiffit has been reproduced diagnosed as a fault or directly assignedtodevelopersfor f ixingit won t f ix developersclaim the potential of not f ixing it i.e.
upper bound and the rest of bugsare alltriagedbut require further investigation.
branch coverage following we evaluate fuzzers with branchcoverage astrongercriterion thanlinecoverage fortest adequacy over dl frameworks c sourcecode.
baselines.
in end to end benchmarks we compare neuriwith thestate of the artmodel levelfuzzers namely nnsmith and muffin andthestate of the artoperator levelfuzzer i.e.
deeprel .for ablationstudy we alsoevaluate neuri svariants.
nnsmith performs model generation with over operators whoserulesare manually craftedbydomainexperts.speci f ically theofficial nnsmith performs puresymbolic generationwhere theallsymbolsarematerializedtogetherwhenalloperatorsin thegrapharesymbolicallyinserted.inthispaperwealsopropose concolic generation .
consequently we also implemented a concolic version of nnsmith which immediately materializes thesymbolsperinsertion.becausetheconcolicvariantperforms similarlyasthepure symbolicversion forclarityweomittedthe resultsinevaluation.onereasoncanbethatconcolicinsertion does not bringmore operator supports in nnsmith asneuri.
muffin based on seed models including densenet and lstm performs mutation based model generation.
similar tonnsmith itsupports over60operators byhand craftingthe shapeinferencerules.however muffin createdmodelsarenot guaranteed to be valid for the lack of input constraints.
notably muffin is only implemented on tensorflow.
as a result we only compare muffinagainst othersontensorflow.
deeprelisanoperator levelmutation basedfuzzersimilarto freefuzz.
as an improvement to freefuzz whose seeds purely 663esec fse december3 san francisco ca usa jiaweiliu jinjunpeng yuyao wang andlingming zhang comefrominstrumentation deeprelextendstheseedinvocations by matching similar apis and exchanging their arguments.
neuri u1d45fis a variant of neuriwhere the use of inferred operator rules is disabled.
in other words neuri u1d45fconstructs dnns by eitherusingconcreteoperatorsdeterminedbycollectedrecordsor symbolicoperatorsfromtheoriginal nnsmith andbothmethods share equal probability for being selected.
theneuri u1d456variant disables concrete operators and only uses symbolic operators from automatedinference or nnsmith .
inaddition inrq3wealsocompareourrulesynthesizerwith rosette a solver aided programming system which supports inductiveprogramsynthesis.speci f ically wegiverosette gasthe grammarunderabitvectortheory.thenumberofbitsforthedata and operations is given that the maximum number in records is232 i.e.
int max .next foreachpartialoperatorweletits recordsbetheconstraintsandrunrosettewitha1000 secondtime budget.more speci f ically we onlycomparewith rosetteoverthe inference of shape propagation e.g.
given an partial operator with koutputdimensions bothrosetteand neuriwillrunktimeseach of which trying to search expr u1d458that matches records of u1d45c u1d458.
we did notinferinputconstraintsforrosettesincemultiplematchedpredicatescanbereturnedinonepasswhilerosettedirectlyterminates when the f irstmatchedpredicate isfound.
con f iguration.
we run all experiments on ubuntu .
poweredbya64 threadamdthreadrippercpu 256gbofmemory and tb of pcie ssd.
our approach is evaluated over the upto date frameworks and versions tensorflow v2.
nightly git 5a6fc06bf8 andpytorchv2.
nightly git f7520cb51e .dueto the different tool chain f lavours we compiled tensorflow with gcc .2andgcov whilepytorchiscompiledwithclang anditssource codebasedcoveragetool .to preciselymeasure the test adequacy of compilation for tensorflow we instrument f ilesintensorflow compiler over800klocs andforpytorch non kernel function f iles under pytorch csrc andaten are conservatively instrumented since pytorch s passes are everywhere .
following prior work we by default run fuzzing for four hours and generate models with f ive operators to balance between efficacyanddebuggability figure 6c .fordetectingresultinconsistency ouroracleusesabsoluteerrorof 3andrelativeerrorof .withmodelsgeneratedassmallas f ivenodes wedidnotsee false positives brought bypropagated f loating pointerrors.
.
rq1 evaluatingcoverage overall coverage.
figure6aand6bshow the coverage growth y axis infourhours x axis .amongall model level fuzzers neuri improves the prior sota i.e.
nnsmith by on tensorflow and15 onpytorch.theresultsindicatethat neuricansynthesize more diverse model structures to exercise various compiler passes.
in addition neurialso outperforms the sota operatorlevelfuzzer deeprel by on tensorflow and on pytorch.
though existing operator level fuzzers is known for constructing invocations for thousands of apis neurishows that by only supporting a smaller but essential set of operators similar andevenbettercodecoveragecanbeachievedthroughmodel level generation.despitethemarginalcoverageimprovementoverdeeprel neuri viamodel levelfuzzing canexplorevarioussystembehaviorsthatarenotcoveredbydeeprel.speci f ically .
of thepytorchbugsand35.
oftensorflowbugsfoundby neuri aremanifestedbymodelsofmultipleoperatorswhichcannotbe coveredbysingle operatorfuzzerslikedeeprel.overall infour hoursneuriautomatically covered .
and .
of the compiler code branches in tensorflow and pytorch respectively.
it isworthnotingthatachieving10 of totalbranch coverageis non trivial giventhecomplexityofdlframeworks.asareference fuzzers forlinux i.e.
alsoamillion locproject commonly achievearound10 ofblockcoverage.themissingbranchescan comefrompassesdesignedforotherhardwaretargets e.g.
gpu whereascpuisusedinourexperiments andunusedexperimental compilation pipelines e.g.
mlir .
ablationstudy.
bycomparing nnsmith withneuri u1d45fandneuri u1d456 weshowthatbothsymbolicandconcreteoperatorsareeffectivefor improvingcoverage .
.
fortensorflowand .
.
for pytorch .
by combining both of them together i.e.
neuri neuri u1d45fandneuri u1d456canbefurtherimprovedtocover .
.
morebranchesfortf pt.thoughthisimprovementmightlook marginal we argue that the additionally covered branches are harder to reach ones as we later show that a certain number of bugsareexclusivelycontributedbyinferredrules.table 1shows thatneurihas a slightly lower validity rate .
and runs .
tf .
pt slower than nnsmith .
the validity rate is lower as some inferred rules are partially correct.
the speed ofneuriisofcourseslowerasitsmodelgenerationtacklesover more symbolic operators than nnsmith and even much more concrete operators.however neuristill achieves better coverage asitgeneratestest caseofhigherqualityoverquantity.notethat annon prefectvalidityratedoesnotintroducefalse positives as we identify and immediately discard an invalid test case if it raises exceptions ineagermode.
speed.the time costs for generating evaluating and saving a testcase are shown in table .
it shows that constrain based model generationisfast takinganaverageof88ms tf 69ms pt for creatingandrunningatest case despitethesingle threadnatureof our model generator.
speci f ically the generation phase on average takes53 tf pt ofthetime whichisoveralldominated by constraint solving.
notably the smt solving time is long tailed thep99showsthatfor1 oftest cases thesolvertimedeteriorates byover12.
tf .
pt comparedto the average.
impact ofmodelsize.
neuriandnnsmith bydefault generate models with nodes.
how about other sizes?
figure 6cshows that the model size used in neuriimpacts the coverage on pytorch tensorflow is omitted for clarity but shares similar trends .
for example onlygeneratingone operatormodels i.e.
single api gets worsecoverage sincecompilerpasseslookformultiple operator patterns e.g.
operatorfusion .onthecontrary thebene f itsconvergeswhenmodelsizegetslargerandlarger e.g.
.
coverage differencein node5 .consequently becausesmallermodels areeasiertodiagnose webydefaultuseamodelsizeof5in neuri.
.
rq2 evaluatingruleinference statistics.
table3displays the statistics of apis partial operators and records at different phases.
the collected row indicates that the developer tests i.e.
the instrumented code incorporate 664neuri diversifyingdnn generationviainductive ruleinference esec fse december3 san francisco ca usa time minute coverage branches 36kbest 335kmax .
neuri neurirneurii deeprelnnsmith muffin a tensorflow0 time minute 47kbest 272kmax .
neuri neurirneurii deeprelnnsmith b pytorch0 time minute .
.
.
.
47kbest 272kmax .
node node node node c impact of model size.
figure hourcoverage trend offuzzing.
table numberofvalidtests generated in4 hours.
tensorflow pytorch validity tests validity tests neuri .
.
neuri u1d45694.
.
neuri u1d45f98.
.
nnsmith muffin .
table testing timebreakdown millisecond .
gen. smt eval.
save total tfavgerage p90 p99 percentage ptavgerage p90 p99 percentage out of apis supported by pytorch jit and out of for xla respectively.
apis are not collected due to the lack oftests e.g.
untestedaliased apis orbeing non tensor apis e.g.
image encoder and decoder apis .
by focusing on these around 63k 34k records can be collected for pytorch tensorflow.
after f iltering out unwanted records pt tf of the records remained for pt tf of the apis.
furthermore data augmentation improves the unique records by pt .
tf outofwhich57 pt tf are counter examples.
withinthe1000 secondbudgetperrule neuricaninfer76 pt tf ofrulesatthepartialoperatorleveland91 pt tf ofrulesattheapilevel .toestimatetheusefulnessof rules weuse fuzzing todenotethenumberforapisthatareused duringfuzzing i.e.
neuri u1d456inonenode .toindicatecorrectness fuzzing denotes those in fuzzing that always construct valid usages.
it turns out pt tf apis out of the inferred onescanbe usedand96 both tendtobeused validly showing the overalleffectiveness ofruleinference.
4the data points of input constraints are fewer because some partial operator have an empty set of input constraintsand arenot visualized.table api partial operator record at differentstages.
api partialop.
record pt tf pt tf pt tf collected filtering681 augment.
inference fuzzing fuzzing figure inferencetimeand symbols ofinferred rules4.
table4 numberofinferredshapepropagationrulesin1000s.
inferredtimeout unsat.
1s 10s 100s 1000s neuri rosette scalability.
figure7depicts the distribution of inferred rules in terms of the inference time and symbol size.
it shows of inferred partial operators have less than pt tf symbols indicating the problem size is small overall.
additionally inferring shapepropagationrulesishighlyaffordable i.e.
ofthemcan be solved within 17ms .
however input constraint inference tends tobemorecostly sincepredicatecandidatesarethoroughlyenumerated in algorithm 1while shape propagation terminates when the f irstfeasiblesolutionisfound.meanwhile theinferencetimeof inputconstraintsgrowswith symbolsbecausepartialoperators withmore symbolsincur more expressionsto validate .
table4compares neuriandrosetteininferringshapepropagation for partial operator in pytorch.
it shows that of partial operators are inferred by neuriwithin one second while rosette takes over seconds for of cases.
speci f ically the unsat 665esec fse december3 san francisco ca usa jiaweiliu jinjunpeng yuyao wang andlingming zhang table sizes ofexpressions after pruning.
u1d434 u1d43c none equiv.
rarity both .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
percent.
.
.
.
presents the number of cases where no expression is applicable afterthefullenumeration.rosettehaszero unsat rulesbecause itfails to f inish the enumerationwithin 1000s.
impact of pruning.
table5shows the number of expressions afterbeingprunedbydifferentmethods.forexample foranpartial operatorwithsixsymbols i.e.
u1d434 u1d43c withoutanypruning therearehundredsofbillionsofexpressionstoverify i.e.
.
recall that we have at most operations .
our pruning method i.e.
the both isabletoprunethesearchspacesmallerby .
the ablation study shows that both equivalence andraritycan sufficientlycontributetoshrinkingthesearchspaceevenifapplied independently.interestingly wefoundthatourmethodpruneseven betterpruningratioforfewersymbols e.g.
.
unpruned for one symbol and .
unpruned for six symbols.
this is a good news since from figure 7we see the number of symbols tend to be small i.e.
approximatelyclusteredwithin 1to .
examples.
we found most partial operators have very simple rules being inferred e.g.
pt tf of which only have expressions with at most one symbol.
nevertheless we show some complicatedrepresentativestoindicatethecapabilityof neuri s rule synthesizer.
given an arbitrary input tensor x say its shape is bothx.flatten andtorch.ravel x f lattenthe tensorintoa1 darraywhoseshapeis whichcanbe inferred by us.
for more complicated cases such as x.unfold dim size step the shape propagation of u1d45cdim u1d456dim size stepcan alsobecorrectlylearnt5.therearealsocaseswherepartiallycorrect rules are learnt.consider avg pool3d x ksize pad where both the input and output have f ive dimensions i.e.
u1d456 u1d441 u1d456 u1d436 u1d456 u1d447 u1d456 u1d43b u1d456 u1d44a u1d45c u1d441 u1d45c u1d436 u1d45c u1d447 u1d45c u1d43b u1d45c u1d44a .
while neuricorrectly inferred that u1d45c u1d447 u1d456 u1d447 2pt kt it over f its the h and w dimension with u1d45c u1d43b u1d456 u1d43b kh min ph due to the lack of records where u1d456 u1d43b kh min ph u1d456 u1d43b 2ph kh.however inour4 hourfuzzingthe corresponding partialoperators were used and didnot leadto any invalidmodels.thisisbecausethisincorrectexpressionstillgets agoodchanceofbeingvalid.forexample nnsmith reveals that smt solvers like z3 tend to return boundary models e.g.
ph which makes the two equations equivalent.
there are of courseuninferred operatorrules.for example one partialoperator oftorch.stack takes hundreds of input tensors where neuri cannothandle hundredsoforientedsymbolsin1000s.
.
rq3 bug finding overview.
infourmonths neurihasfound 100newbugs with f ixed and con f irmed .
links to all bug reports in this 5divisiondemonstrated in thisparagraph is f loordivision i.e.
for integers.table overview ofreported bugsinfour months.
symptom .
total con f irmed fixed won tfix ptinconsistency runtimeerror sanitizer error tfinconsistency runtimeerror sanitizer error total work are included in our artifact6.
of these are found during fuzzingand24arebyproducts e.g.
crashesbycounterexamplesin argumentation .
among the pytorch bugs have been labelled withhighpriority constituting ofallhigh prioritybugs forthe entirepytorchissuetrackerinfourmonths.besides one pytorch bug has been tagged with a cve number when there was only one other cve published in pytorch.
pytorch developers say ...the bugs you ve reported are high quality and ... don tlooklikespeciallyfuzzedsetthat simpossibletosee in practice.
they didreveal a few common themes that areeasy to encounter in practice... meanwhile to report bugs responsibly we discontinued bug reporting to tensorflow when none of our f irst reports con f irmed were f ixedinamonth.hence the bugsshouldbe regardedas alower bound for bug f indingefficacy intensorflow.
uniquebugs.
weillustratethepatternsofexclusivepytorchbugs sincewediscontinuedtensorflowbug f inding foundby neuri during fuzzing i.e.
byproducts not included .
among these fuzzing bugs in pytorch bugs .
are only manifested by models with multiple operators these are not able to be detected by prior single api fuzzers .
meanwhile .
of thebugswouldnotbecoveredby nnsmith foritslimitedapisupports7.
for example torch.reciprocal torch.dstack concatenates the only inputand getsthe reciprocal which shouldhavereturned .however aftercompilationthe thirdoutputelementbecomesnon deterministic.thisiscon f irmed tobeamiscompilationbug8 now f ixed wheretheckernel functiongeneratedbypytorchhaserroneouspointeraliasesforinput and output buffers.
this inconsistency bug is neither detectable by single api testersnor nnsmith unsupportedapis .
in addition bugs .
are exclusive to neuri u1d456 i.e.
the shapesandattributesofthebug inducingmodelsarenotdirectly obtainedfromtherecords butbysolvingconstraintsfrominferred rules.
it shows that enabling rule inference though not bringing surprising coverage improvement .
does help f ind more bugs.
forinstance a high priority bugdetectedby neuri9 now f ixed requirestheinputshapeof torch.histogramdd tobespeci f ically fortriggeringacompilerfailure.theinputshape i.e.
comesfromthesolver providedmodelandnoneofthesixcollected records of torch.histogramdd can trigger the bug.
as another 7muffin sharessimilar limitationsas nnsmith in termsof limitedapi supports and isnot directlycomparableherebecause it only supports tensorflow.
666neuri diversifyingdnn generationviainductive ruleinference esec fse december3 san francisco ca usa example the unfold abs modelpattern meansanyoperators and the inabs means it is an in place operation can manifestaresultinconsistencybug10 now f ixed sincethe graph functionalization in the pytorch compiler was not able to identify certainoperatorpatternsthathavememoryoverlapping.speci f ically itisdetectedbyusingasetofsolver providedarguments i.e.
tensor.unfold .
notably both examples here require operators that are unavailable in nnsmith showing that operator diversityfurther powers modeldiversityto detectmore bugs.
furthermore one heap over f low bug is assigned by pytorch with a cve identi f ication number ghsa 44g2 4gc8 due to its security impact.
this bug is induced by searchsorted arr val sorted idx whichaimstobinarysearch valinarr e.g.
an array where sorted idx are the sorted indices of val.
speci f ically boundarychecksfor sorted idx wereabsentintheprevious implementation.
therefore a large enough index if lucky can lead to a segmentation fault terminating the program without further impact.
however a carefully designed index allows attackers toaccessandstealdatafromothermemoryaddressesbesidesthe array range when performingthe binary search.
won t f ix bugs.
threeofourreportsarerejectedordeprioritized.
forexample bothaninconsistencybugin tf.cast andacrash bug intorch.linalg.eigvals were rejected for using nans asinput incurringunde f inedbehaviours.anotherpytorchjitbug wasdeprioritizedbecausedeveloperssuggestedustouseandtest the newcompiler andconsequentlywe did .
related work inrecentyears fuzzing hasbeenextensivelystudiedfortesting dllibrariesanddlcompilers whichcanbemainlycategorizedinto operatorandmodellevels.operator leveltechniques aimtotesteachdlapiinisolation.freefuzz afullyautomated operator levelfuzzerfortestingdllibraries suchastensorflow andpytorch collectsdlapitracesfromsourcessuchasdeveloper tests andmodelzoo andfurther mutatesthe tracedinputstogenerateadditionalvalid invalidtest casesforfuzzingeachoperator.
similarly docter alsoaimstotesteachdloperatorindividually by extracting their input constraints from documentation incurring manual inspectionof the mined rules for30 of the api arguments.nonetheless theinputconstraintsforeachargument are de f ined by docter as a potential set of types and values which cannotmodel f ine grainedshape constraints.additionally recent work has also been improving the oracles of dl system testing via apirelation andgradientchecking .thougheffectivein bug f inding operator level fuzzing hardly uncover bugs induced bymultiple operators together e.g.
bugsindl compilers.
model level fuzzing techniques generate dlmodels with multipleoperators.thepioneercradle directlyrunspre builtdl models programmed in keras and cross check results from various backends.
built on cradle lemon and audee generate models via pre de f ined mutation rules.
muffin performs layer by layer model generation for testing both training and inference.
recently nnsmith annotates each operator with input constraints and shape transformation and generates well formed models following csmith aided by constraint gencog simpli f ies such annotations via adomain speci f iclanguagewhichstillneedsmanualannotation.
while they complement operator level fuzzing the model mutation generation rules are restrictive e.g.
they typically only target naive shape preserving operators or require certain manualannotations leadingtoalimitedsetofoperatorsbeing used.
this work proposes to infer such operator rules and then apply them to generate valid models with all possible operators.
ourwork can cover asmanyoperators asoperator level fuzzing while creatingvalid models with covered operators fully automatically i.e.
a step forward for bridging the gap between operatorandmodel level fuzzingfor dl systems.
more recently there has been concurrent work on directlyleveraginglargelanguagemodels llms tosynthesizepython programs to construct valid dl models.
such techniques aim to implicitly solvethevalidityconstraintsviadirectlylearningfrom valid samples.
compared with llm based model generation despite the technical complexity by explicitly solving the constraints neuriprovides stronger validity guarantee within the covered model space and is more affordable i.e.
100ms per model on cpu .meanwhile llms trainedoverbillionsoflinesofcodes can be used to easily test beyond the model space carefully crafted and exhaustivelyexploredby neuri.
therefore thesetwoapproaches can be further combinedfor maximizedfuzzinginthe future.
lastly programsynthesishasbeenusedbyrelatedareassuchas synthesizinguser facingtensor manipulationprograms .
similar to nnsmith they also require manual operator speci f ications.
this paper applies inductive program synthesis toinfersuchspeci f ications andcanpotentiallyimproveallmethods targeting model generation e.g.
dl system fuzzing tensormanipulationprogramsynthesisandneuralarchitecturesearch .
conclusion wepresent neuri anovelapproachtoautomaticallyinferoperator rules for diversifying model generation in order to test dl systems.
neurigeneratestest casesbycreatingstructurallyvalidmodelsusingadiversesetofoperatorsforexercisingdeepsystembehaviours.
theprimarysourceofthediversitycomesfromourautomatedrule inferenceandconcolicmodelgeneration.theruleinferenceengine inductively and efficiently discoversoperatorrules for generating valid models symbolically.
meanwhile our concolic model generator further uses concrete operator invocations in combination with the symbolic operators to maximize the model diversity.
as a result neuri f inds many high priority and qualitybugs appreciated by dl framework developers.
we show neuriis promising for long term fuzzing high quality test cases can be generated in millisecondsonasinglethreadandnewoperatorscanbeautomatically integrated.
to date neurihas already detected newbugs for pytorchandtensorflow with81 f ixedorcon f irmed.
data availability our artifact isavailable at zenodo andgithub .