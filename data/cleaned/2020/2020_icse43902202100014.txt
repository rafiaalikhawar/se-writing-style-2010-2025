playing planning poker in crowds human computation of software effort estimates mohammed alhamed school of computing science university of glasgow glasgow united kingdom mohammed.alhamed glasgow.ac.uktim storer school of computing science university of glasgow glasgow united kingdom timothy.storer glasgow.ac.uk abstract reliable cost effective effort estimation remains a considerable challenge for software projects.
recent work has demonstrated that the popular planning poker practice can produce reliable estimates when undertaken within a software team of knowledgeable domain experts.
however the process depends on the availability of experts and can be time consuming to perform making it impractical for large scale or open source projects that may curate many thousands of outstanding tasks.
this paper reports on a full study to investigate the feasibility of using crowd workers supplied with limited information about a task to provide comparably accurate estimates using planning poker.
we describe the design of a crowd planning poker cpp process implemented on amazon mechanical turk and the results of a substantial set of trials involving more than crowd workers and diverse software tasks.
our results show that a carefully organised and selected crowd of workers can produce effort estimates that are of similar accuracy to those of a single expert.
i. i ntroduction reliable software task estimation remains a critical factor in the success of software projects.
the most recent edition of the standish group s chaos survey of software industry practitioners to be made publicly available states that around half of all software projects are delivered later than intended .
in older work emam and koru found that around of cancelled projects were over schedule and were over budget.
even worse underestimation of effort may cause developers to rush potentially resulting in unreliable software and increased error proneness .
several authors have argued that reliable software task estimation is just as important for open source software oss projects .
for example koch argues that established oss projects share many of the characteristics of commercial developments that benefit from reliable task estimations such as project roadmaps release schedules and contributor onboarding activities.
separately asundi notes that oss projects are often not developed in isolation from commercial activities.
many organisations will release part or all of their code base as oss or depend on a community maintained oss system.
obtaining reliable estimates for tasks in these projects helps dependents to understand when new features might be available or bugs fixed.the problem as described by more than one study effort estimation is a challenging task in controlled and centrally managed commercial software development houses .
such difficulties originate from an interplay of factors including software novelty rapid changes in development technologies team competences and the need for creativity in software development .
generally the amount of subjectivity and creativity make predicting the final form of the software product difficult and thus complicate software development planning and estimation.
this difficulty is exacerbated in oss.
such projects typically adopt a bazaar development model in which a large number of different contributors may complete tasks at different points in time joining and leave a project as their interests change.
lee et al.
found that a substantial number of oss contributors on github only submitted a single pull request in the lifetime of a project.
this fluidity and churn makes the development of the necessary project stability and expertise for reliable estimates difficult.
for example only .
of the issues reported in apache foundation issue tracker1has been annotated with estimated or actual efforts.
without such information it becomes difficult to plan software development work in these contexts .
describing the wider problem of task triage hooimeijer and weimer note that the number of bug reports for popular projects typically outstrip the time available from domain experts to triage them.
reviewing the state of some popular open source projects illustrates the scale of this challenge.
the linux kernel firefox web browser and jboss projects respectively have and new issues to be investigated.
our motivation software effort estimation see is at the core of software planning yet existing methods depend on the availability of a stable group of project experts and are not well suited to the dimensions of scale of open source projects.
planning poker is an expert based effort estimation method used in agile development methods such as scrum that has become a popular approach to effort estimation for software tasks.
the most recently published state of agile report states that the practice is used by of the software ieee acm 43rd international conference on software engineering icse .
ieee teams surveyed .
several studies in the literature have also reported that planning poker can provide reliable estimates as compared with single expert estimation.
similar to other delphi methods conventional planning poker depends upon the availability of a team of experts to produce estimates in an iterative consensus building process.
running an expert estimation method such as planning poker is therefore costly.
it requires a meeting of a team of relevant experts to be coordinated which limit planning poker scalability and efficiency.
cohn s description of planning poker process suggests that a team should expect to spend between five and ten minutes per task allowing the team to estimate up to ten stories over a one hour session.
applying this guideline to the linux kernel backlog would require a planning poker session of approximately half a year.
even if only a percentage of these issues are prioritised for resolution the dependency on expert availability imposes significant scaling restrictions.
in a review of surowiecki s keynote talk on the wisdom of the crowds at agile grenning wrote i was wondering how wisdom of crowds would relate to people on agile teams estimation and planning.
i was specifically interested in how his research applied to planning poker a practice used throughout the world on agile teams surowiecki s thesis is that large numbers of relatively inexpert workers can perform as well as small groups of experts if the group activity is appropriately structured.
further crowdsourcing platforms such as mechanical turk have significantly eased the task of recruiting large numbers of workers at relatively low cost.
several studies have demonstrated that crowds can perform effectively on a variety of different types of software project task including requirements management software development and testing .
this paper therefore explores grenning s question to determine whether an inexpert crowd either hired as in crowdsourcing markets or volunteered as in oss projects can produce reliable software task effort estimates and address the scalability of expert based planning poker.
since access to oss community is limited we opted to used crowdsourcing market as a similar oss environment.
we hypothesize that by designing a process that applies human computation to an expert estimation method we can achieve effort estimation that is of comparable accuracy to that of small group planning poker but at scale.
proposed method given our focus on applying human computation we address the following research question rq given a software task that required between half a day and two weeks effort are crowdsourced effort estimates ofcomparable accuracy to those of experts?
to address this question we have developed and evaluated crowd planning poker cpp an estimation practice that can be performed by inexpert crowd workers.
we implement the orchestration of worker activity on the mechanical turk platform.
each crowd worker is presented with initial information about a task to be estimated and then asked to supply acategorical effort estimate and a justification.
once sufficient estimates are received the consensus amongst the crowd is calculated.
if consensus has been achieved the process ends.
otherwise a further round of estimation is undertaken with additional information concerning the range of responses and justifications from the previous round provided to the workers.
the iterative process continues until the crowd reaches consensus or a limit on rounds is reached.
cpp also incorporates a mechanism for filtering low quality estimates based on an evaluation of the behaviour of crowd workers and the justifications that they supply for their estimates.
contribution as far as we are aware grenning never followed up on his inquiry and we are unaware of any other attempt in the literature except for our own pilot study .
our work is therefore a continuation of the work reported there to investigate the application of crowdsourcing to software development task estimation using planning poker.
unlike the pilot study the work includes full evaluation of the approach on a diverse range of thirty nine issues comprising both feature requests and bug fixes.
the issues are selected from three different open source projects jboss spring and apache.
in total crowd planning poker rounds were executed and estimates were received.
actual effort for task completion report in the issue repositories ranged from half a day through to two weeks.
the results of the evaluation demonstrates that crowd workers can produce estimates of comparable accuracy to those of experts.
a replication pack is available for inspection containing all the results generated for the experiment2.
this paper is structured as follows.
section ii reviews the present research in the existing literature.
section iii presents the experimental design for investigating the efficacy of cpp.
the results of the experiments are then presented in section iv.
finally section v summarises our conclusions from the study and our reflections on the next steps in the research.
ii.
r elated work planning poker has been the subject of a number of empirical studies in the academic literature.
mol kken stvold et al.
found that using planning poker to combine estimates produced better results in comparison to unstructured or mechanical methods.
more recently mahnic and hovelja found a similar result in a study of teams of students in a software engineering course.
in both cases the studies found that diverse groups of estimators result in more accurate estimates.
gandomani et al.
also conducted an empirical study of planning poker comparing it to wideband delphi and expert estimation and concluded that planning poker performed marginally better.
gandomani et al.
also noted that both estimation methods were useful in reducing underestimation.
several studies have attempted to apply automated statistical or machine learning techniques to software task effort estimation.
such methods are potentially attractive since they 2could substantially reduce the cost of producing estimates compared with expert based approaches.
early approaches such as function point analysis and cocomo developed cost models based on empirical analysis of large data sets of software projects.
however both these approaches require considerable expert judgement in producing estimates.
more recently several attempts have been made to apply machine learning methods .
however a comparison by usman et al.
suggested that expert estimation techniques still outperform machine learning based approaches.
similarly wen et al.
found only two studies that reported that machine learning based approached outperform experts and neither study compared machine learning techniques to planning poker or other delphi like methods.
thus to date a scalable method for producing reliable software task estimates remains elusive.
apart from the pilot study no previous research has investigated the application of human computation to planning poker.
human computation could present a solution to the issue of scale as several studies have demonstrated that the technique can be applied to a wide variety of other software engineering practices including requirements elicitation source code implementation and usability testing .
a particular concern in the application of crowdsourcing however is to ensure the quality of work produced by crowd workers.
numerous methods have been investigated including using a gold standard machine learning classifiers another crowd to assess the quality of the work and associated data such as worker behaviour .
none of these approaches are entirely satisfactory in the context of software task estimation.
in general there isn t an oracle available for generating a gold standard estimate for a software task.
in addition crowd workers may not return to perform repeated estimation tasks making a quality analysis based on previous performance more difficult.
finally employing a further crowd of workers to assess the quality of a submission while feasible increases the cost of an estimate.
alternative approaches seek proxy information concerning the quality of a submission rather than the submission itself.
of particular relevance mcdonnell et al.
and kutlu et al.
proposed that crowd workers be asked to supply a rationale for their decision alongside the supplied value.
this work shows that obtaining rationales improves the quality of judgements without a substantial increase in task time.
in addition dumitrache et al.
showed that rationales are useful for uncovering the reasons for subjective disagreement amongst crowd workers and reaching a subsequent consensus.
separately rzeszotarski and kittur and kazai and zitouni report the analysis of logs of crowd worker interactions with the task user interface in order to model their behaviour and engagement with the task.
in these approaches worker interactions with the computer interface are captured.
a machine learning classifier is trained to predict the quality of a worker s submission based on the captured behaviour.iii.
e xperimental design in this section we present our experimental design to compare the performance of a crowd in producing software task estimates with those produced by project expert estimation.
we explain the metric used to compare accuracy of estimates between experts and crowds describe the software tasks that formed the experimental objects of our study rehearse the in person planning poker practice and describe our crowd planning poker cpp adaptation and our technique for filtering estimates provided by the crowd workers based on the quality of an associated justification and their behaviour.
we also briefly describe the outcome of an initial pilot study that assisted in the design of the cpp process.
a. baseline and ground truth the purpose of the experiment is to determine whether the cpp practice performed by crowd workers can produce estimates comparable to those of experts.
therefore it was necessary to obtain a set of software tasks that had been annotated with both an expert estimated and actual cost effort providing an experimental baseline and ground truth respectively.
three open source projects jboss apache and spring integration were found to satisfy these criteria.
after searching the project s issue tracker systems issues were found to be annotated by an expert time estimate and an actual spent time in person hours.
although these communities have published their issue reporting documentation the method for producing either the estimate or calculating the actual time cost are not stated.
the researchers attempted to contact several members of the communities to determine the exact estimation process.
as explained by those who responded they rely on their experience to predict the issues estimates.
for example one response was we tried to experiment but always fallen back into guts feeling based on experience and only sometimes challenge each other.
me as a tester usually take into account time for a learning about issue and deployment needed b deploy manually first c reproduce issue fix d automate deployment create tests e whatever else is needed f extra time buffer to mitigate unexpected problems g holidays or people time off should be taken into account as well might be covered by f already but you have to think about it this might take a long time depending on difficulty of environment setup ldap kerberos dns whatever service needs to support given product tool moreover the issues history log confirms that costs have been determined by one or more of the issue assignees.
therefore the estimated time cost reported by the development team on the issue is referred to as an expert estimate in this study.
instead of using a literal person hours planning poker is often used with a relative unit for cost estimation such as story 3table i adopted time based categories and their boundaries for a software task effort estimate .
category low middle high one hour half a day a day half a week a week two weeks points .
cohn states that approximate person effort categories are more appropriate because it is often unrealistic to expect person hour precision estimates to be accurate for software tasks.
further cohn argues that teams eventually develop a tacit interpretation of the relationship between the relative categorical estimate and actual person time costs as the completed tasks are compared to the team s available person hour budget over a number of sprints.
in addition estimate categories often adopt a metaphor that suggests increasing uncertainty with estimate magnitude.
for example grenning suggests using a fibonacci sequence to indicate the margin of error between estimate sizes increases with magnitude.
to map this approach to cpp it was necessary to employ categorical units of which the crowd workers were likely to have a shared understanding without prior communication.
therefore person hour costs reported on the issues were translated into approximate person day and person week categories labelled as one hour half a day one day half a week one week two weeks and more than two weeks.
the translation followed the same scheme as in the community issue tracker system jira where a working day is equal to hours and a working week equal to hours.
this enabled comparison between cpp estimates and the person hour costs reported on the issues table i .
to draw boundaries between the scale categories a relative mid point between the two categories was selected.
table i illustrates the low middle and high possible person hour for each category.
before commencing with the experiment the set of issues was filtered to avoid issues that required less than minutes or more than two weeks to complete issues contained less than words in the description issues or had received no comments and so were assumed to not be of interest to the community issues .
some issues were removed due to more than one filter.
after the filtering step there were issues left as candidates.
thirty nine issues were randomly selected from the filtered data set for use in the experiment.
to ensure that a diverse range of effort magnitudes were included issues were first organised into effort categories ranging from one hour through to two weeks as described above.
issues were then selected randomly from these categories for inclusion in the sample.
urls for the selected issues are included in thetable ii comparison of estimation error metrics of the whole population issues filtered set of issues and selected sample .
mean absolute error hours median magnitude of relative errormean magnitude of relative error all filtered sample replication pack.
issues selected were found to comprise a mixture of bugs feature requests and enhancements.
b. accuracy measurements following and usman et al.
the magnitude of relative error mre and its mean mmre and median mdmre are used to express the accuracy of the estimates in the experiments.
mre is calculated as mre je e0 ej where eis the person hour actual effort as recorded on the issue tracker and e0is the person hour effort estimate either as recorded by the expert on the issue tracker or produced by the crowd during the experiment.
mmre and mdmre are used to represent a summary of the error for either crowd or expert estimates in the result tables.
however before proceeding it was necessary to check whether the expert estimates for the selected issues were representative of the whole data set.
the selected sample might represent an artificially low baseline if the estimates they contain are less accurate than those for the population of issues as a whole.
to do this the mean absolute error mmre and mdmre were calculated for the three sets of issues all estimated issues filtered issues random sample as shown in table ii.
for expert estimates the mre was calculated directly from the effort estimates reported in the respective project s issue tracker.
for crowd workers the categorical estimates from individual estimates were translated back to person hours at the mid point for the category as shown in table i. the results show that the average estimation performance by experts in the sample is slightly better than for the whole or filtered set of issues.
this assessment demonstrates that the selected baseline expert estimates in the sample is suitable for use in the study.
c. crowd planning poker workflow planning poker is an expert estimation technique similar to older methods such as delphi and popularly associated with the scrum software development process .
the objective of the method is to achieve consensus amongst a group of experts whilst minimising bias that might arise from individual estimates and ensuring that conflicting opinions are discussed and resolved.
4each member of the development team has a set of cards each labelled with a possible cost estimate.
different units of costs can be used including t shirt size person hours or story points.
cohn proposes using story points as a means of comparing relative user story complexity rather than producing an absolute estimate.
in cohn s approach story point cards are labelled .
a final card labelled with infinity can be used to signal that the task under consideration is too complex to be reliably estimated.
team members start estimating the effort for a task issue individually and pick the card with an appropriate label to make an estimate.
then the team members reveal their cards simultaneously and check for estimation consistency.
if there is no consensus between the estimates then the team members explain their views to each other.
in particular the estimators with the lowest and highest estimates are asked to explain their reasoning.
additional rounds of estimation and discussion are then performed until they reach a consistent estimation about the issue.
cohn recommends that if consensus hasn t been reached after three rounds of estimation then the team should revisit the issue separately.
the adaptation of planning poker to crowdsourcing in our cpp design is shown in the diagram in figure .
first initial information about the issue title and description is presented to the crowd worker on a web based user interface.
additional contextual information can be revealed by clicking on a corresponding button.
this information includes contextual project details such as definitions of ambiguous terms and abbreviations or more information about project specific terms such as the name of a software component that appears in the description.
the crowd worker can also access comments that were posted on the issue.
further information can be searched by the developer using a search dialog provided on the user interface.
next the worker is asked to select a category for their estimate and provide an accompanying justification.
the cpp application collects this information along with a log of the worker s behaviour on the cpp user interface.
a quality evaluation is then performed on the submitted information according to the procedure summarized in section iii e. submissions that are classified as low quality are eliminated from any further use.
once a sufficient number of estimates are received the consensus of the crowd worker estimates is calculated fleiss kappa to determine if another round is required.
if a further round is required the crowd workers are offered a summary of the low median and high estimates from the previous round along with the justifications provided.
the provision of this supplementary information mimics the design of inperson planning poker.
we refer to the summary of previous round as a seed answer that is fed back to the crowd workers in a similar way to the discussion that takes place in in person planning poker.
crowd workers the study subjects were recruited from the amazon mechanical turk platform .
only workers with a self declared experience of at least two years of softwareengineering were permitted to participate.
each estimation session employed a group of between and workers.
a custom web application has been developed to implement the experiment using the mechanical turk api to interact with the market platform.
all data including worker interactions with the user interface the estimate and the justification are captured using this application.
d. pilot study summary having no prior literature to rely on it was decided to implement a small pilot study of cpp prior to proceeding to the full study .
the objective of the pilot study was to test the design of the cpp workflow.
it was also desirable to test the selected size of the crowd to determine if the number of crowd workers was sufficient to provide a reliable estimate and to explore the quality of estimates and justifications provided by workers.
the selected nine issues for the pilot experiment has an average of comments by different developers for each issue.
the flesch kincaid readability scale grade for the description averaged at .
overall the design of the pilot study was similar to the workflow described above.
however several adjustments were made to simplify the conduct of the pilot study.
in particular estimation took place over a fixed number of three rounds for all trials.
the fixed number of trials enabled observation of the consensus forming process.
an early observation from the pilot study was that a significant proportion approximately of the submissions were of low quality in terms of worker engagement with the task and the justification supplied alongside an estimate.
the pilot also confirmed that estimation accuracy was correlated with low submission quality.
to address this issue within the pilot we developed a quality model for worker behaviour and used it to manually filter out low quality estimates.
the model was able to distinguish between spammers non expert workers and domain experts using the quality model.
in particular we discovered that crowd workers who provided good justifications for their estimates wouldn t necessarily spend much time reviewing contextual information.
we speculated that such workers had reviewed similar issues before and therefore had less need for additional information.
table iii shows the results of the pilot study once the low quality estimates had been removed.
the results were encouraging.
estimates were received of which were accepted for use in the estimation process.
as a consequence crowd workers were able to predict the same estimate as of the expert for seven issues out of nine.
crowd workers also gave some insightful justifications for their estimates.
on the basis of the pilot study results we proceeded to further develop the cpp practice by incorporating an automated quality assessment of worker behaviour as summarized in section iii e below.
5software issue information startestimate ef fort and w rite justificationread issue info max roundsreview previous estimatesyes no consensus?
endrecruit crowd workersfig.
.
general model of the crowdsourcing planning poker task table iii summary of pilot results including number of estimates received accepted ambiguous and rejected outcome for each trial and level of agreement achieved within the crowd .
estimatesactual effortexpert estimatecrowd estimate trial all accepted ambiguous rejected category category mre category mre half day half week one week half day half day one week half day one hour half day half week two weeks half week half week half week half week half week half day half week two weeks two weeks half week two weeks one week two weeks two weeks two weeks one weeks total e. an overview of monitoring quality of crowd work on subjective tasks based on the results of the pilot study described above a multi component quality model was developed for the quality of the workers submissions.
the model combines a quality assessment of the workers justification for their estimate with an evaluation of workers behaviour whilst working on the task utilising a log trace of worker interactions with the cpp user interface .
while previous research take all the traced events into consideration our model used selected events from the trace of worker actions with predefined weights as summarised in in table iv.
some of the actions are binary such as whether the worker clicked on the button to reveal addition information or not.
other actions are associated with particular properties such as how long the worker spent in total on the task.
the values for these events weights were based on observation of crowd behaviour during the pilot study .
the justification provided by the crowd worker was evaluated for the presence of four components a task breakdown a time assignment for each working block a general discussion about the task topic and an explanation of the estimation process applied.
all the submissions from the pilot study were processed according to the model.
the pilot submissions were then manually labelled according to the following definitions.
accepted estimates the crowd worker s rationale contains at least two justification components e.g.
a breakdown oftable iv summary of actions used for scoring behaviour event target properties weight type experience field words click extra info btn.
click issue comment btn.
click terms definition btn.
click project info btn.
spend extra info stage second click google search btn.
type justification field words spend browser window min the required work with time specification for each block.
the weighted behaviour of the crowd worker behind the submission is .
ambiguous estimates the crowd worker s rationale contain at least one justification component and there is a relationship to the estimation process e.g.
mentioning the task in place.
the weighted behaviour of the crowd worker behind the submission is between .
rejected estimates the crowd worker s rationale is completely unrelated to the task and issue topic.
the weighted behaviour of the crowd worker behind the submission is .
the labelling of pilot submission was undertaken by a team of four researchers who first completed the task separately before reviewing each submission collectively to reach consensus.
the authors role was limited to explaining labelling instructions and facilitating the labelling meetings.
the model 6crowd input seed answerexperience question justification questionworking system surveillance monitor quality feedback crowd w orkersinfluence controlask recordscrowd output quality classifier ambiguous assignments disengaged assignmentsconsidered assignmentcrowd assignment crowd behaviorjustification experience feedsfeedsanswer feedsfeedsfig.
.
crowd planning poker quality model values and labelling of pilot submissions was then used to train a random forest classifier from the scikit learn library to automatically accept or reject submissions.
testing the classifier resulted in an f score of .
.
figure illustrates how the quality model was used within the overall cpp process.
during estimation the surveillance monitor implemented within the cpp user interface gathers information about the worker behaviour which is then submitted for the task along with the justification.
the quality classifier is then used to filter the received estimates.
accepted estimates were forwarded to a further round of estimation if required whereas ambiguous and rejected estimates were removed by this process.
both accepted and ambiguous estimates were paid during the study since both were considered to have engaged with the assignment presented in mechanical turk.
workers who were classified as ambiguous or disengaged automatically received notifications that their submissions might be rejected giving the worker an opportunity to improve it by making changes.
thus the worker was engaged in an instant feedback loop until reaching their submission reached the required quality level or they decided to withdraw from the task.
in both cases the burden of checking the quality of crowd submissions was eliminated.
however as the classifier is not a accurate rejected crowd workers were given an option to appeal to ensure that crowd workers were treated fairly for the purposes of the research.
iv.
r esults and evaluation thirty trials were conducted one per selected issue as summarised in table v. according to munoz and bangdiwala interpretation of fleiss kappa all trials proceeded until an almost perfect level .
of agreement had been reached amongst the crowd workers measured using fleiss kappa .
the crowd workers reached a consensus within three rounds in all trials with nine trials ending after a single round ten trials ending after two rounds and eleven trials requiring three rounds of estimation.
each round of cpp received between and estimates with an average of estimates received in each round resulting in a total of between and estimates for each trial.
each round was kept open until a minimum of five estimates of sufficient quality had been received.
unlike the earlier pilot study the proportion of rejected estimates was much lower averaging across all trials and reaching intrial and trial .
the reduction in low quality submission is likely due to the automatic quality assessment and feedback process summarized in section iii e. table v also shows a comparison between the final aggregate estimate produced by the crowd for each trial the expert baseline estimate and actual effort ground truth for the issue as reported in the source project s issue tracker.
the category one hour half day etc.
of the final estimate and actual effort are reported in all cases.
further the mre and mean mre mmre are shown for both the crowd and expert estimates relative to the actual effort.
the next two sections reviews these results with respect to the original research questions.
a. crowd performance compared with experts the research question for this work concerns the ability of the crowds of in expert workers to produce estimates of a similar accuracy to those produced by single experts.
the results of the trials conducted are reported in table v. the table reports the total number of rounds for each trial along with the number of accepted and rejected submissions.
the table also shows the actual categorical effort required for the task concerned and the expert s estimate both as reported on the issue tracker and converted to a category as well as the estimate produced by the crowd.
estimates that are in bold indicate the estimate that was closest to the reported effort either expert or crowd or both if the error was equal .
estimates are underlined if the correct category was also estimated.
as can be seen from the table the crowd workers correctly predicted the effort category for of the trials as compared with fourteen of the issues by the expert estimator .
by this comparison expert estimators out perform crowds.
however an alternative analysis would be to consider which prediction crowd or expert was most accurate for each of the issues.
here the crowd workers produced the same estimates as experts in trials crowd workers were more accurate estimates in trials and experts more accurate in trials.
this suggests that the overall estimation performance of crowd and experts were similar.
we further checked this comparison by investigating whether a statistically significant difference existed between the distributions of mres for both crowd and expert produced estimates.
first we applied the shapiro wilk test to both mre distributions to determine if either were normal.
the result of the test for crowds w .
p .645e and experts w .
p .987e indicate that both were nonnormal.
we therefore applied the mann whitney u test since both distributions are assumed to be independent.
applying this test to the two distributions resulted in being unable to reject the null hypothesis w p .
indicating that there is no evidence of a statistically significant difference between the mre distributions and thus that the two effort estimation techniques have similar accuracy.
in a final analysis we compared the mean mre mmre of crowd estimates .
to the mmre of expert estimates 7table v summary of trial results including number of estimates received accepted and rejected outcome for each round and overall trial and level of agreement achieved within the crowd .
ap raand suaabbreviations in the agreement column stands for almost perfect agreement and substantial agreement according to munoz and bangdiwala .
estimatescrowd agreementactual effortexpert estimatecrowd estimate trialnumber of roundall accepted rejected fleiss kappa category category mre category mre apra .
a day half a week half a day apra .
one hour a day a day apra .
one hour one hour a day apra .
half a day half a day a day apra .
one hour half a day one hour apra .
one hour one hour half a day apra .
half a week half a week half a day apra .
one hour a day half a day sua .
half a day half a day half a day apra .
half a day a day a day sua .
one hour half a week half a day apra .
one hour one hour half a day apra .
half a week half a week a day apra .
half a day half a day a day apra .
half a week half a week half a day apra .
one hour half a week one hour apra .
half a day one week half a week apra .
half a day two weeks a day apra .
half a day half a week a day apra .
one hour half a day half a week sua .
half a day half a day half a day apra .
one hour half a day one hour apra .
half a day a day half a day apra .
a day half a week half a week sua .
one hour one hour half a day apra .
a day half a week half a week sua .
half a day half a day half a day sua .
a day half a week half a week sua .
one hour one hour a day apra .
half a day half a day one hour total across all the issues and found that crowd workers error is less than the experts by .
.
this suggests that crowd workers are more likely to under estimate by a category as compared to experts who are more likely to over estimate using person hours .
more research is required to investigate this phenomenon.
overall the results also demonstrate that the cpp process can effectively discriminate between tasks of different orders of magnitude ranging from half a day through to two weeks.
b. cpp efficiency in this section a brief discussion of the cpp running costs.
whether employed in a commercial or open source setting measuring the costs in terms of worker time is necessary to assess scalability.
the total amount of time that crowd workers took to produce an estimate through cpp ranged from and minutes including idle time.
unsurprisingly the number of rounds in a trial had a significant influenced on the time taken with trial requiring just a single round and lastingjust four minutes for example.
these results suggest that producing an estimate from a crowd takes some additional time compared with a planning poker process conducted by a group of experts as described in section i. expert estimation may also be considerably faster when the expert group already has a good understanding of the task to be estimated and can rapidly achieve consensus without the need for discussion.
nevertheless the results demonstrate that crowds can produce estimates relatively quickly and on demand.
in addition the work demonstrates that cpp can estimate multiple tasks in parallel as compared with in person planning poker where only one issue can be considered at a time.
conducting the experiment trials results in an average cost of .
to produce a final estimate for one issue.
again this figure is influenced by the number of rounds taken in a trial .
this cost would appear to compare very favourably with the cost of running a planning poker session within a software team.
assuming a team of five developer with an average hourly salary of excluding other costs can estimate 8tasks in hour then the average cost per estimate would be .
thus the results of the trials demonstrate the potential for a significant cost saving in the context of playing cpp in a commerical software development houses.
c. beyond estimates crowd insights an additional benefit of requesting a rationale from crowd workers when they supply their estimate is that further insight and analysis of the task to be estimated can be obtained.
many of the workers provided useful information about how to approach the task.
such advice and guidance was often very detailed for example on a task concerning the creation of a preview mode for sites using the apache maven site plugin msite a crowd worker wrote this seems like a good case for building at the dom level to implement the changes in parallel for the previews.
if that is in fact the case it would probably take about a day to get a working prototype.
if not...then a day would also probably be enough to know that this simply cannot be done.
the crowd worker provides a suggestion that the resolution of the issue can be done by monitoring a page s dom for changes to create a preview.
they also include a suggestion that a prototype should be created first to determine whether the feature is feasible.
for another issue concerning the implementation of a new indexing mechanism for a jboss workspace the crowd worker provides a detailed breakdown of the work to be done .
how to determine and what is the most efficient and accurate query for nodes and necessary information?
.
initial testing for viability of indexing nodes no lost data consistency etc .
deeper testing incl.
stress testing at higher node counts ensure all threads are deleted etc.
in particular the crowd worker emphasises the importance of different types of testing noting that non functional testing should be treated separately from the design and functional testing of the feature.
these examples were intriguing as we hadn t anticipated that crowd workers would provide insights with significant domain specific knowledge.
these suggestions and explanations have the potential to be of significant assistance to a team during the wider triage process for a software task that occurs alongside estimation.
further work is needed to understand the extent to which this expertise can be leveraged and focused.
d. threats to validity a limitation of the study is employing issues created for open source projects.
this decision was necessary as the experiment required a source of software tasks that could be provided to anonymous crowd workers and that had been annotated with expert estimated and actual work cost.
this meant there was a risk that the crowd workers could access the issue trackers themselves and simply supply the actualreported cost creating a threat to validity of the reliability results.
this risk was mitigated in several ways.
first the issue identifiers were not supplied to the crowd workers and issues were selected from issue trackers that required user registration.
this created an additional step to deter workers.
second workers were asked for a categorical submission rather than an absolute person hour value creating an additional step if the source issue was accessed.
finally workers were encouraged to supply their estimate and it was clear that payment was not contingent on supplying the correct result.
consequently there is no evidence in the behaviour logs that the workers accessed project issue trackers although this may have occurred outside the cpp user interface.
a second external threat to validity concerns the steps taken to filter the selected issues from the open source projects as described in section iii a. our results are therefore limited to assessing the estimation performance of crowd planning poker on issues that met our filtering criteria.
further the analysis in ii showed that the mean and median mre for the expert estimates the experimental baseline were significantly lower in our filtered sample compared with the unfiltered sample.
this suggests that experts perform worse when making estimates on issues that were lower quality according to the filtering critiera.
therefore the baseline for evaluating crowd planning poker for these issues would be lower compared with the filtered issues.
however we are unable to determine within the scope of the current work how each the filters might impact estimate accuracy for crowd workers.
we briefly discuss future experimental work connected with this in the next section.
v. c onclusions this paper has presented the first study of the application of crowdsourcing to planning poker for the production of software task estimates answering grenning s speculation more than a decade ago.
the work demonstrates that crowd workers organised in a crowd planning poker process can reliably produce software task estimates compared to experts and at substantially reduced cost compared to small teams of domain experts.
the crowd workers were able to discriminate between tasks of varying complexity and provide useful insights as to the resolution of the task.
these results therefore present several opportunities for future research directions.
first an observed benefit of cpp compared to in person planning poker is the ability to obtain results on demand rather than needing to wait for a team s regular planning session.
in addition we noted that the crowd workers often provided useful insights as to how the best approach to take to resolve the issue and the sub tasks that this might involve.
therefore cpp could be used by a software team to obtain an initial estimate for a task along with some initial guidance prior to the task being triaged by a team member.
alternatively cpp could be used to quickly flag issues that lack sufficient information for an accurate estimate to be made either within a crowd or by experts.
we plan 9further studies to understand how a software team could incorporate crowd estimates within existing triage workflows.
related to this possibility is the need to assess the extent to which software teams can publish software project tasks to a crowd for estimation when the issues may contain potentially sensitive or private information.
to date our research has focused on estimation of tasks as issues drawn from open source projects that are not affected by this concern but do have large backlogs of unresolved issues.
conversely small software teams working in a commercial setting may be reluctant to publish the full details of an issue to be resolved for fear of releasing commercially sensitive information such as product directions.
as a further work in this direction we intend to investigate the extent to which issues can be obfuscated to address this concern without reducing the reliability of the estimate.
similarly we are investigating the possibility of measuring the specificity of an issue as issues that concern less project specific activities may be less sensitive for a project.
an alternative approach would be to consider other sources of recruitment of the crowd workers.
large software organisations may host multiple projects and employ many hundreds or thousands of developers.
similarly successful oss projects attract similar numbers of volunteers.
rather than employing crowd workers on sites such as mechanical turk such development efforts might leverage the resources available within their own organisations.
such an approach might also further enhance the accuracy of estimates.