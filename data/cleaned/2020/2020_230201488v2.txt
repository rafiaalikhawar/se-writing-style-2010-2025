perfect is the enemy of test oracle ali reza ibrahimzada university of illinois urbana champaign usa alirezai illinois.eduyigit varli middle east technical university turkey yigit.varli metu.edu.tr dilara tekinoglu university of massachusetts amherst usa dtekinoglu umass.edureyhaneh jabbarvand university of illinois urbana champaign usa reyhaneh illinois.edu abstract automation of test oracles is one of the most challenging facets of software testing but remains comparatively less addressed compared to automated test input generation.
test oracles rely on a ground truth that can distinguish between the correct and buggy behavior to determine whether a test fails detects a bug or passes.
what makes the oracle problem challenging and undecidable is the assumption that the ground truth should know the exact expected correct or buggy behavior.
however we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior but how these two might differ.
this paper presents seer a learning based approach that in the absence of test assertions or other types of oracle can determine whether a unit test passes or fails on a given method under test mut .
to build the groundtruth seer jointly embeds unit tests and the implementation of muts into a uni f ied vector space in such a way that the neural representation of tests are similar to that of muts they pass on them but dissimilar to muts they fail on them.
the classi f ier built on top of this vector representation serves as the oracle to generate fail labels when test inputs detect a bug in mut or pass labels otherwise.
our extensive experiments on applying seer to more than 5k unit tests from a diverse set of open source java projects show that the produced oracle is effective in predicting the fail or pass labels achieving an overall accuracy precision recall and f1 measure of and generalizable predicting the labels for the unit test of projects that were not in training or validation set with negligible performance drop and efficient detecting the existence of bugs in only .
milliseconds on average.
moreover by interpreting the neural model and looking at it beyond a closed box solution we con f irm that the oracle is valid i.e.
it predicts the labels through learning relevant features.
ccs concepts software and its engineering software testing and debugging computing methodologies neural networks .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro f it or commercial advantage and that copies bear this notice and the full citation on the f irst page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci f ic permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
software testing test oracle test automation deep learning acm reference format ali reza ibrahimzada yigit varli dilara tekinoglu and reyhaneh jabbarvand.
.
perfect is the enemy of test oracle.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa 12pages.
introduction a unit test similar to the example in figure 1consists of four main components test input e1 e2 and e3 mut invocation obj.sort test output and test oracle assertequals .
given a ground truth that knows the program s expected correct or buggy behavior for given inputs oracles can determine test results i.e.
whether a test passes or fails.
for example the ground truth in the example of figure 1identi f ies the sorted output for given inputs to be e1 e2 e3 .
consequently the assertion oracle checks if the produced output matches the expected one to generate the test result.
construction of the ground truth can be a challenging task.
furthermore the absence of automated ground truths demands humans to decide whether the generated outputs are correct demonstrating a signi f icant bottleneck that inhibits absolute test automation .
test public void testadd exampleobject obj new exampleobject obj.add e1 obj.add e2 obj.add e3 string output obj.sort assert.
assertequals output e1 e2 e3 figure a simple junit test consists of four main components input mut invocation output and assertion to automatically build the ground truth for test oracles traditional and machine learning based techniques rely on existing or derived formal speci f ications assertions program invariants and metamorphic relations for identifying the correct behavior.
some other techniques determine the patterns corresponding to speci f ic types of bugs observed during test execution as an indicator of the buggy behavior .
the commonality between these techniques is their emphasis on identifying the exact correct or buggy behavior to build the groundtruth.
however identifying the exact behavior and output is an 70esec fse november singapore singapore ali reza ibrahimzada yigit varli dilara tekinoglu and reyhaneh jabbarvand correct implementation f x x x x public double example double x double output if x output x x x else output math.
abs x x x return output buggy implementation f x x x x public double example buggy double x double output output math.
abs x x x return output buggy correct c a test public void test1 double o1 example buggy .
assert.
asserttrue msg o1 test public void test2 double o2 example buggy double o3 example buggy assert.
assertequals o2 o3 b inputoutput figure a correct top and buggy bottom implementations of the mathematical function u1d453 u1d465 u1d465 u1d465 u1d465 b visualization of the correct and buggy implementations behavior c junit tests with assertions to assess correctness of the buggy implementation undecidable problem thereby such techniques only partially validate the program.
that is the program can only be validated under a subset of test inputs with known expected outputs or limited properties of the program determined by the invariants or metamorphic relations can be validated.
while partial oracles enhance test automation to some degree they may not guarantee the existence of a successful test driver.
the key insight in our research is that one can still create an accurate test oracle without knowing the explicit relationship between inputs and outputs under correct or buggy behavior.
instead the ground truth will determine how different the test inputs are correlated to outputs under the correct and buggy behavior.
an oracle based on this ground truth eliminates the need for assert statements and identifying the exact expected output in assertions enhancing unit testing to a great extent .
in this paper we present seer1 an automated oracle to predict unit test results.
speci f ically given a pair of u1d461 u1d456 u1d45a u1d456 where u1d461 u1d456 represents a unit test without any assertions and u1d45a u1d456denotes the implementation of mut seer automatically determines whether the test passes or fails on the mut.
to construct the ground truth seer leverages joint embedding to distinguish between the neural representation of correct and buggy muts.
a classi f ier on top of this embedding learns the correlation between inputs and outputs to predict test results.
this paper makes the following contributions a novel domain speci f ic joint embedding of the unit tests and muts which semantically separates muts neural representations based on whether unit tests pass or fail on them.
design of an interpretable dl model that serves as a test oracle to generate passing or failing labels for unit tests without assertions.
the interpretability enables us to go beyond the usage of dl as a closed box technique and verify if the model predicts labels by looking at the relevant tokens in the implementation of muts.
while it is out of the scope of this paper the relevant tokens involved in the model s decision can be further used by developers to localize the detected bugs.
an extensive empirical evaluation on widely used open source java programs demonstrating that seer is effective achieves an overall accuracy precision recall and f1 measure of and generalizable predicting the labels for the unit 1a person who can see what the future holds through supernatural insight.test of projects that were not in training or validation set with negligible performance drop and efficient once trained it detects the existence of bugs in only .
milliseconds on average.
seer s implementation and artifacts are publicly available .
the remainder of this paper is organized as follows.
section illustrates a motivating example.
section 3provides an overview of seer while section 4describes details of the proposed technique.
section 5presents the evaluation results.
the paper concludes with a discussion of the related research and future work.
illustrative example to illustrate the limitations of prior work and explain the intuition behind our research we use two code examples shown in figure 2a.
the code snippet on the top is the correct implementation of a mathematical function that computes the output as u1d465 u1d465 u1d465 .
the buggy version on the other hand computes the output as u1d465 u1d465 u1d465 due to the displacement of a single parenthesis to the end of the assignment instead of after variable x. formally speaking the behavior of a code is a function u1d435 u1d43c u1d442 that maps inputs in u1d43cto corresponding outputs in u1d442.
in our example the mapping functions representing the explicit behavior of correct and buggy implementations are depicted as blue and yellow graphs in figure b. if such a function is known an oracle can use it as a ground truth to distinguish the buggy and correct behavior.
however in reality muts take multiple complex inputs e.g.
arrays and user de f ined objects resulting in n dimensional mappings between inputs and outputs that are infeasible to determine.
therefore test oracles rely on partial ground truths.
no matter how we build the ground truth the oracle s decision for test inputs belonging to should be fail due to different behavior of the correct and buggy implementations in this range.
suppose that we have two junit tests shown in figure c to assess the correctness of the example buggy method.
the groundtruth for identifying the expected output in the assert statement of test1 is based on dynamic invariant detection while the groundtruth for the assert statement in test2 is based on a metamorphic relation u1d453 u1d465 u1d453 u1d465 .
dynamic invariant detection techniques rely on the execution traces of the existing code .
since our mut is buggy the invariant only captures properties of the buggy behavior i.e.
u1d45c u1d462 u1d461 u1d45d u1d462 u1d461 .
by checking the generated invariant in the assertion 71perfect is the enemy of test oracle esec fse november singapore singapore embedding spacepublic double example double x double output if x output x x x else output math.
abs x x x return output public double example buggy double x double output output math.
abs x x x return output test public void test1 double o1 example buggy .
test public void test2 double o2 example buggy double o3 example buggy test public void test3 double o4 example buggy figure the intuition behind the joint embedding of tests and muts with the goal of separating the representation of buggy and correct muts with respect to tests the test passes for u1d465 .
while it should fail to demonstrate the bug.
similarly the metamorphic relation of u1d453 u1d465 u1d453 u1d465 holds for both correct and buggy implementations thereby the assertion wrongly decides the test inputs in the nonoverlapping range as passed.
this example shows that identifying the explicit correct or buggy behavior to build a ground truth which has been the focus of prior work has notable limitations.
in this research instead of realizing how the inputs are explicitly related to outputs under either correct or buggy behavior we aim to learn how inputs are differently correlated to outputs for failing and passing pairs of u1d461 u1d456 u1d45a u1d456 .
here u1d461 u1d456represents a unit test without any assertions and u1d45a u1d456denotes the implementation of the mut.
to that end seer learns the vector representation of both mut and test so that the tests have a similar vector representation to the muts they pass on them but dissimilar vector representation to muts they can reveal their bug i.e.
fail on them.
such joint embedding separates the representation of buggy and correct muts in the n dimensional vector space.
figure 3shows the intuition behind the joint embedding in seer .
here since test inputs in test1 andtest2 can reveal the bug in example buggy they are closer to the correct mut and farther from the buggy mut in the embedding space i.e.
they have a similar vector representation as correct mut but dissimilar from buggy mut.
on the other hand test3 that cannot reveal the bug and should pass on both the buggy and correct muts has the same distance from the correct and buggy muts in the embedding space.
compared to figure b there is no explicit relationship between test inputs and outputs under correct or buggy behavior but the embedding representation of correct and buggy muts are distinguished based on how the output they generate are correlated differently to the inputs of passing or failing tests.
framework overview figure 4provides an overview of seer framework consisting of four major components method extractor dataset augmentor learning module and interpreter .seer requires a high quality and large dataset of u1d461 u1d456 u1d45a u1d456 instances to train the oracle.
given a set of programs and their corresponding test suites the method extractor component builds such a dataset by extracting the implementation of mut invocations in tests through a lightweight static analysis details in .
.
at the next step method extractor creates labeled tuples in the form of angbracketleft u1d461 u1d456 u1d45a u1d456 u1d459 u1d456 angbracketright where u1d461 u1d456represents a unit test u1d45a u1d456denotes the implementation of mut and u1d459 u1d456shows the testresult outcome which could be u1d443 pass or u1d439 fail .
dataset augmentor component then takes the generated dataset and augments it with additional instances to diversify the bugs and account for imbalanced labels details in .
.
once the training dataset is ready seer feeds it to the learning module to train the oracle through two phases.
in the phase training the learning module learns the vector representation of the test u1d461 u1d456and the mut u1d45a u1d456through joint embedding by minimizing the distance among passing tuples angbracketleft u1d461 u1d456 u1d45a u1d456 u1d443 angbracketright while maximizing the distance among failing tuples angbracketleft u1d461 u1d456 u1d45a u1d456 u1d439 angbracketright detail in .
.
as a result the vector representation of a test is similar to the muts it passes on them but dissimilar to the muts it fails on.
after learning the discriminative vector representations learning module leverages transfer learning and trains a classi f ier on top of the embedding network which serves as our test oracle.
to predict the label seer takes a unit test and the program under test as an input and extracts the implementation of invoked mut s .
given the produced pair of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketright the embedding network f irst computes their vector representations and the classi f ier predicts the label indicating whether a test passes on the given mut or fails.
seer goes beyond the use of dl as a closed box approach and interprets the learned model for two purposes to verify if the embedding does its job in separating the representation of muts based on whether a test passes or fails on them and to verify the validity of the model by checking if the code tokens that impacted the oracle s decision are relevant .
.
in the next section we describe the details of seer s components.
4seer this section will f irst explain how to prepare the inputs to the learning module followed by the details about seer s neural architecture dataset curation and model interpretation.
.
method extractor seer slearning module requires labeled pairs of u1d461 u1d456 u1d45a u1d456 to realize the correlation between the inputs provided by unit tests u1d461 u1d456 and outputs produced by mut u1d45a u1d456 .
given a test suite u1d447 u1d4611 u1d4612 ... u1d461 u1d45b consists of u1d45bunit tests and the program under test u1d443 u1d45a1 u1d45a2 ... u1d45a u1d458 consists of u1d458developer written methods2 method extractor extracts u1d45a u1d456 the implementation of a mut directly called in the body of the unit test u1d461 u1d456.
method extraction can be performed statically i.e.
extracting the whole body of the muts regardless of the statements covered by a given test or dynamically i.e.
only considering the executed statements by a test.
while the latter is more intuitive in helping the model focus on the executed lines for predicting test verdicts recent studies have shown that neural models learn the semantics of the code and context more effectively if provided global information .
consequently the model extractor performs a lightweight f low sensitive analysis on a given unit test u1d461 u1d456 identi f ies the methodinvocation that belongs to the program under test and extracts the corresponding method signature and the body.
if a test invokes multiple methods model extractor concatenates the extracted information for the muts in the order of invocation.
in the illustrative example of figure method extractor identi f ies 2we exclude third party apis as their code may not be available.
72esec fse november singapore singapore ali reza ibrahimzada yigit varli dilara tekinoglu and reyhaneh jabbarvand test suits programs datasetdataset augmentor embedding networkoracle classifier embedding analysis attended tokensattention analysis hypothesis verificationmethod extractor learning moduleinterpretertest verdictpp ffp ?
junit test program call graphcall graphtraining testing t c p t c p t c p t c f t c f t c ?
figure overview of the seer framework example buggy method as u1d45a u1d456and extracts the whole text of the method including the method signature and body.
.
learning module the neural architecture of seer is shown in figure .
learning the neural model that serves as an oracle happens in two phases.
in phase training seer learns the vector representation of unit tests and muts in such a way that the representation of buggy and correct muts are different.
at the next step the representation of u1d461 u1d456 u1d45a u1d456 pairs will be fed into a classi f ier helping seer to learn the correlation between inputs provided by u1d461 u1d456and outputs produced by u1d45a u1d456.
since the produced representation of buggy and correct muts are different the oracle ultimately learns how differently the inputs are correlated to outputs under the correct and buggy behavior.
during the phase training seer learns the vector representation of unit tests and muts through joint embedding .
joint embedding also known as multi modal embedding has been widely used to embed heterogeneous data into a uni f ied vector space so that semantically similar concepts across the two modalities reside closer in the embedding space.
for example in computer vision researchers have used convolutional neural network cnn and recurrent neural network rnn to jointly embed images and text into the same vector space for labeling images .
we adopt the concept of joint embedding in our problem to semantically separate the representation of correct and buggy muts concerning the result of tests.
speci f ically we hypothesize that by jointly embedding the unit tests and muts into a uni f ied vector space so that tests have similar vector representations to the muts they pass on them but are different from the muts they fail on them the resulting embedding separates the representation of correct and buggy muts.
the joint embedding of unit test u1d461 u1d456 and implementation of mut u1d45a u1d456 can be formulated as follows u1d45a u1d456 u1d719 u1d437 u1d45a u1d456 u1d43d u1d437 u1d45a u1d456 u1d437 u1d461 u1d456 u1d437 u1d461 u1d456 u1d713 u1d461 u1d456 token embeddingpoolingdm dm dtcos dm dt cos dm dt margin ranking loss concatenated feature representation feed forward softmax with wce loss tm m positional encodingdtdm label pass fail phase 2phase add norm feed forward add norm multi head attentiontransformer encoderfigure overall architecture of seer where u1d719is an embedding function to map u1d45a u1d456into a ddimensional vector space u1d437 u1d713is an embedding function to map u1d461 u1d456into the same vector space u1d437 and u1d43d is a similarity measure to score the matching degrees of u1d437 u1d45a u1d456and u1d437 u1d461 u1d456in order to put u1d45a u1d456 and u1d461 u1d456closer or farther in the embedding space.
seer uses the cosine similarity metric to measure the similarity between the vector representations of u1d461 u1d456and u1d45a u1d456.
a small cosine similarity means two vectors are closer together in d dimensional embedding spaces while a bigger cosine similarity means the vectors point to different angles i.e.
are farther from each other in the embedding space.
to learn and semantically separate the representation of correct and buggy u1d45a u1d456s with respects to the test results seer minimizes the ranking loss as follows l u1d703 summationdisplay.
angbracketleft u1d461 u1d456 u1d45a u1d456 u1d45a u1d456 angbracketright u1d45a u1d44e u1d465 u1d450 u1d45c u1d460 u1d437 u1d461 u1d456 u1d437 u1d45a u1d456 u1d450 u1d45c u1d460 u1d437 u1d461 u1d456 u1d437 u1d45a u1d456 u1d6fc where u1d437 u1d461 u1d456is the vector representation of u1d461 u1d456 u1d437 u1d45a u1d456 is the vector representation of a mut that u1d461 u1d456passes on it and u1d437 u1d45a u1d456 is the vector representation of a mut that u1d461 u1d456fails on it.
the u1d703and u1d6fcrepresent model parameters and a constant margin value respectively.
intuitively by minimizing the margin ranking loss seer learns to minimize the distance between u1d437 u1d461 u1d456and u1d437 u1d45a u1d456 while maximizing the distance between u1d437 u1d461 u1d456and u1d437 u1d45a u1d456 .
after learning the vector representation of tests u1d437 u1d461 u1d456 and muts u1d437 u1d45a u1d456 in phase seer concatenates them to create a single continuous feature representation for the u1d461 u1d456 u1d45a u1d456 pair.
the resulting combined feature vector is fed into a series of fully connected layers in phase training to decode the learned features into a speci f ic target class i.e.
pass or fail.
.
dataset curation training of seer requires a large and high quality dataset i.e.
a dataset consists of passing and failing angbracketleft u1d461 u1d456 u1d450 u1d456 angbracketrightpairs representing a diverse set of bugs across different projects.
to construct the 73perfect is the enemy of test oracle esec fse november singapore singapore dataset we started with the defects4j which is a collection of reproducible bugs in large and widely used java projects.
each bug in this dataset is accompanied by the buggy and f ixed versions of subject programs as well as developer written passing and failing tests.
our rationales to build the dataset based on defects4j are the bugs are isolated and reproducible making it easier for the neural model to learn relevant features and it contains failing developer written tests helping with the generation of a balanced dataset since the automated generation of failure triggering tests using randoop and evosuite is not guaranteed.
a signi f icant limitation of defects4j dataset is the complexity of the bugs i.e.
the majority of bugs involve only one statement in the code.
this issue can degrade the performance of seer in two ways.
first of all the model may treat small changes in the code as noise and may not include them in learning achieving a lower performance .
more importantly a model trained on simple bugs may not generalize to complex more realistic bugs.
inspired by the power of mutation testing in curating high quality training datasets and the fact that higher order mutants are more representative of complex bugs dataset augmentor component of seer takes a passing angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpair as input and mutates the mut repeatedly at different locations to generate higher order mutants.
algorithm 1explains our dataset augmentation process.
the algorithm takes the u1d440 u1d448 u1d447 and u1d45c u1d45f u1d451 u1d452 u1d45f the maximum number of times we mutate a given mut as an input and generates a higherorder mutant u1d43b u1d442 u1d440 .
to that end it f irst identi f ies unique pairs of u1d440 u1d462 u1d461 u1d44e u1d44f u1d459 u1d452 u1d460 angbracketleft u1d45c u1d45d u1d459 u1d45c u1d450 angbracketrightthat demonstrate the locations u1d459 u1d45c u1d450in mut where a mutation operator u1d45c u1d45dcan be applied line .
next it mutates the mut once at a time using these pairs lines .
after each mutation the algorithm checks whether or not the generated mutant is compilable line .
the algorithm continues the mutation using the next u1d45a u1d462 u1d461 u1d44e u1d44f u1d459 u1d452 lines if the mutant is compilable.
otherwise it reverts the mutation and terminates with the produced u1d43b u1d442 u1d440 lines .
mutation continues until the mut is mutated u1d45c u1d45f u1d451 u1d452 u1d45f times or it has been mutated at all the mutable locations.
.
interpretation without interpretation one cannot trust the performance of ml and speci f ically dl models as their learning depends on millions of parameters.
speci f ically such intelligent models can create unrealistically good predictions but based on learning from irrelevant features due to the noise in the dataset or data leakage problem .
to ensure the trustworthiness of seer we validate the following two hypotheses by interpreting the learning module hypothesis .the oracle looks at relevant tokens in the mut to predict a test result and hypothesis .
the embedding network separates buggy and correct muts in the embedding space by distinguishing their vector representations.
we will discuss the details of attention analysis andembedding analysis to investigate the correctness of hypothesis andhypothesis respectively.
.
.
a t tention analysis.
attention mechanism which was initially proposed to overcome the long sequence problem in recurrent neural networks rnns is a method for helping dl models to identify the importance of single features in a feature sequence as they perform their tasks.
attention mechanism serves two purposes in neural architectures f irst it helps with the model s performance.algorithm dataset augmentation algorithm input mut order output a higher order mutant of mut hom 1mutables getuniquemutables u1d440 u1d448 u1d447 2counter order 3hom mut 4foreach mutable mutables do mutant mutate mut mutable .op mutable .loc ifcounter 0then ifiscompilable mutant then hom mutant counter counter else return u1d43b u1d442 u1d440 else break 14return u1d43b u1d442 u1d440 more importantly it has been extensively used to resolve the interpretability of deep neural models.
the initial implementations of the attention mechanism were neural layers between the encoder and decoder components in the neural architecture producing attention weight vectors u1d434 u1d447 u1d4640 ... u1d464 u1d45b as an output.
in the context of neural code analysis u1d464 u1d456is the probability that given a statement with u1d45btokens how important is the token at location u1d456 in the statement when predicting a label.
the higher the attention weight for a feature the more the model attends to it when making a prediction.
seer uses multi head attention also known as self attention to consider the relative importance of a token in the statement when learning.
the output of a self attention layer is an u1d45b u1d45bmatrix u1d446 u1d434 ... where u1d464 u1d456 u1d457represents the how important is the token at location u1d456given a speci f ic token at location u1d457.
figure 6shows the difference between these two attention mechanisms for the buggy statement in our math.absoutput x x x attention vector self attention matrixmath.absoutput x x x .
.
.
.
.
.
.
.
.
figure the importance of self attention in the oracle problem 74esec fse november singapore singapore ali reza ibrahimzada yigit varli dilara tekinoglu and reyhaneh jabbarvand algorithm attention analysis input mut s tokens u1d447 u1d458 u1d45b u1d4500 ... u1d450 u1d45b input mut s statements u1d446 u1d45a u1d461 u1d4600 ... u1d460 u1d45a input u1d446 u1d434 ... input attention threshold u1d458 output attended tokens u1d434 u1d447 u1d458 u1d45b attended statements u1d434 u1d446 u1d45a u1d461 1atkn 2asmt 3foreach u1d45f u1d45c u1d464 u1d446 u1d434do u1d459 u1d45c u1d450 u1d44e u1d459 u1d434 u1d447 u1d458 u1d45b angbracketleft u1d450 u1d456 u1d456 u1d45b u1d451 u1d456 angbracketright u1d456 u1d45b u1d451 u1d456is index of u1d450 u1d456in u1d447 u1d458 u1d45b u1d459 u1d45c u1d450 u1d44e u1d459 u1d434 u1d447 u1d458 u1d45b getmosta t tended row tkn k foreach angbracketleft u1d450 u1d456 u1d456 u1d45b u1d451 u1d456 angbracketright u1d459 u1d45c u1d450 u1d44e u1d459 u1d434 u1d447 u1d458 u1d45b do if u1d434 u1d447 u1d458 u1d45b.
u1d450 u1d45c u1d45b u1d461 u1d44e u1d456 u1d45b u1d460 angbracketleft u1d450 u1d456 u1d456 u1d45b u1d451 u1d456 angbracketright then u1d434 u1d447 u1d458 u1d45b u1d434 u1d447 u1d458 u1d45b u1d450 u1d456 8foreach u1d460 u1d456 u1d446 u1d45a u1d461 do if u1d460 u1d456 u1d434 u1d447 u1d458 u1d45b u1d458then u1d434 u1d446 u1d45a u1d461 u1d434 u1d446 u1d45a u1d461 u1d460 u1d456 illustrative example for the sake of space and readability figure shows only a subset of sa corresponding to the buggy statement .
figure a .
as shown in this f igure self attention is more successful at capturing the importance of closing parenthesis with respect to open ones compared to the traditional attention mechanism.
algorithm 2presents seer s approach for attention analysis i.e.
analyzing the u1d446 u1d434matrix to identify which tokens and statements attended the most in the mut to predict the test result.
for a given pair of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketright algorithm 2takes mut s tokens u1d4500 ... u1d450 u1d45b self attention matrix u1d446 u1d434 ... and attention threshold value u1d458 as an input to identify the set of attended tokens u1d434 u1d447 u1d458 u1d45b and attended statements u1d434 u1d446 u1d45a u1d461 as outputs.
to that end the algorithm traverses u1d446 u1d434matrix row by row lines identi f ies the top u1d458 most attended tokens top u1d458tokens with the highest attention weight value line and merges the attended tokens per each row for the entire matrix along with their corresponding indices lines .
the outcome of merge is the set of attended tokens u1d434 u1d447 u1d458 u1d45b .
when merging considering the indices is speci f ically important as similar tokens at different indices might be attended differently.
in the example of figure while the token appears multiple times in sa at different indices its highest attention is in the last index.
finally the algorithm iterates over mut s statements u1d4600 ... u1d460 u1d45a and determines the statements that u1d458 of their tokens overlap with the attended tokens in u1d434 u1d447 u1d458 u1d45b lines .
such statements indicate the buggy statements in cases that the predicted label for a angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpair is fail .
the intuition here is that since the number of buggy lines is limited according to the generalized pigeonhole principle there is at least one statement with more than u1d458 tokens among attended tokens u1d434 u1d447 u1d458 u1d45b .
.
.
embedding analysis.
seer relies on visualization techniques to validate the separation of the buggy and correct mut representations in the embedding space.
given the high dimensionality of embedding vectors however the f irst step in the embedding analysis is reducing the dimension of representations.
dimensionality reduction algorithms such as pca lda and tsne latent discriminant axisfigure the intuition behind lda dimensionality reduction concentrate on placing dissimilar data points far apart in a lower dimension representation.
among the most popular dimensionality reduction algorithms seer uses linear discriminant analysis lda as it recognizes the class labels and maximizes the separation between classes during the dimensionality reduction.
figure 7shows the intuition behind how lda performs high dimensionality reduction.
the scattered plots represent the distribution of buggy yellow and correct blue muts in high dimensional embedding space and the curves represent that of in the lower dimension.
as demonstrated by figure if the distributions of reduced dimension instances of two classes overlap they are not separated correctly in the higher dimension.
otherwise the instances of buggy and correct classes are separated in the high dimensional embedding space.
evaluation to evaluate effectiveness of seer we investigate the following research questions rq1 effectiveness .
how effective are the proposed techniques in predicting accurate passing or failing test labels?
what type of bugs the proposed oracle can detect and what bugs are harder for the oracle to detect?
rq2 generalization .
to what extent the proposed technique can predict test labels for the java projects it has not been trained on?
rq3 interpretation .
can embedding truly distinguish between the representation of muts for passing and failing test mut pairs?
what features impact the oracle s decision?
rq4 performance .
what are the performance characteristics of the proposed technique?
.
experimental setup we will explain the details of our experimental setup for the sake of reproducibility.
moroever we have made all artifacts of seer publicly available on github .
.
.
dataset table 1shows details about the properties of the dataset used for training validation and testing of seer .
our dataset is built on top of defects4j which consists of 835bugs in17widely used java projects.
in addition to the data collected from defects4j we also augmented the dataset with higher order mutants and automatically generated randoop tests.
the former helps diversify the bugs in the dataset and the latter generates tests that either pass or fail on the newly added bugs.
finally we 75perfect is the enemy of test oracle esec fse november singapore singapore table properties of the dataset.
projects bugs mutantsdataset tests pass fail contribution defects4j higher order mutants developer tests randoop tests developer tests randoop tests pass fail pass fail pass fail pass fail compress .
lang .
chart .
math .
codec .
closure .
jacksondatabind .
time .
jsoup .
cli .
csv .
jacksoncore .
gson .
jxpath .
mockito .
jacksonxml .
collections .
total removed all the assert statements from the developer written and automatically generated tests to avoid bias in learning from them.
as shown in table the f inal augmented dataset consists of 385passing pairs and 228failing pairs of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketright making a total of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs in the dataset.
for phase training we construct angbracketleft u1d461 u1d456 u1d45a u1d456 u1d45a u1d456 angbracketrighttuples by merging passing and failing angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs for common tests.
that is for a given test u1d461 u1d456 we f ind all the u1d45apassing pairs and u1d45bfailing pairs of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketright.
if u1d45aand u1d45bare non zero we get total u1d45a u1d45btuples of angbracketleft u1d461 u1d456 u1d45a u1d456 u1d45a u1d456 angbracketright.
this provides us with 759tuples of angbracketleft u1d461 u1d456 u1d45a u1d456 u1d45a u1d456 angbracketrightfor phase training divided into90 training validation and testing instances.
for phase training we similarly divided the original dataset with instances represented by table .
.
.
learning module configuration we implemented seer s learning module using pytorch open source library.
multiple factors can affect the neural models learning process and f inal performance.
for the loss function which determines how well the algorithm approaches learning from the training data we used margin ranking loss mrl in phase training and weighted crossentropy loss wcel in phase training.
mrl has been shown to outperform cross entropy loss in learning the embeddings and putting data instances of the same target class closer to each other than instances from other classes .
since our embedding goal is similar i.e.
to put passing pairs of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightcloser to each other compared to failing ones mrl was a reasonable loss function for learning the mut and test embeddings.
for phase training we chose weighted cross entropy loss rather than cross entropy loss which is commonly used in classi f ication problems since our dataset has more failing pairs of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketright compared to passing pairs.
to enhance the performance we utilize adamw optimizer which has been shown to outperform adam optimizer to update the network weights and minimize this loss function iteratively.the other factors that affect the model s performance are hyperparameters and over f itting.
we followed a guided hyperparameter tuning to f ind a con f iguration for the model that results in the best performance on the validation data.
one of the most important hyperparameters is the learning rate which controls how much to change the model in response to the estimated error each time the model updates weights.
choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck whereas a larger value may result in an unstable training process.
the learning rate of seer slearning module for phase and phase training are .
u1d452 4and1.
u1d452 respectively.
the difference in learning rates is because phase learning is incremental compared to phase and a similar learning rate results in large i.e.
nan loss function values.
furthermore we used fold crossvalidation to avoid over f itting and implemented early stopping criteria to terminate the training.
that is we repeated the training validation for 10times on different training and validation sets and chose the model that achieved the best performance.
to automatically terminate the learning our patience level was 5epochs i.e.
if the validation loss of the model did not improve in 5consecutive epochs we assumed that learning had reached an optimum level.
.
rq1 effectiveness for this research question we divided 613pairs of angbracketleft u1d447 u1d436 u1d439 angbracketright instances in our dataset into training validation and testing instances.
to that end we downsampled such instances for each project by and used the remaining if possible.
the only exception was the collections project which we included its few instances only in the training set.
we select accuracy precision recall and f1 score as metrics to measure the effectiveness of seer in predicting correct labels.
table 2shows the result for this experiment under seer with embedding columns.
these results are obtained through a fold cross validation i.e.
downsampling repeated 10times.
76esec fse november singapore singapore ali reza ibrahimzada yigit varli dilara tekinoglu and reyhaneh jabbarvand table effectiveness and generalization of seer in predicting test labels.
tp fp tn and fn stands for true positive false positive true negative and false negative respectively.
subjectsseer with embedding pass tp fn fail tn fp compress .
.
.
.
lang .
.
.
.
chart .
.
.
.
math .
.
.
.
codec .
.
.
.
closure .
.
.
.
jacksondatabind .
.
time jsoup .
.
cli .
.
csv .
.
.
.
jacksoncore .
.
.
.
gson .
.
jxpath mockito jacksonxml collections n a n a n a n a total .
.
.
.
each row in table 2shows one of our subject projects and the percentage of instances they have correctly predicted for different versions of seer .
these results con f irm the original implementation of seer illustrated in figure 5can effectively predict passing and failing labels for the test suite of each subject program achieving accuracy precision recall and90 f1 score .
despite an overall good performance seer did not perform well on some projects those marked by asterisks in table2.
our investigation showed that due to the low contribution of these projects to the dataset the test data instances from them were either none e.g.
collections project or very few.
consequently the effect size of classi f ication was very large.
false negatives are not big issues in our proposed technique asseer is interpretable and developers can quickly check the attended tokens to verify the false negative.
to understand the reasons for false positives we compared the true negative and false positive instances from the following perspectives test type.
the unit tests in our dataset are either developer written tests or automatically generated by randoop.
majority of u1d461 u1d456s for true negative instances belonged to randoop.
however for false positives half of the u1d461 u1d456s are developer written tests while the other half are randoop tests.
as a result there is no signi f icant correlation between the false positive instances and test type.
test tokens.
the average number of test tokens for false positive instances is 74compared to 85for true negatives which shows thatseer performs better when tests are longer.
we believe that this is potentially because the representation of longer tests are unique compared to shorter tests making it easier for the model to predict a correct label for them.
mut tokens.
the average number of mut tokens for false positive instances is 89compared to 131for true negatives which shows that seer performs better when mut s implementation has more tokens and statements.
similar to our argument about test tokens short mut sequences carry less semantic information making it harder for the model to predict test results.
bug type.
seer correctly predicts the test results for of the higher order mutants.
this ratio for the real bugs from the defects4j dataset is .
given that we found no signi f icant correlation between the number of buggy lines in false positive and true negative instances we believe this happens because real bugs are unique.
that is while higher order mutation injects bugs at different locations and considers different combinations of mutation operators the operators are limited making it easier forseer to learn the bugs that involve those operators.
we argue that this is not a limitation of seer but the dataset which can be resolved by including more real bugs in the training dataset.
.
rq2 generalization in the previous research question we showed that seer can effectively serve as an oracle on the unseen angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs from the projects that were in the training dataset.
in this research question we go one step forward to investigate how seer generalizes to predict test labels for angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs where u1d45a u1d456belongs to a project that was not in the training data.
to that end we computed the contribution of each project column tests in table and divided the dataset into high contribution projects with contribution values greater than and low contribution projects.
we used all the instances of the projects in the high contribution dataset for training and validation.
then we tested the trained model on the projects in the low contribution dataset.
for this research question we only consider the precision and recall values to evaluate the performance of seer as the low contribution dataset is highly imbalanced passing angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs are .
more than failing pairs .
compared to the original precision and recall values computed in rq1 and93 the value of seer s performance metrics on unseen projects are and .
given that unseen projects have different statistical distributions compared to the projects used for training i.e.
different tokens and hence vocabularies the performance drop is expected due to the out of distribution ood problem .
our further investigation of the misclassi f ied instances con f irmed that the model s performance attention threshold discovered buggy statement figure the percentage of attended buggy statements with respect to attention threshold 77perfect is the enemy of test oracle esec fse november singapore singapore public static string encodebase64string byte binarydata return stringutils.newstringutf8 encodebase64 binarydata true stringpublic static encodebase64string byte return stringutils.newstringutf8 binarydata true binarydata string true binarydata figure visualization of the self attention for the method encodebase64string in codec project demonstrating that seer has paid the highest attention to the buggy token i.e.
true was better on unseen projects whose vocabularies had a higher overlap with the vocabularies of the projects used for training seer compared to that of for projects with less overlap in vocabularies.
these results demonstrate that seer can achieve comparable performance on unseen projects whose vocabularies overlap with the projects for training the oracle .
.
rq3 interpretation recall that the goal of seer s interpretation is two fold.
first we interpret the embedding network to verify if it has correctly learned to separate the representation of correct and buggy muts.
more importantly we interpret the oracle to identify which features i.e.
tokens in the mut and which statements were mostly attended when predicting a label.
from this information we can determine whether the attended features are relevant to the decision and whether the model s performance is valid.
.
.
a t tention analysis.
given a threshold number u1d458 seer s attention analysis algorithm algorithm produces a set of top u1d458 attended tokens and attended statements in the mut.
to con f irm that seer has attended to relevant tokens for predicting labels we measured the percentage of the buggy statements that are among attended statements in the mut.
speci f ically we computed this metric for true negative test instances i.e.
angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs for which theseer correctly identi f ied to fail.
figure 8shows the percentage of buggy statements that were among the attended statements and how this percentage changes in response to the change of threshold value.
these results demonstrate that seer has indeed attended to relevant tokens to predict test labels and even with a small threshold value of it can correctly identify of the buggy statements in the subject muts.
one interesting observation here is that increasing the threshold may not result in better bug localization.
for example increasing the threshold to public boolean equals object obj if this obj return true if obj null getclass !
obj.getclass return true ziparchiveentry other ziparchiveentry obj if name null if other.name !
null return false else if !name.equals other.name return false return false boolean if obj return true return true other return false return falsereturn falseif boolean if obj return true return true other return false return false return falseiffigure visualization of the self attention for the method equals demonstrating that seer has attended to buggy tokens at multiple locations or50 results in non buggy statements being among the attended statements decreasing the contribution of buggy statements among attended statements.
we also manually investigated the heatmap visualization of the self attention matrix for the failing angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs to qualitatively con f irm if seer has considered relevant tokens to predict labels.
figure 9illustrates such case where the buggy mut is encodebase64string of the codec project.
as shown in figure the bug is due to feeding an incorrect argument to encodebase64 method i.e.
true instead of false .
by looking at the heatmap visualization of the self attention matrix we can see that seer has paid the most attention to the buggy token when predicting the fail label for this test instance for the sake of space and readability we have merged some of the tokens and adjusted the weights in the heatmap visualization .
as another example where the bug is more complex and involves multiple tokens or statements consider the buggy mut and its corresponding heatmap3visualization of self attention in figure .
in this example the return values of the two highlighted return statements are incorrect.
looking at the 3the tokens are merged in this heatmap and only the most attended tokens are labeled.
78esec fse november singapore singapore ali reza ibrahimzada yigit varli dilara tekinoglu and reyhaneh jabbarvand linear discriminant axisdistribution .
.
.
buggy mut correct mut figure linear discriminant analysis lda results.
the yellow graph represents the distribution of buggy muts and the dashed blue graph represents the distribution of correct muts in the reduced dimension heatmap we can see that the tokens of these two statements are among the most attended tokens to predict the fail label.
.
.
embedding analysis.
figure 11shows the result of embedding analysis as discussed in section .
.
.
the blue and yellow distributions show the distinction between the embeddings of correct and buggy muts respectively after lda dimensionality reduction.
as demonstrated by this f igure the distribution of correct and buggy muts are almost distinct in the low dimension space which con f irms seer s ability to semantically distinguish the representation of buggy and correct muts in the embedding space.
there are some overlapping instances near u1d465 0between the correct and buggy muts distributions.
such overlap indicates that the embeddings of a few buggy and correct muts are close to each other in the embedding space with 200dimensions.
by manually investigating those instances we realized that they belong to angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs that seer failed to predict a correct label.
.
rq4 performance to answer this research question we evaluated the time required for phase and phase training as well as the time for testing the oracle.
we ran the experiments on a tesla t4 gpu with 16gb gddr6 memory.
for a batch size of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightpairs a single epoch took 447and1 745seconds on average for training phase and phase respectively.
with the patience level of f ive epochs for the early termination criteria phase and phase training took and30epochs to complete respectively resulting in a total of hours of training.
given that the seer is generalizable the one time training of the model is reasonable.
after training it takes seer only .5milliseconds on average to predict the passing or failing label for a given pair of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketright.
related work state of the art test oracle automation techniques can be divided into three main categories implicit speci f ied and derived oracles.
implicit oracle relies on some implicit knowledge to identify whether a test passes or fails.
examples of such implicit knowledge are bufferover f low almost always yields an error excessive cpu usage is a likely indicator of server disruptions and unnecessary battery usage is evidence of energy defects in mobile apps.
while quite effective and automated implicit oracles can only determine the presence of limited categories of bugs.
speci f ied oracle determines the expected output of test execution and compares it with the actual output to decide whether a test passes or fails.
to identify the expected output these oracles require existence of formal speci f ications or contracts pre and post conditions for the system under test.
the performance and usability of such oracle highly depend on the availability completeness and quality of speci f ications.
however for many ever changing software systems speci f ications and contracts either do not exist or fall out of date.
even if automated techniques generate speci f ications they are usually quite abstract and inferring concrete test outputs from them is not guaranteed or is imprecise .
state of the art ml enabled techniques alleviate such limitations by predicting meaningful speci f ications or assert statements .
compared to seer these techniques evaluate a limited set of properties related to program behavior only at a certain point the assertions .
derived oracle decides the passing or failure of a test by distinguishing the system s correct from incorrect behavior rather than knowing the exact output.
the correct and incorrect behavior can be inferred from some meta data such as execution logs provided as properties of the intended functionality metamorphic relations or checked against other versions of the software .
derived oracles are pragmatic but are generally incomplete i.e.
can only identify test outputs for a subset of inputs.
seer while considered as a derived oracle alleviates this problem through a domainspeci f ic embedding i.e.
semantically separating the buggy and correct code in the embedding space with respect to test results.
consequently the neural model that serves as the oracle considers that general knowledge to predict a passing or failing verdicts.
concluding remarks test oracle automation has been one of the most challenging problems in the software engineering community yet it has received less attention compared to test input generation.
this paper proposed seer a novel dl enabled technique to move one step forward in advancing automated test oracle constructions.
seer predicts a passing or failing verdict for a given pair of angbracketleft u1d461 u1d456 u1d45a u1d456 angbracketrightby learning the semantic correlation between inputs and outputs from a highquality and diverse dataset.
our experimental results show that the learned oracle is accurate and efficient in predicting test results and generalizable to the projects it has not seen during training.
currently we are considering several directions for future work.
based on the promising results of our produced domain speci f ic representations for code and tests we will explore its application in other software analysis tasks such as vulnerability detection bug localization and program repair.
also we are planning to expand seer to system tests.
system tests are more complex and bigger than unit tests which may entail changing the seer s architecture to graph neural networks gnn to better capture the code semantics and representations.
79perfect is the enemy of test oracle esec fse november singapore singapore