quickly generating diverse valid test inputs with reinforcement learning sameer reddy university of california berkeley usa sameer.reddy berkeley.educaroline lemieux university of california berkeley usa clemieux cs.berkeley.edu rohan padhye university of california berkeley usa rohanpadhye cs.berkeley.edukoushik sen university of california berkeley usa ksen cs.berkeley.edu abstract property based testing is a popular approach for validating the logic of a program.
an effective property based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver.
however when the test driver requires strict validity constraints on the inputs completely random input generation fails to generate enough valid inputs.
existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and or test driver.
however collecting such information reduces the speed at which tests can be executed.
in this paper we propose and study a blackbox approach for generating valid test inputs.
we first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs.
this formalization highlights the role of a guide which governs the space of choices within a random input generator.
we then propose a solution based on reinforcement learning rl using a tabular on policy rl approach to guide the generator.
we evaluate this approach rlcheck against pure random input generation as well as a state of the art greybox evolutionary algorithm on four real world benchmarks.
we find that in the same time budget rlcheck generates an order of magnitude more diverse valid inputs than the baselines.
acm reference format sameer reddy caroline lemieux rohan padhye and koushik sen. .
quickly generating diverse valid test inputs with reinforcement learning.
in42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa 12pages.
introduction property based testing is a powerful method for testing programs expecting highly structured inputs.
popularized by quickcheck the method has grown thanks to implementations in many different languages including prominent languages such as python javascript and java .
property based testing allows users to specify a test as x x p x q x wherexis permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
icse may seoul republic of korea copyright held by the owner author s .
acm isbn .
domain of inputs and p qare arbitrary predicates.
the preconditionp x is some sort of validity constraint onx.
a property testing tool quickly generates many inputs x xand runs them through a test driver.
if the tool generates an xfor which p x holds but q x does not hold then the test fails.
using a property based testing tool requires two main steps.
first the user needs to write a parameterized test driver i.e.
the programmatic representation of p x q x .
second the user needs to specify the generator forx x. a generator forxis a nondeterministic program returning inputs x x. testing frameworks typically provide generators for standard types e.g.
primitive types or a standard collections of primitives .
if xis a user defined type the user may need to write their own generator.
for property based testing to be effective the generator must produce a diverse set of inputs x x satisfying the validity constraint p x .
this is a central conflict in property based testing.
on the one hand a simple generator is easier for the user to write but not necessarily effective.
on the other hand a generator that produces diverse valid inputs is good for testing but very tedious to write.
further generators specialized to a particular validity function p x cannot be reused to test other properties on xwith different validity constraints say p x .
we thus want to solve the following problem given a generator gof inputs x xand a validity function p x automatically guide the generator to produce a variety of inputs xsatisfying p x .
one way to solve this problem is to do whitebox analysis of the generator and or the implementations of p x andq x .
a constraint solver can be used to generate inputs x xthat are guaranteed to satisfy p x which also exercise different code paths within the implementation of q x .
another set of approaches adopted in recent greybox fuzzing inspired work is to collect code coverage during test execution.
this information can be used in an evolutionary algorithm that can generate inputs that are likely satisfy p x while optimizing to increase code coverage through q x .
both sets of approaches require instrumenting the program under test thus they are limited in terms of performance.
purely whitebox approaches have trouble scaling to complex validity functions due to the path explosion problem and even greybox fuzzing techniques require instrumentation that can lead to slowdowns thereby reducing the rate at which tests can be executed.
this is in conflict with the quickcheck approach in which properties can be quickly validated without instrumenting the test ieee acm 42nd international conference on software engineering icse program.
in this paper we address this gap by investigating a blackbox approach for guiding a generator gto produce a large number of diverse valid inputs in a very short amount of time.
in this paper we first formalize the problem of guiding the random choices made by a generator for effective property testing as the diversifying guidance problem.
second we notice that the diversifying guidance problem is similar to problems solved by reinforcement learning given a sequence of prior choices state what is the next choice action that the generator should make in order to maximize the probability of generating a new xsatisfying p x get high reward ?
we thus explore whether reinforcement learning can solve the diversifying guidance problem.
we propose an on policy table based approach which adapts its choices on the fly during testing time.
we call our technique rlcheck .
we implement rlcheck in java and evaluate it to state of the art approaches zest and quickcheck.
rlcheck s implementation is open source and available at rlcheck.
we compare the techniques on four real world benchmarks used in the original evaluation of zest two benchmarks that operate on schema compliant xml inputs and two that operate on valid javascript programs.
we find that rlcheck generates .
more diverse valid inputs than state of the art within the same tight time budget.
all methods are competitive in terms of valid branch coverage but the simple rl algorithm we explore in rlcheck may be biased to certain parts of the valid input space.
we also find that a basic addition of greybox feedback to rlcheck does not produce improvements that are worth the instrumentation slowdown.
in summary we make the following contributions we formalize the problem of making a generator produce many valid inputs as the diversifying guidance problem.
we propose a reinforcement learning approach to solve the diversifying guidance problem named rlcheck .
we evaluate rlcheck against the state of the art zest tool on its valid test input generation ability.
a motivating example property based testing tools allow users to quickly test a program by running it with many inputs.
originally introduced for haskell property based testing has since been ported to many other programming languages .
using a property based testing tool involves two main tasks.
first the user must write a parameterized test driver which takes input xof some typexand runs test code checking a property p x q x .
we say xisvalid if it satisfies p x .
for example in figure the test driver test insert line is given a binary tree tree and an integer to add as input.
if tree is a binary search tree line the driver inserts to add into tree line and asserts that tree is still a binary search tree after the insert line .
the assume at line 16terminates the test silently if tree is not a binary search tree.
the assert at line 18is violated if treeis not a binary search tree after the insertion.
thus the test driver implements p x q x for the validity constraint p x xis a binary search tree and the post condition q x after inserting to add xis a binary search tree by raising an assertion failure when p x q x is falsified.
in this example the predicate q x is specified explicitly via assertions.
q x can also be implicitly1from generators import generate int 3def generate tree depth value random.select tree binarytree value ifdepth max depth and random.select tree.left generate tree depth ifdepth max depth and random.select tree.right generate tree depth return tree given tree generate tree to add generate int def test insert tree to add assume is bst tree bst insert tree to add assert is bst tree figure pseudocode for a property based test.
generate tree generates a random binary tree and test insert tests whether inserting a given integer into a given tree preserves the binary search tree property.
random.select d returns a random value from d. defined e.g.
q x may be the property that the program does not crash with a segfault when executed on x. second the user must specify how to generate random inputs for the test driver.
this must be done by writing or specifying a generator a non deterministic function returns a random input of a given type in each of its executions.
property based testing frameworks typically provide generators for basic types such as primitive types and predefined containers of primitives e.g.
generate int in figure .
if the test function takes a user defined data structure such as the tree in figure line the user writes their own generator.
for many types writing a basic generator is fairly straightforward.
in figure generate tree generates a random binary tree by choosing a value for the root node line choosing whether or not to add a left child line and recursively calling generate tree line and choosing whether or not to add a right child line and recursively calling generate tree line .
we have deliberately kept this generator simple in order to have a small motivating example.
the user can now run test insert on many different trees to try and validate p x q x the assume in line 16effectively filters out invalid non bst inputs.
unfortunately this rejection sampling is not an effective strategy if p x is too strict.
if the generator has no knowledge of p x it will first of all very rarely generate valid inputs.
so in a fixed time budget very few valid inputs will be generated.
the second issue is that the generator may not generate very diverse valid inputs.
that is the only valid inputs the generator has a non negligible probability of generating may be very small valid inputs these will not exercise a variety of behaviors in the code under test.
for example out of generated binary trees the generator in figure 1only generates binary search trees of size and only one binary search tree of size and respectively.
overall the generator has very low probability 1411of generating complex valid inputs which greatly decreases the efficacy of the property based test.
fundamentally the input generated by a generator is governed by the choices taken at various choice points.
for example in figure at line the generator makes a choice of which integer value to put in the current node and it chooses to make a left or right child at lines 7and10 respectively.
depending on the prior sequence of choices taken by the generator only a subset of the possible choices at a particular choice point may result in a valid input.
for example ifp x is the binary search tree invariant when generating the right child of a node with value n the only values for the child node that can result in a valid bst are those greater than n. while narrowing the choice space in this manner is straightforward for bsts manually encoding these restrictions is tedious and errorprone for complex real world validity functions.
overall we see that the problem of guiding gto produce many valid inputs boils down to the problem of narrowing the choice space at each choice point in the generator.
we call this the diversifying guidance problem.
we formalize this problem in section 3and propose a solution based on reinforcement learning in section .
problem definition in property based testing a generator gis a non deterministic program returning elements of a given space x. for example in figure xis the set of binary trees of depth up to max depth with nodes having integer values between inclusive.
in particular a generator g s non determinism is entirely controlled by the values at returned at different choice points in the generator.
a choice point pis a tuple l c where l lis a program location and c c is a finite domain of choices.
for example there are three choice points in the generator in figure line the choice of node value line whether to generate a left child and line whether to generate a right child.
during execution each time the generator reaches a choice point l c it makes a choice c c. every execution of the generator and thus every value produced by the generator corresponds to a sequence of choices made at these choice points say c1 c2 .
.
.
cn.
for example the execution through generate bst in figure which produces the tree corresponds to the following sequence of choices c1 c2 .
.
.c9 choice index choice taken choice point c1 line c2 true line c3 line c4 false line c5 false line c6 true line c7 line c8 false line c9 false line as the generator executes each time it reaches a choice point p l c it will have already made some choices c1 c2 .
.
.ck.
traditional quickcheck generators like the one in figure will simply choose a random c cat choice point pregardless of the prefix of choices c1 c2 .
.
.ck.
going back to our running example suppose the generator has reached the choice point choosing the value of the left child of i.e.
choosing what to put in the ?
?
that is the generator has made the choices and must now choose a value from at the choice point in line .
the generator is equally likely to pick any number from this range.
since only of the numbers from are smaller than it has at most an chance of producing a valid bst.
to increase the probability of generating valid inputs the choice at this point should be made not randomly but according to a guide.
in particular according to a guide which restricts the choice space to only those choices which will result in a binary search tree.
first we formalize the concept making choices according to a guide.
definition .
following a guide .we say that a generator g follows a guide c p n cif during its tthexecution given a sequence of past choices c1 c2 .
.
.
ck and the current choice point p l c the generator gmakes the choice p t .
suppose we have a validity function x true false which maps elements output by the generator to their validity.
for example is bst is a validity function for the generator in figure .
thevalidity guidance problem is the problem of finding a guide that leads the generator to produce valid elements of x definition .
validity guidance problem .letgbe a generator producing elements in space x. let x true false be a validity function.
the validity guidance problem is the problem of creating a guide such that ifgfollows then x true for any x xgenerated by g. note that a solution to the validity guide problem is not necessarily useful for testing.
in particular the guide could simply hard code a particular sequence of choices through the generator which results in a valid element x x. instead we want to generate valid inputs with diverse characteristics.
for example we may want to generate unique valid inputs or valid inputs of different lengths or valid inputs that exercise different execution paths in the test program.
we use the notation x to denote an input s characteristic of interest such as identity length or code coverage.
definition .
diversifying guidance problem .letgbe a generator producing elements in space x. let x true false be a validity function and be a characteristic function.
the diversifying guidance problem is the problem of creating a guide such that ifgfollows andxt x is the set of inputs generated by gafter texecutions x x xt x is maximized.
if is the identity function then a solution to the diversifying guidance problem is a guide which maximizes the number of unique valid inputs generated.
method in this section we describe our proposed solution to diversifying guidance problem.
in particular our proposed guide uses reinforcement learning to makes its choices.
we begin with a basic background on monte carlo control the table based reinforcement learning method used in this paper.
we then describe how it can be used to solve the diversifying guidance problem.
.
reinforcement learning we begin by defining a version of the problem solved by reinforcement learning which is relevant to our task at hand.
we use a slightly nontraditional notation for consistency with the previous and next sections.
what we call choices are typically called actions and what we call a learner is typically called an agent.
we assume an learner in some environment.
the learner can perceive the state sof the environment where sis in some set of statess.
at the first point in time the learner is at an initial state s0 s. at each point in time the learner can make a choice c c which will bring it to some new state s s. eventually the agent gets into some terminal state sterm s indicating the end of an episode.
an episode is the sequence of state choice pairs made from the beginning of time up to the terminal state i.e.
e s0 c0 s1 c1 .
.
.
st ct where the choice ctin state stbrings the learner to the terminal state sterm.
finally we assume we are given a reward rfor a given episode e. a larger reward is better.
the problem to solve is the following.
given a state space s choicesc and reward r find a policy which maximizes the expected reward to the learner.
that is find a such that if the learner at each state s s makes the choice c s then the expected reward e e from the resulting episode eis maximized.
.
.
monte carlo control.
one approach to solving the policylearning problem above is by on policy monte carlo control .
the technique is on policy because the policy the learner is optimizing is the same one it is using to control its actions.
thus a monte carlo control learner ldefines both a policy where s outputs a choice cfor the given state s as well as an update procedure that improves after each episode.
algorithm 1shows pseudocode for a monte carlo control mcc learner l. in the algorithm we subscript the choice space state space and qandcounts with lto emphasize these are independent for each mcc learner.
we will drop the subscript lwhen talking about a single learner.
the basic idea is as follows.
we are trying to learn a policy for state spacesand choicesc.
the policy is greedy with probability it makes random choices line otherwise it makes the choices that will maximize the value function q line .
the value function q models the expected reward at the end of the episode from the choice cin state s. it is initialized to for each s c pair line so the first episode follows a totally random policy.
q is exactly the average rewards seen for each episode econtaining s c .
thus at the end of each episode e for each s c e line the running average for the rewards observed with action s c is updated to include the new reward r line .algorithm a monte carlo control learner l. implements a policy land an update function update lwhich updates ltowards the optimal policy after each episode.
input choice spacecl state spacesl and l el initialize episode for s c s l cldo counts l ql procedure state s ifuniformrandom then c random c else c arg maxc clql break ties arbitrarily el append el s c return choice procedure update reward r t len el for0 t tdo s c el ql r ql counts l counts l update avg.
reward counts l counts l el if the reward function producing risstationary i.e.
fixed then it can be shown that this update procedure always improves the policy.
that is if is the original policy and is the policy after the update the expected reward from a learner following is greater than or equal to the expected reward from a learner following .
sutton and barto provide a proof.
we draw attention to some specifics of our implementation that diverge from what may appear in textbooks.
.
.
algorithmic changes.
firstly we update an episode with a single reward rwhich is distributed to all state action pairs.
this is because as will be seen in later sections we only observe rewards at the end of an episode i.e there are no intermediate rewards provided in our method.
secondly we do not use a discount factor on the reward r. this is because the sequence of choices in an input generation do not lend themselves to a natural absolute ordering.
we cannot assume later decisions are more important than earlier ones which the discount factor implicitly does.
.
rlcheck mcc with diversity reward we now return to our problem space of generating inputs with a generator g. notice that the guides we defined in definition .
have a similar function to the learners in section .
given some state p t make a choice c. this leads to the natural idea of implementing a guide as an mcc learner rewarding the learner with some r x after the generator produces input x. however note that for the guide at each choice point p l c only a subset of choices c c can be taken.
further each choice point has a unique task for example choosing whether to generate a left child figure line or a right child figure line .
thus it is natural to define a separate learner 1413lpfor each choice point p and call update lponce for each learner after every execution of the generator.
finally in section we defined a guide using a sequence c to influence its actions while in section .
we assumed a finite set of statess.
thus we need a state abstraction function definition .
state abstraction function .astate abstraction function a c s for a generator gis a deterministic function mapping an arbitrary length choice sequence to a finite state spaces.acan rely on gto retrieve for any ci the choice point pat which ciwas made.
we will return to the state abstraction function in section .
.
we can now define a monte carlo control guide.
definition .
monte carlo control guide .assume a generator gproducing inputs in x a state abstraction function a and a reward function r x r. a monte carlo control guide consists of a set of monte carlo control learners lp .
each learner lpis associated with a choice point p l c ing.
let t lpbelp s policy after t 1calls to update lp ref.
algorithm .
then is p t t lp a .
finally after gproduces an input x the guide callsupdate lp r x for each of its learners lp.
now to use a monte carlo control guide mcc guide to solve the diversifying guidance problem only the state abstraction function a ref.
section .
and the reward function rneed to be specified.
we construct a reward function as follows.
let be the validity function and the characteristic function of interest.
if xbe the set of inputs previously generated by g then let x x x be the set of characteristics of all the previously generated inputs.
then the reward function ris r x runique if x x rvalid if x x rinvalid if x our technique rlcheck is thus make a generator gfollow an mcc guide with the reward function rabove.
note that this reward function is nonstationary that is it is not fixed across time.
if x then generating any x x such that x holds will result in the reward runique re generating the same x in the next step will only result in the reward rvalid.
this means the assumptions underlying the classic proof of policy improvement do not hold .
thus rlcheck s guide is not guaranteed to improve to an optimal policy.
instead it practices a form of online learning adjusting its policy over time.
.
state abstraction a key element in enabling mcc to solve the diversifying guidance problem is the state abstraction function definition .
which determines the current state given a sequence of past choices.
the choice of aimpacts the ability of the mcc guide to learn an effective policy.
on one extreme if acollapses all sequences into the same abstract state e.g.
a then a learner lpessentially attempts to find a single best choice c clpfor choice point p regardless2 ?c1 c2 true c3 c4 false c5 falsec6 true c7 ?s1 s2 l s3 l s4 l 1s5 s6 r figure a partially generated binary tree left and its corresponding choice sequence arranged by influence right .
of state.
on the other extreme if ais the identity function i.e.
a then the state space is infinite so for every previously unseen sequence of choices the learner s policy is random.
the ideal ais the abstraction function that maximizes expected reward.
however computing such an ais not tractable since it requires inverting an arbitrary validity function x .
instead we apply the following heuristic in many input generators a good representation for the state snafter making the nthchoice cnis some function of a past subsequence of choices that influence the choice cn.
the meaning of influence depends on the type of input being generated and the nature of the validity function.
for example figure 2shows a partially generated binary tree on the left.
on the right we show the choices made in the binarytree generator ref.
fig.
leading to this partial tree c1 c2 true c3 c4 false c5 false c6 true arranged by influence where a choice in the construction of a child node is influenced by choices constructing its parent node.
with this influence heuristic the best value for the next choice c7 which determines the value assigned to the right child should depend on the choice c1 which decided that the root node had value as well as the choice c6 which made the decision to insert a right child.
the best value for this choice c7does not necessarily depend on choices c2 c5 which were involved in the creation of the left sub tree.
therefore the state s6 in which the choice c7is to be made can be represented as a sequence .
here fvis a function associated c1 s choice point the node value choice point at line 4of fig.
and fris a function associated with c6 s choice point the right child choice point at line 10of fig.
.
in figure the state s6after applying these functions is we will define the functions fvandfrfor this figure later in this section.
an additional consideration when representing state as a sequence derived from past choices is that such sequences can become very long.
we need to restrict the state space to being finite.
again a reasonable heuristic is to use a trimmed representation of the sequence which incorporates information from up to the last w choices that influence the current choice.
wis a fixed integer that determines the size of a sliding window.
we can build a state abstraction function that follows these considerations in the following manner.
first build a choice abstraction function fpfor each choice point p which maps each cto an abstract choice.
then for c1 c2 .
.
.
cn build sn a so that sn if tailw sk fp cn for some k notherwise 14141def concat tail state value return state 4def gen tree state depth value guide.select state idx state concat tail state value tree binarytree value ifdepth max depth and guide.select state idx left state concat tail state l tree.left gen tree left state depth ifdepth max depth and guide.select state idx right state concat tail state r tree.right gen tree right state depth return tree figure pseudo code for a binary tree generator which follows guide and builds a tree based state abstraction.
random sequence tree tree l r state abstraction method01000020000300004000050000number of generated inputsvalid unique valid figure number of unique valid inputs generated by state abstraction.
random is a no rl baseline.
where is the concatenation operator and tailw s takes the last w elements of s. assume cnwas taken at choice point p. we can build both very basic and very complex state abstractions in this manner.
for example we can get a cn w .
.
.
cn cnby taking fp idfor all and choosing k n 1always.
this would be a simple sliding window of the last wchoices.
the states s1 s6that annotate the edges in figure 2are derived using the choice point abstraction functions fv c cfor the value choice point fr c rfor the right child choice point and fl x l for the left child choice point.
the kis chosen as k largest k n which is a choice from the parent node .
while programatically deriving this kfrom a choice sequence is tedious it is quite easy to do inline in the generator.
the generator figure 3shows a modified version of the generator from figure which updates an explicit state value at each to compute exactly this state abstraction function lines it also uses guides to select arbitrary values lines .
tree size0.
.11101001000unique valid inputs generatedrandom sequence tree tree l rfigure distribution of unique valid tree sizes by state abstraction.
random is a no rl baseline.
.
.
case study.
we evaluate the effect the state abstraction function has on the ability of rlcheck to produce unique valid inputs for the bst example.
we evaluate three state abstraction functions sequence the sliding window abstraction which retains choices from the sibling nodes i.e.
a cn w .
.
.
cn cn.
tree l r the abstraction function illustrated in figure 2and implemented in figure .
tree which chooses klike tree l r but has fp idfor all choice points and thus produces the same state for the left and right subtree of a node.
for example taking w 4and the choices to be abstracted c1 .
.
.
c6 from figure sequence will give tree state will give and tree l r will give .
we evaluate each of these abstraction techniques for generating bsts with maximum depth i.e.
links with .25and rewards eq.
rinvalid rvalid andrunique .
we set w for the abstraction function since there are at least two elements in the state for each parent node this means the learners cannot simply memorize the decisions for the full depth of the tree.
results.
figures 4and5show the results for our experiments.
in each experiment we let each technique generate trees.
the results show the averages and standard errors over trials.
we compare to a baseline random which just runs the generator from figure .
figure 4illustrates that no matter the state abstraction function chosen rlcheck generates many more valid and unique valid inputs than the random baseline tree l r generates more unique valid inputs than random.
within the abstraction techniques tree generates the fewest unique valid inputs.
sequence appears to be better able to distinguish whether it is generating a left or right child than tree probably because the tree state is identical for the left and right child choice points.
tree l r generates the fewest valid inputs but the most unique valid inputs more than sequence.
these unique valid inputs are also more complex those generated with other state abstractions.
figure 5shows for each technique the average number of unique valid trees generated of each size.
note the log scale.
the tree size is the number of nodes in the tree.
we see that tree l r is consistently able to generate orders of magnitude more trees of sizes than the other techniques.
since we reward uniqueness therlcheck is encouraged to generate larger trees as it exhausts the space of smaller trees.
these results suggest that tree l r has 1415enough information to generate valid trees and then combine these successes into more unique valid trees.
overall we see that even with a na ve state abstraction function rlcheck generates nearly an order of magnitude more unique valid inputs than the random baseline.
however a well constructed influence style state abstraction yields more diverse valid inputs.
evaluation in this section we evaluate how rlcheck our mcc based solution to the diversifying guidance problem performs.
in particular we focus on the following research questions rq1 does rlcheck quickly find many diverse valid inputs for realworld benchmarks compared to state of the art?
rq2 does rlcheck find valid inputs covering many different behaviors for real world benchmarks?
rq3 does adding coverage feedback improve the ability of rlcheck to generate diverse valid inputs for real world benchmarks?
implementation.
to answer these research questions we implemented algorithm 1in java and rlcheck on top of the open source jqf platform.
jqf provides a mechanism for customizing input generation for quickcheck style property tests.
baseline techniques.
we compare rlcheck to two different methods junit quickcheck or simply quickcheck the baseline generator based testing technique which calls the generator with a randomized guide and zest also built on top of jqf which uses an evolutionary algorithm based on coverage and validity feedback to guide input generators.
unlike rlcheck and quickcheck zest is a greybox technique it relies on program instrumentation to get code coverage from each test execution.
benchmarks.
we compare the techniques on four real world java benchmarks used in the original evaluation of zest apache ant apache maven google closure compiler and mozilla rhino.
these benchmarks rely on two generators ant and maven use an xml generator whereas closure and rhino use a generator for javascript asts.
the validity functions for each of these four benchmarks is distinct ant expects a valid build.xml configuration maven expects a valid pom.xml configuration the closure expects an es6 compliant javascript program that can be optimized and rhino expects a javascript program that can be statically translated to java bytecode.
overall ant has the strictest validity function and rhino has the least strict validity function.
design choices.
in our main evaluation we simply use identity as the characteristic function to which inputs get runique .
thus rlcheck simply tries to maximize the number of unique valid inputs.
this allows us to run rlcheck at full speed without instrumentation and generate more inputs in a fixed time budget.
in section .3we compare this choice to a greybox version of rlcheck where x takes into account the branch coverage achieved by input x. we instantiate our reward function eq.
with runique rvalid 0andrinvalid .
this incentivizes rlcheck to prioritize exploration of new unique valid inputs while penalizing strategies that lead to producing invalid inputs.
additionally we set .
in algorithm which allows rlcheck to explore at random with reasonably high probability.we first modified the base generators provided by jqf for xml and javascript to transform choice points with infinite domains to finite domains.
these are the generators we use for evaluation of zest and quickcheck.
we then built guide based generators with the same choice points as these base generators.
for the guidebased generators we built the state abstraction inline like it is built in figure .
for each benchmark the state abstraction function is similar to that in figure 3as it maintains abstractions of the parent choices.
we set w 5for the state window size.
the generator code is available at experiments.
we sought to answer our research questions in a property based testing context where we expect to be able to run the test generator for a short amount of time.
thus we chose minutes as a timeout.
to account for variability in the results we ran trials for each technique.
the experiments in section .1and5.
were run on gcp compute engine using a single vm instance with 8vcpus and gb ram.
the experiments in section .3were run on a machine with 16gb ram and an amd ryzen cpu.
.
generating diverse valid inputs to answer rq1 we need to measure whether rlcheck generates a higher number of unique valid inputs compared to our baselines.
on these large scale benchmarks where the test driver does nontrivial work simple uniqueness at the byte or string level is not the most relevant measure of input diversity.
what we are interested in is inputs with diverse coverage.
so we measure inputs with different traces a commonly used metric for input coverage diversity in the fuzz testing literature sometimes these traces are called paths but this is a misnomer .
the trace of an input xis a set of pairs b c where bis a branch and cis the number of times that branches is executed by x bucketed to base orders of magnitude.
let x give the trace of of x. ifx1 takes the path a b a then x1 a b .
ifx2takes the path a a a b then ais hit the same base order of magnitude times so x2 a b .
we call valid inputs with different traces diverse valid inputs.
the results are shown in figures 6and7.
figure 6shows at each time the percentage of all generated inputs that are diverse valid inputs.
for techniques that are only able to generate a fixed number of diverse valid inputs this percentage would steadily decrease over time.
in figures 6cand6d we see an abrupt decrease at the beginning of fuzzing for zest and quickcheck and for closure we see a continuing decrease in the percentage over time for these techniques.
in figures 6b 6c and 6dsee that rlcheck quickly converges to a high percentage of diverse valid inputs being generated and maintains this until timeout.
rlcheck also generates a large quantity of diverse valid inputs.
figure 7shows the total number of diverse valid inputs generated by each technique we see that rlcheck generates multiple order of magnitude more diverse valid inputs compared to our baselines.
the exception is on rhino figure rlcheck only has a .
increase over quickcheck.
rhino s validity function is relatively easy to satisfy most javascript asts are considered valid inputs for translation therefore speed is the main factor in generating valid inputs for this benchmark.
consequently the blackbox techniques time min .
.
.
.
.
diverse validquickcheck zest rlcheck rlcheck a ant at least valid time min .
.
.
.
diverse validquickcheck zest rlcheck b maven time min diverse validquickcheck zest rlcheck c rhino time min diverse validquickcheck zest rlcheck d closure figure percent of total generated inputs which are diverse valids i.e.
have different traces .
higher is better.
time min 010k20k30kdiverse validsquickcheck zest rlcheck rlcheck a ant at least valid time min 050k100k150kdiverse validsquickcheck zest rlcheck b maven time min 050k100k150k200kdiverse validsquickcheck zest rlcheck c rhino time min 050k100k150kdiverse validsquickcheck zest rlcheck d closure figure number of diverse valid inputs i.e.
inputs with different traces generated by each technique.
higher is better.
rlcheck and quickcheck outperform the instrumentation based zest technique on the rhino benchmark.
on both metrics the increase in ant is less pronounced and very variable.
the variation in percentage for ant is quite wide because it is hard to get a first valid input for rlcheck and quickcheck and in some cases rlcheck did not get this within the five minute time budget.
for an understanding of the effect on the results rlcheck shows the results for only those runs that find at least one valid input.
the mean value for rlcheck is much higher but the high standard errors remain due to the fact that these runs find the first valid input being at different times.
for such extremely strict validity functions rlcheck has difficulty finding a first valid input compared to coverage guided techniques.
this is a limitation of rlcheck a good policy can only be found after some valid inputs have been discovered.
for completeness we also ran longer experiments of hour to see if zest or quickcheck would catch up to rlcheck .
in hour rlcheck generates between more diverse valid inputs than zest on all benchmarks and outperforms quickcheck on all benchmarks.
furthermore rlcheck continues to generate a higher percentage of generated diverse valid inputs after one hour.
in particular the large improvements that are seen in figures 6are all maintained at roughly the same rate except for rhino.
in the case of rhino zest improves its percentage of diverse valid inputs from to after one hour while rlcheck continues to generate diverse valid inputs throughout.
rq1 rlcheck quickly converges to generating a high percentage of diverse valid inputs and on most benchmarks generates orders of magnitude more diverse valid inputs than our baselines.
.
covering different valid behaviors section .1shows that rlcheck generates many more diverse valid inputs than the baselines i.e.
solves the diversifying guidance problem more effectively.
a natural question is whether the valid inputs generated by each method cover the same set of input behaviors rq2 .
for this we can compare the cumulative branch coverage achieved by the valid inputs generated by each technique.
figure 8shows the coverage achieved by all valid inputs for each benchmark until timeout.
the results are much more mixed than the results in section .
.
on the closure benchmark quickcheck and rlcheck achieve the same amount of branch coverage by valid inputs.
on rhino quickcheck dominates slightly.
on maven rlcheck takes an early lead in coverage but zest s coverage guided algorithm surpasses it at timeout.
on ant figure 8a rlcheck appears to perform poorly but this is mostly an artifact of rlcheck s bad luck in finding a first valid input.
again for comparison s sake rlcheck shows the results for only those runs that generate valid inputs we see that rlcheck s branch coverage is slightly above zest s on these runs.
the overall clearest trend from figure 8is that rlcheck s branch coverage seems to quickly peak and then flatten compared to the other techniques.
this suggests that our mcc based algorithm while it is exploring diverse valid inputs may still be tuned too much towards exploiting the knowledge from the first set of valid inputs it generates.
we discuss in section 6some possible avenues to explore in terms of the rl algorithm.
rq2 no method achieves the highest branch coverage on all benchmarks.
rlcheck s plateauing branch coverage suggests that it may be learning to generate diverse inputs with similar features rather than discovering new behavior.
time min 01k2k3kbranch cov.
by validsquickcheck zest rlcheck rlcheck a ant time min 02004006008001kbranch cov.
by validsquickcheck zest rlcheck b maven time min 01k2k3k4k5kbranch cov.
by validsquickcheck zest rlcheck c rhino time min 02k4k6k8kbranch cov.
by validsquickcheck zest rlcheck d closure figure number of branches covered by valid inputs generated by each technique.
higher is better .
greybox information given that rlcheck is able to attain its objective as defined by the diversifying guidance problem generating large numbers of unique valid inputs section .
but does not achieve the highest branch coverage over all benchmarks section .
a natural question is to ask whether choosing a different one that is coverage aware could help increase the diversity of behaviors discovered.
this is what we seek to answer in rq3.
for this experiment we re ran rlcheck both blackbox i.e.
with bb id and with greybox information using b x the set of all branches covered by the input x .
thus greybox rlcheck is rewarded when it discovers a valid input that covers a distinct set of branches compared to all generated inputs.
note that this does not reward the guide more for generating an input which covers a wholly uncovered branch compared to an input that covers a new combination of already seen branches.
again we ran each method for trials timing out at minutes.
figures 9shows the number of diverse valid inputs valid inputs with distinct traces generated the the blackbox and greybox versions of rlcheck and figure 10shows the branch coverage by valid inputs for these two versions.
we see universally across all benchmarks and both metrics that blackbox rlcheck outperforms greybox rlcheck .
this suggests that the slowdown incurred by instrumentation is not worth the increased information rlcheck gets in the greybox setting.
the difference is less striking for branch coverage than number of diverse valid inputs generated because fewer inputs are required to get the same cumulative branch coverage.
we see much lower variation in ant in this experiment because on all runs blackbox rlcheck was able to generate at least one valid input for ant.
we chose random seeds at random in both experiments so this is simply a quirk of experimentation.
rq3 adding greybox feedback to rlcheck in terms of the characteristic function causes a large slowdown but no huge gains in number of valid inputs or coverage achieved.
overall rlcheck performs best as a black box technique.
discussion performance.
tabular methods such as ours do not scale well for large choice or state spaces.
let sandcdenote state and choice space sizes respectively.
the monte carlo control algorithm requireso sc space overall and requires o c time for evaluating the policy function .
this is because all the algorithmic decisionmaking is backed by a large q table with s centries.
becauseof these constraints we had to restrict our state and choice spaces forrlcheck .
for example in our javascript implementation when selecting integer values we restricted our choice space to the range .
.
.
rather than a larger range like .
.
.
.
this was necessary to generate inputs in a reasonable amount of time with this generator.
function approximation methods such as replacing the q table with a neural network may be necessary for dealing with larger more complex state and choice spaces.
increasing exploration.
in section .2we observed that the branch coverage achieved by rlcheck generated valid inputs tends to quickly plateau even for benchmarks where the other methods could achieve higher branch coverage figs 8b 8c .
this suggests that even with a high mcc may still be too exploitative for the diversifying guidance problem.
one approach to increase exploration would be to allow the learners to forget old episodes so choices made early in the testing session that are not necessary to input validity do not persist throughout the session.
curiositybased approaches which strongly encourage exploration and avoid revisiting states may also be applicable.
fine tuning.
in our experiments we heuristically chose the and kvalues and then kept them fixed across benchmarks.
we noted the importance of a large and modest kvalue for both generating unique valid inputs and so quickly.
we also chose the reward function heuristically and in our design process we noticed how that this choice significantly affected performance and particularly the distribution of invalid valid and unique valid inputs generated.
it may be valuable to fine tune these hyperparameters and reward functions for different benchmarks.
bootstrapping.
in section .1we saw that rlcheck had difficulty generating a first valid input for very strict validity functions ant .
this limitation could be overcome by allowing rlcheck to be bootstrapped i.e.
given a sequence of choices that produces a valid input at the beginning of testing.
this choice sequence could be user provided as long as there exists a relatively short sequence of choices resulting in a valid input.
relevance of diverse valid inputs for testing.
in aswering rq1 we established that rlcheck was able to generate orders of magnitude more diverse valid inputs.
a natural question is whether this metric is relevant for testing goals such as bug finding.
while we did not conduct an extensive study of bug finding ability as part of our research questions we did look at rlcheck s bug finding ability on our benchmark programs as compared to quickcheck in zest.
time min 010k20k30kdiverse validsblackbox rlcheck greybox rlcheck a ant time min 050k100k150k200kdiverse validsblackbox rlcheck greybox rlcheck b maven time min 0100k200k300kdiverse validsblackbox rlcheck greybox rlcheck c rhino time min 0100k200k300kdiverse validsblackbox rlcheck greybox rlcheck d closure figure number of diverse valid inputs generated by each technique.
higher is better.
time min 01k2k3kbranch cov.
by validsblackbox rlcheck greybox rlcheck a ant time min 0200400600800branch cov.
by validsblackbox rlcheck greybox rlcheck b maven time min 01k2k3k4kbranch cov.
by validsblackbox rlcheck greybox rlcheck c rhino time min 2k4k6k8kbranch cov.
by validsblackbox rlcheck greybox rlcheck d closure figure number of branches covered by valid inputs generated by each technique.
higher is better.
during our evaluation runs the techniques found a subset of the bugs described in the zest paper .
table 1lists for each bug that was discovered during out evaluation runs the average time to discovery ttd and reliability percent of runs on which the bug was found for each method.
bugs are deduplicated as done in the zest paper by exception type.
we see that on the ant where rlcheck found more diverse valid inputs than quickcheck it found bug faster and more often than quickcheck.
it was also faster than zest.
on closure where rlcheck found more diverse valid inputs than zest it was also faster at finding fault .
in contrast on rhino rlcheck only found .
more unique valid inputs than quickcheck.
in fact as shown in figure 6c over of generatorgenerated inputs already satisfied the validity function.
thus we see that the plain generator based approach quickcheck had the best fault discovery of the three methods.
this benchmark is representative of situations where the generator is already fairly well tuned for the validity function of the program under test.
overall we believe these results suggest but are not conclusive in showing that order of magnitude increases in input diversity result in better fault discovery.
related work the problem of automatically generating test inputs that satisfy some criteria has been studied for over four decades.
symbolic execution as well as its dynamic and concolic variants attempt to generate test inputs that reach program points of interest by collecting and solving symbolic constraints on inputs.
despite numerous advances in improving the precision and performance of these techniques the path explosion problem remains a key challenge that limits scalability to large complex constraints.table average time to discovery ttd and reliability rel.
the percentage of runs on which the bug was found for bugs found by each technique during our experiments.
bugs are deduplicated by benchmark and exception type.
dash indicates bug was not found.
rlcheck quickcheck zest bug id ttd rel.
ttd rel.
ttd rel.
ant 41s 178s 123s closure 1s .2s 23s rhino 95s 62s 276s rhino 11s 1s 30s rhino 3s 80s rhino 96s fuzz testing is a popular method to generate byte sequence test inputs.
the key idea is to generate a huge quantity of test inputs at random without incurring much cost for each individual input.
input validity requirements can be addressed either via userprovided input format specifications or by mutating existing valid inputs .
coverage guided fuzzing popularized by tools such as afl improves the effectiveness of mutation based fuzz testing by instrumenting the program under test and incorporating feedback in the form of code coverage achieved by each test execution the feedback is used to perform an evolutionary search for test inputs that cover various behaviors in the test program.
search based software testing generates inputs which optimize some objective function by using optimization techniques such as hill climbing or simulated annealing.
these 1419techniques work well when the objective function is a smooth function of the input characteristics.
quickcheck introduced the idea of formulating tests as properties x p x q x which could be validated to some degree by randomly generating many instances of xthat satisfy p x .
of course the main challenge is in ensuring that p x is satisfied often enough.
some researchers have attempted to write generators that produce diverse valid inputs by construction such as for testing compilers but solutions turn out to be highly complex domain specific implementations.
for some domains effective generators can also be automatically synthesized .
targeted property based testing biases hand written input generators for numeric utility values.
domain specific languages such as luck enable strong coupling of generators and validity predicates.
in contrast we address the problem of biasing arbitrary input generators towards producing inputs that satisfy arbitrary validity functions without any prior domain knowledge.
recently techniques such as zest crowbar and fuzzchick have combined ideas from coverage guided mutationbased fuzzing with generative techniques such as quickcheck.
although code coverage guidance helps discover new program behaviors it comes at the cost of program instrumentation which significantly reduces the number of test inputs that can be executed per unit time.
in contrast we address the problem of generating valid test inputs when considering the program as a black box.
there has been increasing interest in using machine learning to improve fuzz testing saavedra et al .
provide a survey.
b ttinger et al .
propose a deep reinforcement learning approach to fuzz testing.
this work uses the reinforcement learning agent to given a subsequence of the input file as state perform a mutation action on that subsequence.
instead of learning to mutate serialized input strings directly rlcheck employs reinforcement learning on generators for highly structured inputs.
alisp is a language for specifying reinforcement learning problems in a hierarchical manner.
similarly to our work these hierarchical learning programs contain choices points where an agent learns the best choice to take given the current state.
alisp splits value functions into three functions value of the current action current subroutine and overall execution.
it provides functionality for the user to specify which parts of the state each of these values depends on which bears some similarity to our notion of context.
the input generator domain has some key differences from the hierarchical learning domain.
notably we get reward only at the end of the episode when an input is generated.
thus it is unclear whether rlcheck value functions can be effectively split into alisp s three components.
conclusion in this paper we investigated a reinforcement learning approach to guiding input generators to generate more valid inputs for propertybased testing.
we began by formalizing the problem of generating many unique valid inputs for property based testing as the diversifying guidance problem.
we proposed rlcheck where generators follow a monte carlo control mcc based guide to generate inputs.
we found that rlcheck has great performance in terms of generating many diverse valid inputs on real world benchmarks.however mcc seems to be prone to overfitting to a certain space of valid inputs.
we believe more exploration oriented rl approaches could be better suited to provide the guidance in rlcheck .