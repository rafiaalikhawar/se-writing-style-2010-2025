cofi consistency guided fault injection for cloud systems haicheng chen department of computer science and engineering the ohio state university united states chen.
osu.eduwensheng dou state key lab of computer science institute of software chinese academy of sciences university of chinese academy of sciences china wsdou otcaix.iscas.ac.cn dong wang state key lab of computer science institute of software chinese academy of sciences university of chinese academy of sciences china wangdong18 otcaix.iscas.ac.cnfeng qin department of computer science and engineering the ohio state university united states qin.
osu.edu abstract network partitions are inevitable in large scale cloud systems.
despitedeveloper seffortsinhandlingnetworkpartitionsthroughout designing implementingandtestingcloudsystems bugscausedby networkpartitions i.e.
partitionbugs stillexistandcausesevere failures in production clusters.
it is challenging to expose thesepartition bugs because they often require network partitions to startandstopat specific timings.
in this paper we propose consistency guided faultinjection cofi a novel technique that systematically injects network partitionstoeffectivelyexposepartitionbugs.weobservethat network partitions can leave cloud systems in inconsistent states where partitionbugsaremorelikelytooccur.basedonthisobservation cofi first infers invariants i.e.
consistent states among different nodes in a cloudsystem.
once detecting violations to theinferred invariants i.e.
inconsistentstates whilerunningthecloudsystem cofiinjectsnetworkpartitionstopreventthecloudsystemfromre coveringbacktoconsistentstates andthoroughlytestswhetherthecloudsystemstillproceedscorrectlyatinconsistentstates.wehave applied cofi to three widely deployed cloud systems i.e.
cassandra hdfs andyarn.cofihasdetected12previously unknown bugs and four of them have been confirmed by developers.
ccs concepts computer systems organization cloud computing reliability software and its engineering software testing and debugging .
keywords cloud system netwrok partition fault injection testing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
reference format haicheng chen wensheng dou dong wang and feng qin.
.
cofi consistency guidedfaultinjectionforcloudsystems.in 35thieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa 12pages.
introduction cloud systems are playing an increasingly important role in our dailylife.amajorityoffortune500companiesadoptcloudstorage tohosttheirdata .manypopularsocialmediawebsitesare backed up by cloud services .
as a result the dependability ofcloudsystemshasbecomemoreimportantthanever.whencloud systemsfail theconsequencesareusuallysevere.forexample a three hour aws outage in led to a million dollar loss fors p500companies .asanotherexample whenfacebook service failed in many los angeles residents called .
cloud systems run on networked commodity machines where network partitions can occur as frequently as once a week and one incidentmaylastforminutesorevenhours .therefore dependable cloud systems must handle network partitionscorrectly.unfortunately thisisachallengingtasksincenetworkpartitions can happen to any node and can startandstop at any time.eventhoughdevelopersstrivetohandlenetworkpartitions throughoutdesigning implementing andtestingacloudsystem network partitions can still lead to cloud system failures .
weuseathree nodereplica basedcloudsysteminfigure 1to illustrate the typical process when a cloud system encounters a networkpartition.normally differentnodesinacloudsystemkeep their states consistent by exchanging messages phase .
whenanetworkpartitionstarts thenodesonthedifferentsidesofthepartition become disconnected.
this may occur when the three nodes have consistent states phase .
however the two nodes on the left side may change their states e.g.
serving a data updaterequest from the client causing inconsistency between the two sides phase2tophase3 .similarly anetworkpartitionmayoccur when the left two nodes are already inconsistent with the right node phase1tophase3 .thisispossiblebecauseincloudsystemsnodestatesareupdatedasynchronously.whenanetworkpartition starts cloud systems often have various built in mechanisms to recover from inconsistent states e.g.
repeatedly trying to connect the partitioned nodes for recovery.
thus cloud systems can still 35th ieee acm international conference on automated software engineering ase figure1 anetworkpartition seffectonathree nodecluster.
each circle represents one node.
circles with the same pat tern i.e.
solid hollow horizontal stripe or vertical stripe represent nodes that are consistent.
in phases and thelightnings represent network partitions and the dotted ar rows represent the message attempts failed by the networkpartition.
operate without the partitioned nodes.
when the network heals thetwosidesofthepartitionmayhaveinconsistentstates phase4 .
in this case the cloud system will try to recover the whole cluster back to a consistent state phase .
in this paper we refer to thecloud system bugs triggered by network partitions as partition bugs.
a partition bug can occur if the cloud system cannot handle thenetworkdisconnectioninphases2or3 orcannothandlethe inconsistency caused by the network partition in phases or .
fault injection is a common technique for testing cloud systemsagainstadversarialconditions e.g.
nodecrashesandnetwork partitions.
many works inject node crashes when testing cloud systems .
however node crashes and network partitions are fundamentally different.
first node crashes and network partitionsoftenexercisedifferentrecoveryoperationsofacloudsystem.
second crashing a node will remove its in memory state while partitioninganodewillnot.so thesenode crashinginjectiontechniquesareinapplicabletoexposepartitionbugsincloudsystems.
recently afewtoolshavebeenproposedtoinjectnetworkfailures when testing cloud systems.
for example namazu randomly drops network packages with a configured probability .
however suchafaultmodelresemblesanunreliablenetwork whichisdifferentfromnetworkpartitions.neat andjepsen caninject network partitions when testing cloud systems but they rely on developers to specify when a network partition starts and stops.
inthispaper wepropose consistency guided faultinjection cofi an automated technique to expose partition bugs by systematically injecting network partitions into cloud systems.
we find that partitionbugsaremorelikelytooccurwhencloudsystemsarerunning at inconsistent states i.e.
phases and in figure due to two main reasons.
first node communications at incon sistent states are harder to reason about than those at consistent states.second cloudsystemsstrivetorecoversystemstatesfrom inconsistency as quickly as possible leaving smaller time windows fordeveloperstotestcloudsystemsatinconsistentstates.basedonthisobservation cofi smainidea istoinject network partitionsto thoroughly exercise cloud systems at inconsistent states.
specifically cofifirstemploysdistributedprograminvariants to represent the consistent states in a cloud system.
a distributed program invariant or invariant for short is a propertythatmustholdwhenmultiplenodesareeachatcertainprogram point.usinginvariantstorepresentconsistentstatesallowscofito automatically identify consistent states in different cloud systems.
!
!
figure the triggering process of cassandra .
this partition bug can only be triggered when a network partition starts and stops at specific timings.
then cofimonitorsthecloudsystem sruntimestatesandstarts anetworkpartitionwhenaninconsistentstateoccurs i.e.
aninvariant is temporarily violated.
starting the network partition at inconsistentstatescanpreventthecloudsystemfromrecovering backtoconsistentstates sothatcoficanthoroughlytestthecloud system at inconsistent states.
finally cofi systematically explores thestoppingpointofthenetworkpartitionbasedonmessagetypes.hence cofienablesmoreefficienttestingofthecloudsystemthan exhaustively stopping the partition for every message.
wehaveimplementedaprototypeofcofiandappliedittomultipleversionsofthreepopularcloudsystems namelycassandra hdfs andyarn .coficansuccessfullydetect4known bugs and12 unknown bugsthathave neverbeenreportedbefore.
moreover the triggering processes of out of these bugs have timingrequirementsonboththestartingandthestoppingpoints of their network partitions.
at the time of this writing developers have confirmed four of the unknown bugs reported by cofi.
in summary we make the following contributions in this paper.
we propose consistency guided fault injection cofi a novel approachtoexpose partitionbugsbysystematicallyinjecting network partitions at inconsistent states of cloud systems.
cofi can efficientlyexposepartition bugsthathavetiming requirements on the starting and stopping points of network partitions.
we implement a prototype of cofi1 and evaluate cofi using multiple versions of three popular cloud systems namely cassandra hdfs and yarn.
cofi detects previously unknown bugs and four of them have been confirmed by developers.
motivation and challenges in this section we first motivate consistency guided fault injection i.e.
cofi with a real world example.
then we use the motivating example to discuss how cofi addresses its main challenges.
.
a motivating example .
.
thetriggeringprocess.
figure2showsthetriggeringprocess ofcassandra a partition bug that manifests when two cassandranodesexchangegossipmessagesataspecificinconsistent state.
in a cassandra cluster each node obtains the status of itspeers through exchanging gossip messages.
updates to a node s statusareorderedusingvectorclocks.if c1 c2andc3arethethree 537nodes in a cluster both c2andc3will know that c1is running normally step .theclusterisnowataconsistentstate phase1 in figure .
when a user decommissions c1 c1will change its status to left increases its clock value from t0tot1 and then notifies its peers about the update.
when c2receives the update message itwillmodifyitslocalcopyaccordingly step sincethe incomingmessagehasagreaterclockvalue t1 t0 .however due to a network partition c3does not receive any message about the update step .asaresult c3willfalselybelievethat c1isstill runningnormally.atthismoment theclusterbecomespartitioned andinconsistent phase3infigure .aftercertainamountoftime c2dropsc1 sstatus togetherwiththeclockvalue step .atthis point the network partition heals allowing c2andc3to exchange messages at the inconsistent state where c2forgets about c1while c3thinksc1is running normally phase in figure .
we use normal to denote this inconsistent state.
c3then propagates to c2an outdated value of c1 s status step .
sincec2now knows nothingabout c1 itwillblindlyaccept c3 svalue eventhoughitis outdated t0 t1 .asaresult c1reappearstoberunningnormally after it has already been decommissioned!
.
.
timingrequirementsonnetworkpartitions.
totriggerthis bug c2andc3need to exchange a gossip message at the inconsistent state normal .
this requires the network partition to start after step 1and before step as well as to stop after step 4and before step .
first ifthe network partition startsbefore step c3willnotconsider c1asrunningnormally.second ifthenetwork partition starts after step 3or stops before step c2andc3will eventuallyagreethat c1hasleft.finally ifthenetworkpartition does not stop before step c2andc3will not exchange gossip messages at state normal .
the bug will not be triggered if any of the aforementioned situations happens.
such a complex timing requirement on the network partition makes the bug difficult to be exposed using random or developer specified fault injection.
.
challenges and solutions totriggerthepartitionbuginourmotivatingexample cofiinjects network partitions to thoroughly test the cloud system at inconsistent states.
cofi needs to address the following three challenges.
.
.
challenge how to represent and decide consistent states?
theconsistencyofasystemstateiscloselyrelatedtothespecific protocols that individual cloud system adopts.
for example cas sandra uses paxos to replicate user data while hdfs uses replication pipeline .
moreover even for the same protocol twocloudsystemsmayhavetheirownimplementationssuchascassandra s and google spanner s paxos implementations .
cofiemploys distributedprograminvariants torepresent theconsistentstatesinacloudsystem.adistributedprograminvariant is a property that must hold when multiple nodes are each atcertainprogrampoint.foraselectedinvariant acloudsystemstateisconsistentifitsatisfiestheinvariant.otherwise thestate is inconsistent.
we can use the following invariant to represent the consistentstatesinourmotivatingexample i.e.
c2andc3agree onc1 s status status c2 status c3fromfigure 2wecanseethat ataconsistentstate e.g.
step the aboveinvariantissatisfied.conversely ataninconsistentstate e.g.
step2 the above invariant is violated.
since the invariants can be automatically extracted from cloud systems such consistent states can be easily applied to different protocols and implementations.
.
.
challenge when to start a network partition?
an intuitive idea is to start a network partition at every point during the sys tem execution since it simulates the real world scenario that thenetwork can be partitioned at any time.
however this approach isimpracticalduetoextremelyhighoverhead.inourmotivating example step 3alone lasts for more than one minute containing toomanyexecutionpointstoexplore.instead cofiinjectsanetworkpartitionassoonasitdetectsaninconsistentstateatruntime.
forexample usingtheinvariantin .
.
torepresentconsistent states cofi will start the network partition after c2updates its localcopyof c1 sstatusto left step andbefore c3updates itslocalcopyof c1 sstatusto left .toprovidemaximumchances ofexposingpartitionbugs cofiinjectsnetworkpartitionsatthe messagelevel insteadofattheuseroperationlevelasemployedby other tools by failing the message exchanges among nodes.
therefore thegossipmessagesatstep 3willbefailed keeping c2 andc3inconsistent.
.
.
challenge3 whentostopanetworkpartition?
toexercisea cloudsystematinconsistentstates onecantrytostopthenetwork partition at every execution point i.e.
enabling message exchange before each message is sent .
however this approach of exhaustivelysearchingallpossiblepointstostopthenetworkpartitionhas very high overhead.
for instance step 3alone consists of more than gossip messages.
trying to stop the network partition before each of them will be inefficient for exposing our motivating bug.toaddressthisissue coficlassifiesmessagesintodifferent typesandsystematicallyexploresthetimingofstoppinganetwork partition for each type of messages instead of each message.
af ter the classification the gossip messages at step 3are grouped into only a few types drastically reducing the number of stopping points to explore.
consistency guided fault injection inthissection wefirstdiscusscofi sfaultmodel.then weexplain cofi s workflow and explain its major steps.
.
fault model cofitestsacloudsystembyinjectingaperiodof temporary network partition to onenode in the system.
here temporary means that a started network partition will stop at certain point.
note thatstarting a network partition at inconsistent states only tests thecloud system at phase in figure .
to thoroughly test a cloud system at inconsistent states cofi also stops the network partitiontotestthecloudsystematphase4 i.e.
exchangingmessagesamong inconsistent nodes.
thespecificsofourfaultmodelareasfollows.first inatestrun only one node will be partitioned.
second the network partitioncanstartandstopatanytime controlledbycofi butcofiwill onlystart andstop thenetworkpartition onceper testrun.third duringthenetworkpartition allthemessagesbeingsentfromor !
!
figure3 cofi sworkflow.thetwosolidboxesrepresentthe two stages of cofi s workflow.
the three dotted boxes rep resent the configurable inputs to each stage.
deliveredtothepartitionednodewillbefailed.cofifocusesonthis simple network partition model because it is realistic and more importantly cloud systems are expected to correctly handle such a simplefaultmodelattheminimum.itisworthnotingthatcoficanbeextendedtotestmorecomplicatedfaultmodels e.g.
partitioning multiple nodes and simplex partition .
we leave them as future work.
.
cofi in a nutshell figure3presentsanoverviewofcofi sworkflow.cofiworksin two stages invariant mining andfault injection.
in the invariant mining stage cofi first runs the cloud system using the workload andrecordstheruntimevaluesoftheinterestingvariables.then cofi mines distributed program invariants from the recorded variablevalues.theminedinvariantswillbeusedtoguidestartingandstoppingnetworkpartitionsinthefaultinjectionstage.specifically for each invariant which represents consistent system states cofi systematically explores the scenarios of network partitions thatstart at an inconsistent state where the invariant is violated and stop at a later execution point.
during each testing run cofi uses thecheckertodetectincorrectsystembehaviors e.g.
systemdown.
when the checker fails cofi will generate a detailed bug report to help developers diagnose the failure.
the bug report containsinformation about the executed workload the failure symptom the runtime values of the invariant related variables as well as the messages failed by the network partition.
.
specifying interesting variables .
.
which variables are interesting?
in the invariant mining stage cofi mines distributed program invariants based on the runtime values ofsome interesting variables.which variables are interesting?
we observe that two categories of variables can betterrepresentthestateofacloudsystem namely systemmetadata e.g.
the status of a cassandra node as shown in our motivating example and user metadata e.g.
the location of a container in yarn .thismeta information iscriticalbecauseanythingwrong with this meta information may seriously affect the reliability of a cloudsystem.moreover aninterestingvariableshould havedata flow from or to the network so that cofi can exercise the cloud system in inconsistent states by controlling the timing of the network partition.
for instance the status variable in our motivating example is a system metadata that has data flow to and from thenetwork.
by starting the network partition when c2andc3are inconsistent on status step3in figure and stopping the network partition after c2removes its status step4 cofi triggers the bug.
namenode s status on namenode.
2namenode.instance.state namenode status namenode s status on datanode.
4datanode.instance.bpmanager.bpbynameserviceid.bpservices.state namenode status figure two interesting variables in hdfs.
.
.
how to specify interesting variables?
since cofi may access aninterestingvariableoutsideofitsscope werepresentaninterest ingvariableusingthepathtoaccessthevariablefromajava static field java sglobalvariable .werefertothesepathsas accesspaths.
figure4lists two interesting variables i.e.
two accesspaths that refertothesamemetadata inhdfs.specifically thesetwovariables bothstorethestatusofanamenode i.e.
whetherthenamenode isactiveorstandby.whenspecifyinganinterestingvariable one should specify the access path followed by the metadata stored in thevariable.let susethefirstinterestingvariable line2 tofurther explain how theaccess path works.
namenode.instance is astatic field that refers to a namenode object and stateis the namenode object s instance field that stores the namenode s status.
at run time cofiaccesses this interesting variableby first accessing thenamenode.instance object and then accessing the statefield of thatnamenode.instance object.developerscanprovidetheirown interesting variablesto represent systemstates customizing cofi totestthestatestheyareinterestedin.theeffortneededtospecifyaninterestingvariabledependsontheimplementationdetails involved in the access path.
for example it is straightforward toderive the first access path in figure 4because it matches with the namenode s status .
conversely specifying the second access pathrequirestheknowledgethatadatanodestoresinformation about the namenodes in the bpmanager field.
the metadata after eachaccesspathisauser definedidentifier whichhelpscofiselect interesting invariants in the invariant mining stage .
.
.
.
invariant mining in the invariant mining stage cofi first runs the cloud system and records the interesting variables values at the program points thatlikely reflectconsistentsystem states.then cofi groupsthe values recorded on different nodes to reconstruct the consistent states from which invariants are mined.
finally from the mined invariants cofiselectsthe interestingones toguidefaultinjection in the next stage.
.
.
which program points to collect variable values?
to derive consistentstatesinacloudsystem cofiminesdistributedprogram invariants from the interesting variables runtime values at certain programpoints.theconventionistochooseprogrampointslike functionentrances functionexits andloopentrances .however valuesattheseprogrampointsmayreflecttheintermediate resultsofanode slocalcomputation whichdonotrepresentthesystem s consistent states.
instead cofi selects program points right beforeamessageissent before sendprogrampoints andright afteramessageishandled after handleprogrampoints .specifically coficonsiderstheentranceofamessage sendingmethodas !
!
figure5 apartialexecutionof c2andc3ifthenetworkpartition in figure 2does not occur.
abefore sendprogrampoint andconsiderstheexitofamessagehandling method as an after handle program point.
a before send program point reflects the sender node s state when it has applied somechangeslocally andisreadytopropagatesuchachangetoits peers.similarly anafter handleprogrampointreflectsthereceiver node s state when it has finished updating its local state according toareceivedmessage.therefore thesenderandthereceiverofthe samemessageareusuallyconsistentatthepairoftheseprogram points.
figure5illustratessuchapropertyusingourmotivatingexample.
itshowsapartialexecutionof c2andc3ifthenetworkpartition does notoccur and c2sends agossip message to c3at step 3in figure2.
at the before send program point line in figure c2has updated its copy of c1 s status line .
at the after handle program point line c3has also updated its copy of c1 s status accordingtothegossipmessagefrom c2 line7 .asaresult c2and c3are consistent about c1 s status at this pair of program points.
.
.
how to associate values from different nodes?
after collectingvariablevaluesatthebefore sendprogrampointsandthe after handleprogrampoints cofineedstogroupthesevaluestore constructtheconsistentstates.sincethesenderandthereceiverof a message are usually consistent at the corresponding before send and after handle program points cofi groups the variable values at the pair of these program points to reconstruct the consistent state.
for the example in figure cofi groups c2 s variable values at line and c3 s variable values at line for the same gossip message to reconstruct the state.
the constructed state containsall the variables logged at the two before send and after handle programpointinstances.coficonsidersthestateoftwonodes i.e.
pair wise state instead of the state of all the nodes in the cluster for two reasons.
first many properties of distributed protocols canbecapturedinpair wisestates.forinstance forcassandra sgossipprotocol twonodesshouldbeconsistentaftertheyhaveexchangedgossips e.g.
the invariant in .
.
.
second even when a property involves more than two nodes breaking the sub property between anypairoftheinvolvednodeswillbesufficienttoviolatethewhole property.
for example in hdfs s replication protocol all the replicasshouldhavethesamedatawhenawritesucceeds.ifanypair of replicas have different data the whole property no longer holds.
when the interesting variable is status in figure grouping c2 s value at line and c3 s value at line allows cofi to mine the desired invariant i.e.
the invariant in .
.
.
.
.
whichinvariantsaremoreinteresting?
afterconstructingthe pair wisestates cofiemploysdaikon toperformtheactual invariantmining.bydefault daikoncanminemanyinvariants .algorithm1 runningfaultinjectiontestsforaninvariant.
input invariant workload checker 1istates runwithoutpartition invariant 2whileistate istates.next do 3forind istate.inconsistentnodes do passednewmsgtype true allmsgtypes whilepassednewmsgtype do newistates passednewmsgtype runwithpartition istate allmsgtypes istates.add newistates 8function runwithpartition istate allmsgtps do 9workload.start 10partition waiting passednewmsgtp false 11whileworkload.isrunning do ifpartition waiting curstate istate then partition started ifpartition started curmsg nelementallmsgtps then allmsgtps.add curmsg passednewmsgtp true partition stopped 17ifchecker.failed then generatebugreport 19return getnewinconsistentstates passednewmsgtp exploring all these invariants can be very time consuming.
therefore cofi further prunes the mined invariants based on a few heuristics allowingdeveloperstorunmoreinterestingtestswithin a limited budget.
first cofi only selects invariants that involve multiple i.e.
two in our setting nodes.
cross node invariants capture the consistent statesofdifferentnodes.whentheyareviolated theinvolvednodes are inconsistent.
second cofi removes invariants among variables thatrefertodifferentmetadatabecauseitmaynotbemeaningfulto comparetwodifferentmetadata.recordthat whenspecifyingan interestingvariable developersalsospecifythemetadatareferred to by the variable .
.
.
if an invariant involves variables thatrefer to different metadata the invariant will be disregarded.
by default daikonminesmanytypesofinvariants e.g.
theequality ofvariablevalues e.g.
vara varb varc andthemembership relation between two variables e.g.
varelmnt varset .
cofi focuses on equality invariants since it is usually easier to violate theseinvariants i.e.
tocreateinconsistentstates duringthetest runs.
since theinvariant for triggering cassandra asserts the equality of the same metadata on two nodes cofi will select it.
note that cofi s methodology allows using any invariant to represent consistent states pruning out less interesting invariants improves the test efficiency.
.
fault injection inthefaultinjectionstage coficonductsmultipletestrunsforeach mined invariant as shown in algorithm .
more specifically for eachinvariant cofifirstrunsthecloudsystemwithoutinjecting networkpartitiontorecordpossibleinconsistentstatesforstarting 540the network partition in the later runs line .
then cofi iterates over each inconsistent state as the starting point of a network partition line .
for each inconsistent state cofi also explores partitioning different inconsistent nodes line .
for each inconsistentstate partitionednode pair cofirepeatedlyrunsthecloud system to systematically explore different scenarios of network partitions described as follows.
in each test run for exploring one scenario of a network partition cofi first starts the workload line and monitors the cloud system sruntimestate representedbytheinvariant relatedvariables .oncethesystemreachestheselectedinconsistentstateof thecurrentrun cofistartsthenetworkpartitionbystoppingfuturemessagessendingfromordeliveringtothepartitionednode lines12 .cofisystematicallyexploresdifferentnetworkpartition stopping points by stopping the network partition before different types of messages.
foreachmessagetype cofiexplorestwopossiblecases stopping the network partition or maintaining the network partition atthispoint.specifically afterthenetworkpartitionstarts cofi intercepts everymessage thatis sendingfrom ordelivering tothe partitioned node.
if having not seen the type of an interceptedmessage cofi explores the scenario of stopping the partition atthis point which allows the current and future messages to pass lines .
otherwise cofi maintains the network partition by dropping the message.
cofi simulates message drop at the application level instead of the os level.
more details will be explained in .attheendofeachtestrun ifthecheckerfails cofigeneratesa bug report for the failure lines .
cofi terminates exploring the scenarios of the network partition for the pair of inconsistent state partitioned node if there is no new message type encounteredinthelatestrun line5 whichmeanscofihasexploredboth passing and failing all the message types.
due to the non determinism in the cloud system s execution someinconsistentstatesmaynotoccurinthefirstpartition free run but in the later runs.
therefore cofi continues collecting new inconsistentstates ineach testrun line .the newlycollected inconsistentstateswillbeusedasthestartingpointofthenetworkpartitioninthesuccessiveruns line7 .itisalsopossiblethatsome inconsistent states cofi initially collected may not occur in the latertestruns.toaddressthisissue cofiwillretrythetestruns multiple times a configurable parameter for the inconsistent state.
.
.
identifyinginconsistentstates.
toidentifythepossibleinconsistentstatesduringthepartition freerun line1inalgorithm and the runs with network partitions line in algorithm cofi monitorstheruntimevaluesoftheinterestingvariables.specifically cofi synchronously collects the variable values at the before send program points and the after handle program points.
by collecting values at the after handle program points cofi can capture the state change that is caused by a node handling a state changing message.
as shown in our motivating example after c2handles c1 sgossipmessageatstep 2infigure cofiwillimmediately know that status c2has become left .
sometimes a state change is not caused by the node handling a message.
for example step4in figure 2happens after a timeout line in figure .
to capture these state changes cofi collects the variable values at the before send program points.
since cloud systems often employaheartbeatmechanism collectingvariablevaluesatbefore send program points helps cofi periodically refresh its copy of a node s state.
when c2tries to send a gossip after step 4in figure cofi will realize that status c2has become .
.
.
classifyingmessages.
whencofifailsamessageduringa network partition the cloud system can react in two ways thecloud system can simply retry sending the message or initiate a different protocol to perform recovery e.g.
cassandra will initiate hinted handoff when a data replication message fails .
stoppingthe network partition for retried messages is unnecessary since exchanging the same type of messages will not exercise the cloud system differently.
however stopping the network partition in the second scenario can test if the alternative protocol will proceed correctly at inconsistent states.
basedonthisobservation cofisystematicallyexploresstopping the network partition before each typeof messages lines in algorithm instead of each message.
an ideal message type shouldcorrelatewiththecodesegmentsthatwillbeexecutedwhensendingorhandlingthemessage.inthisway stoppingthenetwork partition before different types of messages may exercise different code segments in the cloud system.
cofirepresentsthemessagetypeusingthequad stack sender receiver state where stackis the runtime call stack of the messagesendingmethod senderandreceiver arethesenderandthe receiverofthemessage and stateisthesystemstate i.e.
thevalues oftheinvariant relatedvariables whenthemessageissent.the messagetypeincludesthesendercallstackofamessagebecauseit reflects the execution path on the sender side.
moreover messages sent at different call stacks usually belong to different protocols e.g.
cassandra s gossip and hinted handoff protocols or differentsteps ofthe sameprotocol e.g.
thecommit requeststep and thecommitstepofatwo phasecommitprotocol .therefore handlingthesemessageswillexecutedifferentcodesegmentson the receiver side.
the other three elements in the message type also correlate with the code segments that will be exercised.
in our motivating example the following three types of messages execute different code segments at the receiver side type stack gossip c2 c3 left normal type stack gossip c3 c2 left normal type stack gossip c3 c2 normal specifically a type message will trigger the code that checks the message s vector clock and updates the receiver s state a type 2message will exercise the code that checks the message s vector clock and discards the message and a type message will execute thecodethatblindlyacceptsthevalueinthemessage.forexample all the gossip messages that c2sends toc3at step 3in figure belong to type .
as a result cofi will not redundantly try to stopthenetworkpartitionbeforeeachoftheseequivalentgossip messages.
.
workload and checker workloads drive cofi to exercise a target cloud system.
they cancomefromvarioussourcesrangingfromsimpleunitteststocarefully crafted test cases for stress testing.
although cofi can bedrivenbydifferentworkloads cofiismosteffectivewhenthe workload includes cross node operations that repeatedly read and 541write the interesting variables in different ways.
with such a workload cofi can explore more network partition scenarios.
for each of our tested cloud system we implement a few such workloads usingcommonadminoperations e.g.
resourcemanagerfailover in yarn and user operations e.g.
file movement in hdfs .
moreover developers can flexibly implement checkers to assert thesystempropertiestheycareabout.weprovidedefaultcheckers incofi oneperworkload.specifically ourcheckerscheckforbothgeneralfailures i.e.
fatalentries errorentries andexceptions inexecutionlogs aswellasnodecrashes andoperation specific failures e.g.
returning error code and reading stale data .
implementation cofihasthreecomponents aninstrumentationengine aninvariantminingengine andafaultinjectionengine.theinstrumentationen gineinstrumentsthecloudsystemtoenablereadingtheinteresting variablesatruntimeaswellasinterceptingmessagesendingand message handling method calls.
the invariant mining engine runs thecloudsystemandminesdistributedprograminvariantsfrom the valuesrecorded bythe instrumentedcode.
thefault injection enginerunstheworkloadandthecheckeronthecloudsystemand interacts with the instrumented code to inject network partitions.
.
code instrumentation we build cofi s instrumentation engine using javassist a java bytecodeinstrumentation toolkit.to enableaccessinginteresting variables at run time the instrumentation engine adds a getter method for each field in the target system.
to intercept messagesending method calls the instrumentation engine adds a call to cofi sbeforesend api at the beginning of each message sending method i.e.
eachbefore sendprogrampoint .similarly tointerceptmessagehandlingmethodcalls theinstrumentationengine adds a call to cofi s afterhandle api at the end of each message handlingmethod i.e.
eachafter handleprogrampoint .weintegrate cofi with the knowledge of the message sending methods and the message handling methods in popular cloud systems e.g.
sendoneway in cassandra.
developers can configure cofi to instrumentdifferentmessage passingmethods.insidethemessage sendingmethods theinstrumentationenginealsoaddscodetosimulatethenetworkpartition seffectonthelocalnode.inthefault injection stage this code will be executed when the fault injection enginedecidestofailthemessage.developerscanalsoconfigure the effect of the network partition.
by default the instrumented code throws an ioexception .
bothbeforesend andafterhandle takethreeparameters the sender of the message the receiver of the message and the classof the message.
all three parameters are used to generate an id for themessage in theinvariant miningstage.
the senderand the receiver parameters are also sent to the fault injection engine to decide the message type during the fault injection stage.
.
invariant mining intheinvariantminingstage beforesend andafterhandle perform similar tasks both apis first record the interesting variables values through calling the getter methods.
then the apis generate anidforthemessage to sendorthehandledmessage.finally theyassociatethevariablevalueswiththemessageidandwritethem to a log from which the invariant mining engine mines invariants.
note that cofi needs to pair the before send and after handle program points of the same message to reconstruct a system state.
toidentifythesamemessageonthesenderandreceiversides cofi makes two assumptions each communication channel between twonodesisfifo messagesofthesameclassgothroughthe samechannel.takeourmotivatingbugasanexample underthese two assumptions the first gossip message that c2sends toc3is the first gossip message that c3receives from c2.
all of our tested systems satisfy these two assumptions.
with these assumptions cofi constructs the message id to be the concatenation of the message ssender receiver class andthecounter i.inthisway a message will have the same id on the sender and receiver sides.
whenmininginvariants theinvariantminingenginefirstgroups the variable values at the before send program point and the afterhandle program point of the same message to form a system state.
theenginethenconcatenatesthestatesofthesameprogrampoint pair i.e.
samesender samereceiver andsamemessageclass to formatraceofthestatesforthatprogrampointpair.afterwards theenginerunsdaikon onthetracestomineinvariants.finally the mined invariants are pruned based on the rules in .
.
.
.
fault injection inthefaultinjectionstage the beforesend andtheafterhandle apisrecordtheruntimevaluesoftheinvariant relatedvariables and report them to the fault injection engine.
the beforesend api also reports the pending message sending event to the fault injectionengine andwaitsfortheengine sdecisiononwhetherthe message should be failed.
if the engine decides to fail the message thebeforesend api will return a false triggering the execution of the instrumented code in its caller i.e.
the message sendingmethod to simulate the network partition e.g.
by throwing an ioexception to signal the caller about the network partition or by returning from the message sending method to simulate a silent message drop.
evaluation ourevaluationaimstoanswerthreeresearchquestions howef fectiveiscofiindetectingpartitionbugsincloudsystems?
how does cofi compare with other approaches for injecting network partitions?
howefficientiscofi?weperformourevaluationusing a cloudlab machine that runs ubuntu .
.
the machine has xeon e5 processors and gb memory.
.
experimental methodology .
.
target cloud systems.
we select three widely used opensourcecloudsystemsasourexperimentsubjects i.e.
cassandra hdfs and yarn .
they represent different kinds of cloud systems.
first they provide different functionalities cassandrais a distributed nosql database hdfs is a distributed file system andyarnisadistributedcomputingframework.moreover these systems adopt differentsystem architectures cassandra is a peer to peersystemwhilehdfsandyarnarecoordinator worker systems.finally tocombatnetworkpartitions thesesystemsimplementdifferentrecoverymechanisms e.g.
hdfsemploysdata 542table1 experimentalsettingsforthetargetsystems.
var showsthenumbersoftheinterestingvariablesforeachsystem.
system operations in the workloads interesting metadata var cassandra .
.5create keyspace column family read write data node status node token keyspace name 3add remove column decommission node hdfs .
.
read write file move file directory datanode id namenode id namenode status hdfs .
.
failover namenode failover datanode data block id data block location yarn .
.
launch stop application failover nodemanager nodemanager id container id container location yarn .
.
failover resourcemanager resourcemanager id resourcemanager status table the known bugs used to evaluate cofi.
s shows whether stopping a network partition is needed to trigger thebug.
operations showstheoperationsintheworkload for exposing each bug.
bugid soperations interesting metadata cassandra checkwrite data drop table columnfamily name cassandra checkdecommission node node status hdfs shutdown datanode datanode id yarn startcluster nodemanager id re replicationtorecoverinconsistentuserdata andcassandra uses gossip to recover inconsistent system metadata .
.
.
detecting partition bugs.
to evaluate cofi s effectiveness weapplycofitoourtargetsystemsandcheckifcoficandetect both known bugs and unknown bugs.
detecting knownbugs.
first we collect several known partitionbugsbyinspectingtherecentlypublishedbugdatasets .ifabugsatisfiesthefollowingrequirements weselectittoevaluate cofi it happens in our target systems.
it only requires partitioningonenodetotrigger.
wecanmanuallyreproducethe bug.
finally we obtained fourpartition bugs as shown in table .
these four known bugs cover all three target systems and have differenttimingrequirementsonthenetworkpartition column s .
table2alsoshowstheoperationsineachbug sworkloadandthe metadatastoredintheinterestingvariableswhichwespecifyfor eachbug.theoperationsareextractedfromthebugreports.the interesting variables are identified through understanding each bug s triggering process.
detectingunknownbugs.
toevaluatecofi seffectivenessin detectingunknownbugs weapplycofitotestthelatestversionsofourtargetsystems.forhdfsandyarn wetestboththeirversion and version since these versions are both widely deployed and underactivedevelopment.forcassandra weonlytestitsversion3 sincethelatestminorreleaseofitsversion2 cassandra .
will nolongerbesupportedaftercassandra snextmajorrelease .
table1lists the selected system versions.
wedesign several workloads for each target systemusing the commonuserandadminoperationsasshownintable .forhdfs and yarn we use the same set of operations for both of their versions.
the operations in each workload follow natural order e.g.
create a table before writing data to it.
for cassandra we implementfourworkloadsonathree nodeclustertotestregular data access paxos data access schema update and node decommission.
for both hdfs and hdfs we design three workloadstotestfilesystemoperations namenodefailover anddatanode failover.
both the file system operations workload and the namenodefailoverworkloadrunonaclusteroftwonamenodesandthreedatanodeswhilethedatanodefailoverworkloadrunsonacluster with two namenodes and four datanodes.
for yarn we create three workloads run a yarn application in a cluster of one resourcemanager and one nodemanager resourcemanager failover inaclusteroftworesourcemanagersandonenodemanager nodemanager failover in a cluster of two resourcemanagers and two nodemanagers.
for yarn we build two workloads resourcemanager failover when a yarn application is running in a cluster oftworesourcemanagersandonenodemanager nodemanager failover when a yarn application is running in a cluster of two resourcemanagers and two nodemanagers.
moreover each yarn cluster also runs on top of an hdfs cluster with one namenode and one datanode.
ourcheckerscheckforbothgeneralfailures i.e.
fatalentries errorentries andexceptionsinexecutionlogs aswellasnode crashes andoperation specificfailures e.g.
returninganerrorcodeandreadingstaledata .toexploremorenetworkpartitionscenarios we limit at most fault injection runs for each invariant.
table1alsoshowsthemetadatastoredintheinterestingvariablesthatwespecifyforeachtargetsystem.forcassandra we specify theinteresting variable thatstores keyspace nameinstead of column family name as for triggering cassandra .
this is because keyspace names are accessed more often than column family names to access a column family one needs to first access the owner keyspace potentially exposing more system behaviors when two nodes are inconsistent on a keyspace name.
to enable accessingtheinterestingvariablesinhdfsandyarn weaddtwo staticfields to each version of hdfs and three staticfields to each version of yarn to refer to the objects of the main components in the system i.e.
namenode datanode resourcemanager nodemanager andapplicationmaster.intotal this onlyinvolves modifying22linesofcodeforallfoursystemversions.themanualeffortsforspecifyingtheinterestingvariablesareacceptable oneofour authors specified all the interesting variables and implementedallthemodificationsinthetargetsystemsinafewhours evenifheonlyhasabasicunderstandingofthesesystems.forthedevelopersofthesesystems specifyinginterestingvariablesshouldtakemuch less time.
.
.
comparing with an alternative approach.
we compare cofi with injecting network partitions randomly.
to be more specific we repeatedly run each workload for the same time as cofi spends when testing the target systems.
during each test run we inject a 543table bugs triggered by cofi.
stop shows whether the network partition needs to stop to trigger the bug.
status shows whether the bug is pending for developer s confirmation has been confirmed by developers or has already been fixed.
random showswhetherthebugistriggeredbyrandomlyinjectingnetworkpartitionsduringourexperiment.notethatthefour known bugs are from older versions of the target systems and we only apply random injection to the latest versions of thesesystems.
bug id failure symptom interesting metadata stopstatus random known bugscassandra thread keeps crashing column family name checkfixed n a cassandra decommissioned node reappears node status checkfixed n a hdfs nullpointerexception datanode id fixed n a yarn nodemanager aborts nodemanager id fixed n a unknownbugscassandra thread crashes node status checkpending check cassandra a created keyspace can t be found node status confirmed check cassandra data read failure node status pending cassandra decommission failure node status pending check cassandra data access failure node status confirmed check hdfs file metadata inaccessible namenode status checkpending check hdfs namenode crashes namenode status checkconfirmed yarn fail to stop a yarn service resourcemanager id checkpending yarn misleading error message container s location checkpending yarn invalid application state transition container s location checkconfirmed yarn invalid application state transition container s location checkpending yarn misleading error message container id checkpending check wedidnotknow cassandra beforecofiexposedit.thisbugispreviouslyreportedbyothersincassandra .
.buttheoriginal bug reporter can no longer trigger it in later versions of cassandra.
we are the first one to report this bug in cassandra .
.
.
network partition randomly.
the scenario of the network partition isdeterminedbeforeeachtestrun.first werandomlyselectanode toinjectnetworkpartition.then wedecidewhentostartandstop the network partition.
the starting point and the stopping point of the network partition are represented using the time offset from the start of a test run.
we randomly choose a time offset between 0andthelongesttestdurationtobethestartingpointofthenetwork partition and randomly select a time offset between the starting pointand thelongesttest durationtobe thestoppingpoint ofthe network partition.
the longest test duration will be updated when a longer test run occurs.
if a test run finishes before the startingpoint stopping point is reached the network partition does not start stop in that test run.
.
detecting partition bugs .
.
overall experimental results.
table3lists the partition bugs triggered bycofi.
in this table we show the detailedinformation abouteachbug includingthebug sidinjira bugid thefailure symptomofthebug failuresymptom theinterestingmetadata in the invariant that guides cofi to expose the bug interesting metadata whether the bug requires stopping the network partition to trigger stop whether the bug has been confirmed or fixed by developers status and whether the bug is also triggered byrandomlyinjectingnetworkpartitionsduringourexperiment random .
asshownintable cofisuccessfullydetectsallfourknown bugsusingtheworkloadsspecifiedinthebug sjirareport demonstrating cofi s effectiveness in detecting known partition bugs.
table3alsoshowsthat cofiidentifies12partitionbugsusingthe interesting variables and the simple workloads common user andadminoperations thatwespecifiedforeachsystem .all12 bugsarepreviouslyunknowninthesystemversionswetest.atthetime of writing four of the unknown bugs have been confirmed by developers.theexposedunknownbugshavedifferentsymptoms including severe failures like node crashes and data access failures.
notethatthesebugsonlyrelyonasmallsetofinterestingmetadata i.e.
cassandra s node status hdfs s namenode status yarn s resourcemanager id container location and container id.
in table3 we can also see that cofi is effective in exposing partitionbugsthatrequiresthenetworkpartitiontostopatcertain points.
specifically out of bugs can only be triggered bystopping the network partition at certain timing.
cofi exposesthese bugs by systematically stopping the network partition for each type of messages.
on the contrary it is challenging to expose these bugs using existing techniques that rely on developers to specify when the network partition starts and stops.
.
.
false positive analysis.
table4showsthe detailed statistics of applying cofi to the latest versions of the target systems.
in total cofireports49 uniquetestfailuresinthesesystems column failures .
of these failures are caused by the unknown bugs listed in table columnbugs .
the remaining failures are mostly false positives column false pos.
while one failure cannot be reproduced for diagnosis column can t repr.
.
wefurtherinvestigatethefalsepositivesreportedby cofi.we find that most out of of these false positives are caused by ourcheckersassertingforoperationsuccesswhiletheoperation has to fail.
for example in one of the false positives in cassan dra a data read with quorum consistency out of fails with a nohostavailable error.thisfailurematcheswithcassandra s 544table4 thenumberofuniquetestfailuresineachsystem.a failurecanbeabug afalsepositive falsepos.
orundecided if it cannot be reproduced for diagnosis can t repr.
.
system failures bugs false pos.
can t repr.
cassandra hdfs hdfs yarn yarn total the two failures in hdfs and the two failures in hdfs share the same two root causes hdfs andhdfs .
thefailureinyarn 2sharesthesamerootcause yarn with one of the failures in yarn .
figure the triggering process of hdfs .
specificationbecausethecoordinatornodeforthe readrequestis partitioned from the other two nodes.
as a result it does not have enough peers hosts to serve the request.
these false positives raise a challenge ingenerating oracles for fault injection tests.
specifically the correct system behaviors may vary in different fault scenarios.
for example if the nohostavailable erroroccurswhenonlyanon coordinatornodeispartitioned the failure is a bug because the coordinator should have enough peerstoservethereadrequest.automaticallygeneratingoracles for fault injection tests will be a challenge for the future research.
.
.
casestudy.
wenowshowanexampleofhowpartitionbugs can occur under intricate network partition scenarios and how cofi schoiceofthepartitionstartingpointsandtheexplorationof the stopping points help cofi expose partition bugs.
figure6showsapartitionbug hdfs whichistriggeredby a temporary network partition that occurs when a failover attempt is being rolled back.
this is a previously unknown partition bug reported by cofi.
in a cluster with two namenodes nn2has just became active in a failover attempt step .
due to a network partition nn2fails to respond to the failover command which triggers a rollback step .
normally as the network partition recovers nn2shouldsafelychangebacktostandby.however abug in the rollback process unconditionally terminates nn2during the rollback step .asaresult theclusterlosesahealthynamenode only because of an untimely transient network partition!
cofitriggersthisbugwhenusingtheinvariantthat nn2and the datanode in the cluster dn agree on nn2 s status in consistent states.
after nn2becomes active but before it informs dn table the overhead of cofi.
systeminvariants test runs mined selected iterations time cassandra 222h 20m hdfs 24h 09m hdfs 39h 41m yarn 240h 11m yarn 58h 59m total 585h 20m the system is inconsistent.
at this point cofi starts a network partitionon nn2.duringcofi ssystematicexplorationofdifferent partitionstoppingpoints itwillteststoppingthenetworkpartitionbeforestep whichthentriggersthebug.inourexperiment cofi exposes this bug using only runs when testing hdfs .
as this example shows the correctness of a protocol implementation can behardtoanalyzeunderintricatenetworkpartitionscenarios.cofi canhelpexposepartitionbugsintheimplementationbysystematically exploring different network partition scenarios.
it is also worthnotingthattotriggerthisbugthenetworkpartitionneeds to start and stop in the middle of one failover command issued bytheadmin.therefore injectingnetworkpartitionsattheuser operation level i.e.
starting and stopping the network partition before or after the failover command cannot trigger this bug.
.
comparing with random fault injection table3also lists the bugs triggered by randomly injecting network partitions using the policy described in .
.
.
we find that cofi is more effective in exposing partition bugs than random injection.
specifically the random injection only triggerred out of bugs triggered bycofi.
moreover the randominjection did not trigger any bugs that are new to cofi.
it is also worth noting that if the randominjectiondoesnotstopthenetworkpartition 3outofthese bugs will not be triggered.
to understand whyrandom injection is noteffective in triggering partition bugs we further analyze the triggering process of fourrepresentativebugs cassandra hdfs hdfs andyarn to compute the probability to trigger them randomly.first weassumethatthenodetopartitioniscorrectlyselected.then basedona concreteexecution wecomputetheprobability of selecting the right starting point of the network partitionp start andtheconditionalprobabilityofselectingtheright stopping point based on the selected starting point p end start .
finally theprobabilitytorandomlytriggerabugiscomputedas p bu p start p end start .
we find that the two bugs triggeredbybothcofiandtherandominjectionhavehighlikelyhoods to trigger p ca .
p hf .
while the other two bugs only triggered by cofi have much lower probabilities p hf .
p yn .
.therefore both our experiment and our analysis suggest that cofi is more effective than random injection in triggering partition bugs.
.
overhead analysis to measure the overhead of cofi we record several metrics while applying cofi to detect unknown bugs.
the metrics include the 545numberofinvariantsminedandselectedbycofi aswellasthetest iterations and wall clock time cofi spends in testing each target system.
table 5shows the values of these metrics.
in the fault injection stage there are in total test runs for allfivesystemversions column iterations whichtakesabout585 hourstofinish column time .specifically theaveragetimefor each test run column time column iterations in cassandra and hdfs is about to minutes.
on average yarn requires more than minutes to finish one test run.
this is because yarn employs a wait and retry mechanism for many operations.
the testtimecanbeshortenedbyconfiguringyarntoreducethewait time and thenumber of max retries.
giventhat cloud systems are complicated the above result demonstrates that cofi is efficient to be used for real world cloud system testing.
table5also shows in the invariant mining stage cofi mines distributed program invariants from the target systems columnmined .
after applying cofi s invariant pruning strategy only237invariantsremain column selected .that ssaid about ofinvariantsareremovedbycofi sinvariantspruningstrategy significantly reducing the number of invariants to test.
discussion we now discuss limitations and potential threats in our approach.
cofi sfaultmodel.
cofi focuses on a simple fault model one temporary network partition occurs on one node.
hence cofi can misssomepartitionbugs e.g.
theonestriggeredbypartitioning multiple nodes.
eventhough cofi is not complete our evaluation shows that cofi is effective in detecting partition bugs.
extending cofi to support more fault models can be our future work.
identifying interesting variables.
cofi s effectiveness and efficiency depend on the quality of the specified variables.
specifying uninteresting variables e.g.
variables for local file systemdata may prevent cofi from injecting network partitions at in teresting inconsistent states.
moreover specifying variables thatare updated at the same time e.g.
nodeidandnodeid chararray willcausecofitotestredundantnetworkpartitionscenarios.to helpidentifyinterestingvariables wesuggestdeveloperstospecify metadata variables that interactwith network.
moreover cofi provides default interesting variables for our target systems.
in the future it is needed to automate the variable identification process.
monitoringvariables.
cofidoesnotusecomplicatedprogram analysisorexpensivesynchronizationtocollectconsistentstates ofthecloudsystem.instead cofiemploysasimpleheuristic i.e.
usingthebefore sendandtheafter handleprogrampointsofthe same message to construct system states.
this heuristic may cause cofi to collect an inaccurate state e.g.
if the variables are asynchronouslyupdatedinthemeantime.inthefuture thisinaccuracy can be removed by adding synchronization across the cluster.
threats to validity.
due to resource limitation we evaluate cofi using five versions of three popular cloud systems.
therefore ourexperimentalresultsmaynotreflectthesituationinothercloud systems e.g.
distributed streaming systems.
however we strive tobeunbiasedbyselectingsystemswithdifferentfunctionalities i.e.
a distributednosql database adistributed filesystem anda distributedcomputingframework andarchitectures i.e.
peer topeer vs. coordinator worker .
related work fault injection.
fault injection is a commonly used technique for exposing fault triggered bugs.
in recent years many fault injection techniques have been proposed to expose bugs in cloud systems.neat and jepsen both inject network partitions when testing cloud systems.
however they rely on developers to specify when a network partition starts and stops which makes them less desirable for exposing partition bugs that have strict timingrequirements on network partitions.
moreover both tools injectnetwork partitions at the user operation level which limits their effectiveness in exposing intricate partition bugs.
on the contrary cofi systematicallyand smartlyexplores differentstarting points and stopping points of network partitions at message level which enables cofi to expose partition bugs effectively.
other fault injection techniques include randomly dropping networkpackets injectingnodecrashes injecting filesystemfaults injectingapifailures andreordering networkmessages .cofiiscomplementarytothesetechniques since it focuses on a different and important fault model for cloud systems i.e.
network partitions.
faultinjectionhasalsobeencommonlyusedtotesthowgeneral software behaves at adversarial scenarios such as power faults andadversarialinputs .thesetechniquesdo not focus on exposing partition bugs in cloud systems.
bugdetectiontechniquesforcloudsystems.
besidesfault injection many other techniques have been proposed to detectbugs in cloud systems.
for example distributed model checkersexplore all possible interleavings among network messages and local computation to expose bugs in the cloud system implementations .
while being powerful they still suffer from thestatespaceexplosionproblem.sometoolscandetectbugsby statically analyzing the source code of the cloud systems .
partitionbugsinvolvecomplexinteractionbetweenmultiplenodes in the cloud system which is challenging to analyze statically.
conclusion we present consistency guided fault injection cofi a novel technique that injects network partitions to expose partition bugs in cloudsystems.cofiisthefirstfaultinjectiontechniquethatcontrols both the starting point and the stopping point of the injected networkpartition.ourevaluationonpopularcloudsystemsshows thatcofiisbotheffectiveinexposingpartitionbugsandefficient to be used in real world cloud system testing.