a comprehensive study on challenges in deploying deep learning based software zhenpeng chen key lab of high confidence software technology moe peking university beijing china czp pku.edu.cnyanbin cao key lab of high confidence software technology moe peking university beijing china caoyanbin pku.edu.cnyuanqiang liu key lab of high confidence software technology moe peking university beijing china yuanqiangliu pku.edu.cn haoyu wang beijing university of posts and telecommunications beijing china haoyuwang bupt.edu.cntao xie key lab of high confidence software technology moe peking university beijing china taoxie pku.edu.cnxuanzhe liu key lab of high confidence software technology moe peking university beijing china xzl pku.edu.cn abstract deep learning dl becomes increasingly pervasive being used in a wide range of software applications.
these software applications named as dl based software in short as dl software integrate dl models trained using a large data corpus with dl programs written based on dl frameworks such as tensorflow and keras.
a dl program encodes the network structure of a desirable dl model and the process by which the model is trained using the training data.
to help developers of dl software meet the new challenges posed by dl enormous research efforts in software engineering have been devoted.
existing studies focus on the development of dl software and extensively analyze faults in dl programs.
however the deployment of dl software has not been comprehensively studied.
to fill this knowledge gap this paper presents a comprehensive study on understanding challenges in deploying dl software.
we mine and analyze relevant posts from stack overflow a popular q a website for developers and show the increasing popularity and high difficulty of dl software deployment among developers.
we build a taxonomy of specific challenges encountered by developers in the process of dl software deployment through manual inspection of sampled posts and report a series of actionable implications for researchers developers and dl framework vendors.
ccs concepts software and its engineering software creation and management computing methodologies artificial intelligence general and reference empirical studies .
corresponding author xuanzhe liu xzl pku.edu.cn .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
deep learning software deployment stack overflow acm reference format zhenpeng chen yanbin cao yuanqiang liu haoyu wang tao xie and xuanzhe liu.
.
a comprehensive study on challenges in deploying deep learning based software.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
introduction deep learning dl has been used in a wide range of software applications from different domains including natural language processing speech recognition image processing disease diagnosis autonomous driving etc.
these software applications named as dl based software in short as dl software integrate dl models trained using a large data corpus with dl programs.
to implement dl programs developers rely on dl frameworks e.g.
tensorflow and keras which encode the structure of desirable dl models and the process by which the models are trained using the training data.
the increasing dependence of current software applications on dl as in dl software makes it a crucial topic in the software engineering se research community.
specifically many research efforts have been devoted to characterizing the new challenges that dl poses to software development .
to characterize the challenges that developers encounter in this process various studies focus on analyzing faults in dl programs.
for instance islam et al.
have presented a comprehensive study of faults in dl programs written based on tensorflow tf keras pytorch theano and caffe frameworks.
recently the great demand of deploying dl software to different platforms for real usage also poses new challenges tosoftware deployment i.e.
deploying dl software on a specific platform.
for example a computation intensive dl model in dl software can be executed efficiently on pc platforms with the gpu support but it cannot be directly deployed and executed on platforms with limited computing power such as mobile devices.
to facilitate this deployment process some dl frameworks such asarxiv .00760v4 nov 2020esec fse november virtual event usa zhenpeng chen yanbin cao yuanqiang liu haoyu wang tao xie and xuanzhe liu tf lite and core ml are rolled out by major vendors.
furthermore se researchers and practitioners also begin to focus on dl software deployment.
for example guo et al.
have investigated the changes in prediction accuracy and performance when dl models trained on pc platforms are deployed to mobile devices and browsers and unveiled that the deployment still suffers from compatibility and reliability issues.
additionally dl software deployment also poses some specific programming challenges to developers such as converting dl models to the formats expected by the deployment platforms these challenges are frequently asked in developers q a forums .
despite some efforts made to the best of our knowledge a fundamental question remains under investigated what specific challenges do developers face when deploying dl software ?
to bridge the knowledge gap this paper presents the first comprehensive empirical study on identifying challenges in deploying dl software.
given surging interest in dl and the importance of dl software deployment this study can aid developers to avoid common pitfalls and make researchers and dl framework vendors1better positioned to help software engineers perform the deployment task in a more targeted way.
besides mobile devices and browsers that have been considered in previous work in this work we also take into account server cloud platforms where a large number of dl software applications are deployed .
to understand what struggles that developers face when they deploy dl software we analyze the relevant posts from a variety of developers on stack overflow so which is one of the most popular q a forums for developers.
when developers have troubles in solving programming issues that they meet they often seek technological advice from peers on so .
therefore it has been a common practice for researchers to understand the challenges that developers encounter when dealing with different engineering tasks from so posts as shown in recent work .
our study collects and analyzes so posts regarding deploying dl software to server cloud mobile and browser platforms.
based on these posts we focus our study on the following research questions.
rq1 popularity trend.
through quantitative analysis we find that dl software deployment is gaining increasing attention and find evidence about the timeliness and urgency of our study.
rq2 difficulty.
we measure the difficulty of dl software deployment using well adopted metrics in se.
results show that the deployment of dl software is more challenging compared to other aspects related to dl software.
this finding motivates us to further unveil the specific challenges encountered by developers in dl software deployment.
rq3 taxonomy of challenges.
to identify specific challenges in dl software deployment we randomly sample a set of relevant so posts for manual examination.
for each post we qualitatively extract the challenge behind it.
finally we build a comprehensive taxonomy consisting of categories linked to challenges in deploying dl software to server cloud mobile and browser platforms.
the resulting taxonomy indicates that dl software deployment faces a wide spectrum of challenges.
1unless explicitly stated framework vendors in this paper refer to vendors of deployment related frameworks such as tf lite and core ml.
traineddlmodelstrainexportedmodelsforserver cloudconvertedmodelsformobileconvertedmodelsforbrowsertrainingdata server cloudplatforms mobileplatforms browserplatformsdlsoftwaredevelopmentdlsoftwaredeploymentfigure dl software development and deployment.
furthermore we discuss actionable implications derived from our results for researchers developers and dl framework vendors.
in addition this paper offers a dataset of posts related to dl software deployment as an additional contribution to the research community for other researchers to replicate and build upon.
background we first briefly describe the current practice of dl software development and deployment.
figure distinguishes the two processes.
dl software development.
to integrate dl capabilities into software applications developers make use of state of the art dl frameworks e.g.
tf and keras in the software development process.
specifically developers use these frameworks to create the structure of dl models and specify run time configuration e.g.
hyper parameters .
in a dl model multiple layers of transformation functions are used to convert input to output with each layer learning successively higher level of abstractions in the data.
then large scale data i.e.
the training data is used to train i.e.
adjust the weights of the multiple layers.
finally validation data which is different from the training data is used to tune the model.
due to the space limit we show only the model training phase in figure .
dl software deployment.
after dl software has been well validated and tested it is ready to be deployed to different platforms for real usage.
the deployment process focuses on platform adaptations i.e.
adapting dl software for the deployment platform.
the most popular way is to deploy dl software on the server or cloud platforms .
this way enables developers to invoke services powered by dl techniques via simply calling an api endpoint.
some frameworks e.g.
tf serving and platforms e.g.
google cloud ml engine can facilitate this deployment.
in addition there is a rising demand in deploying dl software to mobile devices and browsers .
for mobile platforms due to their limited computing power memory size and energy capacity models that are trained on pc platforms and used in the dl software cannot be deployed directly to the mobile platforms in some cases.
therefore some lightweight dl frameworks such as tf lite for android and core ml for ios are specifically designed for converting pre trained dl models to the formats supported by mobile platforms.
in addition it is a common practice to perform model quantization before deploying dl models to mobile devices in order to reduce memory cost and computing overhead .
for model quantization tf lite supports only converting model weights from floating points to bit integers while core ml allows flexible quantization modes such as bits to bits .
fora comprehensive study on challenges in deploying deep learning based software esec fse november virtual event usa downloadstackoverflowdatasetidentifyrelevantquestionsdeterminepopularitytrenddeterminedifficultyconstructtaxonomyofchallenges12345rq1rq2rq3 figure an overview of the methodology.
browsers some solutions e.g.
tf.js are proposed for deploying dl models under web environments.
scope.
we focus our analysis on dl software deployment.
specifically we analyze the challenges in deploying dl software to different platforms including server cloud mobile and browser platforms.
any issues related to this process are within our scope.
however challenges related to dl software development e.g.
model training are not considered in this study.
methodology to understand the challenges in deploying dl software we analyze the relevant questions posted on stack overflow so where developers seek technological advice about unresolved issues.
we show an overview of the methodology of our study in figure .
step download stack overflow dataset .in the first step of this study we download so dataset from the official stack exchange data dump on december .
the dataset covers the so posts generated from july to december .
the meta data of each post includes its identifier post type i.e.
question or answer creation date tags title body identifier of the accepted answer if the post is a question etc.
each question has one to five tags based on its topics.
the developer who posted a question can mark an answer as an accepted answer to indicate that it works for the question.
among all the questions in the dataset denoted as the seta .
have an accepted answer.
step identify relevant questions .in this study we select three representative deployment platforms of dl software for study including server cloud mobile and browser platforms.
since questions related to dl software deployment may be contained in dl related questions we first identify so questions related to dl.
following previous work we extract questions tagged with at least one of the top five popular dl frameworks i.e.
tf keras pytorch theano and caffe from aand denote the extracted questions as the set b. then we identify the relevant questions for each kind of platform respectively.
server cloud.
we first define a vocabulary of words related to server cloud platforms i.e.
cloud server and serving .
then we perform a case insensitive search of the three terms within the title and body excluding code snippets of each question in band denote the questions that contain at least one of the terms as the set c. since questions in cmay contain some noise that is not related to deployment e.g.
questions about training dl models on the server we filter out those that do not contain the word deploy and finally questions remain in c. to further complement c we extract questions tagged with tf serving google cloud ml engine and amazon sagemaker from a. tf serving is a dl framework that is specifically designed for deploying dl software to servers google cloud ml engine and amazon sagemaker are two popular cloud platforms for training dl models and deploying dl software.
since the two platforms are rolled out by two major cloud service vendors i.e.
google and amazon we believe that they are representative.
for questions tagged with the two platforms we filter out those that do not contain the word deploy as they also support model training.
then we add the remaining questions as well as all questions tagged with tf serving into cand remove the duplicate questions.
finally we have questions about dl software deployment to server cloud platforms in the set c. mobile.
we define a vocabulary of words related to mobile devices i.e.
mobile android and ios and extract the questions that contain at least one of the three words from bin a case insensitive way.
we denote the extracted questions as the question setd.
then following previous work we also consider two dl frameworks specifically designed for dl software deployment to mobile platforms i.e.
tf lite and core ml .
we extract the questions tagged with at least one of the two frameworks fromaand then add these questions into d. finally we remove the duplicate questions and have questions about dl software deployment to mobile platforms in the set d. browser.
we extract questions that contain the word browser frombin a case insensitive way and denote the extracted questions as the sete.
in addition following previous work we also take tf.js which can be used for deploying dl models on browsers into consideration.
different from tf lite which supports only deployment tf.js also supports developing dl models.
however since dl on browsers is still at dawn questions tagged with tf.js inaare too few only .
if we employ strict keyword matching to filter out questions that do not contain deploy as above only out of questions can remain.
to keep as many relevant questions as possible instead of keyword matching we employ manual inspection here.
specifically we add all the questions intoeand exclude the duplicate questions.
then two authors of this paper examine the remaining questions independently and determine whether or not each question is about dl software deployment.
the inter rater agreement measured as cohen s kappa is .
which indicates almost perfect agreement.
then the conflicts are resolved through discussion and the questions considered as non deployment issues are excluded from e. finally we have questions about dl software deployment to browser platforms in the set e. step determine popularity trend .to illustrate the popularity trend of dl software deployment following previous work we calculate the number of users and questions related to the topic per year.
specifically the metrics are calculated based on the question setsc d ande for each of the past five years i.e.
from to .
step answers the research question rq1 .
step determine difficulty .we measure the difficulty of deploying dl software using two metrics widely adopted by previous work including the percentage of questions with no accepted answer no acc.
and the response time needed to receiveesec fse november virtual event usa zhenpeng chen yanbin cao yuanqiang liu haoyu wang tao xie and xuanzhe liu an accepted answer.
in this step we use the questions related to other aspects of dl software in short as non deployment questions as the baseline for comparison.
to this end we exclude the deployment related questions i.e.
questions in c d ande from the dl related questions i.e.
questions in b and use the remaining questions as the non deployment questions.
for the first metric we employ proportion test to ensure the statistical significance of comparison.
this test is used for testing null hypothesis that proportions in multiple groups are the same and thus the test is appropriate for the comparison in no acc.
for the second metric we select the questions that have received accepted answers and then show the distribution and the median value of the response time needed to receive an accept answer for both deployment and non deployment questions.
step answers the research question rq2 .
step construct taxonomy of challenges .in this step we manually analyze the questions related to dl software deployment in order to construct the taxonomy of challenges.
following previous work to ensure a confidence level and a confidence interval we randomly sample server cloud related questions fromcand mobile related questions from d. since browser related questions in eare not too many we use all the questions in it for manual analysis.
in total we get a dataset of questions that are used for taxonomy construction.
the size of this dataset is comparable and even larger than those used in existing studies that also require manual analysis of so posts.
next we present our procedures of taxonomy construction.
pilot construction.
first we randomly sample of the questions for a pilot construction of the taxonomy.
the taxonomy for each kind of platform is constructed individually based on its corresponding samples.
we follow an open coding procedure to inductively create the categories and subcategories of our taxonomy in a bottom up way by analyzing the sampled questions.
the first two authors named as inspectors who both have four years of dl experiences jointly participate in the pilot construction.
the detailed procedure is described below.
the inspectors read and reread all the questions in order to be familiar with them.
in this process the inspectors take all the elements of each question including the title body code snippets comments answers tags and even urls mentioned by questioners and answerers for careful inspection.
questions not related to dl software deployment are classified as false positives .
for a relevant question if the inspectors cannot identify the specific challenge behind it they mark it as unclear questions which as well as false positives are not included into the taxonomy.
for the remaining questions the inspectors assign short phrases as initial codes to indicate the challenges behind these questions.
specifically for those questions that are raised without attempts mainly in the form of how e.g.
how to process raw data in tf serving the inspectors can often clearly identify the challenges from the question descriptions for those questions that describe the faults or unexpected results that developers encounter in practice the inspectors identify their causes as the challenges.
for example if a developer reports an error that she encounters when making predictions and the inspectors can find that the cause is the wrong format of input data from the question descriptions comments oranswers the inspectors consider setting the format of input data as the challenge behind this question.
then the inspectors proceed to group similar codes into categories and create a hierarchical taxonomy of challenges.
the grouping process is iterative in which the inspectors continuously go back and forth between categories and questions to refine the taxonomy.
a question is assigned to all related categories if it is related to multiple challenges.
all conflicts are discussed and resolved by introducing three arbitrators.
the arbitrator for server cloud deployment is a practitioner who has four years of experience in deploying dl software to servers cloud platforms.
the two arbitrators for mobile and browser deployment are graduate students who have two years of experience in deploying dl software to mobile devices and browsers respectively.
both of these arbitrators have published papers related to dl software deployment in top tier conferences.
the arbitrators finally approve all categories in the taxonomy.
reliability analysis and extended construction.
based on the coding schema in the pilot construction the first two authors then independently label the remaining questions for reliability analysis.
each question is labeled with false positives unclear questions or the identified leaf categories in the taxonomy.
questions that cannot be classified into the current taxonomy are added into a new category named pending .
the inter rater agreement during the independent labeling is .
measured by cohen s kappa indicating almost perfect agreement and demonstrating the reliability of our coding schema and procedure.
the conflicts of labeling are then discussed and resolved by the aforementioned three arbitrators.
for the questions classified as pending the arbitrators help further identify the challenges behind the questions and determine whether new categories need to be added.
finally new leaf categories are added and all questions in pending are assigned into the taxonomy.
in summary among the sampled questions are marked as false positives and as unclear questions .
in addition there are questions each of which is assigned into two categories.
the remaining sampled questions i.e.
for server cloud deployment for mobile deployment and for browser deployment are all covered in the final taxonomy.
the entire manual construction process takes about man hours.
step answers the research question rq3 .
rq1 popularity trend figure shows the popularity trend of deploying dl software in terms of the number of users and questions on so.
the figure indicates that this topic is gaining increasing attention demonstrating the timeliness and urgency of this study.
for deploying dl software on server cloud platforms we observe that users and questions increase in a steady trend.
in most major vendors roll out their dl frameworks for mobile devices .
as a result we can observe that both the number of users and the number of questions related to mobile deployment in increase by more than compared to .
for deploying dl software on browsers questions start to appear in due to the release of tf.js in .
as found by ma et al.
dl in browsers is still ata comprehensive study on challenges in deploying deep learning based software esec fse november virtual event usa a trend of users b trend of questions figure the popularity trend of deploying dl software.
figure time needed to receive an accepted answer.
dawn.
therefore the users and questions related to dl are still not so many as shown in figure .
rq2 difficulty for deployment and other aspects in short of non deployment of dl software the percentages of relevant questions with no accepted answer no acc.
are .
and .
respectively.
the significance of this difference is ensured by the result of proportion test 2 .
df p value .2e indicating that questions related to dl software deployment are more difficult to answer than those related to other aspects of dl software.
more specifically for server cloud mobile and browser deployment the values of no acc.
are .
.
and .
respectively.
in terms of this metric questions about deploying dl software are also more difficult to resolve than other well studied challenging topics in se such as big data no acc.
.
concurrency no acc.
.
and mobile no acc.
.
.
figure presents the boxplot of response time needed to receive an accepted answer for deployment and non deployment related questions.
we can observe that the time needed for non deployment questions is mostly concentrated below minutes while deployment questions have a wider spread.
furthermore we find that the median response time for deployment questions i.e.
.
minutes is about times the time needed for non deployment questions only .
minutes .
more specifically the median response time for server cloud mobile and browser related questions is .
.
and .
minutes respectively.
in previous work researchers find that the median response time needed for other challenging topics including big data concurrency and mobile is about and minutes respectively.
in contrast questions related to deploying dl software need longer time to receive accepted answers.
in summary we find that questions related to dl software deployment are difficult to resolve partly demonstrating the findingin previous work that model deployment is the most challenging phase in the life cycle of machine learning ml and motivating us to further identify the specific challenges behind deploying dl software.
rq3 taxonomy of challenges figure illustrates the hierarchical taxonomy of challenges in dl software deployment.
as shown in figure developers have difficulty in a broad spectrum of issues.
note that although the identified challenges are about deploying dl software to specific platforms not all relevant issues occur on corresponding platforms.
for example to deploy dl software to mobile devices the model conversion task can be done on pc platforms.
we group the full taxonomy into three sub taxonomies that correspond to the challenges in deploying dl software to server cloud mobile and browser platforms respectively.
each sub taxonomy is then organized into three level categories including the root categories e.g.
server cloud the inner categories e.g.
model export and the leaf categories e.g.
model quantization .
in total we have root categories inner categories and leaf categories.
we show the percentages for questions related to each category in the parentheses.
then we describe and exemplify each inner category.
.
common challenges in server cloud mobile and browser to avoid duplicate descriptions we first present the common inner categories in server cloud mobile and browser .
.
.
general questions.
this category shows general challenges that do not involve a specific step in the deployment process and contains several leaf categories as follows.
entire procedure of deployment.
this category refers to general questions about the entire procedure of deployment mainly raised without practical attempts.
these questions are mainly in the form of how such as how can i use that model in android for image classification .
in such questions developers often complain about the documentation e.g.
there is no documentation given for this model .
answerers mainly handle these questions by providing existing tutorials or documentation like information that does not appear elsewhere or translate the jargon heavy documentation into case specific guidance phrased in a developer friendly way.
compared to server cloud .
and mobile .
browser contains relatively fewer such questions .
.
a possible explanation is that since dl in browsers is still in the early stage developers are mainly stuck in dl s primary usage rather than being eager to explore how to apply dl to various scenarios.
conceptual questions.
this category includes questions about basic concepts or background knowledge related to dl software deployment such as is there any difference between these neural network classifier and neural network in machine learning model type used in ios .
this category of questions is also observed in previous work that analyzes challenges faced by developers in other topics through so questions.
for server cloud andmobile this category accounts for .
and .
respectively indicating that developers find even the basics of dl software deployment challenging.
for browser this category is missing.
since tf.js also supports model training we filter out the conceptualesec fse november virtual event usa zhenpeng chen yanbin cao yuanqiang liu haoyu wang tao xie and xuanzhe liu dataprocessing .
procedure .
settingsize shapeofinputdata .
settingformat datatypeofinputdata .
parsingoutput .
migratingpre processing .
authenticatingclient .
procedure .
parsingrequest .
serving .
modelloadin g .
configurationofbatching .
serving multiple models simultaneous l y .
bidirectionalstreaming .
server cloud modelupdate .
environment .
installing buildingframeworks .
avoidingversi o n incompatibility .
configuration of environment variables .
limitationsofplatforms frameworks .
modelexport .
procedure .
specificationofmodelinformation .
exportofunsupportedmodels .
selection usageofapis .
modelquantization .
request .
procedure .
settingrequestparameters body .
batchingrequest .
generalquestions .
entireprocedureofdeployment .
conceptualquestions .
modelupdate .
dataextraction .
generalquestions .
entireprocedureofdeployment .
conceptualquestions .
limitationsofframeworks .
avoidingversionincompatibility .
configurationofinput outputinformation .
dlintegrationintoprojects .
importing loadingmodels .
buildconfiguration .
inferencespeed .
modelconversion .
procedure .
savingmode l s .
conversionofunsupportedmodels .
modelquantization .
specificationofmodelinformation .
selection usageofapis .
parsingconvertedmodels .
dllibrarycompilation .
usage of prebuilt libraries .
register of unsupported operators .
buildconfiguration .
procedure .
dataprocessing .
settingsize shapeofinputdata .
settingformat datatypeofinputdata .
parsingoutput .
migratingpre processing .
mobile gettinginformationofexposedmodel .
modelupdate .
dataextraction .
modelloading .
loadingfromlocalstorage .
loadingfromahttpendpoint .
asynchronousloading .
selection usageofapis .
improvingloadingspeed .
procedure .
inferencespeed .
environment .
importinglibraries .
avoidingversion incompatibility .
procedure .
specificationofmodelinformation .
conversionofunsupportedmodels .
selection usageofapis .
savingmode l s .
generalquestions .
entireprocedureofdeployment .
limitationsofframeworks .
procedure .
dataprocessing .
settingsize shapeofinputdata .
settingformat datatypeofinputdata .
migratingpre processing .
browser modelconversion .
procedure .
threadmanagement .
modelsecurity .
modelsecurity .
procedure .
dataloading .
figure taxonomy of challenges in deploying dl software.a comprehensive study on challenges in deploying deep learning based software esec fse november virtual event usa questions about tf.js during manual inspection as we cannot discern whether these questions occur during training or deployment.
however it does not mean that there is no conceptual questions about browser deployment.
limitations of platforms frameworks.
this category is about limitations of relevant platforms or dl frameworks.
for example a senior software engineer working on the google cloud ml platform team apologizes for the failure that a developer encounters admitting that the platform currently does not support batch prediction .
besides some issues reflect bugs in current deployment related frameworks.
for instance an issue reveals a bug in the tococonvert.from keras model file method of tf lite .
.
.
model export and model conversion.
both categories cover challenges in converting dl models in dl software into the formats supported by deployment platforms.
model export directly saves the trained model into the expected format and it is a common way for deploying dl models to server cloud platforms.
in contrast model conversion needs two steps saving the trained model into a format supported by the deployment frameworks using these frameworks to convert the saved model into the format supported by mobile devices or browsers.
considering the similar functionalities of model export and model conversion we put them together for description.
model export represents .
of questions in server cloud while model conversion covers the most encountered category of challenges in mobile and the third most encountered category of challenges in browser accounting for .
and .
respectively.
we next present representative leaf categories under the two categories.
procedure.
different from entire procedure of deployment which asks about the entire deployment process questions in procedure are about the procedure of a specific step in the process.
an example question in procedure under model conversion is how can i convert this file into a .coreml file .
due to page limit we do not repeat the descriptions of procedure in other inner categories.
export conversion of unsupported models.
the support of dl on some platforms is still unfledged.
some standard operators and layers used in the trained model are not supported by deployment frameworks.
for example developers report that lstm is not supported by tf lite and that gaussiannoise is not supported by tf.js .
similarly guo et al.
report that they could not deploy the rnn models i.e.
lstm and gru to mobile platforms due to the unsupported operation error.
in addition when developers attempt to export or convert models with custom operators or layers the developers also encounter difficulties .
specification of model information.
when exporting or converting dl models to expected formats developers need to specify model information.
for instance tf serving requires developers to construct a signature to specify names of the input and output tensors and the method of inference i.e.
regression prediction or classification .
incorrect specification would result in errors .
sometimes developers directly use off the shelf models that have been well trained and released online for deployment but the developers have no idea about the models information e.g.
names of the input and output tensors making the model export conversion task challenging.selection usage of apis.
there are so many apis provided by different frameworks for developers to export and convert models to various formats.
therefore it is challenging for developers to select and use these apis correctly according to their demand.
for example a developer is confused about the relationship between tensorflow saver exporter and save model and says frankly that she feels more confused after reading some tutorials.
in addition the addition deprecation and upgrade of apis caused by the update of frameworks also make the selection and usage of apis errorprone .
model quantization.
model quantization reduces precision representations of model weights in order to reduce memory cost and computing overhead of dl models .
it is mainly used for deployment to mobile devices due to their limitations of computing power memory size and energy capacity.
for this technique developers have difficulty in configuration of relevant parameters .
in addition developers call for support of more quantization options.
for instance tf lite supports only bit quantization i.e.
converting model weights from floating points to bit integers but developers may need more bits for quantization .
.
.
data processing.
this category covers challenges in converting raw data into the input format needed by dl models in dl software i.e.
pre processing and converting the model output into expected formats i.e.
post processing .
this category accounts for the most questions .
in server cloud .
for mobile andbrowser it covers .
and .
of questions respectively.
we next describe the representative leaf categories under data processing .
setting size shape format datatype of input data.
it is a common challenge in data pre processing to set the size shape and format datatype of data.
a faulty behavior manifests when the input data has an unexpected size shape e.g.
a image instead of a image format e.g.
encoding an image in thebase64 format instead of converting it to a list or datatype e.g.
float instead of int .
migrating pre processing.
when ml dl models are developed data pre processing is often considered as an individual phase and thus may not be included inside the model structure.
in this case code for data pre processing needs to be migrated during the deployment process in order to keep consistent behaviors of software before and after deployment.
for instance when developers deploy a dl application with pre processing implemented with python and out of the dl model to an android device the developers may need to re implement pre processing using a new language e.g.
java or c c .
forgetting to re implement preprocessing or re implementing it incorrectly can lead to faulty behaviors.
in addition an alternative to keep data preprocessing consistent is to add it into the structure of dl models.
for this option developers face challenges such as how to add layers before the input layer of model restored from a .pb file to decode jpeg encoded strings and feed the result into the current input tensor .
parsing output.
this category includes challenges in converting the output of dl models to expected or human readable results such as parsing the output array or tensor to get the actual predicted class.esec fse november virtual event usa zhenpeng chen yanbin cao yuanqiang liu haoyu wang tao xie and xuanzhe liu .
.
model update.
once dl software is deployed for real usage the dl software can receive feedback e.g.
bad cases from users.
the feedback can be used to update weights of the dl model in dl software for further performance improvement.
many challenges such as periodical automated model update on clouds and model update or re training on mobile devices emerge from the efforts to achieve this goal.
this category covers .
.
and .
of questions in server cloud mobile and browser respectively.
.
common challenges in mobile and browser .
.
data extraction.
to deploy dl software successfully developers need to consider any stage that may affect the final performance including data extraction.
this category is observed only inmobile andbrowser accounting for .
and .
of questions respectively.
this finding indicates the difficulty of extracting data in mobile devices and browsers.
.
.
inference speed.
compared to server cloud platforms mobile and browser platforms have weaker computing power.
as a result the inference speed of the deployed software has been a challenge in mobile devices .
and browsers .
.
.
.
model security.
dl models in dl software are often stored in unencrypted formats resulting in a risk that competitors may disassemble and reuse the models.
to alleviate this risk and ensure model security developers attempt multiple approaches such as obfuscating code or libraries .
any challenges related to model security are included in this category.
this category is observed only in mobile and browser since models deployed to these platforms are easier to obtain.
in contrast models deployed on server cloud platforms are hidden behind api calls.
.
common challenges in server cloud and browser environment.
this category includes challenges in setting up the environment for dl software deployment and accounts for .
and .
of questions in server cloud andbrowser respectively.
for mobile its environment related questions are mainly distributed indl library compilation and dl integration into projects categories that will be introduced later.
when deploying dl software to server cloud platforms developers need to configure various environment variables whose diverse options make the configuration task challenging.
in addition for the server deployment developers also need to install or build necessary frameworks such as tf serving.
issues that occur in this phase are included in installing building frameworks .
similarly when deploying dl software to browsers some developers have difficulty in importing libraries e.g.
i am developing a chrome extension where i use my trained keras model.
for this i need to import a library tensorflow.js.
how should i do that .
besides these challenges the rapid evolution of dl frameworks makes the version compatibility of frameworks libraries challenging for developers.
for instance an error reported on so is caused by that the tf used to train and save the model has an incompatible version with tf serving used for deployment .
similarly humbatova et al.
mention that version incompatibility betweendifferent libraries and frameworks is one of the main concerns of practitioners in developing dl software.
.
remaining challenges in server cloud .
.
request.
this category covers challenges in making requests in the client and accounts for .
of questions in server cloud .
forrequest developers have difficulty in configuring the request body sending multiple requests at a single time i.e.
batching request getting information of serving models via request etc.
.
.
serving.
this category concerns challenges related to serving dl software on the server cloud platforms and accounts for .
of questions.
to make a dl model in dl software servable developers first need to load the dl model where issues such as loading time and memory usage may emerge.
in addition many developers encounter difficulties in authenticating the client and parsing the request .
sometimes developers need to serve multiple different models to provide diverse services or serve different versions of one model at the same time but the developers find that the implementation is not such easy accounting for .
of questions .
similarly zhang et al.
demonstrate that multiple model maintenance is one of the main challenges in dl software deployment and maintenance in the server side.
finally we want to mention a specific configuration issue in this category i.e.
configuration of batching .
to process requests in batches developers need to configure relevant parameters manually.
we observe this issue in .
of questions e.g.
i know that the batch.config file needs to be fine tuned a bunch by hand and i have messed with it a lot and tuned numbers around but nothing seems to actually effect runtimes .
.
remaining challenges in mobile .
.
dl library compilation.
this category includes challenges in compiling dl libraries for target mobile devices and covers .
of questions in mobile .
since core ml is well supported by ios developers can use core ml directly without installing or building it.
for tf lite pre built libraries are officially provided for developers convenience.
however developers still need to compile tf lite from source code by themselves in some cases e.g.
deploying models containing unsupported operators .
since the operators supported by tf lite are still insufficient to meet developers demand developers sometimes need to register unsupported operators manually to add them into the run time library.
it may be challenging for developers who are unfamiliar with tf lite.
in addition for compilation developers need to configure build command lines and edit configuration files i.e.
build configuration .
wrong configurations can result in build failure or library incompatibility with target platforms.
.
.
dl integration into projects.
this category includes challenges in integrating dl libraries and models into mobile software projects.
it accounts for .
in mobile .
to integrate dl libraries and build projects developers need to edit build configuration files i.e.
build configuration being a common challenge .
for both android and ios developers.
to integrate dl models into projects developers face challenges in importing and loading models .
.a comprehensive study on challenges in deploying deep learning based software esec fse november virtual event usa for example in an xcode project for ios developers can drag a model into the project navigator and then xcode can parse and import the model automatically .
however some developers encounter errors during this process .
when it comes to an android project the importing process is more complicated.
for instance if developers load a tf lite model with c or java they need to set the information e.g.
datatype and size of input and output tensors manually .
but some developers fail in this configuration .
in addition developers have difficulty in the thread management .
when integrating dl models into projects e.g.
i am building an android application that has three threads running three different models would it be possible to still enable inter op parallelism threads and set to for a quad core device .
.
remaining challenges in browser model loading.
this category includes challenges in loading dl models in browsers being the most common challenges in browser deployment accounting for .
of questions .
for browsers tf.js provides a tf.loadlayersmodel method to support loading models from local storage http endpoints and indexeddb.
among the three ways we observe that the main challenge lies in loading from local storage .
.
in the official document of tf.js local storage refers to the browser s local storage which is interpreted in a hyperlink contained in the document as that the stored data is saved across browser sessions .
however nearly all bad cases inloading from local storage attempt to load models from local file systems.
in fact tf.loadlayersmodel uses the fetch method under the hood.
fetch is used to get a file served by a server and cannot be used directly with local files.
to work with local files developers first need to serve them on a server.
in addition many developers do not have a good grasp of the asynchronous loading .
.
in a scenario when a developer loads a dl model in chrome and then uses the model to make predictions she receives loadedmodel.predict is not a function error since the model has not been successfully loaded .
since model loading is an asynchronous process in tf.js developers need to either use await or.then to wait for the model to be completely loaded before using it for further actions.
.
unclear questions although unclear questions are not included in our taxonomy we also manually examine them to seek for some insights.
all unclear questions have no accepted answers and do not have informative discussions or question descriptions to help us determine the challenges behind the questions.
among these unclear questions report unexpected results or errors when making predictions using the deployed models.
however no anomalies occur at any phase before the phase of making predictions making it rather difficult to discover the underlying challenges.
in fact various issues can result in the errors or unexpected results in this phase.
take the server deployment as an example.
during the manual inspection we find that errors occurring in making predictions can be attributed to the improper handling of various challenges such as version incompatibility between libraries used for training and deployment i.e.
environment wrong specification of modelinformation i.e.
model export mismatched format of input data i.e.
data processing etc.
implications based on the preceding derived findings we next discuss our insights and some practical implications for developers researchers and dl framework vendors.
.
researchers as demonstrated in our study dl software deployment is gaining increasing attention from developers but developers encounter a spectrum of challenges and various unresolved issues.
these findings encourage researchers to develop technology to help developers meet these deployment challenges.
here we briefly discuss some potential opportunities to the research communities based on our results.
automated fault localization.
in section .
we find that of unclear questions report errors when making predictions and that various faults in different phases can result in such errors.
this finding indicates the difficulty in manually locating faults and highlights the needs for researchers to propose automated fault localization tools for dl software deployment.
similarly proactive alerting techniques can be proposed to inform developers about potential errors during the deployment process.
however monitoring and troubleshooting the deployment process is quite difficult because of myriad potential issues including hardware and software failures misconfigurations input data even simply unrealistic user expectations etc.
therefore we encourage researchers to conduct a systematic study to characterize the major types and root causes of faults occurring during deployment of dl software before developing the aforementioned automated tools.
automated configuration.
in our taxonomy many challenges are related to configuration e.g.
specification of model information and configuration of environment variables .
this observation motivates researchers to propose automated configuration techniques to simplify some deployment tasks for developers especially non experts.
in addition automated configuration checkers can be proposed to detect and diagnose misconfigurations based on analyzing the configuration logic requirements and constraints.
implications for other communities.
our results reveal some emerging needs of developers and can provide implications for other research communities such as systems andai.
for example some developers call for more quantization options see model quantization in model conversion.
to help improve current frameworks researchers from the ai community should propose more effective and efficient techniques for model quantization.
in addition to update models on mobile devices see model update system researchers need to propose effective techniques to support model update i.e.
re training on the devices with limited computation power.
.
developers targeted learning of required skills.
dl software deployment lies in the interaction between dl and se.
therefore dl software deployment requires developers with solid knowledge of both fields making this task quite challenging.
our taxonomy can serve as a checklist for developers with varying backgrounds motivating the developers to learn necessary knowledge before reallyesec fse november virtual event usa zhenpeng chen yanbin cao yuanqiang liu haoyu wang tao xie and xuanzhe liu deploying dl software.
for instance an android developer needs to learn necessary knowledge about dl before deploying dl software to mobile devices.
otherwise she may fail in the specification of information about dl models see specification of model information trained by dl developers or data scientists.
similarly when a dl developer who is not skillful in javascript deploys dl models on browsers she may directly load models from local file systems due to the misunderstanding of browsers local storage see section .
.
avoiding common pitfalls.
our study identifies some common pitfalls in dl software deployment.
developers should pay attention to these pitfalls and avoid them accordingly.
for instance when deploying dl software to target platforms developers should remember to migrate the pre processing code and pay attention to version compatibility.
better project management.
our taxonomy presents the distribution of different categories indicating which challenges developers have encountered more.
in a project that involves dl software deployment the project manager can use our taxonomy to assign a task where developers often have challenges e.g.
model conversion to a more knowledgeable developer.
.
framework vendors improving the usability of documentation.
as shown in our results many developers even have difficulty in the entire procedure of deployment i.e.
how to deploy dl software .
for instance such questions account for .
in mobile deployment.
as described earlier developers often complain about the poor documentation in these questions revealing that the usability of relevant documentation should be improved.
specifically dl framework vendors can provide better detailed documentation and tutorials for developers reference.
in addition confused information organization such as hiding explanations of important concepts behind hyperlinks see section .
may result in developers misuse and thus should be avoided.
improving the completeness of documentation.
the prevalence of the conceptual questions category suggests that framework vendors should improve the completeness of the documentation especially considering that dl software deployment requires a wide set of background knowledge and skills.
indeed basic information that might look clear from the vendors perspective may not be easy to digest by the users i.e.
the developers .
the vendors should involve the users in the review of documentation in order to supplement necessary explanations of basic knowledge in the documentation.
this way might help in minimizing developers learning curve and avoiding misunderstanding.
improving the design of apis.
the quality of apis heavily influences the development experience of developers and even correlates with the success of applications that make use of the apis .
our study reveals some apis issues that need the attention of dl framework vendors.
for one functionality framework vendors may provide similar apis for various options see selection usage of apis making some developers confused in practice.
to mitigate this issue framework vendors should better distinguish these apis and clarify their use cases more clearly.
improving functionalities as needed.
we observe that many developers suffer from conversion and export of unsupported models in the deployment process.
for instance in mobile deployment .
of questions are about this challenge.
since it is impractical for framework vendors to support all possible operators at once we suggest that framework vendors can mine so and github to collect related issues reported by developers and then first meet those most urgent operators and models.
threats to validity in this section we discuss some threats to the validity of our study.
selection of tags and keywords.
our automated identification of relevant questions is based on pre selected tags and keywordmatching mechanisms and thus may result in potential research bias.
for tags we mainly follow previous related work to determine which ones to choose.
moreover all tags that we use are about popular frameworks or platforms promising the representativeness of the questions used in this study.
however it is still possible that in other contexts developers discuss issues that we do not encounter.
in addition the keyword matching identification may result in the retrieval of false positives and the loss of posts that do not contain explicit keywords.
the false positives are discarded during our manual examination of data for r3 so false positives do not affect the precision of our final taxonomy.
however due to the large amount of data used for r1 and r2 we do not employ similar manual examination to remove false positives for the two research questions and thus may cause the results of r1 and r2 to be biased.
furthermore although our identified posts with explicit keywords are more representative compared to the implicit posts loss of implicit posts may introduce bias in our results.
selection of data source.
similar to previous studies our work uses so as the only data source to study the challenges that developers encounter.
as a result we may overlook valuable insights from other sources.
in future work we plan to extend our study to diverse data sources and conduct in depth interviews with researchers and practitioners to further validate our results.
however since so contains both novices and experts posts we believe that our results are still valid.
subjectivity of inspection.
the manual analysis in this study presents threats to the validity of our taxonomy.
to minimize this threat two authors are involved in inspecting cases and finally reach agreement with the help of three experienced arbitrators through discussions.
the inter rater agreement is relatively high demonstrating the reliability of the coding schema and procedure.
related work in this section we summarize related studies to position our work within the literature.
challenges that ml dl poses for se.
the rapid development of ml technologies poses new challenges for software developers.
to characterize these challenges thung et al.
collect and analyze bugs in ml systems to study bug severity efforts needed to fix bugs and bug impacts.
alshangiti et al.
demonstrate that ml questions are more difficult to answer than other questions on so and that model deployment is most challenging across all ml phases.
in addition alshangiti et al.
find that dl related topics are most popular among the ml related questions.
in recent years several studies focus on the challenges in dl.
by inspecting dl related posts on so zhang et al.
find that program crashes a comprehensive study on challenges in deploying deep learning based software esec fse november virtual event usa model deployment and implementation questions are the top three most frequently asked questions.
besides several studies characterize faults in software that makes use of dl frameworks.
zhang et al.
collect bugs in tf programs from so and github.
by manual examination zhang et al.
categorize the symptoms and root causes of these bugs and propose strategies to detect and locate dl bugs.
following this work islam et al.
and humbatova et al.
extend the scope to the bugs in programs written based on the top five popular dl frameworks to present more comprehensive results.
inspired by these previous studies we also aim to investigate the challenges that dl poses for se.
however different from these previous studies our study focuses on the deployment process of dl software.
dl software deployment.
to make dl software really accessible for users developers need to deploy dl software to different platforms according to various application scenarios.
a popular way is to deploy dl software to server cloud platforms and then the dl functionality can be accessed as services.
for this deployment way cummaudo et al.
analyze pain points that developers face when using these services.
in other words cummaudo et al.
focus on challenges that occur after the deployment of dl software.
different from this work our study focuses on challenges in the deployment process.
in addition mobile devices have created great opportunities for dl software.
researchers have built numerous dl software applications on mobile devices and proposed various optimization techniques e.g.
model compression and cloud offloading for deploying dl software to mobile platforms.
to bridge the knowledge gap between research and practice xu et al.
conduct the first empirical study on largescale android apps to demystify how dl techniques are adopted in the wild.
in addition in recent years various javascript based dl frameworks have been published to enable dl powered web applications in browsers.
to investigate what and how well we can do with these frameworks ma et al.
select seven javascript based frameworks and measure their performance gap when running different dl tasks on chrome.
the findings show that dl in browsers is still at dawn.
recently guo et al.
put attention on dl software deployment across different platforms and investigate the performance gap when the trained dl models are migrated from pc to mobile devices and web browsers.
the findings unveil that the deployment still suffers from compatibility and reliability issues.
despite these previous efforts specific challenges in deploying dl software are still under investigated and thus our study aims to fill this knowledge gap.
conclusion based on so posts related to dl software deployment in this paper we have presented study findings to show that this task is becoming increasingly popular among software engineers.
furthermore our findings demonstrate that dl software deployment is more challenging than other aspects of dl software and even other challenging topics in se such as big data and concurrency motivating us to identify the specific challenges behind dl software deployment.
to this end we manually inspect sampled so posts to derive a taxonomy of challenges encountered by developers in dl software deployment.
finally we qualitatively discuss ourfindings and infer implications for researchers developers and dl framework vendors with the goal of highlighting good practices and valuable research avenues in deploying dl software.