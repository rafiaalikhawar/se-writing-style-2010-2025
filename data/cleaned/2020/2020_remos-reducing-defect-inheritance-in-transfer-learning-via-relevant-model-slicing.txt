remos reducing defect inheritance in transfer learning via relevant mo del s licing ziqi zhang key laboratory of high confidence software technologies moe school of computer science peking university beijing china ziqi zhang pku.edu.cny uanchun li institute for ai industry research air tsinghua university beijing china liyuanchun air.tsinghua.edu.cnjindong wang microsoft research beijing china jindong.wang microsoft.com bingyan liu key laboratory of high confidence software technologies moe school of computer science peking university beijing china lby cs pku.edu.cnding li key laboratory of high confidence software technologies moe school of computer science peking university beijing china ding li pku.edu.cny ao guo key laboratory of high confidence software technologies moe school of computer science peking university beijing china yaoguo pku.edu.cn xiangqun chen key laboratory of high confidence software technologies moe school of computer science peking university beijing china cherry sei.pku.edu.cny unxin liu institute for ai industry research air tsinghua university beijing china liuyunxin air.tsinghua.edu.cn abstract transfer learning is a popular software reuse technique in the deep learning community that enables developers to build custom models students based on sophisticated pretrained models teachers .
however like vulnerability inheritance in traditional software reuse some defects in the teacher model may also be inherited by students such as well known adversarial vulnerabilities and backdoors.
reducing such defects is challenging since the student is unaware of how the teacher is trained and or attacked.
in this paper we propose remos a relevant model slicing technique to reduce defect inheritance during transfer learning while retaining useful knowledge from the teacher model.
specifically remos computes a model slice a subset of model weights that is relevant to the student task based on the neuron coverage information obtained by profiling the teacher work is done while ziqi zhang was an intern and y uanchun li was a researcher at microsoft.
correspondence to y uanchun li y ao guo.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh p a usa association for computing machinery.
acm isbn .
.
.
.
on the student task.
only the relevant slice is used to fine tune the student model while the irrelevant weights are retrained from scratch to minimize the risk of inheriting defects.
our experiments on seven dnn defects four dnn models and eight datasets demonstrate that remos can reduce inherited defects effectively by to for cv tasks and by to for nlp tasks and efficiently with minimal sacrifice of accuracy on average .
ccs concepts computing methodologies neural networks software and its engineering dynamic analysis security and privacy software security engineering.
keywords program slicing deep neural networks relevant slicing acm reference format ziqi zhang y uanchun li jindong wang bingyan liu ding li y ao guo xiangqun chen and y unxin liu.
.
remos reducing defect inheritance in transfer learning via relevant mo del slicing .
in 44th international conference on software engineering icse may pittsburgh p a usa.
acm new y ork ny usa pages.
.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh p a usa ziqi zhang y uanchun li jindong wang bingyan liu ding li y ao guo xiangqun chen and y unxin liu !
!
!
figure defect inheritance is a common problem in both traditional software reuse and dnn model reuse.
introduction software reuse is popular in traditional programs with examples ranging from copy pasting a piece of code inheriting aparent class to importing a third party library.
despite the great convenience of software reuse the vulnerability of the reused software is one of the most concerning issues .
developers are often unaware of or unable to eliminate the vulnerabilities in thereused code which may lead to unexpected consequences.
for instance the heartbleed bug in the openssl library has led to severe security issues in many applications built upon it .
the opensource nature of many reusable packages increases the attack surface since the attackers can inspect the reused code thoroughly to find flaws to exploit.
the threats and countermeasures of reusing vulnerable modules have been studied by security researchers for a long time .
the deep neural network dnn for short is now considered a special type of software with great breakthroughs in recent years.
to reuse existing models and expedite model development transfer learning has been proposed to reuse the knowledge in one model to build new models.
a typical transfer learning process includes acquiring a public pretrained model e.g.
resnet o r bert adjusting the model architecture and fine tuning the model using target domain data .
borrowing names from the prior literature we call the pretrained models as teachers and the new models built upon them as students .
thanks to the wide availability of sophisticated public models transfer learning is nowadays the default choice for most developers when building their own deep learning applications.
however similar to traditional software reuse transfer learning also leads to concerns about defect inheritance.
figure illustrates the defect inheritance problem in traditional software reuse and model reuse.
in software reuse the developers download the public library and use it in their applications.
similarly in model reuse the developers download the public pretrained model and use transferlearning to obtain their customized models.
in both cases defectsmay be inherited during the software reuse process.
an attackercan download the public library model analyze it to find the defects and attack the deployed application model with the carefully crafted inputs .
the se community takes the security risk of dnns as an important problem as dnn models are extensively usedtable potential defects in the literature that may inherit during transfer learning.
task defect type inheritance rate cvadv ersarial penultimate layer guided .
vulnerability neuron coverage guided .
backdoor latent data poisoning .
nlpadv ersarial greedy word swap .
vulerability word importance ranking .
backdoordata poisoning .
weight poisoning .
in various industrial and research scenarios.
on the one hand dnns are considered as an emerging type of software artifact widely known as software .
.
the widely adapted model reuse is a type of software reuse for dl developers.
on the other hand nowadays dnns are frequently embedded into software applications such as autonomous driving and facial recognition.
the safety of dl models is crucial for the reliability of relevant software.
we surveyed existing literature and summarized seven typical defects that may be inherited during transfer learning.
as shown in table these defects can be categorized into two groups adversarial vulnerability and backdoor.
adversarial vulnerability is an exploratory defect where an attacker can obtain adversarial examples from the public teacher model and fool the students .
backdoor is a causative defect where hidden malicious logic is injected into the teacher model purposely and the student models built upon it may also misbehave under certain backdoor triggers .
according to our measurement study the defects can easily be propagated from the teacher to the students with the inheritance rate ranging from .
to .
.
a major reason for dnn defect inheritance is the indiscriminate training process .
specifically in conventional transfer learning the student model is initialized thoroughly with the teacher s weights including the weights for both student related and defect relateddecision boundaries.
thus the defect related decision boundaries are largely kept during fine tuning since the student dataset is smallscale and irrelevant to the defect related decision boundaries .
the focus of this paper is how to reduce the inherited defects while preserving student related knowledge.
there are mainly two types of solutions for developers to reduce inherited defects and enhance deep learning software reliability and safety.
one is to adopt defect mitigation techniques such as finepruning etc.
after fine tuning which we call fix after transfer approaches.
another is the fix before transfer approach which first randomly initializes the student model then extracts the studentrelated knowledge from the teacher.
the main problem of fix aftertransfer approaches is the poor effectiveness since it is difficult for the student model to remove the inherited defects with limitedtraining data.
the fix before transfer approach is effective in reducing inherited defects but it sacrifices the performance on the student task and applicability because the student model has to be trained from scratch with complicated configurations.
in this paper we try to address the defect inheritance problem from a different angle.
we observe that a teacher model is trained to serve multiple downstream tasks.
however a student model is often authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
remos reducing defect inheritance in t ransfer learning via re levant mo del s licing icse may pittsburgh p a usa trained for a specific task.
thus the teacher model may contain a substantial amount of weights that are not useful for the specific task that the student model tries to address.
having these irrelevant weights enlarges the attack surfaces for backdoors and increases the probability of exposing defects.
based on this observation we propose to selectively reuse the teacher knowledge that is relevant to the student task.
the idea is motivated by relevant slicing which computes a set of program statements that may affect the slicing criterion.
our intuition is similar to the conventional software engineering practice only reusing the relevant code of a program can decrease the probability of having bugs .
however due to the lack of the interpretability of dnn models computing the relevant slice is not easy.
for traditional software the functionalities are specified by the developers through linesof code and the relevance of each line of code can be explicitly computed .
on the contrary the development process of dnn models is quite different.
the decision logic of a model is hidden in millions of weights learned from data.
the semantic meaning of each weight is not understandable making it difficult to compute the relevant part.
to solve the aforementioned problems we introduce a profilingbased approach named remos relevant mo delslicing that is guided by neuron coverage to compute the relevant slice of the pre trained model on the student task.
the relevant slice is further used in fine tuning to reduce the inherited defects.
remos consists of four steps coverage frequency profiling ordinal score computation relevant slice generation and fine tuning.
the first stage records the neuron coverage frequency by iterating all data samples and computes the weight coverage frequency as the fine grained relevance tothe student task.
ordinal score computation combines the weight significance inside the teacher model measured by the magnitude and the relevance to the student task measured by the weight coverage frequency .
the last step finds the model parts that are relevant to the student task and resets the excluded weights.
at last the relevant part of the teacher model is used to initialize the student model and it is fine tuned with the student dataset.
we evaluate remos with both cv and nlp tasks on four models eight datasets and on all seven inherited defects in table .
forcv models the inherited defects are reduced by to for adversarial vulnerability and for backdoors with less than accuracy sacrifice.
for nlp models the inherited defects are reduced by for adversarial vulnerability and to for backdoors with less than accuracy drop.
we also analyze the distribution of the excluded weights of the relevant model slice and find that it mainly slices the high level weights that are irrelevant to the student task.
this paper makes the following research contributions we study the defect inheritance problem in the transfer learning scenario summarize common defects in the existing literature and quantify the inheritance rate of each kind of defect.
we propose remos to reduce defects inherited from teacher models during transfer learning.
our method is inspired byrelevant slicing in traditional software which computes a subset of parameters in the teacher model that are relevant to student tasks while excluding the irrelevant ones to reduce teacher originated defects.
we evaluate remos against seven different types of dnn defects in comparison with several state of the art approaches on various popular dnn architectures and datasets.
the re sults have demonstrated the effectiveness and efficiency of our approach.
problem formulation in this section we formulate the defect inheritance problem describe the threat model and define the defender s goal.
.
dnn defects the dnn defects considered in this paper are defined as follows definition .
dnn defects are the deviation of the actual and expected results of a dnn model produced by certain input samples.
specifically we focus on two typical dnn defects that are widely discussed in the literature including adversarial vulnerability and backdoor.
they can be triggered by adversarial inputs and backdoor inputs respectively.
adversarial vulnerability means that a dnn is vulnerable to adversarial inputs that are generated by adding special noise to normal inputs and lead to prediction errors .
the most common way to generate adversarial inputs is to use gradient ascent techniques on the white box model .
in the software engineering community this vulnerability is viewed as a robustness issue and many techniques have been proposed to test improve or verify the dnn robustness.
abackdoor or a trojan horse is a hidden pattern purposely injected into the model that produces unexpected output if a specific trigger is presented in the input.
backdoor inputs can be easilysynthesized by attaching a backdoor trigger to normal inputs.
themost straightforward way to implant backdoor is data poisoning i.e.
adding carefully designed samples into the training data to let the model memorize misleading patterns.
recently researchers have extended the attacks to make the backdoor more effective evasive and transferable .
the two aforementioned defects share a similar goal to disturb the model output under a limited perturbation budget.
this budget is usually computed by a distance function d which measures the difference between a benign input and a malicious input.
the goal of the adversary is to generate a malicious sample x x argmax x primej f x prime wt y s.t.d x prime x where jis the loss function and is a pre defined small value.
for the adversarial vulnerability the distance function is usually the p norm distance between the two inputs d xadv x bardbl xadv x bardblp.
for backdoors the adversary attempts to generate malicious inputs by attaching a trigger sign onto normal inputs.
in this case the distance is the ratio of different input dimensions between xtriand x d xtri x i x .
in the community of se and ml these two defects and the corresponding attacks usually refer to the same thing so we will use the terms defects or vulnerabilities and attacks according to the different context.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh p a usa ziqi zhang y uanchun li jindong wang bingyan liu ding li y ao guo xiangqun chen and y unxin liu .
dnn defect inheritance unlike most existing work that discusses dnn defects of an individual model we focus on the defects propagating between models during transfer learning.
transfer learning scenario.
suppose a developer wants to train a model mswith a dataset ds.
to improve accuracy and reduce training time she decides to use transfer learning to build her model upon a public pretrained model mt.mtandmsare called the teacher model and the student model respectively.
the dataset dt that was used to train mtis called the teacher dataset which is significantly larger than the student dataset i.e.
dt greatermuch ds .
the most common transfer learning method is fine tuning where the developer changes the model output and uses the local student dataset to fine tune the model.
the change made on the model is slight because the knowledge in the teacher model is general enough.
we are interested in the dnn defect inheritance problem in transfer learning which is defined as follows definition .
dnn defect inheritance is the phenomenon that the input samples which can trigger defects mislead the output in the pretrained teacher model mtcan also produce misbehavior in the student model ms. since the teacher models are usually publicly available attackers can easily analyze them to find adversarial inputs or inject backdoors into them.
such teacher originated malicious inputs pose threats to the student models deployed in the real world even though the student models are not white box available.
threat model.
we assume the attacker has read and or write access white box to the pretrained teacher mt and thus he can infer or inject defects in the teacher model.
this assumption is reasonable because many pretrained models are publicly available .
the attacker can publish a malicious pretrained model as well.
we also assume the attacker has basic knowledge of the student model mssuch as the type of the task but has no read or write access to the student msor student dataset ds.
it is because the student models are usually protected and served as a black box.
based on the knowledge of the teacher model mt the attacker can generate malicious inputs that can fool the student model ms. .
goal and challenges defender s goal.
the defender in our scenario is the developer of the student model mswho is unaware of how the teacher model mt is trained.
her goal is to optimize the transfer learning protocol to reduce the defects inherited from mt reduce the misclassification rate on the malicious samples xthat are generated from mt while retaining the student related knowledge in mt.
figure displays the difference of inherited knowledge between conventional transfer learning left and transfer learning with remos right .
our objective is to reduce the common defects that are shared by both the teacher model and the student model.
because the pretrained teacher model is publicly available removing the shared defects will reduce the exposed attack surface of the student model and improve the security level.
specifically we want to achieve the following objectives figure the goal of remos is to reduce the common defectsshared by the teacher and the student.
effectiveness .
the inherited defects in the student model ms should be effectively reduced i.e.
reducing the inheritance rate of the adversarial samples and backdoor samples.
accuracy .
the student model msshould still be able to reuse the student related knowledge in mtto achieve high accuracy on the student dataset ds.
efficiency and utility .
since efficiency and ease of use are major reasons why developers use transfer learning the new protocol should not bring too much overhead or introduce too much complexity.
reducing the inherited defects is challenging mainly due to the limited interpretability of dnns.
a model is usually viewed asa black box whose knowledge can only be changed by feedingdifferent training data.
it is unclear how to identify and separatedefect related knowledge from student related knowledge during transfer learning.
relev ant model slicing inspired by the relevant slicing technique in traditional programs we believe a similar technique for dnn models can help distinguish student related and potential defect related knowledge in a model and further mitigate the defect inheritance problem in transfer learning.
.
relevant slicing for traditional programs relevant slicing is an important slicing technique and has many applications such as debugging regression testing and prioritizing test case .
we first give the formal definition of the relevant slice as follows definition .
relevant slicing given a program pand a slicing criterion a test case tand a target statement s relevant slicing is to compute a subset of program statements that influence or have the potential to influence the statement sduring the execution oft.
relevant slicing can be used to reduce vulnerability in traditional programs.
an example is shown in figure .
suppose the code snippet on the left is a public library function that contains a potential defect.
if the input bis a dividebyzero error may occur at line .
if this piece of code is used without modification in a custom application the bug may be exploited by attackers to pose threats to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
remos reducing defect inheritance in t ransfer learning via re levant mo del s licing icse may pittsburgh p a usa figure an example code snippet left and the relevant slice with respect to the criterion a b w right .
the application.
however with the relevant slice the statements that are not related to the custom application can be removed to reduce the attack surface.
suppose we only care about a limited number of input cases in the custom application thus we can compute the relevant slice concerning the criterion whose inputs are the interested cases.
for example if the interested input case is a b and the criterion is a b w the corresponding relevant slice is shown in the right code snippet of figure .
in the custom application using the original library code left or the relevant slice right does not influence the correctness of the program the output wat line .
however the latter can remove the inherited defect and avoid the potential security risk dividebyzero error .
.
relevant slicing for dnn models similar to the example of reducing inherited defects in traditionalprograms through relevant slicing we think it might be helpful to adapt the concept of relevant slicing to dnn models to reduce defect inheritance.
the idea of slicing a dnn model is not new.
zhang et al.
proposed to represent the decision process of a dnn with a slice a subset of critical neurons and synapses that made a greater contribution to the prediction and used the computed slices to detect adversarial inputs.
although our goal is different we could follow their definition and customize it based on our scenario definition .
relevant model slicing given a dnn model mand a target domain dataset d relevant model slicing is to compute a subset of model weights that are more relevant bounded by a threshold to the inference of samples in dand less relevant to the samples outside d. if we could obtain such a relevant model slice we can use it to reduce inherited defects in transfer learning just like using relevant program slices to reduce potential vulnerability in traditional software.
specifically we can transfer the knowledge in the relevant slice to the students to retain useful knowledge whiling removing the rest of irrelevant parts to avoid the defect related knowledgepropagating into the student model.
the goal can be formally described as finding a subset of the teacher model s weights w wt that when wis used in transfer learning can maximize the accuracy of the student model on both clean data and malicious data that isgenerated from the teacher model.
max w wt summationdisplay.
x y dsi i where t represents the transfer learning process and t w is the weights after training.
i is the indicator function.
the concept of finding a subset of weights can also be found in model pruning approaches that are mainly used to reduce the model size.
for example magnitude based pruning proposes to trim the small magnitude weights to save computation cost.
however such a subset is not a relevant slice because the weights with larger magnitudes are not necessarily only relevant to the target studenttask.
they are relevant to all the domains that are included by the teacher model s domain.
our approach computing the relevant slice as defined in section .
by directly solving equation is difficult due to the unpredictable dnn training process and unavailable prior knowledge about defects.
in this section we introduce a heuristic algorithm to approximately findthe relevant model slice and use it to reduce defect inheritance in transfer learning.
.
overview the workflow of remos consists of four steps coverage frequency profiling ordinal score computation relevant slice generation and fine tuning.
an illustration with a three layer neural network isshown in figure .
for simplicity we assume there are three data samples in the student dataset.
coverage frequency profiling computes the coverage frequency of each weight which represents the relevance to the student task.
the weight coverage frequency is computed based on the neuron coverage frequency profiled over the student dataset.
the first stage of figure displays the neuron coverage frequency in the neurons and the weight coverage frequency on the weights .
the ordinal score computation step computes a score for each weight based on the static information of the teacher model weight magnitude and the dynamic information on the student task weight coverage frequency .
because the range of weight magnitude and the coverage frequency is different remos uses the ordinal score as the yardstick.
in the second stage of figure the ordinal score is displayed in the pink circles on each weight.
relevant slice generation identifies the relevant slice based on the ordinal scores.
the weights with large scores are included in the slice and others are reset.
in the third stage of figure the size of the slice is constrained to be nine.
therefore the connections with red circles are excluded by the slice and are reset.
finally the fine tuning step uses the student dataset to train the model as displayed in the last stage of figure .
this step is the same as the conventional transfer learning except that the student model is initialized with the computed relevant slice.
we then explain why remos can achieve the three objectives in section .
first because remos only contains the weights that are frequently covered by the student dataset the relevance between the slice and the student task is high.
the accuracy of the student model is not harmed and can be recovered by a fast fine tuning phase.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh p a usa ziqi zhang y uanchun li jindong wang bingyan liu ding li y ao guo xiangqun chen and y unxin liu figure the workflow of transfer learning with remos.
second the weights that are less relevant to the student domain are excluded from transfer learning including the large magnitude weights that are potentially more relevant to the defects in the teacher model.
this design improves the effectiveness of reducing defect inheritance.
third in the implementation the relevant slice accounts for a large portion of the teacher model.
most of the student model is initialized by the teacher model which leads to higher efficiency when tuning the student model.
at last our slicing algorithm is not restricted to any specific operator or model architecture.
it can be applied to both convolutional neural networks and transformerbased nlp models.
.
coverage frequency profiling we first illustrate the formulation of the dnn model.
a deep neural network model can be abstracted to be a set of layers m l1 l2 lk where kis the number of layers called depth .
l1 is the input layer.
each layer lkconsists of several neurons lk nk nk nk tk andtkdenotes the number of neurons in the layer lk.
layer lk k contains a weight matrix wk rtk tk that connects all the neurons of the preceding layer to the current layer.
each value weight wk i j wkconnects the neuron nk ito the neuron nk j. for presentation simplicity we mainly concentrates on the linear layers in dnn models to describe our methodology.
however our technique is also applicable to other model architectures as shown in the experiments of section .
lethkdenote the neuron activation internal feature of the k th layer.
hk iis the activation value of neuron nk i. the computation process of each layer can be formulated as omitting the bias term for simplicity hk relu wk hk where relu is a popular activation function.
we define the output of the model mon the input xas the set of all internal features run m x h2 h3 hk .
we then define the coverage metric of dnn.
we focus on the widely used neuron coverage nc .
let cov represent the neuron coverage function which takes the internal features as input andoutputs the indexes of the covered neuron.
the neuron coverage function finds the neurons whose activation values are larger than athreshold .
the neuron coverage on the sample xis formulated as cov x cov run m x cov h1 h2 hk vi vi i .
each vector virepresents which neurons at layer liare covered by the data x. remos first iterates the student dataset and records the coverage information for each sample.
then the neuron coverage frequency on the dataset dsis computed as cov ds summationdisplay.
x dscov x i i k .
the coverage frequency of the weight wk i j is the sum of the neuron coverage frequency of two neurons that this weight connects covw ds k i j cov ds k i cov ds k j the intuition is that if one neuron is frequently covered by ds the relevant weights should be frequently covered as well.
because the student dataset is often small profiling does not cost much time or computation resources.
.
ordinal score computation this step computes the ordinal scores to determine which weights should be included in the relevant slice.
the score combines thestatic information of the teacher model weight magnitude andthe dynamic information on the student task weight coverage frequency .
under the premise that the knowledge in the slice is sufficient for fine tuning we want the slice to contain as little relevant knowledge as possible.
in this way the risk of containing defect related knowledge is reduced.
the weight magnitude is an important measurement of the teacher s knowledge the larger the weight magnitude is the more significant it is to the teacher .
therefore we use the absolute weight values inside the slice as an indicator of the amount of teacher knowledge.
we formulate the objective of remos as wremos argmax w wtacc t w ds summationdisplay.
w w w our heuristic solution to the above function is to select the weights with small magnitude and large student task relevance.
however one challenge of combining the two indexes is the different value range.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
remos reducing defect inheritance in t ransfer learning via re levant mo del s licing icse may pittsburgh p a usa take resnet18 for example the range of magnitude is .
the range of weight coverage frequency on the mit indoor scenes dataset i s .
to solve this problem remos uses the order of one weight inside the whole model weight set as a unified metric.
formally for the weight wk i j the ordinal magnitude score and the ordinal coverage score is computed by ord magk i j rank wk i j ord covk i j rank covw ds k i j where rank computes the index of the value in the ascending sorted array of all weights.
the ordinal magnitude score ord mag is the position of wk i j in the sorted array.
the ordinal coverage score ord cov is the position of the weight coverage frequency covw ds k i j .
the next step combines the two types of ordinal scores.
the intuition is that the weights with large ordinal coverage scores and small ordinal magnitude scores should be included in the relevant slice.
thus we set the overall ordinal score as the ordinal coverage score minus the ordinal weight score ordk i j ord covk i j ord magk i j. .
relevant slice generation this step identifies which weights should be included in the slice based on the ordinal scores.
specifically we rank the weights by the ordinal score and the weights with larger scores are selected as the relevant slice i.e.
slice ds wk i j ordk i j t .
where is a hyper parameter representing the ratio of weights to be selected into the relevant slice i.e.
w wt .t is the ordinal score threshold to control the size of the slice.
finally the weights in slice ds are used to initialize the student model.
the weights outside of slice ds are regarded as less relevant to the student task and are set to zero to forget defect related knowledge in the teacher.
the initialized student model is fine tuned with the student dataset just like the normal transfer learning process to generate the final student model.
ev aluation to evaluate remos we mainly focus on the following aspects defect mitigation effectiveness how effective is remos to mitigate the inheritance of common defects?
how much accuracy needs to be sacrificed?
how does it compare to other techniques?
in section .
and section .
generalizability is remos general enough for different transfer learning tasks?
we test our method on both cv and nlp tasks different models different datasets and different defects to evaluate the generalizability.
efficiency how much time does remos spend to get a satisfactory student model?
how is its efficiency compared to conventional transfer learning?
in section .
interpretability how is the model trained by remos different from other student models?
why is remos effective and efficient?
in section .
.
experimental setup overall methodology.
in our experiments we directly adopted various attacks in the prior literature to evaluate remos.
we first downloaded pretrained models from the internet as the teacher models.
then we used our technique and other baseline methods to train student models from the teachers.
we followed the prior attack literature acted as the attackers who have access to the teacher models and tried to generate malicious inputs that can fool the student models.
these inputs were used to measure how many defects each student has inherited from the teacher if more defects are inherited the student will have a higher error rate on the malicious inputs.
the models datasets and defects used for cv tasks and nlp tasks are different which will be introduced in the corresponding subsections.
implementation we implement remos on pytorch .
for the ease of customizing dl training procedures.
for the neuroncoverage implementation we slightly changed the code base of evaldnn to profile the coverage frequency.
evaluation metrics.
there are mainly two metrics compared across different student training techniques in our experiments including accuracy acc and defect inheritance rate dir .
the accuracy is computed by acc mean x y dsi .
the defect inheritance rate is computed as the misclassification rate on the malicious dataset dm i.e.dir mean x y dmi .dmis formed by the generated misclassified samples from ds.
the accuracy and defect inheritance rate may differ on different datasets making it difficult to directly compare the performance of different student training techniques.
thus we additionally intro duce the relative accuracy and relative defect inheritance rate forcomparison.
suppose accs ftis the accuracy achieved by the conventional fine tuning technique on student task sandsis the set of student tasks.
for each student training technique the mean relative accuracy is computed as raccm means s accs m accs ft i.e.
the mean accuracy compared to the traditional fine tuning technique and the mean relative defect inheritance rate is rdirm means s dirs m dirs ft .
.
defect mitigation on cv tasks this section and section .
discuss the effectiveness of remos on cv and nlp tasks respectively.
setup.
models and datasets we used two state of the art largescale cnn models resnet18 and resnet50 on five popular transfer learning datasets including mit indoor scenes caltech ucsd birds category flowers stanford actions and stanford dogs .
the selected datasets are representative with the number of classes ranges from to .
the number of parameters of the two models are 11m and 25m .
baselines.
we compare remos with six representative baselines including fine tuning retraining delta mag pruning delta r and renofeation.
among them fine tuning and retraining do not incorporate any defense at all.
delta is a technique that is widely used by deep learning developers to improve accuracy .
this technique encourages the student model to have similar feature maps as the teacher model.
mag pruning is a state of the art fix aftertransfer approach for backdoor removal .
this technique prunes authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh p a usa ziqi zhang y uanchun li jindong wang bingyan liu ding li y ao guo xiangqun chen and y unxin liu the small magnitude weights.
we followed the implementation of and set the pruning ratio to .
delta r and renofeation are two representative fix beforetransfer techniques.
delta r similar to delta cooperates feature regulation to learn from the teacher.
renofeation is a recent work to mitigate adversarial vulnerability inheritance on cv tasks which cooperates feature regulation internal dropout and stochastic weighted average to train the student model.
we used the configurations learning rate momentum etc.
that can achieve optimal accuracy on the student tasks.
for the fixafter transfer techniques we set the learning rate to 1e .
for the fix before transfer we set a larger learning rate to 1e .
we set the number of training iterations as 30k and keep it the same across all techniques.
defect simulation.
to simulate the situation of defect inheritance we need to generate malicious inputs based on the teacher model and test whether they lead to misclassification on the student model.
we consider two kinds of adversarial attacks including the penultimate layer guided adversarial attacks and the neuroncoverage guided adversarial attacks.
for the former one we followedthe state of the art attack introduced by rezaei et al.
to generate a set of malicious inputs dm.
we selected this attack because it is one of the known strongest attacks in white box setting .
for the neuron coverage guided attacks motivated by the prior work on dnn testing we combined various coverage metrics and different strategies to generate dm.
the coverage metrics include neuron coverage nc top k neuron coverage tknc and strong neuron activation coverage snac .
tknc covers the neurons that have the highest kactivation values we select kas follows .
snac covers the neurons that the activation value is high enough inside the layer.
the strategies include deepx plore randomly selects inactivated neurons as the target neuronsto activate dlfuzz selects the most covered neurons and dlfuzz rr combines three strategies in a round robin manner .
for each sample we optimized the input gradient to cover as many internal neurons as possible.
we selected the last sample during the optimization procedure as the malicious sample.
the simulation of backdoor defects was slightly more complicated.
we followed the settings of latent backdoor et al.
and poisoned the training data of the teacher model.
we assumed theattacker had a training dataset of a similar data distribution to the student dataset.
some samples in the attacker s dataset were attached by a trigger sign e.g.
a firefox logo and labeled mistakenly onpurpose.
by tuning the pretrained model on the attacker s dataset we obtained a backdoored pretrained model.
then the backdoored pretrained model was used as the teacher to generate student models.
results.
the defect reduction effectiveness of remos on penultimatelayer guided adversarial samples neuron coverage guided adversarial samples and backdoor samples are shown in figure figure and figure respectively.
generally remos significantly reduces defect inheritance with minimal accuracy loss.
the discussion on each kind of defect is detailed below.
adversarial vulnerability.
we first discuss the two types of adversarial samples.
figure shows the defect mitigation results on penultimate layer guided adversarial samples.
we plot the results onall of the five datasets.
the first row displays the student accuracy and the second row shows the defect inheritance rate.
conventional transfer learning achieves high accuracy on the student datasets although at the cost of high dir s. in figure out of cases have higher dir than .
the phenomenon implies that the inheritance of adversarial vulnerability is practical and the developers should pay attention to it.
retraining has the lowest dir but the accuracy is significantly lower raccm .
.
delta can partially increase the accuracy in some cases.
however it s dir s are significantly higher over on resnet18 .
mag pruning is ineffective to reduce the inherited adversarial vulnerability rdirm .
which demonstrates that the low magnitude weights are not critical to the teacher model s knowledge including the defect related knowledge.
we can also observe that generally the dir s of resnet50 are lower than resnet18.
it s probably because that the increased model capacity helps to increase the robustness.
this observation aligns with the prior work .
remos only sacrifices less than model accuracy and reduces over inherited defects than the conventional fine tuning the rdirmis .
for resnet18 and .
for resnet50 .
besides in many cases the dir of remos are comparable with the students trained from scratch minimum value for dir .
the two fix before transfer techniques del t a r and renofeation can partially improve the accuracy of the student models based on retraining but the performance is still lower than the fine tuned students.
besides we can observe that such techniques have lower accuracies on resnet50 than resnet18.
we think the reason is that the increased complexity of resnet50 makes it more difficult to train on the small scale student dataset.
similarly the defect reduction results on neuron coverage guided adversarial samples are shown in figure .
for the fine tuned students the dir s averagely .
are lower than that of penultimatelayer guided samples averagely .
.
we think the reason isthat when generating adversarial samples targeting on the mid dle layers is less effective than targeting on the penultimate layer.
the latter can directly fool the last fc layer.
on the contrary the turbulence made on the intermediate layers may be corrected bysubsequent layers which are trained to have fault tolerance .
forneuron coverage guided samples the target layers contain a large number of intermediate layers.
it can also be observed that the inheritance rates differ between various coverage metrics and strategies.
for the coverage strategies the defects discovered by dlfuzz inherit most in transfer learning averagely .
.
it means that selecting the most covered neurons can generate easier to inherit adversarial samples.
for the coverage metrics tknc is the worst effective the average dir is .
.
we think the reason is that the top kneurons are too little compared to the enormous neurons inside the model.
remos reduces the inheritance rates in all cases by a large margin.
the rdirmis .
over the two datasets.
on average of inherited defects that are generated by the neuron coverage guided techniques can be eliminated by remos.
backdoor.
the backdoor mitigation result is shown in figure .
the average inheritance rate is .
for fine tuning and .
for mag pruning.
it means the backdoor knowledge is partially encoded in the low magnitude weights.
remos reduces the inherited defects to a similar level of retraining the rdirmis .
.
it means authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
remos reducing defect inheritance in t ransfer learning via re levant mo del s licing icse may pittsburgh p a usa figure the accuracy acc and adversarial vulnerability inheritance rate dir of remos and other student training techniques.
figure the inheritance rate of adversarial inputs generated by different neuron coverage guided test generators on resnet18.
figure the accuracy acc and backdoor inheritance rate dir of remos and other baselines on resnet50.
that the large magnitude weights excluded by remos are critical to the backdoor knowledge.
after resetting such weights the backdoor defect is mitigated by a large margin.
.
defect mitigation on nlp tasks setup.
models and datasets.
we used two popular models bert and roberta in the experiment .
remos can also be used for other nlp pretrained models.
the datasets are sst andimdb for backdoor attacks and sst and qnli for adversarial attacks following the prior settings .
baselines.
we took two baselines from the cv experiments conventional fine tuning and magnitude based pruning.
the other baselines are not designed for nlp models and cannot achieve reasonable accuracy thus were excluded from the comparison.
defect simulation.
the backdoor implementation followed the prior work which considers two scenarios full domain knowledge fdk and domain shift ds .
fdk means that the attacker knows the student dataset and uses the identical dataset to embed the backdoor.
ds means the attack dataset is different from the victim dataset.
we used sst and imdb as the attack dataset and the victim dataset to form the four scenarios.
we adopted two effective attack techniques from data poisoning and weight poisoning.
for adversarial vulnerability we assumed the fdk scenario.
we adopted two public implementations from textattack kuleshov and textfooler .
kuleshov uses a greedy word swap strategy to find the adversarial samples.
textfooler uses the greedy word importance ranking algorithm.
results.
the defect reduction results of remos on the backdoor and adversarial samples are displayed in table and table respectively.
the defect inheritance rate is reduced by and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh p a usa ziqi zhang y uanchun li jindong wang bingyan liu ding li y ao guo xiangqun chen and y unxin liu figure the convergence progress of conventional transfer learning retraining and remos.
for two kinds of backdoor samples.
remos can also reduce inherited adversarial vulnerabilities.
then we will summarize the results on the two defects.
backdoor.
as shown in table the backdoor attack is very effective and all dir s of fine tuning are over meaning that the backdoor inheritance problem in nlp tasks is more severe than that in cv .
we think there are two reasons.
first for nlp tasks the high dimensional trigger vector translated from the trigger word is more distinct than triggers cv tasks.
the difference between vectorsis larger than the difference between cv images pixel values are be tween and after normalized .
second nlp models contain much more parameters.
bert has 110m parameters while resnet50 has 25m parameters .
the more redundant parameters may be easier to convey the backdoor logic.
with remos the rdirmis .
for data poisoning and .
for weight poisoning and the relative accuracy raccmis about .
.
the rdirmof mag pruning is .
.
both remos and magpruning are less effective on nlp models than the cv models.
this phenomenon implies that reducing the nlp backdoor inheritance might be more difficult.
adversarial vulnerability.
table displays the results on the inherited adversarial vulnerability.
for simplicity we report the average dir over two attacks.
for fine tuned students the average dir is .
.
similar to the backdoor the dir s on nlp models are higher than the cv models.
mag pruning can partially reduce thedir therdirmis .
.
the rdirmof remos is much lower .
with slight accuracy drop raccm .
.
.
efficiency and overhead we compare the training efficiency of remos with fine tuning and retraining in figure .
the convergence speed of remos is close to fine tuning and significantly faster than retraining.
both remos and fine tuning achieve the optimal accuracy within minutes on the scenes dataset and within minutes on the birds dataset.
since the training process of remos does not introduce additional time cost the overhead of remos is just the slice computation time.
the profiling step in slice computation is similar to performing a forwarding pass with the training dataset which only takes several minutes.
after profiling computing the ordinal scores and selecting the relevant weights take less than a minute on resnet18 and about three minutes on resnet50.
meanwhile remos can be easily integrated into the conventional transfer learning pipeline without anyfigure the distribution of weight changes during training.figure the magnitude of weights excluded by remos in each layer.
task specific configuration.
therefore we believe remos is efficient and easy to use enough for student model developers.
.
interpreting the slice to further understand the efficiency and effectiveness of remos we inspect the weight pattern of the student model.
first we measured the weight changes of the student models before and after training.
specifically we computed the weight difference ws wtbetween the student model and the teacher model.
the result is shown infigure .
for conventional transfer learning fine tuning most of the weight changes are around .
.
it means that most weights do nothave to change too much to fit the student dataset.
retraining suffers from low performance because the weight changes are much larger and the small student dataset is not enough.
remos significantly reduces the number of weights that require large range adjustment thus the convergence speed is high.
we also studied the magnitude distribution of the weights excluded from the relevant slice i.e.
the weights that are reset to zero in remos .
the result is shown in figure .
as the layer depth increases more weights with higher magnitude are excluded.
this re sult is intuitive because in a dnn model the weights at deep layers are usually related to high level features thus may be less relevant to the specific student tasks.
instead some of them may contribute a lot to certain defect related decision logic.
remos excludes these weights from the student model thus it can reduce defect inheritance while retaining useful knowledge.
related work the transferability of both adversarial attacks and backdoor attacks has been extensively studied before.
wang et al.
and rezaei et al.
proposed to generate adversarial inputs targeting intermediate layers of the teacher models.
these techniques are similar to neuron coverage guided dnn testing techniques.
badnets and trojannn have demonstrated the backdoor attacks are inheritable in their experiments.
v arious approaches have also been proposed to make the backdoors more transferable.
similarly jiet al.
presented model reuse attacks wherein malicious primitive models may infect host ml systems .
to mitigate defect inheritance one way is to improve the robustness of teacher models and making the robustness transferable .
however in this paper we focus on the student model developers side and consider how to avoid inheriting defects from public teacher models that may be insecure.
a straightforward solution for student model developers to reduce defect inheritance is fix after transfer in which the developers authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
remos reducing defect inheritance in t ransfer learning via re levant mo del s licing icse may pittsburgh p a usa table the defect reduction effectiveness of remos against two backdoor attacks on nlp tasks.
for each model we include four situations where the attacker s dataset may be the same or different as the student dataset.
model datasetdata poisoning weight poisoning fine tune mag prune remos fine tune mag prune remos acc dir acc dir acc dir acc dir acc dir acc dir ber tfdksst to sst .
.
.
.
.
.
.
.
.
.
.
.
imdb to imdb .
.
.
.
.
.
.
.
.
.
.
.
dssst to imdb .
.
.
.
.
.
.
.
.
.
.
.
imdb to sst .
.
.
.
.
.
.
.
.
.
.
.
robertafdksst to sst .
.
.
.
.
.
.
.
.
.
.
.
imdb to imdb .
.
.
.
.
.
.
.
.
.
.
.
dssst to imdb .
.
.
.
.
.
.
.
.
.
.
.83imdb to sst .
.
.
.
.
.
.
.
.
.
.
.
average relative v alue .
.
.
.
.
.
.
.
table the accuracy acc and adversarial vulnerability inheritance rate dir of remos on nlp tasks.
model dataset fine tune mag prune remos ber tsst 2acc .
.
.
dir .
.
.
qnliacc .
.
.
dir .
.
.
robertasst 2acc .
.
.
dir .
.
.
qnliacc .
.
.
dir .
.
.
average relative v alueraccm .
.
rdirm .
.
first generate a student model following the standard transfer learning procedure and then use normal defect mitigation techniques to remove the defects.
however due to the shortage of data and limited knowledge about defects such techniques like adver sarial training are uneasy and ineffective.
fine pruning is probably the most easy to use method against backdoors.
nevertheless the experiments show that it can not guarantee effectiveness against adversarial vulnerabilities.
the fix before transfer approach initializes the student model from scratch and extracts knowledge from the teacher model during transfer learning.
this technique suffers from the performance of the student model and the utility.
renofeation uses knowledge distillation together with several heuristic tricks to recover the model accuracy.
nevertheless its performance is inferior and the utility is constrained to cnn models.
threat to v alidity and discussions one possible threat to validity is that the studied samples in theexperiment may not be representative.
we tried our best to mini mize this threat by covering as diverse vulnerabilities as possible including vulnerabilities on different domains cv and nlp different model architectures resnet and bert and different datasets eight datasets in total .
in this paper we focus on the scenario of two models the teacher and the student .
however there are three models in the real world backdoor setting.
the generalized teacher model is the publiclyrecognized model by the community.
this model can be fine tunedby an attacker to inject backdoor and become a domain teacher model.
the domain teacher model is published on the internet downloaded by a deep learning developer and further fine tuned into the domain student model which inherits the defects .
we simplify the three model setting into two models because from the dl developers perspective the generalized teacher and the domain teacher are essentially the same.
both of them may contain vulnerabilities that may be inherited by the developers student model.
for the retraining baseline we think it is not suitable for the orinary dl developers although at some point retraining achieves enough accuracy.
retraining large dl models from scratch requires abundant dataset multiple expensive gpu and various trainingtricks.
it is difficult for ordinary dl developers to meet all these requirements.
thus reusing public model on the internet becomes a widely accepted choice.
one intuitive solution to reduce the inherited defects is to dig into the teacher model and deal with the important vulnerabilities.because the teacher models are widely accessible on the internet there should exist common vulnerabilities that have been understood by the public.
these vulnerabilities are called universal adversarial vulnerability and only accounts for a small part of the adversarial vulnerabilities in the teacher model.
thus this solution maybe not effective enough to reduce as much inherited defects as possible.
conclusion this paper proposes remos a relevant model slicing technique to reduce defect inheritance during transfer learning.
remos only reusesthe relevant parts inside the teacher model and retrains the irrelevant parts to forget the defect related knowledge.
the relevant slice iscomputed based on the neuron coverage information by profiling the teacher model on the student dataset.
extensive experiments on seven dnn defects four dnn models and eight datasets demonstrate the effectiveness of the approach.
remos can reduce inherited defects by to for cv tasks and by to for nlp tasks with an accuracy sacrifice less than .