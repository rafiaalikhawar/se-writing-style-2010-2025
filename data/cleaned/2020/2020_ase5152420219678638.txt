unsupervised labeling and extraction of phrase based concepts in vulnerability descriptions sofonias yitagesu college of intelligence and computing tianjin university tianjin china sofoniasyitagesu yahoo.comzhenchang xing research school of computer science australian national university data61 csiro australia zhenchang.xing anu.edu.auxiaowang zhang college of intelligence and computing tianjin university tianjin china xiaowangzhang tju.edu.cn zhiyong feng college of intelligence and computing tianjin university tianjin china zyfeng tju.edu.cnxiaohong li college of intelligence and computing tianjin university tianjin china xiaohongli tju.edu.cnlinyi han college of intelligence and computing tianjin university tianjin china hanly2 foxmail.com abstract people usually describe the key characteristics of software vulnerabilities in natural language mixed with domainspecific names and concepts.
this textual nature poses a signifi cant challenge for the automatic analysis of vulnerabilities.
au tomatic extraction of key vulnerability aspects is highly desirablebut demands significant effort to manually label data for modeltraining.
in this paper we propose an unsupervised approachto label and extract important vulnerability concepts in texturalvulnerability descriptions tvds .
we focus on three types ofphrase based vulnerability concepts root cause attack vector and impact as they are much more difficult to label and extractthan name or number based entities i.e.
vendor product andversion .
our approach is based on a key observation that thesame type of phrases no matter how they differ in sentencestructures and phrase expressions usually share syntacticallysimilar paths in the sentence parsing trees.
therefore we proposetwo path representations absolute paths and relative paths anduse an auto encoder to encode such syntactic similarities.
toaddress the discrete nature of our paths we enhance traditionalvariational auto encoder v ae with gumble max trick forcategorical data distribution and thus creates a categorical v ae cav ae .
in the latent space of absolute and relative paths we further use fit tsne and clustering techniques to generateclusters of the same type of concepts.
our evaluation confirmsthe effectiveness of our cav ae for encoding path representationsand the accuracy of vulnerability concepts in the resultingclusters.
in a concept classification task our unsupervisedlylabeled vulnerability concepts outperform the two manuallylabeled datasets from previous work.
index t erms textual vulnerability descriptions vulnerability concepts unsupervised representation learning conceptlabeling and extraction.
i. i ntroduction software vulnerabilities once disclosed can be documented in security databases such as nvd ibmxforce exploitdb .
people usually describe the key characteristicsof a vulnerability in natural languages such as the examplesshown in fig.
.
key characteristics often include vulnerableproduct and versions product vendor and root cause attack corresponding author.
fig.
.
motivating example reported vulnerabilities in nvd vector and impact of the vulnerability.
although these vul nerability databases provide rich information about knownvulnerabilities security analysts have to manually identify andextract key information of their interests from textual vulnera bility descriptions tvd .
automatic information extraction ishighly desirable to expedite vulnerability analysis and securityresearch for example finding all vulnerabilities of a productwith certain impact or establishing traceability links betweenrelated vulnerabilities in different databases or detecting dis crepancies between vulnerability reports regarding the samevulnerability created by different people .
the desired automatic information extraction is faced with two challenges.
first tvds have large variations in bothsentence structure and phrase expression.
as the examplesin fig.
show whether a key entity or concept is men tioned or the order of different entities and concepts beingmentioned vary greatly from one tvd to another.
for exam ple tvd1 starts with buffer overflow root cause whileother tvds start with the vendor product or componentnames.
tvd1 tvd2 tvd3 have similar sentence structures but tvd4 tvd5 tvd6 have different sentence structures.
36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee furthermore aspect descriptions vary greatly in details and phrase expressions.
for example impact can be expressed allow to which result in or leading to .
all such sentence andphrase variations make it hard to develop a comprehensive setof linguistic rules for concept extraction in tvds.
machine learning techniques can learn distinct features from tvds for automatic information extraction.
for example dong et al.
trains the named entity recognition ner model for extracting product names and versions in tvds.however machine learning techniques require tvds withlabeled entities and concepts for model training.
unlike abun dant labeled general text there are no large datasets of labeledtvds for training information extraction models especiallyfor phrase based concepts like root cause attack vector andimpact.
as shown in fig.
tvds are full of software specificnames number sequences and domain specific terms.
modelstrained with only general text but not vulnerability descriptionssuffer from sharp performance degrade .
however manuallabeling a large corpus of tvds is a labor intensive task.
to develop novel ways to automatic information extraction in tvds we make two key observations over tvds.
first vendor product and version information are almost alwayspresent in tvds because they are the most basic informationfor identifying a vulnerability.
product and vendor are usuallydistinct names and known vulnerable product and vendornames are well archived in common platform enumeration cpe dictionary.
v ersions are number sequences and havedistinct orthographic features.
therefore it is easy to developa gazetteer or rules to precisely recognize vendor product and version in tvds.
second although the surface formsof the root cause attack vector and impact vary greatlyat both sentence and phase level the syntactic relationshipsbetween a specific type of phrase based concepts and ven dor product version are stable and manifest high regularities.such syntactic relationships can be encoded in similar pathsof part of speech pos tags in the sentence parsing tree.take tvd1 and tvd3 in fig.
as an example.
the rootcause buffer overflow and generates rsa key pairs areincompletely different sentence structures and have very dif ferent phrase expressions.
however their absolution paths inthe sentence parsing trees of tvd1 and tvd3 are root s np np and root s vp np np and their relative pathsfrom andres huggel and alibaba are np pp np pp np np and np pp np pp np np whichare very similar see section ii d for how we construct pathrepresentations .
based on the above two observations we propose an unsupervised method for labeling and extracting phrase based con cepts in tvds.
first we build a gazetteer based on cpe dic tionary and version orthographic features and use gazetteer based matching to identify vendor product and version intvds.
then we use a vulnerability specific pos tagger which can generate meaningful pos tags for tvds.
next weobtain the parsing tree of a tvd sentence.
for each terminalnoun phrase in the tree we obtain an absolute path of postags from tree root to this noun phrase and a relative pathof pos tags from each found vendor product version phraseto this noun phrase.
we focus on noun phrases because theyusually constitute the most informative part of the root cause attack vector and impact.
we build a vocabulary of frequentabsolute paths and a vocabulary of frequent relative paths.
aspaths are discrete data we use categorical v ariational auto encoder cav ae to learn path embeddings in anunsupervised way.
cav ae places paths in a latent space inwhich we use clustering algorithms to cluster similar paths which usually represent the same type of concepts.
finally based on the human label of a path cluster our approach labelsand extracts all noun phrases in tvds corresponding to thepaths in the cluster as a certain type of vulnerability concept.we conduct extensive experiments to evaluate our approachon different setups which achieves .
and84 accuracy for attack vector impact root cause and others respectively.
this paper makes the following three contributions we make two key observations on the data characteristicsof tvds which inspire the design of our unsupervisedmethod for concept labeling and extraction.
we design and implement a novel method for labelingand extracting phrase based concepts in tvds based onunsupervised machine learning techniques categoricalv ariational autoencoder and clustering .
we evaluate our approach on a dataset of vul nerability sentences from nvd.
our evaluation showsthe high accuracy of the root cause attack method andimpact labeled using our approach and the effectivenessof these labeled concepts in the training concept classifier.
we provide all our implementations it can be found at labeling src master ii.
o ur approach this section describes our proposed approach for unsupervised labeling and extraction of phrase based concepts intvds as shown in fig.
.
in this work we are interestedin three phrase based concepts root cause attack vector andimpact.
we crawl and build a tvd corpus from vulnera bility databases e.g.
nvd or cve .
we use asoftware specific tokenizer a vulnerability specific postagger and the context free grammar cfg sentenceparser by stanford parser to process the tvds and obtainall noun phrases in the tvds.
we build a gazetteer of vendors products and versions from the cpe dictionary and wikipediaand use it to identify vendor product and version noun phrases referred to as vpv noun phrases in tvds.
if vpv nounphrases are found we calculate relative paths from each vpvnoun phrase to the rest of non vpv noun phrases and alsocalculate absolute paths from tree root to these non vpv nounphrases.
distinct frequent absolute paths and relative paths arefed into a cav ae for unsupervised representation learning.
inthe latent space of absolute paths and relative paths clusteringalgorithms e.g.
fit tsne and dbscan are used to clustersimilar paths which usually represent same type of concepts.
944fig.
.
approach overview finally based on the human label of a path cluster our approach labels all noun phrases in tvds corresponding tothe paths in the cluster and outputs these labeled noun phrasesas concerning concepts.
a. phrase based vulnerability concepts the concepts that describe vulnerability characteristics in tvds are usually noun phrases.
for example a denial of service application crash is a noun phrase that describesthe impact of an attack.
t able i types of phrase based vulnerability concepts type examples root cause not properly close client connections attack v ector via a series of interrupted requests to a large object url impact a denial of service application crash table i shows three types of phrase based vulnerability concepts and some representative examples we aim to extractin this work.
these vulnerability concepts give essential in formation to security analysts and software developers aboutthe root cause attack vector or means and impact orconsequence of a vulnerability and attack .
rootcause is a program error or software weakness such as bufferoverflow not properly close client connections that canbe exploited to attack a system or network.
attack v ectordescribes various methods of executing an attack for example via images with crafted iptc metadata.
impact describes thepossible consequences of an attack such as denial of service or proxy server resource consumption.
we also consider anothers type which includes all noun phrases not belonging tothe root cause attack vector or impact.
as a by product ourapproach also identifies three types of entities i.e.
vendor product and version through gazetteer based matching.
b. sentence tokenization pos tagging and parsing we use the vulnerability specific tokenizer implemented in to tokenize tvd sentences.
we used a pre trained vulnerability specific pos tagger to assign pos tags totokens in tvds.
we use the stanford parser 1to convert a annotated tvd sentence into a parsing tree.
the parsergenerates grammatical syntactic constructs of phrases such asa sentence s prepositional phrase pp noun phrases np verb phrases vp adjective phrases adjp adverb phrases advp conjunctions phrases conjp with their particularpos tags.
in this work we focus on a terminal ngram wordsheader by noun phrases np .
although there are noun phrasesthat do not contain tvd concepts almost all tvd conceptsare expressed as part of a noun phrase.
c. gazetteer based v endor product v ersion recognition gazetteers are a dictionary of names or name patterns.
in this work we build a gazetteer for the vendor product and version respectively.
information sources are the cpedictionary and wikipedia.
our gazetteer includes not onlyfull names but also known abbreviations and synonyms forexample os unix.
for version in addition to specificversion numbers we also abstract them into number patternslike digit.digit.digit where a digit is a placeholder for any digit.
assume there are some vendor product or version noun phrases in some branches of a sentence parsing tree.
we usea gazetteer based searching method that hierarchically drillsdown to sub branches of the parsing tree until it finds anexact match of a name or name pattern in the gazetteer.
if amatch is found the algorithm returns the discovered gazetteername e.g.
vendor andreas huggel product exiv2 and v ersion .
.
we refer to the found vendor product versionphrases as vpv phrases.
as the examples in fig.
show atvd may mention all three types of information e.g.
tvd1 tvd3 and tvd5 or just some types e.g.
tvd2 .
we use thefound vpv phrases as landmarks to recognize other typesof vulnerability information such as root cause attack vector and impact which are non vpv noun phrases.
if no vpvphrases are found in a tvd sentence this sentence will bediscarded.
but we never see tvds that do not mention anyvendor product or version information in our dataset.
d. path representation of non vpv phrases based on the sentence parsing tree and the found vpv phrases we construct an absolution path and some relative paths one for each vpv phrase for each non vpv phrase.
945fig.
.
illustration of path construction and path pattern t able ii absolute and rela tive pat h represent a tion tvd1 absolute paths relative paths x0 buffer overflow root s np np np pp np pp np np x2 not null terminate strings root s vp np np pp np pp np s vp np x4 the sscanf function root s vp pp s vp np np np pp np pp np s vp pp s vp np np x6 denial of service application crash root s vp pp s vp np sbar s vp s vp vp np np pp np pp np s vp pp s vp np sbar s vp s vp vp x11 via crafted iptc root s vp np sbar s vp s vp vp np pp np pp np np pp np pp np s vp pp s vp np sbar s vp s vp vp pp np tvd2 x2 not properly close client connections root s vp np np np pp np s vp np np x4 proxy server resource consumption root s vp np sbar s vp s vp vp np np pp np s vp np sbar s vp s vp vp np x5 a denial of service root s vp np sbar s vp s vp vp np np np pp np s vp np sbar s vp s vp vp np np x6 service proxy server resource consumption root s vp np sbar s vp s vp vp np pp np np pp np s vp np sbar s vp s vp vp np pp np x9 interrupted requests to a large object root s vp np sbar s vp s vp vp np pp np pp np np pp np s vp np sbar s vp s vp vp np pp np pp np tvd3 x2 an exponent of root s np pp np np np pp np pp np vp s np pp np x3 rsa key pairs root s vp np np np pp np pp np np x5 transactions that are sent in cleartext root s vp np sbar s vp pp np np pp np pp np sbar s vp pp np tvd4 x1 reset arbitrary passwords root s vp pp s vp np sbar s vp s vp vp np np pp np s vp np sbar s vp s vp vp np x2 remote attackers root s vp s np np pp np s vp s np x3 arbitrary passwords root s vp s vp vp np np pp np s vp s vp vp np x4 an associated e mail address root s vp s vp vp sbar s np np pp np s vp s vp vp sbar s np tvd5 x1 another tab then switching back to the original root s np pp np np np np pp adjp np np x2 the screen definition root s np pp np adjp pp np sbar s np np np sbar s np x3 local users root s vp s np np np pp adjp np pp np s vp s np x4 certain options root s vp s vp vp np np np pp adjp np pp np s vp s vp vp np tvd6 x0 mishandles a short descriptor root s vp np np pp np np pp np pp np np x1 leading to out of bounds memory access root s vp np sbar s vp s vp vp np pp np np pp np s vp np sbar s vp s vp vp np pp np x3 out of bounds memory access root s vp np sbar s vp s vp vp np np pp np s vp np sbar s vp s vp vp np absolute path representation in a sentence parsing tree an absolution path is identified by the sequence of pos tags from the tree root to the node of a terminal noun phrase.
fig.
3shows partially the parsing trees of tvd1 tvd2 and tvd3.first we determine terminal noun phrases as a sequence ofterminal words in a branch of the parsing tree rooted at anoun phrase np node.
for example np1 buffer overflow and np5 not null terminate strings are two terminal nounphrases in the parse tree of tvd1.
np4 ras key pairs isa terminal noun phrase in the parsing tree of tvd3.
giventhe np node of a terminal noun phrase an absolute path canbe constructed by traversing the parsing tree from the rootnode to this np node.
for example the absolution path for buffer overflow is root0 s0 np0 np1.
as we do not careabout the specific index of nodes we can abstract this path asroot s np np .
in the same way the absolute path for rsakey pairs is constructed as root s vp np np .
we make a key observation on the absolution path repre sentation.
that is absolute paths can abstract away the surfacedifferences of sentence structures and phrase expressions andsame type non vpv concepts likely have the same or similarabsolute paths.
for example the absolute paths for not nullterminate strings in tvd1 and rsa key pairs in tvd3differ only in one np and both two phrases are the root cause.we refer to such similar paths as path patterns.
note that thelocations of similar paths in the parsing trees the index ofnodes on the paths and specific expressions of correspondingterminal noun phrases can all be very different.
table ii shows more examples.
tvd1 and tvd2 have non vpv phrases tvd3 and tvd6 have non vpv phrases and tvd4 and tvd5 have non vpv phrases.
for example terminal noun phrases such as not properly close clientconnections in tvd2 and rsa key pairs in tvd3 bothhave the same absolution path i.e.
root s vp np np andthey are mostly root causes.
the terminal noun phrase suchas remote attackers in tvd4 and local users in tvd5 946both have the same absolution path i.e.
root s vp s np and they are mostly attack methods.
the terminal noun phrasesuch as arbitrary passwords in tvd4 and certain options in tvd5 both have the same absolution path i.e.
root s vp s vp vp np and they are mostly impacts.
in the sameway terminal noun phrases such as interrupted requests to alarge object in tvd2 and via crafted iptc in tvd1 bothhave the same absolution path i.e.
root s vp np sbar s vp s vp vp np pp np pp np and they are mostly relatedto attack vectors of vulnerability concepts.
on the other hand phrases such as a denial of service and service proxy server resource consumption in tvd2have nearly similar absolute path patterns i.e.
root s vp np sbar s vp s vp vp np np they only differ with pp np and these phrases are mostly consequences of vulnera bility i.e.
impacts.
relative path attributes a relative path is identified by the sequence of pos tags from a vpv phrase i.e.
vendor product or version to a non vpv noun phrase.
if the vpvphrase and the non vpv phrase share the same branch of theparsing tree i.e.
they have a common ancestor all commonancestor nodes are truncated from their absolute paths.
thisgives truncated paths.
then the truncated path of the vpvphrase is reversed to obtain the reversed truncated path.
therelative path is then obtained by concatenating the reversed truncated path of the vpv phrase the lowest common ancestornode and the truncated path of the non vpv phrase.
take the version phrase .
and the non vpv phrase buffer overflow in tvd1 in fig.
as an example.
theabsolute path of .
is root s np pp np pp np andthe absolute path of buffer overflow is root s np np .common ancestor nodes root s np will be removed andthe lowest common ancestor is np .
the reversed truncatedpath of .
is np pp np pp and the truncated path of buffer overflow is np .
finally the relative path from .
to buffer overflow is np pp np pp np np .
in the same way the relative path from version phrase .
.
to not properlyclose client connections is constructed as root np pp np s vp np np .
relative paths capture syntactic relationships between found vpv phrases in a tvd sentence and non vpv phrases.
simi lar to absolute paths relative paths also abstract differencesin sentence structures and phrase expressions and same type non vpv concepts likely share similar relative paths i.e.
syntactic relationships with found vendor product andversion phrases in tvds.
for example buffer overflow in tvd1 mishandles a short descriptor in tvd6 and rsa key pairs in tvd3 have similar relative paths i.e.
np pp np pp np np and they are all root cause.
as an other example the relative path of proxy server resourceconsumption in tvd2 out of bounds memory access intvd6 and reset arbitrary passwords in tvd4 have a similarrelative path pattern i.e.
np pp np s vp np sbar s vp s vp vp np and they are all impacts of vulnerability.e.
unsupervised representation learning although absolute and relative paths abstract away many detailed sentences and phrase differences they are still toofine grained to determine their similarities.
there are generallya large number of distinct paths in a large tvd corpus andsimilar paths vary greatly in length and pos tags.
therefore it is hard to measure path similarity using approximate stringmatching methods.
in this work we consider each distinct pathas a word in a vocabulary and use an auto encoding techniqueto project discrete paths i.e.
high dimensional one hot vector into a low dimensional continuous vector space in which pathsimilarity can be measured using euclidean distance.
categorical v ariational autoencoders cavae autoencoder is a network architecture which include an encoderand a decoder.
the encoder compresses a high dimensionalinput vector xinto a low dimensional continuous vector in a latent feature space.
the decoder then tries to decode thelow dimensional continuous vector and reconstruct the high dimensional input vector x prime.
the encoder and decoder are trained together with the reconstruction loss between xand x prime that is how similar the reconstructed output matches the input.
the model training is unsupervised in the sense that itrequires only input paths but no label is needed.
we can alsosay the model training is supervised by the reconstructed pathsas similar as the input paths.
once the model is trained we usethe trained encoder to embed unseen paths in the latent vectorspace.
due to the reconstruction goal similar paths would havesimilar latent vectors which can be further clustered.
we adopte the network structure of v ariational autoencoder v ae .the v ae assumes gaussian dis tribution.
however it does not fit our data as our paths arehighly sparse categorical data.
therefore we extend v ae withgumbel max trick which suits modeling highly sparse categorical data with encoder decoder architecture.by using gumbel softmax trick we can samplefrom the latent space feed these samples to the decoder and generate new output.
essentially gumbel softmax approximate categorical samples and compute gradients viathe re parameterization trick.
we use binary cross entropy tocompute the loss between the reconstructed paths and the inputpaths.
if the decoder does not reconstruct the input satisfac torily it incurs much reconstruction loss.
we also computethe kl divergence for non differentiable gradient andwe minimize both at the same time.
input to cavae we build two dictionaries one for distinct absolute paths and the other for distinct relative paths.for each path we count its occurrence frequency in the entirecorpus.
we sort the paths by their occurrence frequenciesand take up to paths cover paths that occur at leasttwice in our corpus to build the two vocabularies absolute path vocabulary and relative path vocabulary.
each path inthe vocabulary is regarded as a word and represented ina one hot vector.
given each non vpv phrase in a tvdsentence we represent the phrase s absolute path as the one hot vector of this path.
we represent the phrase s relative paths 947by concatenating the one hot vectors of the relative paths of this phrase.
we have two cav aes jointly trained one forembedding absolute paths and the other for embedding relativepaths.
we concatenate the latent vectors of the two cav aesas the latent vector of the input non vpv phrase.
f .
clustering and concept labeling the trained cav ae encoder can be used to place all nonvpv phrases in a tvd corpus into the latent space in terms of the phrases absolute path and relative paths.
we can clusternon vpv phrases based on their closeness in the latent space.we use three different clustering settings cavae fit tsne cavae f i t tsne d b s c a n and cavae d b s c a n .
fit tsne is a t distributed stochastic neighbor embedding method for visualizing high dimensional data points by giving each data point a locationin a two dimensional map.
the cavae fit tsne setting allows us to observe the latent space learned bythe cav ae.
the two dimensional map can also be regardedas a clustering output.
cavae f i t tsne dbscan further applies dbscan density based spatial clustering of applications with noise to the two dimensional map of fit tsne to cluster data points in thistwo dimensional space.
cavae d b s c a n applies dbscan directly to high dimensional late space by cav ae.rahmahs algorithm is applied to determine the bestparameters for the dbscan algorithm.
based on the observation of path patterns and their relationships with vulnerability concept types we assume that non vpv phrases with similar paths in a cluster should representa distinct concept type.
the last step of our approach is toask a security expert to sample non vpv phrases in eachcluster and label the cluster with the concept type rootcause attack vector or impact that the majority of non vpvphrases represent.
if the cluster is a mix of several differentconcept types with similar phrase amounts the cluster will belabeled as others.
based on the cluster labels and the phraseboundaries our approach can label and extract root cause attack vector and impact phrases from the tvds in the corpus as visualized in fig.
.
as a by product vpv phrases detectedby gazetteer matching can also be extracted.
iii.
e v alua tion we conduct a series of experiment to investigate the following research questions rq1 how robust is our cav ae on embedding syntactic structures of absolute paths and relative paths of non vpvphrases?
we answer this question by study cluster propertiesin the latent space.
rq2 is our hypothesis that syntactically similar paths leads to same type of non vpv concepts valid?
we answer thisquestion by manually validating the types of non vpv phrasesin a cluster against the human labeled cluster type.
rq3 to what extent variant model designs affect the accuracy of unsupervised labeling?
we replace the key modelcomponents with alternative designs and compare the perfor mance of model variants.
rq4 how well can our unsupervised labeled data support machine learning tasks?
we compare the performance of avulnerability concept classifier trained with our unsupervisedlabeled data and manually labeled data from .
a. experimental setup dataset we crawl a tvd corpus from the nvd website.
the corpus includes nvd entries from .
we remove a small number of nvd entries that donot reference cve id as they may not represent securityvulnerabilities.
we keep the textual content of each nvd entryas tvds but remove reference code fragments if any .
wealso remove reject disputed and reserved nvd entries as they may create confusion because theirdetails vary accordingly.
after text processing we obtain a textcorpus consisting of nvd entries sentenceswith in total tokens.
from this corpus we obtainabout terminal noun phrases.
if the tvd does notcontain at least one vpv phrase we discard this tvd.
we obtain a total of non vpv noun phrases.
from these non vpv noun phrases we collect unique phraseswithin the sentences followed by distinct phrases across thesentence.
we obtain distinct non vpv noun phrasesacross the corpus.
for each distinct non vpv phrase wecalculate its absolute path and relative paths and then countpath frequencies in the corpus.
the path frequencies follow along tail distribution with about of paths paths occurring or more times.
note that a recurring path doesnot mean the phrases represented by the path are all the same see table ii for examples .
in this work we consider the pathoccurring only once as accidental and do not consider themin unsupervised path representation learning.
in this work the vocabulary size will cover all recurring paths inour corpus.
therefore we set the upper bound of absolute path vocabulary and relative path vocabulary to andexperiment with the vocabulary size and5000.
these vocabulary sizes cover 485and non vpv phrases respectively.
model configurations our cav ae is implemented on tensorflow and keras .
the input to cav ae is a5 dimensional matrix where the first row is the one hotvector of the absolute path and the rest four rows are theone hot vectors of relative paths.
in our dataset a non vpvphrase has at most relative paths.
zero padding is applied forsmaller vocabulary sizes and if the non vpv phrase has lessthan relative paths.
the encoder and decoder networks areconfigured with fully connected layers each of which has800 neurons.
we set the network structure as forthe encoder reversed for the decoder .
that is the latent spaceis dimensional continuous vector.
the activation functionis relus .
during training we update model parametersusing a gumbel softmax back propagation algorithm.
in all cases the input data is partitioned into training validation and test dataset our models 948t able iii resul ts of loss function model losses and v alida tion loss in each itera tion .
steps iteration temperature model loss validation loss .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fig.
.
training curves of reconstruction loss are optimized using adam with an initial learning rate set to .
and decay every epoch by a factor of .
.
wechoose the model parameters that show the best accuracy onthe validation set and we report the score on the test set.
agrid search is employed to select appropriate hyperparametervalues.
in all cases a fold cross validation methodology isapplied.
then the fold results are averaged and used toselect the best models and variants for comparison.
we usethe default hyperparameters of fit tsne and dbscan.
b. cavae robustness rq1 motivation the encoder of cav ae learns to embed absolute paths and relative paths into the latent space.
the decoder reconstructs the input path using the latent featurevector from the latent space.
the quality of the reconstructiondepends on the quality of the encoded features in the latentspace.
therefore we measure the reconstruction quality toevaluate the quality of latent space projection.
approach we estimate the reconstruction loss to measure how much information is lost during the decoder s recon struction phase.
we report the log likelihood reconstructionloss to measure the cav ae model performance and thequality of the encoded features in the latent space.
the lowerthe reconstruction loss of the model the higher the fidelity oflatent space projection.
result table iii and fig.
show the values of loss varying with the number of iterations during the trainingprocess of the cav ae model on the absolute and relativepaths.
the red line represents model loss and the blue linerepresents a validation loss in fig.
right .
overall when themodel is iterated to nearly steps the reconstruction erroris optimized to be small.
as the training process continues to20 the model loss drops rapidly from .
to .
andvalidation loss from .
to .
.
when the model losserror decreases gradually we can see that the reconstructionerror increases a little.
with further training of the model the reconstruction error is optimized to a stable and optimalstate in the end.
a significant loss reduction is observed indicating that our model addresses how two different prob ability distributions are minimized.
the result shows that theproposed cav ae achieves a small .
log likelihood onthe validation set which shows the effectiveness of our modelto improve the quality of latent space clustering.
fig.
left shows that the temperature parameter gradually decreases from .
to .
.
this result allows usto control how closely samples from the gumbel softmaxdistribution approximate data points from the cate gorical distribution.
as the softmax temperature approaches samples from the gumbel softmax distribution becomeone hot and the gumbel softmax becomes the categoricaldistribution.
the result shows that the temperature gradually decreases to be .
and continues to optimize to a stable state.
our results indicate that cav ae are robust in both learning syntactic structures of absolute and relative path s and dimension reduction thus justifying the use ofcav ae for the subsequent latent space clustering.
c. the quality of resulting concept clusters rq2 motivation our unsupervised labeling approach is based on the hypothesis that syntactic relationships betweenvpv phrases and non vpv phrases help to determine the con cept type of non vpv phrases.
we want to confirm the validityof our hypothesis.
in our approach discrete syntactic pathsare projected into a latent space.
therefore if our assumptionholds non vpv phrases with similar paths should be closein the latent space.
therefore we validate our hypothesis byexamining the accuracy of non vpv phrases in the resultingclusters through both data visualization and manual checking.
approach first we visualize the latent space with fittsne i.e.
cavae fit tsne .
fit tsne shows the resulting clusters in a dimensional map.
we alsoapply dbscan directly on the latent space cavae dbscan and on the dimensional map cavae fit tsne dbscan and visualize the resulting clusters again on a dimensional map.
we would expect tosee four major clusters which would correspond to root cause attack vector impact and others respectively.
next we sample the non vpv phrases corresponding to the paths in the resulting clusters and manually examine ifthe concept type of the sampled non vpv phrases matchesthe cluster label given by the human.
we perform this manualchecking for all three clustering settings.
as we have a largenumber of tvds and phrases to examine we adopt a statisticalsampling method .
in particular we examine the minimumnumber min of data instances for each cluster to ensure 949fig.
.
visualization of resulting clusters a cavae fit tsne b cavae fi t tsne d b s c a n and c cavae dbscan respectively.
that the estimated accuracy is in a error margin at a confidence level.
we hire two annotators one from our research group not involved in this study and one from a local security company to annotate the concept type of each sampled non vpv phrase.we request participants to use four concept labels root cause attack vector impact or others.
the two annotators first inde pendently evaluate the label accuracy of the sampled phrases a binary decision .
then we compute cohens kappa to evaluate the inter rater agreement.
when two annotatorsdisagree on the labels they discuss to make a final judgment.based on the final labels we compute the accuracy of non vpv phrases of each cluster.
results fig.
shows the dimensional visualization of the clusters obtained in the three experimental settings.although the shapes and locations of the resulting clustersare different in different settings we can clearly observe fourmajor clusters with a very small percentage of outliers inall three settings.
in the visualization red points correspondto root cause rose points to impact yellow points to attackvector and green points to other types.
the clusters repre senting root cause and impact are compacted.
the clustersrepresenting attack vectors and other types albeit more sparse are still well distinguishable.
the black points are outliers.
wedesign the cav ae to be robust in noise filtering and structurallearning noise phrases are further identified with dbscanclustering.
any phrase that is randomly selected by rahmahsalgorithm is not found to be a core point of the clusters or a borderline phrase is classified as an outlier phrase and isnot assigned to any cluster.
these outliers are removed fromthe training data we provided.
table iv reports the label accuracy of non vpv phrases in the four clusters.
the column min is the number of phraseswe randomly sample and examine from each distinct clusteraccording to .
the columns aa1 and aa2 show the accu racy results determined by the two annotators independently and the column af is the final accuracy after resolving theirdisagreements.
the cohen s kappa metrics for each type ofconcept between the two annotators are all .
which indicates a reasonable inter rater agreement.
we observe thatthe disagreement is due to the attack vector class sometimeshas similar structures with impact class.
the concept type withthe highest accuracy is selected to be the label of a cluster.
according to the manual annotation the majority of nonvpv phrases in each of the four major clusters indeed corre t able iv the concept type accuracy of resul ting clusters approaches type min aa1 aa1 af cav ae f i t tsneattack v ector .
.
impact .
.
root cause .
others .
.
.
cav ae f i t sne d b s c a nattack v ector impact .
.
root cause others cav ae d b s c a nattack v ector .
.
impact root cause .
.
others t able v accuracy of model variants variant model deign accuracy cav ae absolute path and relative paths .
cav ae ap only absolute path cav ae we word embeddings of phrases as input trv ae not designed to handle categorical data spond to one of the four concept types respectively.
we seehigh label accuracies for all four clusters in allexperimental settings.
the accuracies in the three experimentalsettings are close.
applying dbscan to the latent space orthe fit tsne s dimensional space can improve the label ac curacies slightly.
cavae fit tsne d bsca n is overall the best setting which achieves .
and accuracy for attack vector impact root cause andothers respectively.
our cluster visualization and manual checking of nonvpv phrases in the resulting clusters confirm the effec tiveness of unsupervised labeling method.
this confirmsthe validity of our hypothesis that syntactically similarnon vpv phrases in terms of absolute paths and relativepaths usually represent the same type of vulnerabilityconcepts.
d. comparison of v ariant model designs rq3 motivation our model design makes several special considerations encode syntactic relationships between vpvphrases and non vpv phrases use path representation toabstract away differences of sentence structures and phraseexpressions enhance v ae with gumbel softmax to handlecategorical path representation.
we want to investigate howthese special model design considerations affect the perfor mance of clustering same type of non vpv phrases.
approach we create three model variants cav aeap which is trained only with absolute paths but not relativepaths of non vpv phrases cav ae we which takes asinput of the pretrained word2vec word embeddings ofnon vpv phrases rather than their path representations trv ae which is a traditional v ae that is not designed to 950t able vi comp arisons with concept classifica tion methods dataset prec.
recall f1 labeling joshi et al.
.
.
.
labeling bridges et al.
.
.
.
our unsupervised labeling .
.
.
handle categorical data.
we perform phrase clustering by cavae f i t tsne d b s c a n and label the resulting four clusters with the four concept types.
we usethe manually annotated non vpv phrases from rq2 as theground truth to evaluate the clustering accuracy.
due to thespace limitation we report only the overall accuracy for thefour concept types i.e.
root cause attack vector impact andothers as a whole.
result table v presents the accuracy results.
compared with the accuracy of the original cav ae .
the accuracies of all the three model variants degrade inparticular cav ae ap cav ae we and tra ve .
cav ae we has the largest accuracy drop whichindicates that representing phrases in word embeddings isnot appropriate for unsupervised representation learning.
thiscould be because word embeddings contain too much syntacticand semantic information which is irrelevant and hard to re construct by auto encoder.
furthermore word embeddings arealready dense continuous vectors on which further dimensionreduction could be difficult.
in contrast our path representationabstracts away detailed differences at both sentence and phraselevels which allows path patterns to emerge.
as such itbecomes easy for auto encoder to embed and reconstruct pathpatterns.
however the performance drop of tra ve indicates that just the auto encoder is still not enough to effectively embed pathpatterns due to its categorical nature.
therefore our gumbel softmax enhancement is necessary.
finally without consider ing relative paths cav ae ap still performs reasonably well.as shown in table ii absolute paths themselves can capturemany regularities among non vpv phrases and thus have goodpredictive power for the concept type of non vpv phrases.however taking into account relative paths between vpvphrases and non vpv phrases can further boost the modelperformance.
this shows that absolute paths and relative pathsrepresent complementary features.
our results suggest that all the three special model design considerations contribute to boost our model per formance.
e. usefulness of unsupervised labeled data rq4 motivation our approach provides a novel unsupervised way to label and extract key vulnerability phrases e.g.
rootcause attack vector and impact from tvds.
after confirmingthe high accuracy of our unsupervised information extractionapproach we want to further investigate how well the phrasesobtained through our unsupervised labeling may be used astraining data for machine learning techniques as opposed tomanually labeled concept phrases in tvds.
approach closest to our work the two previous studies also investigate concept labeling and extraction intvds.
different from our work they manually label conceptsin tvds and thus produce two manually labeled datasets ofvulnerability concepts referred to as the bridges dataset and the joshi dataset .
in joshi s work they label vul nerability means and consequence which correspond to rootcause attack vector and impact in our terminology.
bridgesmanually created a gazetteer of vulnerability relevant terms and extracted short phrases that give pertinent informationabout a vulnerability.
the bridges dataset consists of tvds labeled with vulnerability relevant term which match es the union of attack means and consequences labelsof joshi.
the concept type of vulnerability relevant term forthese categories is ambiguous and too general to use hence to suit the concept types in our terminology we manuallynormalize the vulnerability relevant term labels to rootcause attack vector and impact.
the joshi dataset consistsof cve tvds tokens labeled with attack means and consequences which are equivalent to rootcause attack vector and impact phrases in our approach.
for a fair comparison we use only the tvds labeled in these three labels of the two datasets.
when comparing withbridges et al.
or joshi et al.
we use only the tvdsin their respective dataset.
we use our approach to label theroot cause attack vector and impact phrases in these tvds and obtain and root cause attack vectorand impact phrases for the bridges dataset and and root cause attack vector and impact phrases for thejoshi dataset.
in this work we consider a simple conceptclassification task that predicts the concept type for an inputphrase.
we use the random forest classifier and trainthe classifier with the phrases from the bridges dataset thejoshi dataset and our unsupervised labels respectively.
wecompare prediction precision recall and f1.
we perform fold cross validation and report average performance metrics.
results table vi presents the classification results.
the classifier trained with the phrases extracted through ourunsupervised labeling achieves the best precision .
recall .
and f1 .
followed by the classifiertrained with the bridges dataset and then the classifier trainedwith the joshi dataset.
by comparing the phrases in the threedatasets we find that the consistent boundary and typing byour method is important for correct classification.
in contrast human labeling tends to be more inconsistent.
for example inour method a denial of service application crash a denialof service and application crash are labeled as impacts andtheir boundaries are consistent with their structural similarity.in contrast the human labeling identified only a denial ofservice as impact ignoring other structurally similar phrases.
951t able vii error anal ysis each cluster subca tegories are labeled as root cause r a tt ack vector a imp act i and others o respectivel y .
cluster cluster cluster cluster rai o ra i o ra i o rai o .
.
.
.
.
.
.
.
.
.
the vulnerability phrases in tvds extracted by our unsupervised labeling method constitute a high quality dataset for the training concept classifier.
due to the con sistent boundary and typing by our information extractionmethod the classifier trained with our extracted phrasesoutperforms the classifier trained with manually labeledphrases.
f .
threats to v alidity error analysis despite having the best results of the experiments our approach generates some incorrectly classifiedconcept phrases which is discussed in this section.
our approach processes unstructured tvds which are full of software specific names number sequences and domain specific terms see fig.
.
however our approach is not sig nificantly affected.
we believe that we minimize these issuesby using vulnerability specific tokenizer and pos tagger .our path representation and auto encoding for noise reductionfurther minimize the problems of domain specific terms.
table vii shows the breakdown of phrase labels in the four clusters by cavae fit tsne d bsca n. our manual analysis shows that the incorrectly labeled concepts aremostly from the attack vector class which rarely mixes withthe phrases from the impact class.
for example the phrase service application crash via images root s vp pp s vp np sbar s vp s vp vp np pp np is a mix of impactand attack vector phrases.
the phrase service applicationcrash root s vp pp s vp np sbar s vp s vp vp np pp np np is an impact whereas via image root s vp pp s vp np sbar s vp s vp vp np pp np pp np is theattack vector.
the model is confused to correctly cluster eitherof them.
as can be seen from the long path the two phrasesdiffer with pp np .
the error is due to our cav ae learns thecommon structures of paths and places them close to eachother in the latent space.
however the phrase boundary isdefined by the parser in the early phases.
although the cfgparser has reasonable assumptions for grammatically incorrectsentences it is sometimes hard to parse tvd sentences dueto domain specific vocabularies and jargon.
to investigate the level of errors we randomly select noun phrases from each cluster from the labeled clusteringresults and re annotated them from scratch.
we observe thatin each human generated concept cluster .
root cause .
attack vector .
impact are correctly labeled which is close to the cluster quality by our unsupervisedlabeling.
this shows that it is a significant achievement for ourapproach which confirms path representation and latent spaceclustering are effective for vulnerability concept labeling.
internal v alidity in our approach the first threat to internal validity may begin when vulnerabilities are reportedto the vulnerability databases and cybersecurity blogs byonline users contributors i.e.
the natural characteristics ofthese online data feeds are noisy.
we cannot avoid mistakesdue to human characterizing at the source data reporting which may affect our approach in recognizing vulnerability relevant concepts.
the popular vulnerability databases suchas nvd have analysts who manually fix such errors beforevulnerability reports are publicly accessible in their repository.in the future we will extend our approach to minimize thethreat related to more noisy vulnerability information fromonline contributors.
the other threat to internal validity is that of manually labeling.
concept phrases are challenging to determine in somecases.
we invite annotators to manually labeling concepts.this document is regarded as ground truth data to evaluatethe quality of clusters.
we obtain the agreement between theannotators to be over for the root cause of vulnerabilities.however for attack vector and impact the agreement is while for attack vector it is indicating that there aredisagreements for annotating attack vector.
we ask the twoannotators to discuss and resolve their disagreements.
external validity it is concerned with whether the experimental results can be generalized to other datasets.
tomaximize the validity to some extent we crawl tvd fromnvd and cve databases.
although these vulnerability reportscontain tokens other vulnerability databases suchas osvdb ibmx force are not included in this study dueto the significant effort needed to validate the cluster quality.still the general structure of tvd and reporting formats aremore or less similar due to the universal characteristics ofvulnerabilities.
however there may be intrinsic differences incharacterizing vulnerabilities in other vulnerability database which deserve further investigation.
iv .
r ela ted work recently neural network based named entity recognition ner models achieve great success in the newswire domainand related artifacts however they suffer from sharpperformance decline as the input moves to tvd .
the main reason is that tvd often contains textcomposed of mixed language studded with codes and domain specific vocabularies.
these models do not use tvd trainingdata which results in the trained neural models lack of do main knowledge to identify the vulnerability relevant conceptscritical for security analyses .
to tackle this problem neural models have heavily relied on human annotations and hand crafted features .
for exam ple a crf based system and lstm basedneural network models are proposed to identifycybersecurity related entities and concepts.
however trainingneural models for concept extraction requires a large amountof manually labeling training data obtaining large labeled 952datasets is particularly challenging in specific domains such as tvd where human annotations are expensive and time consuming.
one way of minimizing human annotation is touse external knowledge bases which are limited to particularentities e.g.
cpe for vulnerable products and product ven dors .
many other vulnerability concepts are phrases such asroot cause attack vector and impact could not have a matchingentity by an external knowledge base.
in the cybersecurity domain there has been growing concern in developing entity and concept extraction methods.more et al.
attempt to annotate entities in the cve de scriptions using off the shelf ner tools.
mulwad et al.
extend his idea by proposing a prototype system using ansvm classifier.
joshi et al.
proposed a framework toextract cybersecurity relevant entities from the nvd.
unlikeour approach their approach involves hand annotating a smallcorpus fed into the stanford ner s for training a crf entityextractor.
bridges et al.
implemented a mem trained withthe average perceptron algorithm which has been proven to bebetter than .
in their work cybersecurity relevant concepts such as root cause attack vector and impact are manuallylabeled as vulnerability relevant term which is ambiguousand too general to be of use.
jones et al.
implementeda bootstrapping algorithm to extract security entities from thetext.
however their approach works well for small corpus.mcneil et al.
proposed a bootstrapping algorithm thatlearns heuristics to extract information about exploits andvulnerabilities.
however these models over fit to the trainingdata that is hard to generalize to unseen data.
in contrast this paper proposes a new mechanism i.e.
the use of unsupervised auto encoding and clustering forlabeling phrase based concepts.
the v ae is provenin learning representations in an unsupervised manner andwe use their principles.
however our input data are high dimensional sparse and categorical and back propagationis impossible as traditional v ae works only for continuousdata.
therefore we use gumblemax to back propagatingvia discrete variables.
recently fit sne the recentimplementation of tsne received much attention as it canaddress high volumes of data and reveal meaningful clusteringstructure.
we use the advantage of this algorithm to visualizeand latent space clustering which dbscan then exploits.compared to previous works we provide a pipeline of pre trained models designed to automatically label tvds fromonline data sources in an unsupervised way.
v. c onclusion in this paper we present an unsupervised method for labeling phrases based concepts root cause attack vector and impact in textual vulnerability descriptions that serveas training data for downstream machine learning tasks.
theproposed approach uses path representations from the sentenceparsing tree to abstract aways fine grained differences insentence structures and phrase expressions.
it learns featurerepresentations through categorical v ariational auto encoder cav ae and uses encoded features in the latent space toinitialize the clustering of the same type of phrases.
ourapproach can deal with the mixed texts and linguisticallymalformed phrases.
our evaluation shows that our approachcorrectly labels a broad set of vulnerability relevant phrase based concepts.
we also provide initial evidence of the useful ness of the labels obtained for downstream machine learningtasks.
the findings from this work including the publishedresources such as labeled corpus and trained models willbe at the forefront of future studies which will add insightinto the security domain and support automated processingand research on fast growing vulnerability information.
inthe future we will introduce additional concept categoriesand extend our work for identifying concepts from softwareengineering and cybersecurity discussion forums.
a cknowledgments this work is supported by the national natural science foundation of china nsfc and thejoint project of bayescom.
xiaowang zhang is supported bythe program of peiyang y oung scholars in tianjin university 2019xrx .
r eferences c. mitre national vulnerability database nvd gov .
ibm ibm x force exchange com online accessed june .
o. security exploit database https .
h. gasmi j. laval and a. bouras information extraction of cybersecurity concepts an lstm approach applied sciences vol.
no.
.
s. s. weerawardhana s. mukherjee i. ray and a. e. howe automated extraction of vulnerability information for home computer security inf oundations and practice of security 7th international symposium fps montreal qc canada nov. pp.
.
t. d. g unes l. tran thanh and t. j. norman identifying vulnerabilities in trust and reputation systems in proceedings of the twentyeighth international joint conference on artificial intelligence ijcai2019 macao china aug. pp.
.
h. chen j. liu r. liu n. park and v .
s. subrahmanian vest a system for vulnerability exploit scoring timing in proceedings of the twenty eighth international joint conference on artificial intelligence ijcai macao china aug pp.
.
s. mittal p .
k. das v .
mulwad a. joshi and t. finin cybertwitter using twitter to generate alerts for cybersecurity threats and vulnera bilities in ieee acm international conference on advances in social networks analysis and mining asonam san francisco ca usa aug. pp.
.
l. neil s. mittal and a. joshi mining threat intelligence about open source projects and libraries from code repository issues and bugreports in ieee international conference on intelligence and security informatics isi miami fl usa nov. pp.
.
s. r. v adapalli g. hsieh and k. s. nauer twitterosint automated cybersecurity threat intelligence collection and analysis using twitterdata in proceedings of the international conference on security and management sam pp.
.
a. pingle a. piplai s. mittal a. joshi j. holt and r. zak relext relation extraction using deep learning approaches for cybersecurityknowledge graph improvement in asonam international conference on advances in social networks analysis and mining v ancouver british columbia canada aug. pp.
.
y .
dong w .
guo y .
chen x. xing y .
zhang and g. wang towards the detection of inconsistencies in public security vulnerability reports in28th usenix security symposium usenix security santa clara ca usa august .
usenix association pp.
.
s. k. lim a. o. muis w .
lu and o. c. hui malwaretextdb a database for annotated malware articles in proceedings of the 55th annual meeting of the association for computational linguistics acl v ancouver canada july august v olume long papers pp.
.
s. yitagesu x. zhang z. feng x. li and z. xing automatic part of speech tagging for security vulnerability descriptions in 18th ieee acm international conference on mining software repositories msr madrid spain may pp.
.
d. p .
kingma and m. welling auto encoding variational bayes in 2nd international conference on learning representations iclr banff ab canada apr .
.
d. j. rezende s. mohamed and d. wierstra stochastic backpropagation and approximate inference in deep generative models inproceedings of the 31th international conference on machine learning icml beijing china jun.
pp.
.
e. jang s. gu and b. poole categorical reparameterization with gumbel softmax in 5th international conference on learning representations iclr toulon france april .
c. mitre common vulnerabilities and exposures cve https cve.mitre.org .
d. klein and c. d. manning accurate unlexicalized parsing in proceedings of the 41st annual meeting of the association for computa tional linguistics july sapporo convention center sapporo japan.
acl pp.
.
r. a. bridges k. m. t. huffer c. l. jones m. d. iannacone and j. r. goodall cybersecurity automated information extraction techniques drawbacks of current methods and enhanced extractors in 16th ieee international conference on machine learning and applications icmla cancun mexico dec. pp.
.
a. joshi r. lal t. finin and a. joshi extracting cybersecurity related linked data from text in ieee seventh international conference on semantic computing irvine ca usa sep. pp.
.
e. j. gumbel statistical theory of extreme values and some practical applications a series of lectures.
us government printing office vol.
.
c. j. maddison a. mnih and y .
w .
teh the concrete distribution a continuous relaxation of discrete random variables in 5th international conference on learning representations iclr toulon france april .
c. k. s nderby t. raiko l. maal e s. k. s nderby and o. winther how to train deep variational autoencoders and probabilistic laddernetworks corr vol.
abs .
.
g. c. linderman m. rachh j. g. hoskins s. steinerberger and y .
kluger fast interpolation based t sne for improved visualization ofsingle cell rna seq data nature methods vol.
no.
pp.
.
m. ester h. kriegel j. sander and x. xu a density based algorithm for discovering clusters in large spatial databases with noise inproceedings of the second international conference on knowledge discovery and data mining kdd portland oregon usa pp.
.
n. rahmah and i. s. sitanggang determination of optimal epsilon eps value on dbscan algorithm to clustering data on peatland hotspots insumatra iop conference series earth and environmental science vol.
p. jan .
r. a. bridges c. l. jones m. d. iannacone and j. r. goodall automatic labeling for entity extraction in cyber security corr vol.
abs .
.
m. abadi p .
barham j. chen z. chen a. davis and j. dean tensorflow a system for large scale machine learning in 12th usenix symposium on operating systems design and implementation osdi2016 savannah ga usa nov. pp.
.
f. chollet et al.
keras.
.
available v .
nair and g. e. hinton rectified linear units improve restricted boltzmann machines in proceedings of the 27th international conference on machine learning icml haifa israel jun.
pp.
.
d. p .
kingma and j. ba adam a method for stochastic optimization in3rd international conference on learning representations iclr san diego ca usa may .
r. singh and n. s. mangat elements of survey sampling.
springer science business media vol.
.
j. r. landis and g. g. koch an application of hierarchical kappatype statistics in the assessment of majority agreement among multipleobservers biometrics pp.
.
t. mikolov i. sutskever k. chen g. s. corrado and j. dean distributed representations of words and phrases and their composition ality in advances in neural information processing systems 27th annual conference on neural information processing systems lake tahoe nevada united states dec. pp.
.
l. breiman random forests mach.
learn.
vol.
no.
pp.
.
j. devlin m. chang k. lee and k. toutanova bert pre training of deep bidirectional transformers for language understanding inproceedings of the conference of the north american chapterof the association for computational linguistics human languagetechnologies naacl hlt minneapolis mn usa june pp.
.
z. xiao towards a two phase unsupervised system for cybersecurity concepts extraction in 13th international conference on natural computation fuzzy systems and knowledge discovery icnc fskd guilin china july pp.
.
s. more m. matthews a. joshi and t. finin a knowledge based approach to intrusion detection modeling in ieee symposium on security and privacy workshops san francisco ca usa may pp.
.
x. liao k. y uan x. wang z. li l. xing and r. a. beyah acing the ioc game toward automatic discovery and analysis of open sourcecyber threat intelligence in proceedings of the acm sigsac conference on computer and communications security vienna austria oct. pp.
.
p .
m. vu t. t. nguyen and t. t. nguyen alpaca advanced linguistic pattern and concept analysis framework for software engineering corpo ra in proceedings of the acm southeast conference pp.
.
atefehzafarian a. rokni s. khadivi and s. ghiasifard semisupervised learning for named entity recognition using weakly labeledtraining data in the international symposium on artificial intelligence and signal processing aisp pp.
.
g. kim c. lee j. jo and h. lim automatic extraction of named entities of cyber threats using a deep bi lstm crf network int.
j. mach.
learn.
cybern.
vol.
no.
pp.
.
y .
qin g. shen w .
zhao y .
chen m. y u and x. jin a network security entity recognition method based on feature template and cnn bilstm crf frontiers inf.
technol.
electron.
eng.
vol.
no.
pp.
.
s. k s. s v .
r and k. p .
soman deep learning approach for intelligent named entity recognition of cyber security corr vol.
abs .
.
s. y agcioglu m. s. seyfioglu b. citamak b. bardak s. guldamlasioglu a. y uksel and e. i. tatli detecting cybersecurity events fromnoisy short text corr vol.
abs .
.
v .
mulwad w .
li a. joshi t. finin and k. viswanathan extracting information about security vulnerabilities from web text in proceedings of the ieee wic acm international joint conference on webintelligence and intelligent agent technology workshops wi iat campus scientifique de la doua lyon france aug. pp.
.
m. collins discriminative training methods for hidden markov models theory and experiments with perceptron algorithms in proceedings of the conference on empirical methods in natural languageprocessing emnlp philadelphia pa usa jul.
pp.
.
c. l. jones r. a. bridges k. m. t. huffer and j. r. goodall towards a relation extraction framework for cyber security concepts inproceedings of the 10th annual cyber and information security research conference cisr oak ridge tn usa apr .
pp.
.
n. mcneil r. a. bridges m. d. iannacone b. d. czejdo n. perez and j. r. goodall p ace pattern accurate computationally efficientbootstrapping for timely discovery of cyber security concepts in 12th international conference on machine learning and applications icm la miami fl usa dec. v olume pp.
.
l. van der maaten and g. hinton visualizing data using t sne journal ofmachine learning research vol.
no.
pp.
.