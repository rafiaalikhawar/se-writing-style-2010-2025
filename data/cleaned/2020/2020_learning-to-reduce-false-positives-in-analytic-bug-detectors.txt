learning to reduce false positives in analytic bug detectors anant kharkar microsoft redmond washington usaroshanak zilouchian moghaddam microsoft redmond washington usamatthew jin microsoft redmond washington usa xiaoyu liu microsoft redmond washington usaxin shi microsoft redmond washington usacolin clement microsoft redmond washington usa neel sundaresan microsoft redmond washington usa abstract due to increasingly complex software design and rapid iterative development codedefectsandsecurityvulnerabilitiesareprevalentinmodernsoftware.inresponse programmersrelyonstatic analysistools toregularlyscantheir codebasesandfindpotential bugs.inordertomaximizecoverage however thesetoolsgenerally tend to report a significant number of false positives requiring developers to manually verify each warning.
to address this problem weproposeatransformer basedlearningapproachtoidentify falsepositivebugwarnings.wedemonstratethatourmodelscan improvetheprecisionofstaticanalysisby17.
.inaddition we validated the generalizability of this approach across two major bug types null dereference and resource leak.
ccs concepts software and its engineering software defect analysis computing methodologies natural language generation neural networks.
keywords datasets neural networks gaze detection text tagging acm reference format anant kharkar roshanak zilouchian moghaddam matthew jin xiaoyu liu xinshi colinclement andneelsundaresan.
.learningtoreduce falsepositivesinanalyticbugdetectors.in 44thinternationalconferenceon software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction softwaredefects bugs thatgoundetectedduringthedevelopment processcancausesoftwarefailure resultinginfinancialandreputational harm to companies and a host of problems for users of buggy software.
developers often rely on static analysis tools to scan their codebases and find potential bugs.
despite their benefits static analysis tools are not consistently used in many software projects .
previous work has attributed their inconsistent usage tohighfalsepositiveratesandineffectivepresentationofwarnings .
developing any static analyzer is a non trivial task due to the trade offbetweenprecisionandrecall itischallengingtoreport only correct bugs precision while covering all bugs with a similar pattern coverage recall .
balancing these two objectives manually is difficult and can result in analyzers with high false positive rate lowprecision .analyzerswithhighinitialprecisioncanalsodegradeinpredictiveperformanceasthenatureofbugschangesover time.continuouslyupdatingandmaintainingstaticanalyzersto handle concept drift can be costly .
previous research has investigated various methods to improve staticanalysisfalsepositiverates.inparticular researchershaveexplored eliminating bugs along infeasible paths using syntacticmodel checking eliminating all the bugs that are similar to a false positive based on similarity of modification points and using a two staged error ranking strategy where false positive patterns are learned after manual labeling in the first stage .
our work uniquely contributes to this line of prior work by leveraging state of the art neural models to automatically refine the output of static analyzers.
beyondtraditionalrule basedtools therehasbeensignificant recent work leveraging machine learning for software bug and vulnerability detection in various languages including c c java andjavascript .
however much likerule based analyzers these machine learning models often suffer from low precisionwhenappliedinrealworldsettings.anotherchallenge with some machine learning approaches is the need to develop newmodelstocapturenewtypesofvulnerabilities.unlikethisline of work we do not use machine learning to detect bugs directly.
instead weleveragemachinelearningtoaugmentexistingstatic analyzers.webelievethisstrategyyieldsthebestofbothworlds ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anant kharkar roshanak zilouchian moghaddam matthew jin xiaoyu liu xin shi colin clement and neel sundaresan where machine learning complements the capabilities of current static analyzers.
toaugmentstaticanalyzers weexploredseveralmodels includingafeaturebasedmodelandtwoneuralmodels.ourfeature based model includes a set of carefully handcrafted features extracted fromsourcecode.ourneuralmodelswereinspiredbytherecent successes of transformer models in code search and document generation as well as code completion .
one of our neural models learns from labeled data deepinferenhance while the other is applied in a zero shot setting without the need for further training or finetuning gpt c .
we conducted an experiment with allthe modelson bugs identifiedby infer an interprocedural staticanalyzerthatdetectsbugsinjava c andc .ourresults showthatwecanimprovetheprecisionofinfer sanalysisbyupto .
related work wedescribethepriorworkonstaticanalyzersandtheuseofmachine learning for bug detection.
.
static analysis based bug detection rule based systems and static analyzers have been widely adopted fordetectingsoftwarebugs .however oneofthebarrierstoconsistentusageofstaticanalyzersistheirhighfalsepositive rate .
previous work has explored various ways of reducing this false positive rate.
for instance junker et al.
leveraged syntactic model checking to eliminate infeasible paths program slices .
an implementation of their approach on goanna an static analyzerforc c programsshowedthattheycouldexcludethe majority of false positives.
muske et al.
implemented a partitioning mechanism to partition similar warnings based on the modified variables and modification points.
a whole partition is then considered false positive once its leader is determined as false positive.shenetal.
developedefindbugs whichusesatwostaged error ranking strategy to deal with the false positives issue in findbugs .
efindbugs first reports warnings on a sample program.oncethewarningsaremanuallylabeled thetoollearnswhat bug patterns to eliminate on the second run against the user application.
similarly aletheia learns users preferences from manuallabelingonasmallerset .ourworkuniquelycontributestothis line of prior work by exploring the use of state of the art neuralmodels to automatically refine the output of static analyzers by removing false positives.
.
learning based bug detection beyond rule based tools there has been significant recent work on data driven and machine learning approaches to detect software bugsandvulnerabilities.forinstance russelletal.
proposeda machine learning method for detecting software vulnerabilities in c c codebases.similarly choietal.
trainedamemoryneural network to detect a variety of buffer overruns in c style code.
li etal.
trainedarecurrentneuralnetwork rnn todetecttwospecific types of vulnerabilities related to improper use of library api functions.
bugram leveraged n gram language models to identifylowprobabilitytokensequencesincodeasbugs.pangetal.
trainedamachinelearningmodeltopredictstaticanalyzerlabelsforjavasourcecode.finally deepbugs trainedaclassifierthat distinguishes correct from incorrect code for three classes of bugs swapped function arguments wrong binary operator and wrong operandinabinaryoperation injavascript.however themajority ofmachinelearningsolutionssufferfromlowprecisionwhenappliedonrealworldsettings.anotherchallengewithsomemachine learning approaches is the need to develop new models to capture new types of vulnerabilities.
by leveraging machine learning to augmentexistingstaticanalyzers ourworkcreatesthebestofboth worlds wheremachinelearningwillcomplementthecapabilities of current static analyzers to generate more precise results.
static void main string args varreturnnull returnnull returnnull.value private static nullobj returnnull return null internal class nullobj internal stringvalue get set examples nullderef program.cs error null dereference pointer returnnull couldbenulland is dereferenced at line column .
figure1 anexampleofanulldereferencedetectedbyinfer infer our false positive reduction approach can work with any static analyzerforwhichlabeleddataisavailable.ourexperimentsspecifically targeted infer an interprocedural static analyzer that is used todetectavarietyofbugsinjavaandc .therecentreleaseof infer also added support for bug detection in c projects.
infer uses separation logic a program logic for reasoning about memory manipulations to prove certain memory safety conditions and createprogramstatesummariesforeachmethodinacodebase.infer s analysis examines multiple methods in order to identify bugs in code.whenanalyzingeachmethod inferformulatespre andpostconditions thatdescribe the impactof the method onthe memory stateoftheprogram.whenanalyzingamethodinvocation infer usestheconditionsofthecalleetoformlogicalpredicatesforthe caller.
thus infer analyzes the entire call stack of a program by composinglogicalpredicatesfromallnestedcalleemethods.figure shows an example of a null dereference bug identified by infer.
we decidedto focus oninfer for threereasons.
first unlikethe majorityofcommonanalyzersthatonlyconsiderthecontextofa singlemethod i.e.intraprocedural infer sanalysisis interproceduralanditscontextcanstretchacrossseveralmethods.second as opposedtomanyanalyzersthatrelyondeveloperannotationsto detect certain bugs infer s analysis is automated and does not rely onannotations.finally duetoincrementalchangeanalysis infer can scale well on large production codebases.
like other static analyzers infer is also prone to false positives.
figure2showsanexample inwhichinferreportsthatthevariable authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning to reduce false positives in analytic bug detectors icse may pittsburgh pa usa private voiddumplog file logfile longstartoffset longendoffset arraylist string blobs throwsioexception map string logblobstatus blobidtologrecord newhashmap finaltimer.context context metrics.dumplogtimems.time try dumplog logfile startoffset endoffset blobs blobidtologrecord longtotalinconsistentblobs for string blobid blobidtologrecord.keyset logblobstatus logblobstatus blobidtologrecord.get blobid if !logblobstatus.isconsistent totalinconsistentblobs logger.error inconsistent blob blobid logblobstatus ambry tools src main java com.github.ambry store dumplogtool.java error null dereference object logblobstatus last assigned on line could be null and is dereferenced at line .
figure2 anexampleofafalsepositivewarningfrominfer.inferwarnsthat logblobstatus canbenull.thisoccursif blobid is not a valid key of blobidtologrecord.
the warning is incorrect since blobidcomes from blobidtologrecord s key set.
logblobstatus can be null since it is assigned by calling get on a map if the key is not present in the map get will return null.however thekey blobidcomesfromthe keysetofthesame map meaningthevaluemustexistinthemapand logblobstatus cannot be null.
infer is not able to recognize the coding convention of iterating over a map s key set and incorrectly triggers a nulldereferencewarning.languagemodels whicharetrainedto identify patterns across a large corpus of code can recognize such idioms.
this motivated us to turn to machine learning to detect falsepositivesreportedbyinfer.indeed ourmodelidentifiesthis specific warning as a false positive.
false positive reduction false positive infer warnings share common characteristics and follow patterns in coding conventions as described in the example above.thismotivatedustoturntomachinelearningasameans of capturing these patterns and identifying false positive warnings.
weexperimentedwithseveralmodels includingafeature based modelandtwotransformer basedneuralmodels.below wepresent thedata setwe usedfor training andtesting thesemodels aswell as the details of each model.
.
data collection our data set consists of null dereference warnings generated by running infer on seven java repositories.
null dereference bugs occurwhenapointerthatcanpotentiallybenullisdereferenced.
to create a diverse dataset we chose several open source projects andtwoproprietaryprojects.theprojectsincludeback endservice components ambry azuresdk and nacos buildplugins azure mavenplugins andbrowserautomation playwright .table1summarizestheprojectsinourdataset includingthenumberoftotal warningsandtruepositivesreportedbyvanillainfer.projectaand project b denote the proprietary projects.
eachwarningwasinvestigatedandlabeledasvalid truepositive orinvalid falsepositive byexperienced developers.theneed for manuallabelingpresentsabottlenecktoscalinguptolargerdata sets.
in the end the developers identified of the warnings as true positives .
precision and as false positives.
precision for individual repositories varied between to the lower endofthisrangecancorrespondtopoorexperiencefordeveloperstable summary statistics of null dereference warnings.total warnings and true positives are as reported by vanilla infer.
name lines of total true precision code warnings positives project a .
project b .
ambry .
azure sdk .
playwright .
nacos .
azure maven .
plugins total .
of those projects.
there is significant opportunity for machine learning to benefit the experience by improving precision.
for each warning we record the following information thelabel whether the warning was legitimate or not thelocationofthewarning includesthefilenameandline number where the warning occurred thecode this is the code snippet on the line of warning the error message the error message produced by infer thelocal context around the warning consists of all lines of code from beginning of the surrounding function to the line of the warning.
thenon local context includes the content of functions that were called in the current context.
thenon localcontext enablesustoaccountfortheinterprocedural nature of infer.
to obtain interprocedural information we collect and use the content of certain methods invoked in the local context that can impact the value of the null pointer.
for example forsomenulldereferencewarnings thenullpointeroriginatesas the return value of a method we retrieve the body of this methodas non local context.
figure3demonstratestheimportanceofnon localcontext.infer reports that the variable datacentertoadd assigned on line top can be null.
to investigate this a developer must look into thefinddatacenter methodin adifferentfile bottom .here we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anant kharkar roshanak zilouchian moghaddam matthew jin xiaoyu liu xin shi colin clement and neel sundaresan 461datacenter datacentertoadd hardwarelayout.finddatacenter datacentername 462list disk disksforreplicas allocatedisksforpartition numberofreplicasperdatacenter capacityofreplicasinbytes datacentertoadd attemptnonrackawareonfailure 465partitionlayout.addnewreplicas partition partitionid disksforreplicas 466system.out.println added partition partitionid to datacenter datacentername publicdatacenter finddatacenter string datacentername for datacenter datacenter datacenters if datacenter.getname .comparetoignorecase datacentername returndatacenter return null ambry clustermap src main java com.github.ambry.clustermap staticclustermanager.java error null dereference object datacentertoadd last assigned on line could be null and is dereferenced by call to allocatedisksforpartition ... at line .
figure3 anexampleofaninterproceduralbugdetectedbyinfer.inferreportsthat datacentertoadd top line461 canbenull.
todetermineifthisisthecase aninvestigatormustfindtheimplementationof finddatacenter bottom whichisusedto assignthevalueof datacentertoadd.since finddatacenter explicitlycontainstheline return null datacentertoadd canbe null and the warning is reasonable.
canseethat finddatacenter canreturnnullonline204ifnoneof thedatacenters match the argument meaning it is possible for datacentertoadd to be null when it is dereferenced.
therefore in order to determine whether this warning is correct the content of the callee method finddatacenter is necessary.
although it is possibleforanullpointertooriginatefrommultiplenestedmethod calls we found that in most cases collecting the immediate callee was sufficient.
.
feature based false positive reduction asabaseline weextractedfeaturevectorsfromourdataandtrained a classifier to predict whether a warning is a false positive.
our features included whether the non local context explicitly contains the line return null .ifthislineexistsinnon localcontext then it is possible for the callee to return null and the variable that holds the return value in the caller can be null.
whetheranull checkmethodappearsinthecontextofthe warning.forsomeinferwarnings thedereferencedvariable is verified to be non null earlier in the method using special null check methods e.g.
objects.requirenonnull .
since these null check methods belong to external libraries inferisunabletounderstandtheirbehavior resultinginfalse positive warnings.
whetheradereferencedvariableisaclassfield.inpractice infer s logic makes errors when tracking the state of class fields and often incorrectly treats them as nullable.
whetheranimplicitcastofawrapperclasstoaprimitivetype occurs on the warning line.
in our analysis we realized that implicitcastscanbethecauseofmanynullpointerissues.
for example when the code includes a map object with primitive typevalues e.g.hashmapwithdoublevalues the map svaluesmustinsteadbewrapperclassobjects double insteadofprimitives double sincemapsinjavacannottakeprimitives.valuesretrievedfromthemapareoftenstoredinprimitive type variables causing an implicit cast see figure .
if the wrapper object is null this cast operation causes a null dereference.
wetrainedalogisticregressionclassifieronthesefeatures.since thelimitedsizeofourdatapreventsusfromusingasimpletrain testsplit weinsteadused5 foldcross validationfortrainingand evaluation.
in a realistic scenario the model would not have access totrainingdatafromthesameprojectforwhichitismakingpredictions.
however as shown in table the projects that comprise ourdatasetvarywidelyinthenumberofwarnings andattempting to separate repos across different folds would result in insufficient training data in several folds.
to partially mitigate this issue we ensurethatallwarningsfromthesamefileappearinthesamefold.
1hashmap string double m newhashmap string double 2m.put bla newdouble .
below line will cause an implicit cast operation doublev m.get bla figure4 exampleofanimplicitcastofawrapperclass dou ble toprimitivetype double inline4.calling get onthe map mreturnsadouble whichisimplicitlycasttodoubleto comply with the type of v. .
neural false positive reduction engineeredfeatures whileeasytounderstand areinflexibleand cannot automatically learn new patterns from data.
deep learning models particularly transformers are able to better capture the complexities of modern source code.
transformers are deep neural networksthatleverageattentionmechanismstolearnpatternsin sequential data such as language.
they contain billions of parametersandcan leveragemassivedatasetstolearn representationsof languagepatterns.theyhaveachievedstate of the artresultsfor applications in natural language processing nlp such as machinetranslation questionanswering anddocumentsummarization .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning to reduce false positives in analytic bug detectors icse may pittsburgh pa usa transformers are usually pretrained on a large unlabeled corpus and further finetuned on task specific labeled data to perform classification or language generation.
thewealthofopensourcecodeavailableongithubhasinspired researchers to train a variety of transformers on open source code andfinetunethemtosupportmanydownstreamtaskssuchascode completion documentation generation automated code review software traceability and code search using natural language.
twomajorcategoriesoftransformersareauto generativemodels and auto regressive models.
auto generative models such as bert are trained to reproduce their inputs while auto regressive model produce the next token in the sequence.
in this work we leverage two transformers to help reduce infer s false positive rate.
thefirstmodel deepinferenhance isacustomizedversionofcodeberta anauto generativeencodermodelsimilartoroberta .
the second model is gpt c an auto regressive model with only decoder layers similar to gpt and gpt .
both models require only source code as input rather than any intermediate code structure such as syntax trees or control flow graphs.
.
.
deepinferenhance.
codebertais a pre trainedtransformer based on the roberta architecture and developed by huggingface .
the model was pre trained on codesearchnet a multilingual source code corpus of million functions with comments and docstrings from github.
codesearchnet consists of functions from go java javascript php python and ruby.
codebertawasinspiredbythesuccessofcodebert anapplication ofthebertarchitecturetosourcecode.codebertwasalsotrained oncodesearchnetandyieldedstate of the artresultsfortaskssuch as code search and documentation generation.
furthermore codebert s promisingresults in zero shotsettings showed thepower of its representations.
wedecidedtousethemorelightweightandefficientcodeberta architecture.however wewereinterestedinapplyinginfertoboth javaandc code andc wasnotablyabsentfromcodeberta s training dataset.
therefore we pretrained an identical codeberta model on a corpus of million java and c functions that we collected from github.
like the original codeberta our model is pretrained using a masked language modeling mlm objective.
encoder based transformers like codeberta which incorporate information from both sides of the current position can learn to create efficient representations of their entire input.
through transferlearning theserepresentationscanthenbeusedtosolve morespecifictasks.wesoughttotransferourpretrainedmodel s learnedrepresentationstothetaskofidentifyingfalsepositiveinfer warnings.
therefore we added a sequence classification head to this model in order to classify warnings as true positive or false positive.
we finetuned the model on our dataset of infer warnings by freezing all layers except for the classification head.
the inputs forfinetuningarestringsofcodecontext andthelabelsareboolean indicators of valid or invalid warnings.
our final model consists of a layer encoder and layer classification head with a total of million parameters.
we call this model deepinferenhance.
.
.
gpt c. unlikeauto generativemodels whichlearnrepresentations to reproduce their input auto regressive generative modelslearntocreatenewtext.gpt 3isoneexampleofsuchagen erative model .
because of the scarcity of labeled infer warnings for supervised learning we turned to generative models and used code completion recommendations as a signal of the legitimacy of inferwarnings.
manynulldereferencewarnings canberesolvedevenifsub optimally byintroducinganullcheckbeforethederef erence.similarly manyresourceleakbugscanbefixedbyexplicitly releasingtheleakedresource.ifagenerativemodelrecommends anull checkorresourcerelease thismayindicate thatthecorre sponding warning is indeed legitimate since the model deemedthat such a fix is necessary.
our intuition is that the model may have a fuzzy understanding that a null check or resource release is required.
to generate these code recommendations we use gpt c a generative transformer based on the well known gpt .
this model was designed and trained for code line completion and represents the state of the art in this field it was implemented as part oftheintellicodecomposewebservice.thismodelisalsomultilingual and was pretrained on c python c java javascript typescript go php ruby andc.gpt ctakesinapartiallywritten method body and uses multi headed self attention to predict the nextline.weuseitinazero shotsetting unlikedeepinferenhance we do not train gpt c ourselves but instead rely on its pretrained parameters.wedonotverifythesyntacticcorrectnessofthegener atedcode butratheruseitasa fuzzy signalonlytodetermineifa warningisvalidornot.fornulldereferencewarnings ifthemodel generates a null check statement at a line before a null pointer warning occurs we consider that warning valid.
gpt c is trained specifically for line completion rather than wholelinegeneration.thismeansthat ratherthangeneratinga full line of code from previous lines gpt c expects an incomplete line of code at the end of its input and generates code to complete thisline.thisincompletetrailinglineisa promptandconsistsof severaltokensatthebeginningofthefinalline.forourobjective ofpredictinginferwarningvalidity weprovidespecificprompts togpt cforeachwarningtype.fornulldereferences theinput promptsaretheprefixesof7differentnullcheckstatements e.g.
ifordebug.assert .foreach prompt weusegpt c withbeam search to generate line completion recommendations.
with a beam size of this results in recommendations per warning.
we also prepend non local context to the input where possible.
figure shows an example input.
each inferwarning hasan associatedfile pathand linenumber thatcorrespondtothemethodwherethewarningoccurs wecall thisthetargetmethod.forbothtransformermodels theinputto the neural network includes the source code of the target method up to but not including the line of the warning.
for gpt c we include two additional components the non local context method body preceding the target method and a line completion prompt immediately following the target method.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anant kharkar roshanak zilouchian moghaddam matthew jin xiaoyu liu xin shi colin clement and neel sundaresan for node datanode nodes if allocateddisks.size numberofreplicas break disk disk datanode.getdiskwithmostcapacity replicasize allocateddisks.add disk disk.freecapacity disk.freecapacity replicasize publicdiskgetdiskwithmostcapacity long replicasize disk mindisk null for disk disk disks if mindisk null mindisk.freecapacity disk.freecapacity disk.freecapacity replicasize mindisk disk returnmindisk public static voidstrategy3 datacenter dc list partition partitions intnumberofpartitions intnumberofreplicas longreplicasize for inti i numberofpartitions i list node nodes dc.nodes collections.shuffle nodes list disk allocateddisks newarraylist disk for nodedatanode nodes if allocateddisks.size numberofreplicas break disk disk datanode.getdiskwithmostcapacity replicasize allocateddisks.add disk if figure an example of a legitimate infer warning top and the corresponding input to gpt c bottom .
infer reports that disk whichisassignedby getdiskwithmostcapacity bottom canbenullandisdereferencedonline207.thegpt cinput is constructed by appending a prompt to the method body preceding this line as well as prepending the non local contextmethod getdiskwithmostcapacity .
here gpt c correctly predicts a null check and therefore this warning is regarded as legitimate by the gpt c based model.
experiments we performed two experiments to better understand how these models perform in a real world setting.
the first experiment summarized in table was focused on comparing effectiveness of our feature based and neural models.
the second experiment focused on verifying the generalizability of our neural approaches when appliedtoadifferentbugtype.sinceourobjectiveistoeliminate falsepositivesreportedbyinfer ourprimarymetrictoevaluateourmodelsistherelativeprecisionimprovementovervanillainfer.we alsomeasurerecallwithrespecttoinfer struepositivewarnings arecallof100 meansthatallofthetruepositivewarningsfrom vanillainferwerereported.byconstruction noneoftheapproaches inthisworkreportnewwarningsbeyondthoseoriginallyreported by infer.
table performance of machine learning for removing false positive null dereference warnings approach precision precision recall baseline .
feature based .
.
.
deepinferenhance .
.
.
gpt c .
.
.
.
experiment comparing feature based and neural models .
.
feature based model.
the simplest data driven approach toidentifyfalsepositiveinferwarningsistomanuallysearchfor patternsinthewarnings.thehandcraftedfeatureswecollectedfor ourlogisticregressionmodelcapturethepatternsthatwediscovered from manual review of our dataset.
this feature based model was able to improve infer s precision by but with significant reduction in recall.
since source code can be inherently complex it is unsurprising that simple handcrafted features are insufficient to identify false positive warnings.
.
.
deepinferenhance.
sincehandcraftedfeaturescannotadequately represent source code we turned to deep learning to au tomatically learn patterns in code that indicate the legitimacy of infer warnings.
we took a traditional supervised transfer learning approach using our dataset of labeled warnings to finetune our deepinferenhance model.
the results show that this model greatlyimprovedprecisionandrecallcomparedtothefeature basedmodel.
transformersaregenerallyfinetunedonmuchlargerdatasets thantheseveralhundredwarningsweused.however deepinferenhance was still able to learn patterns that provided a significant improvementinprecision.onesuchpatternoccurswhen nullis explicitlypassedasanargumenttoamethod.evenifthemethod authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning to reduce false positives in analytic bug detectors icse may pittsburgh pa usa handles null arguments infer still reports a null dereference warning which is often a false positive.
deepinferenhance is able to learn this pattern purely from the code itself.
.
.
gpt c. deepinferenhance is able to boost vanilla infer s precision but requires labeled data.
since this data is expensive to collect we sought a solution that could forego supervised learning altogether.
transformer models such as codebert have shown promisingperformanceinzero shot settingsforsourcecode.our approach is novel due to our interpretation of code completion recommendations by self supervised generative models recom mendations for null checks are a signal that the null dereference warning is legitimate.
with this approach gpt c had the highest precision of our models improving on infer by relative .
with slightly lower recall than deepinferenhance.
whileanalyzingtheresultsfromgpt c weidentifiedseveral patterns in the warnings that gpt c predicts incorrectly.
these patterns included insufficientornonexistentnon localcontext ourinvestigationrevealedthat whennon localcontextisunavailableorcontains insufficientinformation gpt cdoesnotperformwell.forexample methodcallsbelongingtoaninterfacetypecannotberesolveduntil runtime.
therefore we cannot retrieve such methods as non local context.similarly non localcontextcanincludegettermethodsthat return a class field however these methods do not provide any informationaboutthevaluetheclassfieldmayhold.intheexampleinfigure6 inferwarnsthat handlermethod whichisassignedusing gethandlermethod canbenull.
gethandlermethod middle simply returns the handlermethod class field bottom .
in order to correctly determine if the local variable handlermethod can be null we would need to collect not only the bodies of the methods gethandlermethod andcreatemessagingerrormessage but also the constructors and fields of defaultazuremessagehandler .
no reference to the target object nulldereferencewarnings generally fall into two categories with respect to the target pointer.
for some warnings the null pointer is represented by a variable in thesourcecode forotherwarnings thepointerisreturnedfrom a method with no explicit variable to hold its value.
the latter case presents a problem for gpt c recommendations the nullable pointer has no reference in the code before the line where the warning occurs which is not included in the input to the model.
therefore this pointer does not appear in the input to gpt c reducing the chance that gpt c recommends a null check.
excessive sensitivity to the input several infer warnings in asingleprojectcanrefertosimilarcode oftenwiththesametarget variable or method.
in such cases if the instances are truly similar and legitimate all of the instances should be reported as bugs to theenduser.however becausegpt cisverysensitivetominor differencesintheinputsequence itmayreportonlyasubsetofthe warnings as legitimate.
to enforce consistency we group together warningswiththesametargetvariableandlabelthemallaccording to a logical or where all warnings are predicted as legitimate if gpt cpredictsanywarninginthegroupaslegitimate.theresults intable2includethisconsistencypostprocessing.alternatively warnings could be grouped according to code similarity metrics such as edit distance.
.
.
overall result.
deepinferenhanceandgpt cofferatradeoff.
ourobjectiveistoincreaseprecision forwhichgpt cisbest.however deepinferenhance has significantly better recall capturing oflegitimatebugswhilestillprovidinga15 boostinprecision over vanilla infer.
because of its superior recall we would be more likelytorecommenddeepinferenhancetodeveloperswhovalue coverage in addition to precision.
h owever this model requires finetuning whereas gpt c offers the best precision and moderate recall without the need for additional data or further training.
.
experiment verifying the generalizability of neural approaches toverifythegeneralizabilityofourneuralapproachesbeyond thenull pointer bug we evaluated our gpt c model on infer s resourceleak warnings.aresourceleakhappenswhenaprogram does not release resources it has acquired.
the below code snippet shows an example of a resource leak warning where an exception inf.write will cause the program to skip the f.close statement and leak the stream resource.
56public static voidfoo throwsioexception 57fileoutputstream f newfileoutputstream new file w 58f.write an exception here will cause a leak 59f.close in our target java projects infer detected a total of resource leak warnings.
table shows the summary statistics of the iden tified resource leaks infer did not detect any resource leaks in ambry .
table summary statistics of resource leak warnings.total warnings and true positives are as reported by vanillainfer.
name lines of total true precision code warnings positives project a .
project b .
azure sdk .
playwright nacos .
azure maven .
plugins total .
sincethisdatasetwasnotlargeenoughtomeaningfullyfinetune deepinferenhance we decided to only focus on our gpt c model wherenofurthertrainingorfinetuningisrequired.weonlyhadto adjustourpromptinglogic.forresourceleaks weuseprefixes first three characters of the method names close andrelease as theprompts.iftheleakedresourceisassignedtoavariable wealso use this variable name as a prompt.
table shows the results of using gpt c to remove false positives in resource leak warnings.
as shown in the table gpt c can improve infer s precision by .
.
however it fails to identify over a third of legitimate bugs.
one pattern in the missed bugs is that some leaked resources have names or types such as entitynotfoundhttpresponse or changefeedprocessorbuilderimpl that do not clearly indicate authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa anant kharkar roshanak zilouchian moghaddam matthew jin xiaoyu liu xin shi colin clement and neel sundaresan public class defaultazuremessagehandler implements azuremessagehandler nullable private invocablehandlermethod handlermethod private class ?
messagepayloadtype private stringcreatemessagingerrormessage string description invocablehandlermethod handlermethod gethandlermethod stringbuilder sb newstringbuilder description .append n .append endpoint handler details n .append method .append handlermethod.getmethod .append n .append bean .append handlermethod.getbean .append n returnsb.tostring publicinvocablehandlermethod gethandlermethod returnhandlermethod figure6 anexampleofaninferwarningwherenon localcontextdoesnotprovideenoughinformationaboutthevalueofa class field.
infer warns that handlermethod can be null top .
gethandlermethod simply returns the handlermethod class field bottom .
but this is not enough to determine the legitimacy of this warning.
that they are in fact resources and therefore should be released.
types that clearly indicate resources such as those that contain fileorstreamin the name are recognized more often by gpt c. table performance of machine learning for removing false positive resource leak warnings approach precision precision recall gpt c .
.
.
discussion our gpt c model improved infer s precision by .
for null dereferences and by .
for resource leaks.
however it missed someofthecorrectwarningsthatinferdetected witharecallof fornulldereferences and65 for resourceleaks.weidentified several patterns of false negative predictions which resulted in the reducedrecall.onepatternoccurredwhenthenon localcontext wasaclassfieldgettermethod.thesemethodsareoftenasingle return statement which is not sufficient information for gpt c tomakethecorrectprediction.onewaytomitigatethisproblem couldbetoincludeclassfieldsandconstructorsaspartofnon localcontext.
however the current gpt c model is only trained for linecompletionusingsinglemethodbodies.newertransformermodelsforcode whichusesupplementarycontextinadditiontoindividual method bodies can better leverage this context to create morecomplete representations of the program state.
therefore future work should explore training an extended context model for code completion asanevolutionofthegpt cmodelweused.weexpectthatsuchamodelwouldperformbetterinmanydownstreamtasks including for verifying true positive warning from static analyzers.
anotherclassofwarningsforwhichgpt cdidnotperformwell were chained method calls e.g.
foo.bar .baz .
if a warning is triggered on a method call in the middle of a chain gpt c cannot reasonably predict a null check.
since the intermediate method call is not stored in a variable we cannot prompt gpt c to predicta null check for the return value of that method call.
one wayto mitigate this problem is to modify the source code to insert a variable assignment for each method call in the chain.
however a developer would only break the method chain for a null check wherenecessary.therefore breakingthechainmaycreateanabnormal codepattern thatgpt cwill notrecognize.
alternatively thevariableassignmentcouldbeinsertedforonlyonemethodin the chain.
for each method call in the chain we could insert anassignment generate recommendations using gpt c and selectthe recommendation with the highest confidence.
however we decidedonamuchsimplerapproachtomitigatechainedmethod calls simply trust infer s decision and predict such warnings as legitimate bugs.
foranybugdetectionsystem precisionandrecall havesignificantly different downstream impacts for developers.
low precision means that developers waste time analyzing many false positive warnings while low recall means that some legitimate bugs are not identified.
static analyzers have typically favored coverage and recall over precision with the objective of maximizing the number of reported legitimate bugs.
however in practice low precision reducesdeveloperadoptionofanalysistools duetothetime developerswasteoninvestigatingfalsepositives.priorworkhas found that developers mostly use analysis tools in their spare time andtendtofixwarningsinshortworkingsessions.therefore they are primarily driven by time constraints when addressing bugs identifiedbystaticanalyzers .asaresult wechosetofocuson precision rather than recall we believe presenting developers with higher quality warnings will lead to bugs actually being addressed rather than ignored due to a lack of confidence or time constraints.
howeverincertaincases whererecallismoreimportant ourmodels can be used to re rank the warnings so that developers are presented withmore true positivesfirst.
thisallows us topresent all warnings to developers while prioritizing likely legitimate bugs.
we demonstrated the effectiveness of transformer models for two bug types and one tool but we believe this approach should generalizetootherlanguagesandtools.ourexperimentonresource authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
learning to reduce false positives in analytic bug detectors icse may pittsburgh pa usa leaksprovidesevidenceofthis.inaddition ourgpt capproach isnottiedtoaparticularprogramminglanguage staticanalyzer or warning type.
because we use gpt c in a zero shot setting no furthertrainingisrequired.customizationmayinsteadberequired throughuniquewaysofpromptinggpt candspecificsignalsto seekinitsoutputs adjustingpromptsshouldbetheonlychange necessary to apply gpt c to new bug types.
for example one way to apply this technique to buffer overflow bugs in c c couldbe to search for bounds checking recommendations.
even deep inferenhance which requires labeled data for finetuning can be expanded to additional programming languages by pretraining on a larger and more diverse corpus.
inthisworkweappliedlarge scaletransformerstofurtherverify bugs that have already been localized by static analyzers.
whilethismeansthatourapproachwillnotfindbugsbeyondthosereported by the static analyzers it is a cost effective way to leverage transformers for this problem.
large scale transformer models are expensive to train and evaluate and using them to scan everymethod or every line of a project can be prohibitively expensive.by applying these models to resolve warnings that have alreadybeen localized by a static analyzer we ensure that transformersare utilized in a cost effective way .
however there may be other waystolocalizebugsandusetransformers.forexample onecan leverage prior work on bug localization to determine buggy files andonlyexaminethosefilesasopposedtotheentireprogram.
.
threats to validity .
.
dataset size.
static analysis warnings are time intensive to triage since each warning requires a detailed review of the source code involved in the warning.
it is expensive to collect a large datasetoflabeledwarnings whichispreferredwhentrainingtrans formermodels.thisisparticularlyimpactfulfordeepinferenhance which requires labeled data for both finetuning and evaluation.
although we used cross validation to compensate for the limiteddataset size all of our approaches would benefit from a largerdataset.
in order to scale the dataset we must present warningsto project owners for review.
through developer engagements wefoundthatthisraisesacold startproblem inordertoreceive appropriate attention and high quality feedback warnings must havesufficientlyhighprecision orelsedevelopersmaynotengage with warnings shared for labeling purposes.
we believe that the precision improvements of the approaches discussed here serve asasolution tothiscold startproblem andwillallowus toshare warnings with a wider set of projects to scale our dataset.
.
.
evidence of generalizability.
ourapproachofusingmachine learning to augment and complement static analysis is designed genericallytobenefitanyanalyzer.however inthisstudy wefocus on one analyzer infer and two categories of bugs null derefer ence and resource leak for one language java .
to gain wider adoption among developers of diverse projects our approach must demonstrate benefits across additional languages and bug types.
ourexperimentswithresourceleaksareourfirstattempttodemonstratethis.weplantoapplyandevaluateourapproachtoc as well as additional languages as the next step for expanding our approach.
.
data release ourdatasetconsistsofwarningsfrominferforvariousopensource and proprietary software projects.
source code from proprietary projectswasmadeavailabletoussolelyforresearchpurposes.since wedonotownthisdata wecannotreleaseitpublicly.weintendto release data from open source projects after we have worked with eachprojectownerto resolvetheissues orotherwiseverifywith the owners the safety of releasing bug or vulnerability data.
conclusion rule based bug detectors and static analyzers have been widely adopted for detecting security vulnerabilities functional bugs and even performance issues.
however building an analyzer is nontrivialbecauseofthedifficultyofbalancingprecisionandcoverage reportingonlycorrectbugsandensuringthatallsimilarbugsare reported.
themajorityofexistinganalyzersfavorhighercoveragetoensure completeness and therefore they produce more false positive warnings.however frequentfalsepositivewarningsareoneofthe main barriers to wider adoption of static analyzers in the software industry this problem cannot be solved by the analyzers them selves.
to close this gap we augmented static analyzers with a varietyofmachinelearningmodels.weexperimentedwithboth feature basedandneuralmodelsforfalsepositivereduction.our experimentsoninfer awell knowninterproceduralstaticanalyzer showed that leveraging gpt c in a zero shot setting can improve theprecisionofnullpointerwarningsby17.
andresourceleak warnings by .
one immediate direction for future work is to experiment with more warning types and languages to further verify the gener alizability of our approach.
another direction involves training transformers with broader context.
for instance one may include the imports constructors class fields and superclasses in cases of inheritance aspartofthecontextwhiletraining.weexpectthis broader context to increase transformer effectiveness in general and especially in zero shot settings to augment other code analyzers.athirddirectionistoexplorewhetheragenerativetransformer similar to gpt c can be used in conjunction with a static analyzer to suggest fixes for some or all the bugs.