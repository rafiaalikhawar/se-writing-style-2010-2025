causal testing understanding defects root causes brittany johnson university of massachusetts amherst amherst ma usa bjohnson cs.umass.eduyuriy brun university of massachusetts amherst amherst ma usa brun cs.umass.edualexandra meliou university of massachusetts amherst amherst ma usa ameli cs.umass.edu abstract understanding the root cause of a defect is critical to isolating and repairingbuggybehavior.
wepresentcausaltesting anewmethod of root cause analysis that relies on the theory of counterfactual causality to identify a set of executions that likely hold key causal information necessary to understand and repair buggy behavior.
using the defects4j benchmark we find that causal testing could be applied to of real world defects and for of those it can help developers identify the root cause of the defect.
a controlled experimentwith37developersshowsthatcausaltestingimproves participants ability to identify the cause of the defect from of the time with standard testing tools to of the time with causal testing.
theparticipantsreportthatcausaltestingprovidesuseful information they cannot get using tools such as junit.
holmes our prototype open sourceeclipsepluginimplementationofcausal testing is available at ccs concepts software and its engineering software testing and debugging.
keywords causaltesting causality theoryofcounterfactualcausality software debugging test fuzzing automated test generation holmes acm reference format brittany johnson yuriy brun and alexandra meliou.
.
causal testing understandingdefects rootcauses.
in 42ndinternationalconferenceon software engineering icse may seoul republic of korea.acm newyork ny usa 13pages.
introduction debuggingandunderstandingsoftwarebehaviorisanimportant part of building software systems.
to help developers debug many existing approaches such as spectrum based fault localization aim to automatically localize bugs to a specific location in the code .
however finding the relevant line is often not enough to help fix the bug .
instead developers need help identifying and understanding the root cause of buggy behavior.
whiletechniquessuchasdeltadebuggingcanminimizeafailing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthefirstpage.
copyrightsforcomponentsofthisworkownedbyothersthanthe author s mustbehonored.
abstractingwithcreditispermitted.
tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
input and a set of test breaking changes they do not help explain whythe code is faulty .
to address this shortcoming of modern debugging tools this paperpresents causaltesting anoveltechniqueforidentifyingroot causes of failing executions based on the theory of counterfactual causality.
causaltestingtakesamanipulationistapproachtocausal inference modifying and executing tests to observe causal relationshipsandderivecausalclaimsaboutthedefects rootcauses.
givenoneormorefailingexecutions causaltestingconducts causal experiments by modifying the existing tests to produce a small set of executions that differ minimally from the failing ones butdonotexhibitthefaultybehavior.
byobservingabehaviorand then purposefully changing the input to observe the behavioral changes causaltestinginferscausalrelationships thechange in the input causesthe behavioral change.
causal testing looks for two kinds of minimally different executions ones whose inputs are similar and ones whose execution paths are similar.
whenthedifferencesbetweenexecutions eitherintheinputsorinthe execution paths are small but exhibit different test behavior these small causal differences can help developers understand what is causing the faulty behavior.
consider a developer working on a web based geo mapping service such as google maps or mapquest receiving a bug report that the directions between new york ny usa and ren l vesqueblvd.wmontreal qc canada arewrong.
thedeveloper replicates the faulty behavior and hypothesizes potential causes.
maybe the special characters in ren l vesque caused a problem.maybethefirstaddressbeingacityandthesecondaspecificbuild ingcausedamismatchininternaldatatypes.
maybetherouteistoo long and the service s precomputing of some routes is causing theproblem.
maybe construction on the tappan zee bridge along the route has created flawed route information in the database.
there are many possible causes to consider.
the developer decides to step through the faulty execution but the shortest path algorithm coupled with precomputed route caching and many optimizations is complex and it is not clear how the wrong route is produced.
the developer gets lost inside the many libraries and cache calls and the stack trace quickly becomes unmanageable.
suppose instead atoolhadanalyzedthebugreport stestand presentedthedeveloperwiththeinformationinfigure .
thedeveloperwouldquicklyseethatthespecialcharacters thefirstaddressbeingacity thelengthoftheroute andtheconstructionarenottherootcauseoftheproblem.
instead allthefailingtestcaseshaveoneaddressintheunitedstatesandtheotherincanada whereasallthepassingtestcaseshaveboththestartingandendingaddressesinthe same country.
further the tool found a passing and a failing input withminimalexecutiontracedifferences thefailingexecutioncontains acall to the metricconvert pathsofar method butthe passing one ieee acm 42nd international conference on software engineering icse failing new york ny usa to ren l vesque blvd.
w montreal qc canada failing boston ma usa to ren l vesque blvd.
w montreal qc canada failing new york ny usa to harbour square toronto on ca nada passing new york ny usa to dalton st boston ma usa passing toronto on canada to ren l vesque blvd.
w montreal qc canada passing vancouver bc canada to ren l vesque blvd.
w montreal qc canada minimally different execution traces failing passing findsubendpoints sor6 tar7 findsubendpoints sor6 tar7 findsubendpoints sor7 tar8 findsubendpoints sor7 tar8 metricconvert pathsofar findsubendpoints sor8 tar9 findsubendpoints sor8 tar9 figure passing and failing tests for a geo mapping service application and test execution traces.
doesnot.1armedwiththisinformation thedeveloperisnowbetter equipped to find and edit code to address the root cause of the bug.
we implementcausal testingin anopen source proof of concept eclipse plug in holmes that works on java programs and interfaces with junit.
holmes is publicly available at cs.umass.edu .
weevaluatecausaltestingintwoways.
first we use holmes in a controlled experiment.
we asked developers to identify theroot causes of real worlddefects with and without accesstoholmes.
wefoundthatdeveloperscouldidentifytheroot cause86 ofthetimewhenusingholmes butonly80 ofthetime without it.
second we evaluate causal testing s applicability to real worlddefectsbyconsideringdefectsfromreal worldprograms in the defects4j benchmark .
we found that causal testing couldbeappliedto71 ofreal worlddefects andthatfor77 of those it could help developers identify the root cause.
a rich body of prior research aims to help developers debug faulty behavior.
earlier mentioned fault localization techniques rank code locations according tothelikelihoodthattheycontainafault forexampleusingtest cases or static code features .
the test based rankings canbeimproved forexample bygeneratingextratests or by applying statistical causal inference to observational data .
automatedtestgenerationcancreatenewtests whichcanhelpdiscover buggy behavior and debug it and techniques canminimizetestsuites andindividualtests to help deliver the most relevant debugging information to thedeveloper.
these techniques can help developers identify where thebugis.
bycontrast causaltestingfocusesonexplaining why buggy behavior is taking place.
unlike these prior techniques causaltestinggenerates pairsofverysimilartests thatnonetheless exhibit different behavior.
relatedly considering tests that exhibit minimally different behavior bugex focuses on tests that differ slightlyinbranchingbehavior anddarwingeneratesteststhat 1note that prior work such as spectrum based fault localization can identify the differences in the traces of existing tests the key contribution of the tool we describe here is generating the relevant executions with the goal of minimizing input and execution trace differences.pass a version of the program without the defect but fail a version with the defect .
unlike these techniques causal testing requires only a single faulty version of the code and only a singlefailing test and then conducts causal experiments and uses the theory ofcounterfactual causality toproduce minimally different testsandexecutionsthathelpdevelopers understand thecauseof the underlying defect.
therestofthispaperisstructuredasfollows.
section 2illustrates how causal testing can help developers on a real world defect.
sections3and4describe causal testing and holmes respectively.
section5evaluateshowusefulholmesisinidentifyingrootcauses and section 6evaluates how applicable causal testing is to realworlddefects.
section 7discussestheimplicationsofourfindings and limitations and threats to the validity of our work.
finallysection8places our work in the context of related research and section9summarizes our contributions.
motivating example consider amaya a developer who regularly contributes to open source projects.
amaya codes primarily in java and regularly uses theeclipseide andjunit.
amayais workingonaddressingabug report in the apache commons lang project.
the report comes with a failing test see in figure .
figure2showsamaya sideassheworksonthisbug.
amaya runs the test to reproduce the error and junit reports that an exceptionoccurredwhiletryingtocreatethenumber 0xfade see2in figure2 .
amaya looks through the junit failure trace looking for theplacethecodethrewtheexception see 3figure2 .
amayaobserves that the exception comes from within a switchstatment and thatthereisnocaseforthe eattheendof 0xfade.
toaddsuchacase amayaexaminestheother switchcasesandrealizesthateachcaseis makingadifferentkindofnumber e.g.
thecasefor lcreateseither alongorbiginteger.
since 0xfadeis amaya conjectures that this numberfitsinan int andcreatesanewmethodcallto createinteger inside of the case for e. unfortunately the test still fails.
using the debugger to step through the test s execution amaya seesthe numberformatexception thrownonline545 see 3infigure .
sheseesthattherearetwootherlocationstheinputtouches see 4and5in figure during executionthat could beaffectingthe outcome.
shenow realizesthat thecodeon lines497 despite being where the exception was thrown may not be the location of the defect s cause.
she is feeling stuck.
butthen amayaremembersafriendtellingheraboutholmes a causaltestingeclipseplug inthathelpsdevelopersdebug.
holmes tellsher thatthecode failsonthe input 0xfade but passesoninput 0xfade.
the key difference is the lower case x. also according to the execution trace provided by holmes these inputs differ inthe execution of line see 4in figure .
the ifstatement fails to check for the 0xprefix.
now armed with the cause of the defect amaya turns to the internet to find out the hexadecimal specificationandlearnsthatthetestisright 0xand 0xarebothvalid prefixes for hexadecimal numbers.
she augments the ifstatement and the bug is resolved!
holmesimplementscausaltesting anewtechniqueforhelping understand root causes of behavior.
holmes takes a failing testcase or test cases and perturbs its inputs to generate a pool of figure amaya s eclipse ide while she is debugging a defect evidenced by a failing test.
possible inputs.
for example holmes may perturb 0xfadeto0xfade 0xfade edafx 0xfad xfade fade andmanymore.
holmesthenexecutes alltheseinputstofindthosethatpasstheoriginaltest soracle and next selects from the passing test cases a small number such that either their inputs or their execution traces are the most similar to the original failing test case.
those most similar passing test cases help the developer understand the key input difference that makes the test pass.
sometimes holmes may find other failing test cases whose inputs are even more similar to the passing ones than the originalinput anditwouldreportthosetoo.
theideaistoshow the smallest difference that causes the behavior to change.
holmes presents both the static test input and dynamic execution trace information to the developer to compare the minimallydifferent passing and failing executions to better understand the root cause of the bug.
for example for this bug holmes showsthe inputs 0xfadeand 0xfade and the traces of the two executions showingthatthepassingtestentersamethodfrom createinteger that the failing test cases do not dictating to amaya the expected code behavior leading her to fix the bug.
causal testing amaya s debugging experience is based on what actual developers did while debugging real defects in a real world version of apache commons lang taken from the defects4j benchmark .
as the example illustrates software is complex and identifying root causesofprogramfailuresischallenging.
thissectiondescribesour causaltestingapproachtocomputingandpresentingdevelopers with information that can help identify root causes of failures.figure3describesthecausaltestingapproach.
givenafailing test causaltestingconductsaseriesofcausalexperimentsstarting withtheoriginaltestsuite.
causaltestingprovidesexperimental resultstodevelopersintheformofminimally differentpassingand failing tests and traces of their executions.
.
causal experiments with test cases causaltestingmodifiestestcasestoconductcausalexperiments it observes system behavior and then reports the changes to test inputsthatcausesystembehaviortochange.
tocreatethesetest asserttrue createnumber string 9b failed 0xfade numberutils.createnumber 0xfade .intvalue 0xfade 0xfadeedafx0 0xfadxfade fade ... 0xfade 0xfade 0xfade 0xfade 0xfade edafx 0xfade 0xfad 0xfade xfade 0xfade fade 0xfade 0xfade createnumber createnumber str.isblank str.startswith ...... figure causal testing computes minimally different test inputs that nevertheless produce different behavior.
89case modifications and to then identify the modifications that lead to behavioral change causal testing needs a systematic way of perturbing inputs and of measuring test case similarity which we describe in this section.
once the experiments are complete causaltestingreportstothedeveloperalistofminimallydifferent passingandfailingtestcaseinputsandtheirexecutiontraces to help explain root causes of the failing behavior.
.
.
perturbing test inputs.
toconductcausalexperiments causal testingstartswithafailingtest whichweshallcallfromnowon theoriginalfailingtest andidentifiestheclassthistestistesting.
causal testing considers all the tests of that class and generates moretestsusingautomatedtestinputgeneration andtheoracle from the one failing test to create a set of failing and passing tests.
then causaltestingfuzzestheseexistingandgeneratedtestinputstofindadditionalteststhatexhibitexpectedandunexpected behavior.
theoretically itisalsopossibleforcausaltestingtoperturbthe testoracle.
forexample itmightchangethe asserttrueinfigure 3to assertfalse.
however perturbingtestoraclesisunlikelytoproduce meaningfulinformationtoguidethedevelopertotherootcause or atleast islikelytoproducemisleadinginformation.
forexample making a test pass simply by changing the oracle does not provide information about key differences in test inputsthat alter software behavior.
assuch causaltestingfocusesonperturbingtestinputs only.
therearedifferentwayscausaltestingcouldassemblesetsof passing and failing tests.
first causal testing could simply rely on the tests already in the test suite.
second causal testing could use automated test generation to generate a large number of test inputs.
third causal testing could use test fuzzing to change the existing tests inputs to generate new similar inputs.
fuzz testing is an active research area although the term fuzztestingisalsousedtomeansimplygeneratingtests and hasbeenappliedinthesecuritydomaintostress testanapplication and automatically discover vulnerabilities e.g.
.
whileinreal worldsystems existingtestsuitesoftencontain both passing and failing tests these suites are unlikely to havesimilar enough pairs of one passing one failing tests to provideuseful information about the root cause.
still it is worthwhileto consider these tests first before trying to generate more.
as such oursolutiontothechallengeofgeneratingsimilarinputsis to start with all existing tests use multiple fuzzers to fuzzthese tests generate many tests and filter those tests to selecttheonessimilartotheoriginalfailingtest.
asweobserved withholmes our proof of conceptcausal testingtool described in section using multiple input fuzzers provided a diverse set of perturbations increasing the chances that causal testing finds a set of minimally different inputs and that at least one of them would lead to a passing execution.
.
.
input similarity.
giventwoteststhatdifferintheirinputs butsharethesameoracle causaltestingneedstomeasurethesim ilaritybetweenthetwotests asitsgoalistofindpairsofminimally differentteststhatexhibitoppositebehavior.
conceptually toapply thetheoryofcausalinference thetwotestsshoulddifferinonly one factor.
for example imagine a software system that processesapartmentrentalapplications.
iftwoapplicationinputsareiden tical in every way except one entry and the software crashes onone but not on the other this pair of inputs provides one pieceof evidence that the differing entry causesthe software to crash.
otherpairsthatalsoonlydifferinthatoneentrywouldprovide more such evidence.
if the inputs differed in multiple entries it would be harder to know which entry is responsible.
thus to help developers understand root causes causal testing needs to preciselymeasureinputsimilarity.
weproposetwowaystomeasure input similarity syntactic differences andexecution path differences.
static input differences.
the static input similarity can be viewed at different scopes.
first inputs can agree in some and differ in others of their arguments e.g.
parameters of a method call .
agreementacrossmoreargumentsmakesinputsmoresimilar.
second each argument whose values for the two tests differ candiffer to varying degrees.
a measure of that difference dependson the type of the argument.
for arguments of type string the levenshtein distance the minimum number of single characteredits required to change one stringinto the other is a reasonable measure thoughthereareothersaswell suchashammingdistance difference between two values at the bit level .
for numerical arguments their numerical difference or ratio is often a reasonable measure.
we found that relatively simple measures of similarity suffice for general debugging and likely workwell in many domains.
using levenshtein or hamming distance for strings the arithmetic differencefornumericalvalues andsumsofelementsdistancesfor arrays worked reasonably well in practice on the defects from four different real world systems we examined from the defects4j benchmark .
however more generally the semantics of similarity measures are dependent on the domain.
some arguments may play a bigger role than others and the meaning of some types may only make sense in the particular domain.
for example in apartment rental applications a difference in the address may play a much smaller role than a difference in salary or credit history.
as such how the similarity of each argument is measured and how the similarities of the different arguments are weighed are specifictothedomainandmayrequirefinetuningbythedeveloper especiallyforcustomdatatypes e.g.
project specific objecttypes .
still intheend wefoundthatsimple domain agnosticmeasures worked well in the domains we examined.
execution path differences.
along with static differences twoinputs candiffer basedon theirdynamic behaviorat runtime.
one challenge when considering only static input differences isthat a statically similar input may not always yield an outcome thatisrelevanttotheoriginalexecution.
forexample itispossible that two inputs that differ in only one character lead to completely incomparable unrelated executions.
therefore causal testing also collects and compares dynamic information in the form of the execution path the input causes.
beyond simplistic ways to compare executions such as by their lengths comparing the statements and method calls in each execution provides information we found helpful to understanding root causes.
thisalsostrengthensthecausalconnectionbetweenthein putchangeandthebehaviorchange iftwoinputs executions one passing and one failing only differ by one executed statement it is likely that one statement plays an important role in the behavioral 90change.
augmentingmethodcallswiththeirreturnvaluesprovides additionalinsightsinsituationswherethebugisevidentnotbythe sequence of statements executed but in the use of a method that returns an unexpected value.
both static and execution path measures of similarity can be useful in identifying relevant tests that convey useful information todevelopers.
inputsthataresimilarbothstaticallyandintermsofexecutionpathsholdpotentialtoconveyevenmoreusefulinformation astheyhaveevenfewerdifferenceswiththeoriginalfailing test.
therefore causal testing prioritizes tests whose inputs are statically and dynamically similar to the original failing test.
.
communicating root causes to developers after generating and executing test inputs causal testing ranks them by similarity and selects a user specified target number of themostsimilarpassingtestcases.
inourexperience threetests was a good target though at times a time out was necessary because finding three similar passing tests was computationallyinfeasible.
causal testing reports tests as it finds them produce resultsforthedeveloperasquicklyaspossible whileitperforms more computation looking for potentially more results.
causal testing collects the input and the execution traces for each test it executes.
these are of course used for determining testcasesimilarity butalsoholdthekeyinformationintermsof whatdifferencesintestinputsleadtowhatbehavioralchanges.
for thepairsoffailingandpassingtests causaltestingpresentsthe static differences in inputs and the execution traces along with eachmethodcall sargumentsandreturnvalues withdifferences highlighted.
becauseexecutiontracescanget large parsingthem can be difficult for developers showing differences in the tracessimplifies this task.
causal testing displays a minimized trace focused on the differences.
holmes a causal testing prototype we have implemented holmes an open source eclipse plug in causaltestingprototype.
holmesisavailableat umass.edu and consists of four components input and test case generators editdistancecalculators comparers a testexecutor comparator and an output view.
.
input test case generation holmesfirsttaskistocreateasetofcandidatetestcases.
holmes first searches all tests in the current test suite for tests that are similartotheoriginalfailingtestusingstringmatchingtodetermineiftwotestsaresimilar.
morespecifically holmesconvertstheentiretestfiletoastringandparsesitlinebyline.
thisisanapproximation of test similarity.
future work can improve holmes by considering similarityindynamicexecutioninformationbetweenthetwotests orbycreatingnewtestsbyusingtestinputsfromothertestsbut the oracle from the original failing test.
next holmes proceeds to generate new tests.
holmes gets new inputs for generating new tests in two ways test case generation.
holmes uses an existing test case generationtool evosuite .
wechoseevosuitebecause itisastate of the art open sourcetoolthatworkswithjavaand junit.
holmes determines the target class to generatetests from based on the class the original failing test tests.
forexample iftheoriginalfailingtestiscalled numberutilstest holmes tells evosuite to generate tests for numberutils.t o determineifatestisrelatedtotheoriginalfailure holmes searches the generated tests for test cases that call the same methodastheoriginaltest.
fromthisprocess holmeswill get at least one valid input to use during fuzzing.
input fuzzing.
to generate additional inputs for new tests holmes fuzzes existing and generated testinputs.
holmesuses two off the shelf open source fuzzers peach 2and fuzzer3.
to increase the chances that fuzzed inputs will producepassingtests holmesprioritizes whenavailable inputs from passing tests.
holmes fuzzes the original inputandallvalidinputsfromgeneratedtestcases againto increase the chance of finding passing tests.
onceholmesrunstestgenerationandfuzzesthevalidinputs the next step is to determine which of the generated inputs are most similar to the original.
.
test execution edit distance calculation the current holmes implementation uses static input similarity to identifyminimally differenttests.
usingonlystaticinputsimilarity first provided us with a better understanding of how execution informationcouldbecollectedandusedmosteffectively.
intheuser study described in section we semi automated using dynamic execution trace information for evaluating holmes.
future workcan improve holmes by automatically using dynamic execution trace information as described in section .
.
.
to evaluate static input differences holmes first determines the data type of each argument in the method under test this determineshowholmeswillcalculateeditdistance.
forarguments with numerical values holmes calculates the absolute value of the arithmeticdifferencebetweentheoriginalandgeneratedtestinput argument.
for example inputs .
and .
have an edit distance of .
.
for stringand charinputs holmes uses two different metrics.
first holmesdeterminesthehammingdistancebetweenthetwo arguments.
we elected to use hamming distance first because we found it increases the accuracy of the similarity measure for randomly generated inputs.
once holmes identifies inputs thatare similar using the hamming distance it uses the levenshtein distancetofurtherrefineitsfindings inputsthatrequirethefewestcharacterchangestochangefromonetotheotheraremostsimilar.
holmes uses an edit distance threshold of tests whose inputs are morethanalevenshteindistanceof3awayfromtheoriginalfailing tests are considered too different to be reported to the developer.
holmesusestheexecutedtestbehaviortodeterminewhichinputssatisfytheoriginalfailingtest soracle.
then holmesattempts tofurtherminimize thetestdifferencesby foreachoriginalargument iteratively replacing the original value with new input value and executing the modified test to observe if the oracle is satisfied.
holmesiteratestotrytofindthreesimilarpassingteststocompare to the failing one.
.
communicating root causes to developers an important consideration when building a tool is how it will communicatewiththedeveloper .
onceholmeshascomputeda setofpassing andasetoffailing tests itorganizestheinformation for presentation.
holmes organizes tests by whether it passes or fails showing the original failing test at the top of the output window making it easy to compare the differences.
under each test holmespresentsaminimizedtestexecutiontrace.
soastonotoverwhelmthedeveloperwithinformation holmes userinterfaceincludestheoptiontotoggleshowingandhidingtraceinformation.
.
holmes limitations we implemented holmes as a prototype causal testing tool to be used in a controlled experiment with real users see section .
we havethusprioritizedensuringholmesimplementstheaspectsof causal testing we needed to evaluate over fully automating it.
thecurrentversionofholmesautomatestestgeneration execution andstaticeditdistancecalculation.
weusedintrace to collectruntimeexecutiontracesandthen manually incorporated theexecutioninformationwiththetests.
futureversionsofholmes will automate the dynamic trace collection and comparison.
the current version of holmes relies on the defects4j benchmark usedinourevaluations andextendingittootherdefects may require extending holmes or setting those defects projectsup in a particular way.
for simplicity holmes works on single argument tests with stringor primitive arguments.
while this is sufficient for the defects in defects4j benchmark this limitation willneedtobeliftedfortestswithmultiplearguments.
ourholmes prototypeimplementation isopen source toallowothersto build on it and improve it.
causal testing effectiveness wedesignedacontrolleduserstudyexperimentwith37developers to answer the following three research questions rq1 does causal testing improve the developers ability to identify the root causes of defects?
rq2 doescausaltestingimprovethedevelopers abilitytorepair defects?
rq3 do developers find causal testing useful and if so what aspect of causal testing is most useful?
.
user study design causal testing s goal is to help developers determine thecause of a test failure thereby helping developers better understand andeliminate defects from their code.
we designed our user studyand prototype version of holmes to provide evidence of causal testing s usefulness while also providing a foundation of what information is useful for causal testing.
we randomly selected seven defects from defects4j from the apache commons lang project.
we chose apache commons lang becauseit isthemostwidelyknownprojectindefects4j had defectsthat requiredonlylimiteddomain knowledge and can be developed in eclipse.
ouruserstudyconsistedofatrainingtaskandsixexperimental tasks.
each task mapped to one of the seven defects.
each participantstartedwiththetrainingtask andthenperformedsixexperimentaltasks.
thetrainingtaskandthreeoftheexperimentaltasksusedholmesandtheotherthreeexperimentaltasksbelonged to the control group and did not include the use of holmes.
the orderof thetasks andwhich taskswere part ofthe controlgroupand which part of the experimental group were all randomized.
for the training task we provided an eclipse project with a defective code version and single failing test.
we explained how to execute the test suite via junit and how to invoke holmes.
we allowed participants to explore the code and ask questions telling themthat thegoalistochange thecodesothat allthattestspass.
each task that followed was similar to the training task controlgroup tasks did not have access to holmes experimental group tasks did.
werecordedaudioandthescreenforlateranalysis.
weasked participantstocompleteacausalityquestionnaireaftereachtask consisting of two questions what caused test x to fail?
and what changes did you make to fix it?
at then end the participants completed an exit survey with open ended questions such as what information did you findmost helpful when determining what caused tests to fail?
and point likert scale questions such as how useful did you find x?
forthelikert scalequestions wegaveparticipantstheoptions veryuseful somewhatuseful notuseful and misleadingor harmful .
we also gave participants an opportunity to provide additional feedback they saw fit.
prior to our experiment we conducted a pilot of our initial user studydesignwith23studentsfromagraduatesoftwareengineering course.
our pilot study consisted of tasks and a mock up version of holmes.
we used lessons learned and challenges encounteredto finalize the design of our study.
the pilot participants didnot participate in the final study presented here.
all final study materials areavailable online at in the user study materials directory.
.
participants we recruited a total of participants from industry and academia undergraduate students phd students masters students 2industrydevelopers and1researchscientist.
participants pro gramming experience ranged from to years and experience withjavarangedfromafewmonthsto15years.
allparticipants reportedhavingpriorexperiencewitheclipseandjunit.
weanalyzeddatafrom37participants 2undergraduateparticipants p2 and p3 did not follow the instructions so we removed them from our dataset.
.
user study findings we now summarize the results from our study.
rq1 doescausaltestingimprovethedevelopers abilitytoidentify the root causes of defects?
theprimarygoalofcausaltestingistohelpdevelopersidentify the root cause of test failures.
to answer rq1 we analyzed the responsesparticipantsgavetothequestion whatcausedtestx to fail?
we markedresponses aseither correct capturedfull and the true cause or incorrect missing part or all of the true cause .
figure4shows the root cause identification correctness results.
when using holmes developers correctly identified the cause 92defect group correct incorrect total 1control holmes 2control holmes 3control holmes 4control holmes 5control holmes 6control holmes totalcontrol holmes figure distributions of correct and incorrect cause descriptions per defect.
average resolution time in minutes defect control .
.
.
.
.
.0holmes .
.
.
.
.
.
figure5 theaveragetimedeveloperstooktoresolvethedefects inminutes.
of the time out of times .
the control group only identified the cause of the time out of .
fisher s exact test findsthatthesesamplescomefromdifferentdistributionswith83 probability p .
.
for four of the six defects defects and developers using holmes were more accurate when identifying root causesthan the control group.
for defects and participants only incorrectlyidentifiedthecauseapproximately5 ofthetimewhenusingholmes comparedto11 ofthetimewithoutholmes.
for defect participants with holmes identified the correct cause out of of the time without holmes they could only identify thecorrectcause67 12outof18 ofthetime.
ourfindingssuggestthat causaltestingsupportsandimprovesdeveloperability to understand root causes for at least some defects.
rq2 doescausaltestingimprovethedevelopers abilityto repair defects?
while causal testing s main goal is to help developers understandtherootcause thisunderstandingmaybehelpfulinremoving the defect as well.
to answer rq2 we analyzed participants re sponses to the question what changes did you make to fix thecode?
we used the same evaluation criteria and labeling as for rq1.
to determine if causal execution information improves developers ability to debug andrepairdefects we observed the time it tookparticipantstocompleteeachtaskandthecorrectnessoftheir repairs.defect group correct incorrect total 1control holmes 2control holmes 3control holmes 4control holmes 5control holmes 6control holmes totalcontrol holmes figure6 distributionofcorrectandincorrectrepairsimplemented by participants per defect.
figure5showstheaveragetimeittookdeveloperstorepaireach defect.
we omitted times for flawed repair attempts that do not addressthedefect.
onaverage participantstookmoretimewith holmesonallbutonedefect defect3 .
oneexplanationforthis observation is that while holmes helps developers understand the root cause this understanding takes time which can reduce the overallspeed of repair.
figure6shows repair correctness results.
when using holmes developers correctly repairedthe defect of thetime out of while the control group repaired the defect of the time out of .
for two of the six defects defects and developers using holmes repaired the defect correctly more often defect vs. defect vs. .
for defects and developers repaired the defect correctly of the time both with and withoutholmes.
foronedefect defect1 developerswithholmes wereonlyabletorepairthedefectcorrectly86 12outof14 of thetimewhiledeveloperswithoutholmescorrectlyfixeddefects of the time.
holmes did not demonstrate an observable advantage when repairingdefects.
ourfindingssuggestthat causaltestingsometimeshelpsdevelopersrepairdefects butneitherconsistently nor statistically significantly.
rq3 do developers find causal testing useful and if so what aspect of causal testing is most useful?
toanswerrq3 weanalyzedpost evaluationsurveyresponses tothequestionaskingwhichinformationwasmostusefulwhen understanding and debugging the defects.
we extracted and aggregated quantitative and qualitative results regarding information most helpful when determining the cause of and fixing the defects.
wealsoanalyzedthelikert scaleratingsregardingtheusefulnessof junit and the various components of causal execution information.
overall participants found the information provided by holmes more useful than other information available when understanding anddebuggingthedefects.
outof37participants found 93the addition of at least one aspect of holmes more useful than outputprovidedbyjunitalone.
further participantsfound theadditionofholmesatleastasusefulasjunit.
theremaining foundtheadditionofholmesnotasusefulasjunitalone.
thoughmajorityofparticipantsfoundholmes outputmoreuseful junitandinteractivedebuggersareanimportantpartofdebugging.
therefore our expectations would be that causal testing would augment those tools not replace them.
participants found the minimally different passing tests holmes provided the most useful out of participants rated this piece of information as very useful.
the passing and failing test inputs that holmes provided received very useful or useful rankings more often than the test execution traces.
finally 18participants marked either the passing or failing execution traceas not useful.
one participant felt the passing test traces were misleading or harmful during their session they noted that they feltinsomecasestheexecutionpathswerenotassimilarasothers which made interpreting the output more confusing.
togainabetterunderstandingofwhatpartsofcausalexecution informationaremostuseful andwhy wealsoanalyzedparticipants qualitative responses to the questions asked in our post evaluation questionnaire.
what information did you find most helpful when determining what caused tests to fail?
overall participants explicitly mentioned someaspectofholmesasbeingmosthelpful.
for6ofthesepartici pants alltheinformationprovidedbyholmeswasmosthelpfulfor cause identification.
another participants noted that specifically thesimilarpassingandfailingtestsweremosthelpful.
forexample p36statedthesesimilartests whenpresented sidebyside made it easy to catch a bug.
theother6participantsstatedtheexecutiontracesweremost helpful.
oneparticipant sresponsesaidthatthepartsofholmes output that were most helpful was the output showing methodcalls parameters and return values.
this was particularly true whenthereweremultiplemethodcallsinanexecutionaccording to p26 it was useful to see what was being passed to them and what they were returning.
whatinformationdidyoufindmosthelpfulwhendecidingchangestomaketothecode?
overall 14participantsmentionedsomeaspectof holmes as being most helpful.
of these explicitly stated that the similarpassingtestsweremosthelpfuloftheinformationprovided by holmes.
p7 who often manually modified failing tests to better understand expected behavior noted it helped to see what tests werepassing which helpedhim see whatwas actuallyexpected and valid.
fortheother4participants theexecutiontracesweremosthelpful for resolving the defect.
one participant specifically mentioned thatthereturnvaluesintheexecutiontracesforpassingandfailing inputsweremosthelpfulbecausethenhecouldtell whichparts are wrong.
wouldyouliketoaddanyadditionalfeedbacktosupplementyour responses?
many participants used this question as an opportunity to share why they thought holmes was useful.
many reported commentssuchas holmesisgreat!
and reallyhelpful.
formany holmes was most useful because it provided concrete workingexamples of expected and non expected behavior that help with pinpointing the cause of the bug.
a participant noted that without holmes they felt like it was a bit slower to find the reason why the test failed.
another participantnotedthatthetraceprovidedbyholmeswas somewhat more useful than the trace provided by junit.
in free form unprompted comments throughout the study participants often mentioned that the passing and failing tests andtraces were useful for their tasks several participants explicitly mentioned during their session that having the additional passing andfailingtestswere superuseful andsavedthemtimeandeffort in understanding and debugging the defect.
while the qualitative feedback is largely positive it is important to point out that we do not view causal testing tools as a replacement forjunit.
the intent isfor themto complementeach otherandhelpdevelopersunderstandanddebugsoftwarebehavior.
three participants explicitly mentioned that holmes is mostuseful in conjunction with junit and other tools available in the ide.
several participants highlighted the complementary nature of thesetools.
forexample p26explainedthatthoughholmeswas very useful when debugging the code it is most useful with other debugging tools as it does not provide all information.
finally participantsalsosuggestswaystoimproveholmes.
one participantmentionedthatholmesshouldaddtheabilitytoclick on the output and jump to the related code in the ide.
another suggested making thedifferences between the passing andfailing testsvisiblymoreexplicit.
threeparticipantsexplicitlysuggested rather than bolding the entire fuzzed input only bolding the parts thataredifferentfromtheoriginalfailingtest.
ourfindingssuggestthat causaltestingisusefulforbothcauseidentificationand defectresolution andiscomplementarytootherdebuggingtools.
causal testing applicability to real world defects toevaluatetheusefulnessandapplicabilityofcausaltestingtorealworld defects we conducted an evaluation on the defects4j benchmark .
defects4jisacollectionofreproducibledefectsfoundin real world open source java software projects apache commons lang apachecommonsmath closurecompiler jfreechart and joda time.
foreachdefect defects4jprovidesabuggyversionand fixed version of the source code along with the developer written testsuites which includeoneor moretests thatfail onthe buggy version but pass on the fixed version.
wemanuallyexamined330defectsinfourofthefiveprojectsin thedefects4jbenchmarkandcategorizedthembasedonwhether causal testing would work and whether it would be useful inidentifying the root cause of the defect.
we excluded joda time from our analysis because of difficulty reproducing the defects.
.
evaluation process todetermineapplicabilityofcausaltestingtodefectsinthedefects4jbenchmark wefirstimportedthebuggyversionandfixed versionintoeclipse.
wethenexecutedthedeveloper writtentest 4some such difficulties have been documented in the joda time issue tracker 94suitesonthebuggyversiontoidentifythetargetfailingtestsand the methods they tested.
once we identified the target failing tests and methods under test weranholmesusingthetargetfailingtests.
ifholmesranand produced causal test pairs we ran intrace to produce execution traces.
sometimes holmes was unable to produce an output.
in thesecases weattemptedtoevaluateifamorematureversionof holmescouldhaveproducedanoutput.
todothis wemanually madesmallperturbationstothetestinputsinanattempttoproduce reasonably similar passing tests.
we made perturbations based on thetypeofinputandhowamorematurecausaltestingtoolwould work.
for example if the test input was a number we made small changes such as adding and subtracting increments of one from the original value or making thenumber positive or negative.
we then executed the tests and attempted to produce causal test pairs.
in caseswhere holmesor our manualanalysis wasable to producesimilarpassingtests wenextdeterminedifthisinformation could be useful for understanding the root cause of that defect.
to do this we first used the fixed version to determine what we believed to be the root cause.
if we were able to determine the rootcause wethenmadeadeterminationonwhetherthesimilar passing tests and execution information would help developers understand the root cause and repair the defect.
weusedthisprocessandtheproducedinformationtocategorize the defects as we describe next.
.
defect applicability categories we categorized causal testing s applicability to each defect into the following five categories i.works useful and fast.
for these defects causal testing can produce at least one minimally different passing test thatcapturesitsrootcause.
wereasoncausaltestingwould be helpful to developers.
in our estimate the differencebetween the failing and minimally different passing testsis reasonably small that it can be found on a reasonable personalcomputer reasonablyfast.
formostofthesedefects our existing holmes implementation was able to produce the useful output.
ii.works useful butslow.
forthesedefects causaltesting can produce at least one minimally different passing testthat captures its root cause and this would be helpful todevelopers.
however the difference between the tests islarge and in our estimation causal testing would need additionalcomputationresources suchasrunningovernight or access to cloud computing.
for most of these defects our current holmes implementation was unable to produce the necessary output but a more mature version would.
iii.works but is not useful.
for these defects causal testing can produce at least one minimally different passingtest butinourestimation thistestwouldnotbeusefulto understanding the root cause of the defect.
iv.willnotwork.
forthesedefects causaltestingwouldnot beabletoperturbthetests andwouldtellthedeveloperit cannot help right away.
v.wecouldnotmakeadetermination.
becausethedefects in our study are from real world projects some requiredapplicability category project i ii iii iv v total math lang 65chart 26closure total figure distribution of defects across five applicability categoriesdescribed in section .
.
project specific domain knowledge to understand.
as we are not the original projects developers for these defects the lack of domain specific knowledge prevented us from understanding what information would help developers understand the root cause and debug and we elected not to speculate.
assuch weoptednottomakeanestimationof whether causal testing would be helpful for these defects.
.
results figure7shows our defect classificationresults.
of the330 defects wecouldmakeadeterminationfor139.
ofthese causaltestingwould try to produce causal test pairs for .
for the remaining40 causaltestingwouldsimplysayitcannothelp and would not waste the developer s time.
of these defects for causaltestingcanproduceinformationhelpfulinidentifyingtherootcause.
for29 asimplelocalide basedtool would work and for47 atool wouldneed moresubstantial resources such as running overnight or on the cloud.
the remaining23 wouldnotbenefitfromcausaltesting.
ourfindings suggest that causal testing produces results for of realworld defects and for of those it can help developers identify and understand the root cause of the defect.
discussion ourfindingssuggestthatcausaltestingcanbeusefulforunderstanding root causes and debugging defects.
this section discusses implicationsofourfindings aswellasthreatstothevalidityofour studies and limitations of our approach.
encapsulating causality in generated tests.
ouruserstudy foundthathavingpassingandfailingteststhataresimilartothe originalfailintestthatexposedadefectareusefulforunderstandinganddebuggingsoftwaredefects thoughnotalldefects.
participantsfoundthepassingteststhatprovidedexamplesofexpectedbehaviorusefulforunderstandingwhyatestfailed.
thissuggeststhatcausal testing can be used to generate tests that encapsulate causality in understanding defective behavior and that an important aspectof debugging is being able to identify expected behavior when software is behaving unexpectedly.
executioninformationfordefectunderstanding repair.
execution traces can be useful for finding the location of a defect and understanding software behavior .
our study has shown that such traces can also be useful for understandingrootcausesofdefects and insomecases canhighlight theserootcausesexplicitly.
participantsinourstudyfoundcomparing execution traces useful for understanding why the test was 95failing and how the code should behave differently for a fix.
for someparticipants theexecutiontraceinformationwasthemostuseful of all information provided.
these results support further use of execution traces when conducting causal experiments.
causaltestingasacomplementarytestingtechnique.
our findingssupportcausaltestingasacomplementtoexistingdebugging tools such as junit.
understandably participants sometimes foundthemselvesneedinginformationthatholmesdoesnotprovide especially once they understood the root cause and needed to repair the defect.
our findings suggest that causal testing ismost useful for root cause identification.
still a majority of the participants in our study found holmes useful for both cause identification and defect repair despite on average taking longer to resolve defects with holmes.
we speculate that increased familiaritywithcausaltestingwouldimprovedevelopers abilitytouse the right tool at the right time improving debugging efficiency as supported by prior studies .
supporting developers with useful tools.
the goal of software development tools is often to decrease developer effort such that developers will want to use that tool in practice.
however researchsuggeststhatthefirstthingpractitionersconsiderwhende cidingwhethertouseagiventoolisthattool susefulness .
our study shows that participants often took more time to debug when using holmes however despite this and other challenges developersencountered participantsstillgenerallyfoundholmesuseful for both understanding and debugging defects.
this suggests that animportantpartofevaluatingatoolintendedfordeveloperuseis whetherthetoolprovidesusefulinformationincomparisonto orin our case along with existing tools available for the same problem.
.
threats to validity external validity.
our studies used defects4j defects a collection of curated real world defects.
our use of this well known andwidely usedbenchmarkofreal worlddefectsaimstoensure ourresultsgeneralize.
weselecteddefectsfortheuserstudyrandomly from those that worked with our current implementation of holmesandthatrequiredlittleornopriorprojectordomainknowledge with varying levels of difficulty.
the applicability evaluation considered all defects across four projects.
the user study used participants which is within range of higher data confidence and is above average for similar user studies .
ourstudyalsoreliedonparticipantswithdifferent backgrounds and experience.
internalvalidity.
ouruserstudyparticipantswerevolunteers.
thisleadstothepotentialforself selectionbias.
wewereableto recruitadiversesetofparticipants somewhatmitigatingthisthreat.
construct validity.
part of our analysis of whether causal testingwouldapplyandbeusefulfordebuggingspecificdefects was manual.
this leads to the potential for researcher bias.
we minimized this threat by developing and following concrete reproducible methodology and criteria for usefulness.
theuserstudyaskedparticipantstounderstandanddebugcode theyhadnotwritten whichmaynotberepresentativeofasitationinwhichdevelopersaredebuggingcodetheyarefamiliarwith but is representative of a common scenario of developers debugging others code .
weaimedtoselectdefectsforthestudythatrequiredlittle project and domain knowledge.
additionally we did not disclosethetruepurposeoftheuserstudytothesubjectsuntilafter the end of each participant s full session.
.
limitations and future work causal testing mutates tests inputs while keeping the oraclesconstant recall section .
.
.
this process makes an implicit assumptionthatsmallperturbationsoftheinputsshouldnotaffect the expected behavior and thus if small perturbations do affect thebehavior knowingthisinformationisusefultothedeveloper for understanding the root cause of why the faulty behavior is takingplace.
thisassumptioniscommoninmanydomains such as testing autonomous cars and other machine learning based systems .
however it also leads causaltesting limitations.
in particular some changes to the inputs doaffect expected behavior and using the unmodified oracle will not be valid in these cases.
thiscan leadcausal testingto generatepairs ofteststhat donot capturecausalinformationabouttheexpectedbehaviorproperly.
forexample itcould produceatestthat passesbutthatuses the wrongoracleandshould infact fail.
itremainsanopenquestion whether such tests would be helpful for understanding root causes.
the causal test pair still indicates what minimal input change can satisfy the oracle which might still be useful for developers to understand the root causes even if the passing test does not properly capture the expected behavior.
futureworkcouldextendcausaltestingtoincludeoraclemutation.
a fruitful line of research when specifications formal orinformal are available is to extract oracles from those specifica tions.
for example swami can extract test oracles and generate tests from structured natural language specifications and toradacu jdoctor and tcomment candosofrom javadoc specifications.
behavioral domain constraints data constraints or temporal constraints can also act as oracles for the generated tests.
by fuzzing existing tests and focusing on test inputs that are similarto theoriginal failingtest causaltestingattempts tomitigate the risk that the tests oracle will not apply.
in a sense a test s inputsmustsatisfyasetofcriteriafortheoracletoremainvalid andbymodifyingtheinputsonlyslightly asdefinedbystaticor dynamicbehavior ourhopeisthatinsufficientlymanycases thesecriteriawillnotbeviolated.
futureworkcouldconsiderimplementing oracle aware fuzzing that modifies inputs while specifically attempting to keep the oracle valid.
insomecases itmaynotbepossibletogeneratepassingtests bygeneratingnewtests.
forexample codethatneverthrowsan exception cannot have a test pass if that test s oracle expects theexception to be thrown.
in such cases causal testing will not producefalsepositiveresultsforthedeveloper andwillsimplysay no causal information could be produced.
our studies have identified that causal testing is often but not always helpful.
futureworkcanexaminepropertiesofdefectsor testsforwhichcausaltestingismoreeffectiveatproducingcausal information and for which that causal information is more helpful to developers.
this information can in turn be used to improve causal testing.
related work the closest work to causal testing is bugex which is also inspired by counterfactual causality.
given a failing test bugex uses runtimeinformation suchaswhetherabranchistaken tofindpassingandfailingteststhatdifferwithrespecttothatpieceofinformation.
darwin targets regression failures and uses concrete and symbolic execution to synthesize new tests such that each test differsincontrolflowwhenexecutedonthebuggyandthenon buggy versionofthecode.
bycontrast causaltestingrequiresonlyasingleversionofthecode andonlyasinglefailingtest andgenerates pairsofteststhatdiffer minimallyeitherstaticallyordynamically orboth tohelpdevelopersunderstandtherootcauseofthedefect.
deltadebugging aimstohelpdevelopersunderstandthe cause of a set of failing tests.
given a failing test the underlying ddminalgorithmminimizesthattest sinputsuchthatremovingany otherpieceofthetestmakesthetestpass .
deltadebuggingcan also be applied to a set of test breaking code changes to minimize that set although in that scenario multiple subsets that cannot be reduced further are possible because of interactions between code changes .
by contrast causal testing does not minimize an input or a set of changes but rather produces otherinputs notnecessarilysmaller thatdifferminimallybutcauserelevant behavioral changes.
the two techniques are likely complementary in helping developers debug.
whenappliedtocodechanges deltadebuggingrequiresacorrect code version and a set of changes that introduce a bug.
iterative deltadebuggingdoesnotneedthecorrectversion usingtheversion historytoproduceacorrectversion .
again causaltestingis complementary thoughfutureworkcouldextendcausaltesting to consider the development history to guide fuzzing.
faultlocalization alsoknownasautomateddebugging isconcerned with locating the line or lines of code responsible for a failingtest .
spectralfaultlocalizationusesthefrequency with which each code line executes on failing and passing tests casestoidentifythesuspiciouslines .
whentests orfailingtests arenotavailable staticcodeelementsordataaboutthe process that created the software can be used to locate suspicious lines .
accounting for redundancy in test suites can improve spectral fault localization precision .
mimic can also improve fault localization precision by synthesizing additional passingandfailingexecutions andapollocandosobygenerating tests to maximize path constraint similarity .
statistical causalinferenceusesobservationaldatatoimprovefaultlocalization precision .
importantly while statistical causal inference aims to infer causality it does not apply the manipulationist approach thatcausaltestinguses asaresult causaltestingcan makemorepowerfulstatementsaboutthecausalrelationshipsit discovers.
unfortunately research has shown that giving developers the ground truth fault location even from state of the art fault localizationtechniques doesnotimprovethedevelopers ability torepairdefects likelybecauseunderstandingdefectcauses requiresunderstandingmorecodethan justthelinesthatneedto beedited.
bycontrast causaltestingdiscoversthechangestosoftware inputs that causethe behavioral differences and a controlled experimenthasshownpromisethatcausaltestingpositivelyaffects the developers ability to understand defect causes.mutationtestingtargetsadifferentproblemthancausaltesting andtheapproachesdiffersignificantly.
mutationtestingmutates thesourcecodetoevaluatethequalityofatestsuite .
causal testing doesnot mutatesource code it perturbstest inputs and helpsdevelopersidentifyrootcausesofdefects ratherthanimprove testsuites althoughitdoesgeneratenewtests.
inaspecialcase of causal testing when the defect being analyzed is in software whose input is a program e.g.
compiler causal testing may rely on code mutation operators to perturb the inputs.
reproducingfieldfailures isanimportantpartofdebugging complementary to most of the above described techniques includingcausaltesting whichrequireafailingtestcase.
fieldfailures often tell more about software behavior than in house testing .
fuzz testing is the process of changing existing tests to generate moretests though inindustry fuzztestingisoftensynonymous with automated test input generation .
fuzz testing has been usedmostoftentoidentifysecurityvulnerabilities .
fuzzing can be white box relying on the source code or black box relyingonlyonthespecificationorinputschema .
causal testingusesfuzztestingandimprovementstofuzztestingresearch can directly benefit causal testing by helping it to find similartest inputs that lead to different behavior.
fuzzing can be usedon complex inputs such as programs which is necessary to apply causal testing to software withsuch inputs as is the casefor closure one of the subject programs we have studied .
fuzz testingbyitselfdoesnotprovidethedeveloperwithinformationto helpunderstanddefects rootcauses thoughthefailingtestcases it generates can certainly serve as a starting point.
thecentralgoalofautomatedtestgeneration e.g.
evosuite andrandoop andtestfuzzingisfindingnewfailingtestcases.
for example combining fuzz testing delta debugging and tradi tional testing can identify new defects e.g.
in smt solvers .
automatedtestgenerationandfuzzingtypicallygeneratetestinputs which can serve as regression tests or require humans to write test oracles.
without such oracles one cannot know if the tests pass or fail.
recent work on automatically extracting test oracles from code comments can help .
differential testing can also produce oracles by comparing the executions of thesame inputs on multiple implementations of the same specification .
identifying defects by producing failing tests is the precursor to causal testing which uses a failing test to help developers understand the defects root cause.
contributions we have presented causal testing a novel method for identifying rootcausesofsoftwaredefectsthatsupplementsexistingtesting and debugging tools.
causal testing is applicable to of realworld defects in thedefects4j benchmark and for ofthose it canhelpdevelopersidentifytherootcauseofthedefect.
developersusingholmes aproof of conceptimplementationofcausaltesting were more likely to correctly identify root causes than without holmes vs. of the time .
majority of developers who used holmesfounditmostusefulwhenattemptingtounderstandwhy a test failed and in some cases how to repair the defect.
overall causaltestingshowspromiseforimprovingthedebuggingprocess especially when used together with other debugging tools.
97acknowledgments thiswork issupported bythe nationalscience foundationunder grants no.
ccf iis and ccf and by google and oracle labs.