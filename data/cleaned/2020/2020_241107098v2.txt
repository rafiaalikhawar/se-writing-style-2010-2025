a multi agent approach for rest api testing with semantic graphs and llm driven inputs myeongsoo kim tyler stennett saurabh sinha alessandro orso georgia institute of technology atlanta ga usa.
ibm t.j. watson research center yorktown heights ny usa.
1mkim754 2tstennett3 gatech.edu 3sinhas us.ibm.com 4orso cc.gatech.edu abstract as modern web services increasingly rely on rest apis their thorough testing has become crucial.
furthermore the advent of rest api documentation languages such as the openapi specification has led to the emergence of many blackbox rest api testing tools.
however these tools often focus on individual test elements in isolation e.g.
apis parameters values resulting in lower coverage and less effectiveness in fault detection.
to address these limitations we present autoresttest the first black box tool to adopt a dependency embedded multiagent approach for rest api testing that integrates multiagent reinforcement learning marl with a semantic property dependency graph spdg and large language models llms .
our approach treats rest api testing as a separable problem where four agents api dependency parameter and value agents collaborate to optimize api exploration.
llms handle domain specific value generation the spdg model simplifies the search space for dependencies using a similarity score between api operations and marl dynamically optimizes the agents behavior.
our evaluation of autoresttest on real world rest services shows that it outperforms the four leading black box rest api testing tools including those assisted by restgpt which generates realistic test inputs using llms in terms of code coverage operation coverage and fault detection.
notably autoresttest is the only tool able to trigger an internal server error in the spotify service.
our ablation study illustrates that each component of autoresttest the spdg the llm and the agent learning mechanism contributes to its overall effectiveness.
index terms multi agent reinforcement learning for testing automated rest api testing i. i ntroduction modern web services increasingly depend on rest representational state transfer apis for efficient communication and data exchange .
these apis enable seamless interactions between various software systems using standard web protocols .
rest apis function through common internet methods and a design that allows the server and client to operate independently .
the prevalence of rest apis is evident from platforms such as apis guru and rapid api which host extensive collections of api specifications.
in recent years various automated testing tools for rest apis have been developed e.g.
.
these tools follow a sequential process select an operation to test identify operations that depend on the selected operation determine 1also with aws ai labs santa clara ca usa this work was performed before the author joined aws ai labs.parameter combinations and assign values to those parameters.
feedback from response status codes is then used to adjust the exploration policy at each step either encouraging or penalizing specific choices.
although significant research has been dedicated to optimizing each individual step operation selection dependency identification parameter selection and value generation these tools treat each step in isolation rather than as part of a coordinated testing strategy.
this isolated approach can result in suboptimal testing with a high number of invalid requests.
for instance one endpoint might benefit from extensive exploration of different parameter combinations whereas another endpoint might require focused exploration of previously successful parameter combinations with diverse input values.
consequently existing tools often achieve low coverage especially on large rest services as shown in recent studies e.g.
language tool genome nexus and market in the arat rl evaluation and spotify and ohsome in the nlp2rest evaluation .
to overcome these limitations we propose autoresttest a new approach that integrates a semantic property dependency graph spdg and multi agent reinforcement learning marl with large language models llms to enhance rest api testing.
instead of traversing all operations to find dependencies by matching input output properties autoresttest uses an embedded graph that prioritizes the properties by calculating the cosine similarity between input and output names.
specifically autoresttest employs four specialized agents to optimize the testing process.
the dependency agent manages and utilizes the dependencies between operations identified in the spdg guiding the selection of dependent operations for api requests.
the operation agent selects the next api operation to test prioritizing operations likely to yield valuable test results based on previous results such as successfully processed dependent operations.
the parameter agent chooses parameter combinations for the selected operation to explore different configurations.
finally the value agent manages the generation of parameter values using three data sources values from dependent operations llm generated values that satisfy specific constraints using few shot prompting and type based random values.
the value agent learns which data source is most effective for each parameter type and context.arxiv .07098v2 jan 2025the autoresttest agents collaborate to optimize the testing process using the multi agent value decomposition q learning approach .
when selecting actions each agent is employed independently using the epsilon greedy strategy for exploitation exploration balancing.
however during policy updates using the q learning equation autoresttest uses value decomposition to consider the joint actions across all agents.
through centralized policy updates each agent converges toward selecting the optimal action while accounting for the actions of the other agents.
we evaluated autoresttest on real world restful services used in previous studies including well known services such as spotify and compared its performance with that of four state of the art rest testing tools recognized as top performing tools in recent studies restler evomaster morest and aratrl .
to ensure a fair comparison we used enhanced api specifications generated by restgpt which augments rest api documents with realistic input values generated using llms.
to measure effectiveness we used code coverage for the open source services operation coverage for the online services and fault detection ability measured as internal server errors triggered for all the services.
these are the most commonly used metrics in this field .
autoresttest demonstrated superior performance across all coverage metrics compared to existing tools.
it achieved .
method coverage .
branch coverage and .
line coverage significantly outperforming arat rl evomaster restler and morest by margins of .
for closedsource services autoresttest processed api operations compared to operations processed by the other tools.
these results show that autoresttest can perform more comprehensive api testing than the other tools.
in terms of fault detection autoresttest identified operations with internal server errors outperforming aratrl evomaster morest and restler .
notably autoresttest was the only tool that detected an internal server error for spotify .
we reported the errors detected on fdic ohsome and spotify as these are actively maintained projects.
the ohsome error has been confirmed and fixed we are still waiting for feedback from the developers for the other bug reports.
we also performed an ablation study which revealed the importance of autoresttest s key components.
removing temporal difference q learning caused the largest performance drop with method line and branch coverage falling to .
.
.
.
and .
.
respectively.
spdg removal reduced coverage to .
.
.
.
and .
.
while llm removal led to .
.
.
.
and .
.
.
overall removing any component decreased coverage by .
.
demonstrating the significance of each component in contributing to autoresttest s effectiveness.
the main contributions of this work are a novel rest api testing technique that reduces the operation dependency search space using a similarity r e g i s t e r p o s t t a g s customer r e s t c o n t r o l l e r summary c r e a t e c u s t o m e r o p e r a t i o n i d createcustomerusingpost r e q u e s t b o d y d e s c r i p t i o n u s e r c o n t e n t a p p l i c a t i o n j s o n schema t y p e o b j e c t p r o p e r t i e s l i n k s t y p e a r r a y i t e m s r e f components schemas link e m a i l t y p e s t r i n g maxlength p a t t e r n .
.
name t y p e s t r i n g maxlength p a t t e r n password t y p e s t r i n g maxlength minlength p a t t e r n r e s p o n s e s d e s c r i p t i o n c r e a t e d c o n t e n t schema r e f components schemas userdtores d e s c r i p t i o n u n a u t h o r i z e d d e s c r i p t i o n f o r b i d d e n d e s c r i p t i o n not found fig.
.
a part of market api s openapi specification.
based graph model employs multi agent reinforcement learning to consider optimization among the testing steps and leverages llms to generate realistic test inputs.
empirical results showing that autoresttest outperforms state of the art rest api testing tools even when provided with enhanced api specifications by covering more operations achieving higher code coverage and triggering more failures.
an artifact including the autoresttest tool the benchmark services used in the evaluation and detailed empirical results which serves as a comprehensive resource for supporting further research and replication of our results .
ii.
b ackground and motivating example a. rest apis rest apis are a type of web apis that conform to restful architectural principles .
these apis enable data exchange between client and server through established protocols like http .
the foundation of rest lies in several key concepts statelessness cacheability and a uniform interface .
in restful services clients interact with web resources by making http requests.
these resources represent various types of data that a client might wish to create read update or delete.
the api endpoint defined by a specific resource path e.g.
users represents the resource that clients interact with.
different http methods e.g.
post get put delete determine what actions can be performed on that resource such as creating reading updating or deleting data.
each unique combination of an endpoint s resource path and anhttp method constitutes an operation e.g.
get users or post users .
when a web service processes a request it responds with headers a body if applicable and an http status code indicating the result.
successful operations typically return 2xx codes while 4xx codes denote client side errors and a status code indicates an internal server error.
b. openapi specification the openapi specification oas which evolved from swagger in version to the oas in version is a crucial standard for restful api design and documentation.
as an industry standard format oas defines the structure functionality and anticipated behaviors of apis in a standardized and human accessible way.
figure presents a portion of the market api s oas.
this example highlights the structured approach of oas in defining api operations.
for instance it specifies a post request on the register endpoint for creating a new customer.
the request body is expected to be in application json format and must include properties such as links email name and password each with specific validation constraints.
the responses section defines possible outcomes including a created status for a successful request along with unauthorized forbidden and not found status codes for various error conditions.
c. large language models large language models llms like the generative pretrained transformer gpt series are at the forefront of advancements in natural language processing nlp .
llms trained on extensive text collections can understand interpret and generate human like text .
the gpt series including gpt exemplifies these advanced llms demonstrating a remarkable ability to produce human like text for various applications from education to customer service .
their versatility in handling diverse language related tasks from writing coherent text and translating languages to test case generation highlights their advanced capabilities in human like understanding .
d. multi agent reinforcement learning reinforcement learning rl is a branch of machine learning where an agent learns to make decisions by interacting with its environment .
in this process the agent selects actions in various situations states observes the outcomes rewards and learns to choose the best actions to maximize the cumulative reward over time.
rl involves a trial anderror approach where the agent discovers optimal actions by experimenting with different options and adjusting its strategy based on the observed rewards.
additionally the agent must balance between exploring new actions to gain more knowledge and exploiting known actions that provide the best reward based on its current understanding.
this balance between exploration and exploitation is often controlled by parameters such as in the greedy strategy .multi agent reinforcement learning marl is an advanced extension of reinforcement learning that involves multiple agents interacting in a shared environment to achieve individual or collective goals .
marl addresses the complexities of agent interactions including cooperation competition and communication .
techniques such as cooperative learning competitive learning and communication and coordination methods enable agents to develop optimal strategies.
applications of marl span various domains including autonomous driving and robotic coordination where agents must work together or compete to optimize their performance .
marl is expected to be used in areas requiring complex multi agent decision making .
e. motivating example figure depicts the register endpoint in the market api s openapi specification.
this endpoint is vital for creating user credentials needed in other components of the api.
existing rest api testing tools struggle to generate valid requests for this operation due to the strict parameter requirements email name and password are required parameters each with specific constraints on valid values while links is an optional parameter.
moreover these tools fail to prioritize information gained from a successful operation invocation in subsequent requests.
autoresttest addresses these issues through a multi agent approach.
the parameter agent employs reinforcement learning to identify valid parameter combinations learning that email name password is a required parameter set while avoiding invalid combinations that would trigger errors.
for these parameters the value agent first determines the appropriate data source for each parameter choosing between llms dependency values from previous operations or random values because different parameters require different generation strategies to create valid values.
for the register endpoint it selects llm generation as the parameters require specific formats e.g.
email format password rules that basic defaults cannot match and dependency values are not available for this initial operation.
it then generates context aware values that comply with the specification s validation patterns.
after a successful registration the dependency agent utilizes the spdg to identify operations that depend on user credentials e.g.
customer cart customer orders and propagates the registered user s information to these dependent operations.
the operation agent then prioritizes testing these dependent operations while the parameter and value agents reuse the strategies that were successful for the register endpoint.
through value decomposition marl enables efficient policy updates by appropriately distributing rewards to each agent based on their contribution.
iii.
o urapproach a. overall workflow figure illustrates the architecture of autoresttest highlighting its core components the spdg the rest agents andfig.
.
overview of our approach.
the request generator.
the overall workflow consists mainly of two phases initialization and testing execution.
initialization phase the process begins with parsing the openapi specification to extract endpoint information parameters and request response schemas.
using the parsed specification the dependency agent constructs the spdg as a directed graph in which nodes represent api operations and edges represent potential dependencies between operations.
each weighted edge e a b indicates that operation b provides values that can be used by operation a i.e.
adepends onb with the edge weight a value between and representing the semantic similarity between the operations inputs and outputs see section iii c .
these initial dependencies are later validated and refined through the testing process based on actual server responses.
the rest agents operation parameter value and dependency agents are then initialized with their respective qtables.
each agent serves a specific purpose in the testing process the operation agent selects api operations to test the parameter agent determines parameter combinations the value agent generates appropriate parameter values for each parameter and the dependency agent manages operation dependencies from the spdg.
testing execution phase the testing execution follows an iterative process.
in each iteration the operation agent first selects the next api operation based on its learned q values and exploration strategy.
next the parameter agent determines which parameters to include considering both required and optional parameters from the specification.
the value agent then generates parameter values using dependencies identified by the dependency agent llm generated values or default assignments for basic parameter types.
using the spdg the dependency agent identifies any dependencies between the selected parameters and those used in previous operations.
finally the request generator constructs the api request with the mutator component modifying of requests to test error handling and trigger potential response codes similar to prior work .
once the request is sent to the service under test sut and a response is received the response is used to update the q tables of all agents through reinforcement learning refinespdg dependencies and store any responses for the final testing report.
this cycle continues until the testing time budget is exhausted.
the spdg refinement process involves increasing or decreasing edge weights driven by rewards and penalties for dependencies based on server responses successful dependencies validated by 2xx response codes increase edge weights whereas failed dependencies reduce edge weights with heavily penalized edges being effectively removed from consideration.
this continuous refinement helps ensure the accuracy of the dependency graph over time.
b. q learning and agent policy both the spdg and rest agent modules use the qlearning algorithm with value decomposition within their respective agents.
during initialization each agent creates a q table data structure that maps available actions to their expected cumulative rewards.
when request generation begins autoresttest addresses the two primary components of qlearning action selection and policy optimization to facilitate effective communication between agents.
action selection during action selection agents independently choose between exploiting their best known option or exploring new options randomly.
to balance these choices all agents follow an epsilon greedy strategy with probability the agent selects a random action exploration and with probability it selects the action with the highest q value exploitation likely yielding the best results.
autoresttest utilizes epsilon decay to guarantee all actions are adequately explored in its initial stages.
starting with an epsilon value of .
this value decreases linearly to .
over the duration of the tool s operation.
this strategy commonly used in practice has been shown to improve performance by balancing exploration and exploitation .
policy optimization after receiving a response from the server agents update their q table values using the temporaldifference update rule of the q learning algorithm derived from the bellman equation .
this update aims to maximize the expected cumulative reward for each action taken.
the qlearning update rule is given by q s a q s a fig.
.
illustration of spdg construction and refinement.
where represents the temporal difference error calculated as r max a q s a q s a where is the learning rate is the discount factor ris the received reward sis the current state s is the next state a is the current action and a is an action in state s .
given the complexity of the multi agent environment autoresttest leverages value decomposition to optimize the joint cumulative reward which has shown improvements for policy acquisition over independent learning.
this approach assumes that the joint q value can be decomposed additively as follows q s a nx iqi s ai where qiandairepresent the q value and action for agent i in shared state s. the temporal difference error which measures the difference between current q values and the optimal target qvalues can be redefined using value decomposition to reflect the displacement from the optimal joint q value r max a nx iqi s a i nx iqi s ai using this redefined temporal difference error each agent updates its q table to converge towards the optimal joint q value as depicted in the following temporal difference equation qi s ai qi s ai h r max a nx i 1qi s a i nx i 1qi s ai i this value decomposition approach enables each agent to select actions independently while maintaining centralized policy updates simplifying the implementation while enhancing coordination across agents .
for reward delegation the dependency value and parameter agents are optimized to reward actions that generate 2xx status codes whereas the operation agent rewards the selection of operations that generate 4xx and 5xx status codes.
this balance in behavior is intended to maximize coverage by encouraging repeated attempts at creating successful requests for operations that frequently yield 4xx and 5xx status codes.
for the hyperparameters and like arat rl and other related work we use values .
and .
respectively.c.
semantic property dependency graph the construction of the spdg begins with parsing the openapi specification to extract information about api endpoints parameters and request response schemas.
as shown in figure the spdg is initialized as a directed graph where nodes represent api operations and edges represent potential dependencies between operations based on semantic similarities.
the initialization process involves two main steps.
first for each operation in the api specification we create a node containing the operation s id parameters and response schemas.
then we identify potential dependencies between operations by computing semantic similarities between their parameter names inputs and response field names outputs using cosine similarity with pre trained word embeddings e.g.
glove .
when comparing two operations if their similarity score exceeds .
1we create an edge between them with the similarity score as its weight.
to ensure all operations have some potential dependencies to explore if an operation has no edges with scores above the threshold we connect it to the five most similar operations.
phase of figure provides an example of the initial spdg generated by our technique.
for example the edge between get users id and get orders user id has a weight of .
due to the high similarity between the names of the inputs and outputs of these two operations.
dependency agent the dependency agent manages and uses the dependencies between operations captured in the spdg.
it uses a q table to represent the validity of these dependencies operating similarly to a weighted graph where edges are assigned values that reflect confidence in each dependency.
specifically the q table encodes edges from the spdg categorizing dependencies by parameter type query or body and target parameters body or response .
higher qvalues on an edge indicate greater confidence in the reliability of that operation dependency causing the agent to prioritize these relationships on future requests.
for each parameter and body property in a selected operation the dependency agent refers to its q table to identify a dependent parameter body or response as well as the associated operation id.
for example as illustrated in phase of figure when testing get orders user id the agent might use the value for user id from a successful get users id response reflected by the strengthened edge weight of .
.
the dependency agent consults autoresttest s tables storing successful parameters request body properties and 1we selected this value based on previous studies .
2top five is a common threshold in top k similarity matching .decomposed responses to ensure that the selected dependency has available values.
autoresttest recursively deconstructs response objects allowing the dependency agent to access nested properties within response collections.
required parameters without available dependencies are fulfilled using value mappings provided by the value agent.
although the spdg significantly reduces the search space for operation dependencies the reliance on semantic similarity may overlook potential candidates.
to address this the dependency agent permits random dependency queries during exploration.
as shown in phase of figure if a random dependency successfully generates a valid input autoresttest identifies the contributing factor and adds this new edge to the spdg.
for instance during testing the agent might discover that post carts returns a cart id that can be used as input forget orders user id leading to a new edge with weight .
.
similarly when the specification contains undocumented response values e.g.
if post register returns additional user related fields not specified in the openapi document the dependency agent evaluates these new properties for potential dependencies with other operations as demonstrated by the edge between post register and post carts with weight .
in phase .
d. rest agents operation agent this agent is tasked with selecting the next api operation to test.
it employs reinforcement learning to prioritize operations that are likely to yield meaningful test results based on prior experiences.
this agent uses a simplified state model that only tracks whether an operation is available for selection.
the agent s action space encompasses all possible operations in the api specification.
each operation s q value is initialized to and is updated based on response codes from the server.
the q table for the operation agent stores cumulative rewards representing the proportion of unsuccessful requests for each operation in the provided service as discussed in greater detail in the next paragraph.
the operation agent acts as the forerunner of the testing process selecting an operation that dictates the state of the remaining agents.
while the remaining agents coordinate to generate valid test cases for a given operation the operation agent is tasked with identifying unsuccessful operations for retesting.
consequently while the remaining agents update their q value using the value decomposition temporal difference equation equation in section iii b the operation agent updates its q values independently using a structured reward system for server errors 5xx for client errors 4xx excluding and for successful responses 2xx for authentication failures and for invalid methods .
this reward structure encourages the agent to prioritize exploring problematic endpoints while severely penalizing systematically invalid requests and mildly penalizing endpoints with consistently successful requests.
as an example consider the customer registration endpoint in the market api figure .
the q value for the endpoint is initially .
after initial attempts at processing the operationfail the q table value might increase e.g.
to prompting the agent to prioritize further testing on this endpoint.
conversely a successful test case would decrease the q value directing the agent to explore other challenging endpoints.
parameter agent the parameter agent is responsible for selecting parameters for the chosen api operation.
it ensures that parameters used across requests are both valid and varied covering a range of testing scenarios while addressing interparameter dependencies.
for each operation the parameter agent initializes a state containing the operation id available parameters and required parameters and defines its action space as possible parameter combinations.
the q values associated with each state action pair are initialized to .
consider again the market api s customer registration endpoint shown in figure .
the parameter agent initializes with the following state s createcustomerusingpost where the first list contains all available parameters from the request body schema whereas the second list contains only the required parameters according to the endpoint s specification.
the agent initializes a q table for this operation mapping various parameter combinations to q values.
this setup ensures that different parameter combinations limited to by default to account for space restrictions are sufficiently represented in the agent s action space.
the agent updates its q values using the value decomposition temporaldifference equation equation in section iii b and the following reward structure for server errors 5xx for client errors 4xx and for successful responses 2xx .
importantly unused parameter combinations receive a neutral update effectively in the q learning process maintaining their initial q values.
these unused combinations are prioritized over combinations with negative rewards and deprioritized relative to those with positive rewards.
for example for the registration endpoint if the parameter combination email name password consistently yields positive rewards the unused combination email name password links retains its initial q value and may be selected during exploration to test scenarios with optional parameters.
suppose that the q values for these parameter combinations evolved to .
and .
respectively.
this would suggest that the inclusion of the links parameter is problematic and result in a lower q value for the configuration with all parameters.
through this reward scheme autoresttest effectively identifies valid parameter sets and addresses challenges related to undocumented inter parameter dependency requirements particularly in complex scenarios such as user registration where certain parameters must be present and correctly formatted.
value agent this agent is responsible for generating and assigning values to parameters selected by the parameter agent.
for each parameter of an operation it maintains a state containing the operation id parameter name parameter type and openapi schema constraints with its actions corresponding to possible data sources for parameter value assignment.
the q values for each state action pair are initialized to .consider the market api s customer registration endpoint in figure .
the value agent initializes the following states for email name and password parameters s createcustomerusingpost email string pattern .
.
s createcustomerusingpost name string pattern maxlength createcustomerusingpost password string pattern minlength maxlength to generate a diverse set of inputs the value agent can select inputs from the following data sources operation dependency values when selected the value agent collaborates with the dependency agent to map dependent values to the selected parameter.
for example the registration endpoint might reuse email addresses from prior successful registrations to test duplicate user scenarios.
llm values for this data source the value agent creates or parses if already created values using few shot prompting with llms .3for instance the llm might generate john.doe example.com as the input value for the email parameter based on the specified pattern.
random values when this option is selected the value agent generates random values based on the type of the selected parameter.
for example it may create a random sequence of characters for strings a random number between and for integers and a random true false value for boolean types.
once a request is completed with values from the chosen data source the agent updates its q values based on the temporal difference equation equation in section iii b using the same reward strategy as the parameter agent iii d2 to refine its value generation.
for the registration endpoint for instance the q values across parameters show an average of .
for llm values .
for random values and .
for operation dependency values.
in analyzing these q values we observe that because the registration endpoint is required for account creation it is less likely to derive values from operation dependencies which results in a lower q value for the dependency source.
although random values are effective for simpler parameters such as name the llm generated values perform better for pattern constrained fields such as email andpassword .
e. request generator the request generator constructs and dispatches api requests to the sut.
it works closely with the rest agents to ensure that the generated requests are both effective and comprehensive.
upon receiving responses from the sut the response handler processes these results and provides feedback to the rest agents allowing refinement of future requests.
the mutator s purpose is to generate invalid requests to uncover unexpected behaviors e.g.
responses .
this is a crucial part of rest api testing frameworks as state of theart tools employ similar strategies such as mutating parameter 3in this work we used gpt .
turbo with a temperature setting of .
because of its known performance in rest api contexts .types values and headers e.g.
using an invalid content type in the header .
the mutator follows these conventions and mutates of the generated requests a strategy used by the most recent tool .
the request generator interacts with the rest agents to construct api requests relying on them for detailed information about the operation to test the parameters to use and appropriate values for those parameters.
specifically the operation agent selects the api operation to test.
the parameter agent identifies and optimizes the parameters for the chosen operation.
the value agent generates realistic and effective values for the parameters.
using this information the request generator constructs a complete api request.
the request generator then dispatches the request to the sut which processes the request and returns a response.
the response handler analyzes this response to detect any errors or unexpected behaviors.
insights from these analyses are fed back to both the rest agents and the dependency agent allowing them to refine their strategies for future requests.
the interactions between the dependency module the rest agents the request generator and the sut establish a robust feedback loop that enhances the overall effectiveness of the testing process.
this collaborative approach ensures that the generated requests are not only comprehensive but also tailored to uncover potential issues within rest apis.
iv.
e valuation in this section we present the results of empirical studies conducted to assess autoresttest.
our evaluation aims to address the following research questions rq1 how does autoresttest compare with state of theart rest api testing tools in terms of code coverage and operation coverage achieved?
rq2 in terms of error detection how does autoresttest perform in triggering internal server error responses compared to state of the art rest api testing tools?
rq3 how do the main components of autoresttest marl spdg and llm based input generation contribute to its overall performance?
a. experiment setup we conducted our experiments on two cloud vms each equipped with a core intel r xeon r platinum processor with gb ram.
to ensure consistent test conditions we restarted the services and restored their databases in each testing session to eliminate potential state dependency effects across sessions.
we used the default configuration and database settings for each service.
we allocated dedicated resources to each service and testing tool running them sequentially to prevent interference.
throughout the experiments we closely monitored cpu and memory usage to ensure optimal performance without encountering resource constraints.table i rest services used in the evaluation .
rest service lines of code operations features service language tool rest countries genome nexus person controller user management market project tracking system youtube mock fdic spotify ohsome api for our evaluation we relied on the same set of rest api testing tools and services used by arat rl .
accordingly we compared autoresttest with arat rl evomaster morest and restler .
specifically we used the latest released version or the latest commit when a release was unavailable restler v9.
.
evomaster v3.
.
arat rl v0.
morest obtained directly from the authors .
the arat rl benchmark dataset has restful services.
in addition to these services we included the services from the restgpt study .
out of the total services we excluded scs and ncs because they were written by evomaster s authors and we aimed to avoid potential bias.
we also excluded ocvn due to authentication issues.
lastly we excluded omdb which is a toy online service with only one api operation that all testing tools can process in a second.
ultimately we used services features service language tool rest countries genome nexus person controller user management microservice market service project tracking system ohsome youtube mock and spotify.
table i lists the open source services along with the lines of code and the number of api operations in each service.
for a fair comparison because our tool utilizes llm calls we used the enhanced specification generated by restgpt which adds realistic testing inputs to the specification using llms.
moreover we used gpt .
turbo as restgpt utilized this model.
based on a recent survey that describes settings and metrics for rest api testing we used a one hour time budget with ten repetitions to compute the results.
to measure effectiveness and error finding ability we used code coverage open source services only number of successfully processed operations in the specification and number of status codes which are the most popular metrics in the literature.
to collect code coverage we used jacoco .
for the number of processed operations we used the script from the nlp2rest repository .
for the number of status codes we used the script available in the aratrl repository .
this script collects status codes by tracking the http responses and removes the duplicated codes using the server response message for each operation.
b. rq1 effectiveness the effectiveness of autoresttest is evaluated based on its ability to comprehensively cover more code compared totable ii number of operations exercised .
autoresttest arat rl evomaster morest restler fdic ohsome spotify total other tools.
figure illustrates the line branch and method coverage achieved by each testing tool on the nine open source services in our benchmark additionally it shows the average coverage across these apis.
as shown in figure autoresttest outperformed the other tools in terms of method coverage achieving .
coverage on average compared to arat rl .
evomaster .
morest .
and restler .
.
this represents a significant coverage gain ranging from .
to .
percentage points.
similarly autoresttest achieved higher line coverage .
on average compared to the other tools which achieved .
.
and .
respectively.
finally in terms of branch coverage autoresttest again outperformed the other tools achieving .
coverage on average compared to the other tools which achieved .
.
.
and .
respectively.
in our evaluation we also measured the number of processed operations for online services for which source code is unavailable ohsome and spotify.
table ii presents these results which highlight autoresttest s ability to handle a larger number of operations.
specifically autoresttest exercised operations in total compared to arat rl evomaster morest and restler which processed and operations respectively.
these results on achieved code coverage and successfully exercised operations demonstrate autoresttest s effectiveness on a range of different rest apis and how it improves on the state of the art.
in most cases there were notable performance gains in genome nexus person controller user management market youtube ohsome and spotify.
conversely for features service rest countries and project tracking system autoresttest s results did not show much difference compared to the second best performing tool in our set.
these four services have a notable characteristic in common the number of input parameters in their apis is mostly to and the services are therefore easier to test.
this result shows that autoresttest can effectively explore rest apis especially for services with a large search space.
autoresttest achieves considerable gains in code coverage with method coverage increasing between .
to .
percentage points line coverage between .
and .
percentage points and branch coverage between .
and .
percentage points compared to the other tools considered.
the improvement in performance is particularly noticeable on large and complex services with many input parameters.fig.
.
comparison of code coverage metrics across tools and services line branch and method coverage.
table iii service failures triggered response codes .
rest apis autoresttest arat rl evomaster morest restler features service language tool rest countries genome nexus person contoller user management market project tracking system youtube fdic ohsome spotify total c. rq2 fault detection capability we evaluated the fault detection capability of autoresttest by counting how many internal server errors it identified.
table iii shows the number of such errors detected by autoresttest arat rl evomaster morest and restler.
as the data in the table show autoresttest detected a total of internal server errors across the evaluated rest apis far outperforming the other tools on this metric.
aratrl detected errors evomaster and morest detected errors each and restler detected errors.
specifically autoresttest identified significantly more errors in the ohsome service errors compared to aratrl errors with none detected by evomaster morest or restler.
additionally autoresttest was the only tool to detect an error in the spotify service.
it is important to note that both ohsome and spotify are active services spotify for example has million users and the ohsome servicehas recent github commits.
we reported the detected issues and the ohsome errors were accepted whereas we are still waiting for spotify s response .
this improvement in fault detection is somehow expected as autoresttest achieves the highest coverage among the other tools which is strongly correlated with fault finding ability in rest api testing .
to illustrate the utility of autoresttest s specific components in fault detection on a specific example consider the following sequence of operations in the ohsome service which shows the capabilities of the spdg.
autoresttest begins by successfully querying the post elements area ratio endpoint from the ohsome service with its filter2 parameter assigned to node relation .
subsequently autoresttest targets the get users count groupby key endpoint where the dependency agent applies the spdg s semantically created dependency edges to identify a potential connection between thefilter parameter of the new operation and the filter2 parameter of the previous operation.
when the dependency agent reuses the node relation value from the previously successful filter2 parameter in the new request the spdg uncovers an unexpected internal server error.
typically an invalid filter value would trigger a client error but this server side error indicates that the spdg identified a deeper unanticipated fault in the server s value handling.
the other tools overlook the correlation between the two parameters due to either the minor naming differences or improper dependency modeling and fail to expose this error.
for another example autoresttest shows the effectiveness of its llm value generation in the interactions with the spotify api.
the get playlists playlist id trackstable iv code coverage achieved by different tool variants .
method line branch autoresttest .
.
.
.
without llm .
.
.
.
.
.
.
without learning .
.
.
.
.
.
.
without spdg .
.
.
.
.
.
operation in spotify s api requires specific knowledge regarding spotify s playlist id formation.
spotify generates spotify ids for playlists that are typically characters long with constraints on the permitted letters and patterns.
where many tools fail to create valid ids autoresttest s value agent leverages its llm to generate valid spotify ids for playlists.
autoresttest is thus able to successfully query the get playlists playlist id tracks operation with rippling effects across the service.
for instance after retrieving the international standard recording code isrc from the playlist s tracks autoresttest s mutator randomly selects an isrc to use as the user id in the subsequent get users user id playlists operation.
this sequence reveals a hidden dependency conflict that results in a internal server error.
other tools fail to exercise the get playlists playlist id tracks operation entirely and are hence unable to locate this error.
autoresttest outperforms the other tools in terms of fault detection capability.
it identifies a total of instances of internal server errors whereas aratrl evomaster morest and restler detected and errors respectively.
d. rq3 ablation study to understand the contribution of each component in autoresttest we conducted an ablation study in which we removed specific elements of the approach the llm based input generation the agent learning step in marl and the spdg.
because our tool heavily depends on agents it was not feasible to create a reasonable tool without marl entirely.
therefore instead of removing all the agents we only removed the temporal difference q learning from the agents.
table iv presents the impact of these removals on method line and branch coverage.
removing the temporal difference q learning leads to the most significant decrease in overall metrics dropping the coverage rates to .
.
and .
for method line and branch coverage.
the learning step s contribution is crucial in optimizing the testing process through strategic exploration and exploitation of testing paths.
without the multi agent learning step the tool repeated the same requests and failed to properly update its agents with feedback.
the removal of the spdg has the next most significant impact in terms of method and line coverage dropping the performance significantly to .
.
and .
for method line and branch coverage.
this result indicates that the spdg plays a critical role in identifying dependenciesand guiding test generation by reducing the search space thus helping identify the dependent api operations.
removing the llm alone also leads to a substantial decrease in performance with method line and branch coverage dropping to .
.
and .
.
the primary reason for this difference is the llm s ability to generate diverse and appropriate test inputs.
these inputs are essential for exercising the api as they help uncover various parameter and operation dependencies.
furthermore operation and parameter dependencies cannot be accurately identified without resolving parameter constraints when they exist.
notably without the spdg autoresttest exercised only operations for spotify whereas with the spdg it consistently covered operations.
the ablation study shows that each component of autoresttest llm marl and spdg contributes considerably to the effectiveness of the approach and removing any component drops the coverage significantly.
the decrease in method line and branch coverage ranges in percentage points between .
and .
.
and .
and .
and .
indicating that each component plays an important role.
e. threats to validity like for any empirical study there are potential threats to the validity of our results.
regarding construct validity the use of chatgpt .
turbo as our llm component introduces potential data leakage as it may have been trained on api related content potentially affecting our results.
while our ablation study demonstrates the llm s importance this limitation should be considered when interpreting our findings.
additionally technical choices like the mutation rate and chatgpt s default parameters may affect result comparability.
rest apis inherently flaky behavior e.g.
due to network issues introduces possible threats of internal validity.
to mitigate this issue we performed multiple test runs and averaged the results.
we also carefully inspected our code and results to mitigate the risk of implementation errors.
the uneven quality of openapi specifications can also affect the validity of our results.
while this is an inherent and somehow unavoidable issue we have left to future work the investigation of the impact of specification completeness and accuracy on autoresttest s performance.
finally our selection of rest apis benchmarks can affect external validity.
while we used a diverse set of realworld apis autoresttest may perform differently on different benchmarks.
the availability of our dataset and code will allow other researchers to validate and extend our evaluation.
v. r elated work a. automated rest api testing automated testing for rest apis has employed various strategies.
evomaster uses both white box and blackbox techniques and applies evolutionary algorithms to refinetest cases focusing on detecting server errors.
restler generates stateful tests by inferring producer consumer dependencies aiming to uncover server failures.
tools such as resttestgen exploit data dependencies and utilize oracles to validate status codes and response schemas.
morest adopts model based testing to simulate user interactions and generate test cases while restct employs combinatorial testing techniques to explore parameter value combinations systematically.
arat rl introduces reinforcement learning to adapt and refine api testing strategies based on realtime feedback.
techniques such as quickrest schemathesis and restest use property based testing and various oracles to ensure compliance with openapi or graphql specifications.
tools such as dredd fuzz lightyear and tcases provide diverse testing capabilities from validating responses against expected results to detecting vulnerabilities and validating swagger based specifications.
the closest approach to autoresttest that incorporates reinforcement learning is arat rl.
however arat rl does not employ a comprehensive model to represent operation dependencies which reduces its effectiveness.
given the difficulties in embedding potential dependencies into the action space of an agent arat rl considers weighted probabilities using parameter frequency to guide potential dependencies.
this can result in the inefficient prioritization of unrelated operations in subsequent requests.
unlike arat rl autoresttest uses the spdg to narrow the dependency search.
morest with its restful property graph rpg is the closest approach to autoresttest in terms of using a graph model.
however morest is incapable of progressively adapting its requests according to server feedback rendering its dependency sequences less effective.
concurrently to this work two additional rest api testing techniques were proposed deeprest and llamaresttest .
deeprest uses deep reinforcement learning to discover implicit api constraints employing a single agent that learns operation orderings through a reward mechanism.
however deep learning s black box nature makes it difficult to track how and why specific testing decisions are made.
in contrast autoresttest s reward mechanism is more effective because uses multiple specialized agents and is able to track how each agent s decisions contribute to the overall testing strategy.
additionally while deeprest learns dependencies purely through trial and error autoresttest reduces the search space upfront using the spdg.
llamaresttest fine tunes small language models specifically for rest api testing tasks using llama models to identify inter parameter dependencies and generate valid inputs.
llamaresttest mainly focuses on llm driven testing whereas our approach uses llms only to generate parameter values while relying on the spdg for efficient dependency identification and multi agent reinforcement learning for dynamic optimization across all testing steps.
we could not compare with either of these approaches in our evaluation because they had not yet been published when we performed this work.b.
llm based rest api testing and analysis recent advancements in llms have resulted in improved rest api testing and analysis approaches.
nestful provides a benchmark for evaluating llms on nested api calls revealing challenges in handling complex api interactions.
restspecit demonstrates automated specification inference and black box testing with minimal user input while restgpt introduces coarse to fine online planning for improved task decomposition and api selection.
while these approaches primarily focus on rest api analysis autoresttest focuses on testing.
restspecit includes some testing capabilities but it is limited to basic parameter mutation and lacks advanced testing features such as operation parameter dependency identification.
c. reinforcement learning based test case generation recent studies have explored reinforcement learning for software testing particularly for web and mobile applications.
for instance zheng and colleagues proposed a curiosity driven reinforcement learning approach for web client testing and pan and colleagues applied a similar technique for android application testing .
other work includes qbe a q learning based exploration method for android apps and autoblacktest an automatic black box testing approach for interactive applications .
reinforcement learning has also been used specifically for android gui testing .
while these approaches use single agent based techniques autoresttest decomposes the test generation problem into multiple components operation parameter value dependency and uses a multi agent reinforcement algorithm to reward the different components contributions in each reinforcement learning step.
additionally autoresttest introduces the spdg to reduce the search space of operation dependency.
vi.
conclusion and future work in this paper we introduced autoresttest a new technique that leverages multi agent reinforcement learning the semantic property dependency graph and large language models to enhance rest api testing.
by optimizing specialized agents for dependencies operations parameters and values autoresttest addresses the limitations of existing techniques and tools.
our evaluation on state of the art rest services shows that autoresttest can significantly outperform leading rest api testing tools in terms of code coverage operation coverage and fault detection.
furthermore our ablation study confirms the individual contributions of the marl llms and spdg components to the tool s effectiveness.
in future work we will explore the dynamic adaptation of testing strategies optimize performance and scalability e.g.
through fine tuning llms and develop more sophisticated faultclassification approaches.