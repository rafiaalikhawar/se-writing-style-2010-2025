toward a better understanding of probabilistic delta debugging mengxiao zhang zhenyang xu yongqiang tian xinru cheng and chengnian sun school of computer science university of waterloo waterloo canada emails m492zhan uwaterloo.ca zhenyang.xu uwaterloo.ca x59cheng uwaterloo.ca cnsun uwaterloo.ca department of computer science and engineering the hong kong university of science and technology hong kong china email yqtian ust.hk abstract given a list lof elements and a property that lexhibits ddmin is a classic test input minimization algorithm that aims to automatically remove irrelevant elements from l. this algorithm has been widely adopted in domains such as test input minimization and software debloating.
recently probdd a variant of ddmin has been proposed and achieved stateof the art performance.
by employing bayesian optimization probdd estimates the probability of each element in lbeing relevant to and statistically decides which and how many elements should be deleted together each time.
however the theoretical probabilistic model of probdd is rather intricate and the underlying details for the superior performance of probdd have not been adequately explored.
in this paper we conduct the first in depth theoretical analysis of probdd clarifying the trends in probability and subset size changes and simplifying the probability model.
we complement this analysis with empirical experiments including success rate analysis ablation studies and examinations of trade offs and limitations to further comprehend and demystify this stateof the art algorithm.
our success rate analysis reveals how probdd effectively addresses bottlenecks that slow down ddmin by skipping inefficient queries that attempt to delete complements of subsets and previously tried subsets.
the ablation study illustrates that randomness in probdd has no significant impact on efficiency.
these findings provide valuable insights for future research and applications of test input minimization algorithms.
based on the findings above we propose cdd a simplified version of probdd reducing the complexity in both theory and implementation.
cdd assists in 1validating the correctness of our key findings e.g.
that probabilities in probdd essentially serve as monotonically increasing counters for each element and 2identifying the main factors that truly contribute to probdd s superior performance.
our comprehensive evaluations across benchmarks in test input minimization and software debloating demonstrate that cdd can achieve the same performance as probdd despite being much simplified.
index terms program reduction delta debugging software debloating test input minimization i. i ntroduction delta debugging is a seminal family of algorithms designed for software debugging among which ddmin stands out as a classic test input minimization a.k.a.
test input reduction algorithm.
given a list lof elements modeling the test input and a property thatlexhibits ddmin aims to remove elements in lthat are irrelevant to such that the resulting list is smaller than lyet still satisfies .
the ddmin algorithm plays a crucial role in softwaretesting debugging and maintenance since compact and informative bug triggering inputs are easier for developers to effectively identify root causes than large bug triggering inputs with bug irrelevant information .
to minimize a test input ithat satisfies ddmin has been used in two primary manners.
in the first manner iis initially segmented into a list denoted as l which could be segmented based on characters tokens lines etc.subsequently ddmin is directly applied to l .
alternatively ddmin serves as a pivotal component within advanced structure aware test input minimization algorithms including perses hdd creduce and chisel .
these algorithms leverage the inherent structures of ito expedite the minimization process or further reduce its size.
generally these algorithms initiate by parsing iinto a tree structure such as a parse tree.
they then iteratively extract a list lof tree nodes from the tree using heuristics and apply ddmin tolto gradually condense the tree.
both manners underscore the fundamental role of ddmin as the cornerstone of test input minimization.
in the past years different variants of ddmin have been proposed to improve its performance among which probabilistic delta debugging probdd is the state of the art with notable superiority to other algorithms .
when reducing l probdd utilizes a theoretical probabilistic model based on bayesian optimization to predict how likely every element in lis essential to preserve the property by assigning a probability to each element.
probdd prioritizes deleting elements with lower probabilities as such elements generally have a lower possibility of being relevant.
before each deletion attempt an optimal subset of elements is determined by maximizing the expected reduction gain.1if the deletion of this subset fails to preserve the probabilistic model increases the probability assigned to each element in the subset.
as reported aided by such a probabilistic model probdd significantly outperforms ddmin by reducing the execution time and the query number.
however this probabilistic model in probdd is rather intricate and the underlying mechanisms for its superior per1in each attempt the expected reduction gain is defined as the expected number of elements removed.
higher expected reduction gain is preferred as it indicates an expectation to delete more elements through this attempt.
2a query is a run of the property test .arxiv .04735v4 may 2025formance have not been adequately studied.
the original paper of probdd merely showed its performance numbers without deep ablation analysis on such achievements.
specifically the following questions are important to the research field of test input minimization but have not been answered yet.
what role do probabilities play in probdd and can they be simplified without impacting performance?
what specific bottlenecks does probdd overcome to achieve improvement compared to ddmin ?
how does randomness in probdd contribute to the performance improvement?
what are the potential limitations of probdd?
gaining a deeper understanding of the state of the art i.e.
probdd is highly valuable for test input minimization tasks.
by clarifying the intrinsic reasons behind its superiority we can facilitate researchers to understand the essence of the probabilistic model as well as its strengths and limitations.
such demystification in our view paves the way for enlightening future research and guides users to more effectively apply ddmin and its variants for test input minimization.
to this end we conduct the first in depth analysis of probdd starting by theoretically simplifying its probabilistic model.
in the original probdd probabilities are used to calculate the expected reduction gain which is subsequently used to determine the next subset size.
however this process necessitates iterative calculations impeding the simplification and comprehension of probdd.
in our study we initially establish the analytical correlation between the probability and subset size allowing for probabilities and subset sizes to be explicitly calculated through formulas thus eliminating the need for iterative updates.
further through mathematical derivation we discover that the probability and subset size can be considered nearly independent each varying at an approximate ratio on their own.
by theoretical prediction the probability increases approximately by a factor of1 e .
and the subset size can be deduced by this probability thus providing the potential for simplifying probdd.
building upon our theoretical analysis we conducted extensive evaluations of ddmin probdd and cdd across diverse benchmarks.
the experimental results confirm the correctness of our theoretical analysis demonstrating how probdd addresses bottlenecks in ddmin by skipping inefficient queries reveals the impact of randomness on results and highlights the limitations of probdd.
these findings provide valuable guidance for future research and the development of test input minimization algorithms.
based on the aforementioned analysis we propose counterbased delta debugging cdd a simplified version of probdd to explain probdd s high performance.
by replacing probabilities with counters cdd eliminates the probability computations required by probdd thus reducing theoretical and implementation complexity.
our experiments demonstrate that cdd aligns with probdd in both effectiveness and efficiency which validates our previous analysis and findings.
key findings.
through both theoretical analysis and empirical experiments our key findings are through theoretical derivation the probabilities in probdd essentially serve as monotonically increasing counters and can be simplified.
this suggests that the probability mechanism itself may not be a critical factor in probdd s superior performance.
the performance bottlenecks addressed by probdd are inefficient deletion attempts on complements of subsets and previously tried subsets which should be considered to enhance efficiency.
randomness in probdd has no significant impact on the performance.
test input minimization is an np complete problem and randomness in probdd does not produce smaller results.
probdd is faster than ddmin but at the cost of not guaranteeing minimality.3the trade off between effectiveness and efficiency is inevitable and should be leveraged accordingly in different scenarios.
contributions.
we make the following major contributions.
we perform the first in depth theoretical analysis for probdd the state of the art algorithm in test input minimization tasks and identify the latent correlation between the subset size and the probability of elements.
we propose cdd a much simplified version of probdd.
we evaluate ddmin probdd and cdd on 76benchmarks validating the correctness of our theoretical analysis.
additional experiments and statistical analysis on probdd further explain its superior performance reveal the effectiveness of randomness and demonstrate the limitations of probdd.
to enable future research on test input minimization we release the artifact publicly for replication .
additionally we have integrated cdd into the perses project available at paper organization.
the remainder of the paper is structured as follows ii introduces the symbols used in this study and detailed workflow of ddmin and probdd.
iii presents our in depth analysis on probdd simplifying the model of probability and subset size.
iv describes empirical experiments and results from which additional findings are derived.
v introduces cdd which simplifies probdd based on our earlier findings while maintaining equivalent performance.
vii illustrates related work and viii concludes this study.
ii.
p reliminaries to facilitate comprehension table i lists all the important symbols used in this paper.
next this section introduces ddmin and probdd with the running example shown in fig.
.
a. the ddmin algorithm theddmin algorithm is the first algorithm to systematically minimize a bug triggering input to its essence which has been widely adopted in program reduction software debloating and test suites reduction .
it takes the following two inputs 3a list is considered to have minimality if removing any single element from it results in the loss of its property.table i the symbols used in this paper.
symbol description symbol description l the list to minimize s the size of s the property to preserve e s expected reduction gain with the first selements li thei th element of l e euler s number li.p the probability of li r the round number vi a variant of l sr the subset size in round r s a subset of l pr the probability of each element in round r l1 import math sys l2 input sys.argv l3 a int input l4 b math.e l5 c l6 d pow b a l7 c math.log d b l8 crash c a original.l1 import math sys l2 input sys.argv l3 a int input l4 b math.e l5 c l6 d pow b a l7 c math.log d b l8 crash c b by ddmin .l1 import math sys l2 input sys.argv l3 a int input l4 b math.e l5 c l6 d pow b a l7 c math.log d b l8 crash c c by probdd.
fig.
a running example in python.
fig.
a shows the original program represented as a list of elements l1 l2 l8 in which l8 i.e.
crash c triggers the crash.
fig.
b and fig.
c show the minimized results by ddmin and probdd with removed elements masked in gray.
both minimized programs still trigger the crash.
note that probdd cannot consistently guarantee the result in fig.
c and might produce larger results due to its inherent randomness.
l a list of elements representing a bug triggering input.
for example lcan be a list of bytes characters lines tokens or parse tree nodes extracted from the bug triggering input.
a property that lhas.
formally can be defined as a predicate that returns tif a list of elements preserves the property fotherwise.
and returns a minimal subset of lthat still preserves from which excluding any single element will make the minimal subset lose .
this algorithm has been widely used in practice to facilitate developers in debugging .
it generally consists of the following three steps.
initialize.
start by setting the initial subset size sto half of the input list l i.e.
s l .
step minimize to subset.
partition linto subsets of size s. for each subset s check whether salone satisfies .
if yes keep only sand restart from step with l sand the subset size as half of the new l otherwise go to step .
step minimize to complement.
partition linto subsets of size s. for each subset s check whether the complement ofs i.e.
l s e e l e s satisfies .
if yes keep the complement of sand restart from step with l l s otherwise go to step .
step subdivide.
if any of the remaining subsets has at least two elements and thus can be further divided halve the subset size i.e.
s s 2and go back to step .
if no subset can be further divided i.e.
the subset size is ddmin terminates and returns the remaining elements as the result.
round number r.note that we introduce a round number r at the second column of table ii.
within each round the list lis divided into subsets of a fixed size on which step and step are applied.
a new round begins when no further progress can be made with the current subset size.
this round number isnot explicitly present in the original ddmin algorithm but exists implicitly .
in subsequent sections we will also use this concept to introduce and simplify the probdd algorithm.
table ii illustrates the step by step minimization process of ddmin with the running example in fig.
.
initially the input lis .
the ddmin algorithm iteratively generates variants by gradually decreasing the subset size from to .
round s .
at the beginning ddmin splits linto two subsets and generates two variants v 1and v .
however neither of them preserves .
round s .
next ddmin continues to subdivide these two subsets into smaller ones and generates eight variants i.e.
v3 v4 v10 by using these subsets and their complements.
specifically the first four variants v v4 v5 v6 are the subsets and the next four variants v v8 v9 v10 are the complements of these subsets.
again none of these eight variants preserves .
round s .
finally ddmin decreases subset size sfrom to and generates more variants.
this time v which is the complement of the subset l5 preserves .
hence the subset l5 is permanently removed from l. then for each of the remaining subsets l1 l2 l8 ddmin restarts testing the complement of each subset i.e.
from v24to v .
however none of these variants preserves and no subset can be further divided so ddmin terminates with the variant v 23as the final result.
b. probabilistic delta debugging probdd wang et al.
proposed the state of the art algorithm probdd significantly surpassing ddmin in minimizing bugtriggering programs on c compilers and benchmarks in software debloating.
probdd employs bayesian optimization to model the minimization problem.
probdd assigns a probability to each element in l representing its likelihood of being essential for preserving the property .
at each step during the minimization process probdd selects a subset of elements expected to yield the highest expected reduction gain and targets these elements in the subset for deletion.
in this section we outline probdd s workflow in algorithm paving the way for a deeper understanding and analysis of probdd.
initialize line .
inl probdd assigns each element an initial probability p0on line representing the prior likelihood that each element cannot be removed.
step select elements line line .
first probdd sorts the elements in lby probability in ascending order on line and the order of elements with the same probability is determined randomly.
then on line it calculates the subset to be removed in the next attempt via the proposed expected reduction gain e s as shown in equation with e s denoting the expected gain obtained via removing the firsttable ii step by step outcomes from ddmin on the running example.
in each column a variant is generated and tested against the property .
these variants are sequentially generated from left to right.
the first row displays the variant identifier and the second row displays round number rand subset size s. in the following rows the symbol denotes an element is included by a certain variant while gray cells signify that the element have been removed.
for the last row tindicates that the variant still preserves the property whereas findicates not.
initial variants v1 v2 v3v4v5v6v7v8v9v10 v11 v12 v13 v14 v15 v16 v17 v18 v19 v20 v21 v22 v23 v24 v25 v26 v27 v28 v29 v30 element round r s r s r s l1 l2 l3 l4 l5 l6 l7 l8 f f f f f f f f f f f f f f f f f f f f f f t f f f f f f f selements inlselected for deletion and li.pdenoting the current probability of the i th element in l. e s s sy i li.p note that probdd has an invariant that the subset schosen for deletion attempt is always the first selements in l. every time the first s elements are selected as the optimal subset s where s maximizes the expected reduction gain e s elaborated as equation .
s arg max se s step delete the subset line .
if is still preserved after the removal of s probdd removes subset son line i.e.
keeps only the complement of s and proceeds to step .
if cannot be preserved after the removal on lines and probdd updates the probability of each element in the subset svia equation and resumes at step .
it is important to note that if an element lihas been individually deleted but failed its probability li.pwill be set to indicating that this element cannot be removed and will no longer be considered for deletion.
li.p li.p q l s l.p step check termination line .
if every element either has been deleted or possesses a probability of probdd terminates.
if not it returns to step .
round number r.similar to the concept of rounds in ddmin see table ii probdd also has an implicit round number r as introduced on line in algorithm and the second row of table iii.
during a round the subset size is the same and every subset in lis attempted for deletion.
once the probabilities of all elements have been updated the next round begins i.e.
r r 1on line .
table iii illustrates the step by step results of probdd.
following the study of probdd the initial probability p0is set to .
resulting in subsets with a size of as per equation .
round s .
similar to the example in the original paperof probdd we assume probdd selects l1 l4 l5 l8 to delete due to the randomness thus resulting in the variant v .
however v 1fails to exhibit leading to the probability of these selected elements being updated from .
to0.
.
.
based on equation .
next the remaining elements with lower probability i.e.
l2 l3 l6 l7 are prioritized and selected for deletion resulting in v2.
this time the property test passes and these elements are removed.
round s .
given that all probabilities of remaining elements become .
the next subset size becomes .
subsequently subset l1 l5 are attempted to remove in v 3and later subset l4 l8 are attempted to remove in v though no subset can be successfully removed.
after these two attempts all probabilities update to0.
.
.
.
round s .
finally the subset size becomes so each individual element is selected to remove alone.
the elements l4andl1are finally removed from the final result in v 5and v respectively while l5andl8are verified as non removable thus being returned as the final result.
iii.
d elving deeper into probability and size beginning with this section we will systematically present our findings.
each finding will be introduced by first stating the result followed by the explanation.
in this section we theoretically analyze the trend of probability changes across rounds and the approach to derive the optimal subset size.
a. on the probability in probdd finding the probability assigned to each element increases monotonically with the round number r by a factor of approximately .
.
essentially the probability for each element can be expressed as a function of rand p0 i.e.
pr .582r p0 an illustrative example.
the running example illustrated in table iii leads to this finding.
observation reveals that after each element has been attempted for deletion once i.e.
completing one round the probabilities of all remaining elements are updated.
the initial probability is .
after v italgorithm probdd l input l a list to be minimized.
input l b the property to be preserved by l. input p0 the initial probability given by the user.
output the minimized list that still exhibits the property .
initialize the probability of each element with p0 1foreach l ldol.p p0 the round number r initially .
ris not explicitly used in the original probdd algorithm.
it is displayed for demonstrating probdd s implicit principles.
2r 3while l l l.p 1do select elements from lfor deletion attempt.
s selectsubset l check if removing the subset preserves the property temp l s if temp tthen l temp else calculate the factor to update probabilities factor q l s l.p update the probabilities of elements in the subset foreach l sdol.p factor l.p ifall elements probability have been updated then move to the next round.
r r 12return l 13function selectsubset l input l a list of elements to be reduced.
output the subset of elements that maximizes the expected reduction gain.
sort lby ascending probability with elements having the same probability in random order.
sortedl randomizethensort l s currentmaxgain foreach l sortedl do tempsubset s l gain tempsubset q l tempsubset l.p ifgain currentmaxgain then currentmaxgain gain s tempsubset else break return s table iii step by step outcomes from probdd on the running example.
similar to table ii round number subset size and the details of each variants are presented.
for each variant the probability of each element is noted alongside.
initial variants v1 v2 v3 v4 v5 v6 v7 v8 element prob round r s r s r s l1 .
.
.
.
.
.
.
l2 .
.
l3 .
.
l4 .
.
.
.
.
l5 .
.
.
.
.
.
l6 .
.
l7 .
.
l8 .
.
.
.
.
.
.
.
f t f f t f t f changes to .
following v it increases to .
and by the end of v it reaches .
consequently we hypothesize that with each deletion attempt the probability approximately increases in a predictable manner.
through appropriate simplification we can theoretically model this trend and thereby model the entire progression of probability changes.
assumption for theoretical analysis besides the above observation from a concrete example theoretical analysis is necessary.
to refine the mathematical model of probdd for easier representation analysis and derivation we assume that the number of elements in lis always divisible by the subset size.
with this assumption the probability of each element will be updated in the same manner as a result before and after each round the probabilities of all elements are always the same as shown in lemma iii.
.
this assumption is often applicable in practice.
for instance in the running example in table iii before each round the probabilities associated with each remaining element are identical ensuring that all subsets are of identical size.
furthermore the probabilities of elements are updated to the same next value after the round.
lemma iii.
.
if the number of elements in lis always divisible by the subset size then after each round the probabilities of all elements will always remain the same.
proof.
we use mathematical induction to prove this lemma.
base case .initially all probabilities are set to the same value.
hence before the first round the probabilities of all elements are identical.
inductive step .
assume that before a given round the probabilities of all elements are identical induction hypothesis .
after failing to delete a subset s probdd updates the probability of each element of saccording to equation .
this formula depends solely on two factors the current probability of each element of s i.e.
li.p and the size of the subset s .
for li.p by the induction hypothesis all elements have the same probability at the beginning of the round for s if the total number of elements in lis divisible by the subset size then every subset in the round will have the same size s .
therefore both factors li.pand s are identical for all elements in a subset and the probabilities of these elements are updated to the same new value using equation .
furthermore as all subsets undergo the same update process the probabilities of all elements in the list will remain identical at the end of the round.
conclusion .the probabilities of all elements remain identical at the end of this round.
consequently as long as the total number of elements is always divisible by the subset size the probabilities of all elements will remain identical throughout the process.
take the running example in table iii as a demonstration.
during the reduction the number of elements is always divisible by the subset size in each round i.e.
s s s .
therefore starting with an initial probability of .
the probability of each elements remain identical after each round being .
.
and respectively.
while it is not always possible for the number of elements to be divisible by the subset size the elements will still be partitioned as evenly as possible.
however such indivisibilities make the theoretical simplification of probdd nearly impossible.
based on our observation when running probdd being slightly uneven during partitioning does not significantlyaffect probability updates.
moreover we will demonstrate that the simplified algorithm derived from this assumption has no significant difference from probdd in v via thorough experimental evaluation.
probability vs.subset size correlation in the second step we derive the correlation between probability and subset size.
based on the assumption in the previous step the probability of each element is identical and represented as prin round r thus the formula of expected reduction gain from equation can be simplified to e s s pr s given the probability of elements prin the round r srcan be derived through gradient based optimization i.e.
e sr .
therefore the optimal size srto maximize e s is ln pr .
subsequently we can also deduce the next probability to be pr pr pr sr. in summary the correlation between probability and subset size can be simplified as equation and equation in which subset size sris determined by probability pr and probability pr 1in the next round is determined by both prandsr.
sr ln pr pr pr pr sr trend of probability changes through equation pr pralways holds indicating a monotonic increase of the probability of elements.
however there is still room for simplification as srcan be represented by pr implying that pr 1can be represented solely by pr.
lemma iii.
.
pis increased by a factor1 e i.e.
pr pr e p0 e r proof.
given sr ln pr we can deduce that pr e sr. subsequently we substitute printo equation obtaining pr pr pr sr pr e sr sr pr e .
pr equivalently the approximate probability after round rcan be derived given only p0 i.e.
pr p0 e r .582r p0 therefore through empirical observations on the running example coupled with theoretical derivation and simplification we have identified the pattern of probability changes w.r.t.
the round number r i.e.
pr p0 e r .582r p0.b.
on the size of subsets in probdd finding the size of subsets in r th round can be analytically pre determined given only the probability of this round i.e.
sr arg maxs n s pr s which is either ln pr or ln pr .
based on the finding the probability prcan be approximately estimated by the current round number rvia a factor.
consequently we can further derive the subset size srby maximizing the expected reduction gain in probdd.
lemma iii.
.
the optimal subset size srin round ris either ln pr or ln pr .
proof.
the expected reduction gain is determined by the formula e sr sr pr sr which increases initially withsrand then decreases as srgrows further enabling the optimal solution to be identified through derivative analysis.
therefore we can deduce the optimal srby solving e sr .
therefore the optimal size of subsets srin r th round is ln pr which will be rounded to either ln pr or ln pr .
the final subset size should be chosen based on which integer results in a larger expected reduction gain.
lemma iii.
allows the subset size to be analytically predetermined thus providing the potential for simplification of probdd and leading to the proposal of cdd detailed in v .
iv.
e mpirical experiments in addition to the theoretical derivation above we conduct an extensive experimental evaluation on ddmin and probdd to gain deeper insights and achieve further discoveries.
specifically we reproduce the experiments on ddmin and probdd by wang et al.
and then delve deeper into probdd analyzing its randomness the bottlenecks it overcomes and its minimality.
furthermore we evaluate our proposed cdd which will be presented in v validating our previous theoretical analysis.
due to limited space we present the results of both probdd and cdd together within this section but this section primarily focuses on discussing probdd while the next section will focus on cdd.
a. benchmarks to extensively evaluate ddmin probdd and cdd we use the following three benchmark suites 76benchmarks in total covering various use scenarios of minimization algorithms.
bm c large bug triggering programs in c language each of which triggers a real world compiler bug in either llvm or gcc.
the original size of benchmarks ranges from tokens to tokens.
this benchmark suite has been used to evaluate test input minimization work .
bm dbt source programs of command line utilities.
the original size of benchmarks ranges from tokens to tokens.
this benchmark suite was collected by heo et al.
and used to evaluate software debloating techniques .
bm xml xml inputs triggering unique bugs in basex a widely used xml processing tool.
the original size of benchmarks ranges from tokens to tokens.
this benchmark suite is generated via xpress and collected by the authors of this study as the original xml dataset used in probdd paper is not publicly available.
b. evaluation metrics we measure the following aspects as metrics.
final size.
this metric assesses the effectiveness of reduction.
when reducing a list lwith a certain property a smaller final list is preferred indicating that more irrelevant elements have been successfully eliminated.
in all benchmark suites the metric is measured by the number of tokens .
execution time.
the execution time of a minimization algorithm reflects its efficiency.
a minimization algorithm taking less time is more desirable and execution time is measured in seconds .
query number.
this metric further evaluates the algorithm s efficiency.
during the reduction process each time a variant is produced the algorithm verifies whether this variant still preserves the property referred to as a query.
since queries consume time a lower query number is favorable.
p value.
we calculate the p value via wilcoxon signed rank test between every two algorithms to investigate whether the performance differences are significant.
a p value below .
indicates that we can reject the null hypothesis which assumes no difference in performance at the .
significance level.
otherwise we fail to reject the null hypothesis suggesting that the observed difference may not be statistically significant.
c. the wrapping frameworks theddmin algorithm and its variants usually serve as the fundamental algorithm.
to apply them to a concrete scenario an outer wrapping framework is generally needed to handle the structure of the input.
in our evaluation we choose the same wrapping frameworks as those used by probdd paper.
for those tree structured bug triggering inputs i.e.
bm cand bm xml we use picireny .
an implementation of hdd .
picireny parses such inputs into trees and then invokes picire .
an open sourced delta debugging library with ddmin probdd and cdd implemented to reduce each level of the trees.
for software debloating on bm dbt chisel is employed in which ddmin probdd and cdd are integrated.
all experiments are conducted on a server running ubuntu .
.
lts with tb ram and two intel xeon gold cpus .60ghz.
to ensure the reproducibility we employ docker images to release the source code and the configuration.
each benchmark is reduced using a single thread.
following the probdd paper we run each algorithm on each benchmark times and calculate the geometric average results.
d. reproduction study of probdd to comprehensively reproduce the results of probdd we evaluate ddmin and probdd using three benchmark suites containing a total of 76benchmarks.
following the settings of probdd we set the empirically estimated remaining rate as the initialization probability p0 specifically .
for bm c and bm dbt and .5e for bm xml.
the detailed results are shown in table iv and table v. efficiency and effectiveness.
through our reproduction study we find that the performance of probdd aligns with the results reported in the original paper showing that probdd is significantly more efficient than ddmin .
across three benchmark suites probdd requires .
less time and .
fewer queries with p value being 3e and 9e respectively.
moreover we assess the effectiveness by measuring the sizes of the final minimized results.
the effectiveness of ddmin and probdd varies across benchmarks.
however the wilcoxon signed rank test yields a p value of .
which is substantially higher than the .
significance level.
therefore we fail to reject the null hypothesis suggesting no statistically significant difference in overall performance between the two algorithms.
e. impact of randomness in probdd finding randomness has no significant impact on the performance of probdd.
in probdd elements with different probabilities are sorted accordingly while elements with the same probability are randomly shuffled.
however randomness alone intuitively does not ensure a higher probability of escaping local optima and the effect of this randomness on performance has not been thoroughly investigated.
to this end we conduct an ablation study by removing such randomness creating a variant called probdd no random.
we evaluate this variant across all benchmarks.
the results indicate that the randomness does not significantly impact performance.
specifically in terms of final size execution time and query number probdd no random achieves and compared to and of probdd respectively.
the p values of .
.
and .
all exceed .
so we fail to reject the null hypothesis indicating no statistically significant differences.
f .
bottleneck overcome by probdd finding on tree structured inputs inefficient deletion attempts on complements and repeated attempts account for the bottlenecks of ddmin which are overcome by probdd.
in the study of probdd the authors demonstrate that probdd is more efficient than the baseline approach ddmin in tree based reduction scenarios where the inputs are parsed into tree representations before reduction.
therefore to uncover the root cause of this superiority we follow the same application scenario and analyze the behavior of probdd in reducing the tree structured inputs.
to further understand why probdd is more efficient than ddmin we conduct in depth statistical analysis on the query number number of deletion attempts .
intuitively performance bottlenecks lie in those queries with low success rates table iv the final size execution time and query number of ddmin probdd and cdd on bm cand bm dbt.
to address significant variations across benchmarks the geometric mean rather than the arithmetic mean is employed providing a smoother measure of the average.
final size execution time s query number benchmark original size ddmin probdd cdd ddmin probdd cdd ddmin probdd cdd llvm llvm llvm llvm llvm llvm llvm llvm llvm gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc 220bm c mean bzip2 .
.
chown .
date .
grep .
gzip .
.
mkdir .
.
rm .
sort .
tar .
uniq .
669bm dbt mean ddmin probdd cdd0200400600800query number thousand .
.
.
.
.
other complement revisit probdd cdd a on bm c ddmin probdd cdd02004006008001000query number thousand .
.
.
.
.
other complement revisit probdd cdd b on bm dbt ddmin probdd cdd051015202530query number thousand .
.
.
.
.
other complement revisit probdd cdd c on bm xml fig.
visualization of queries within ddmin probdd and cdd.
in ddmin three types of queries are displayed via stacked bars the height of which denotes the query number.
within each bar the number of successful queries total queries and the corresponding success rate are annotated.
impairing ddmin s efficiency.
existing studies also demonstrate the presence of queries with low success rates.
therefore to qualitatively and quantitatively identify the exact bottlenecks impairing ddmin we statistically analyze all the queries in ddmin and categorize them into three types complement queries attempting to remove the complement of a subset.
according to ddmin algorithm given a subset smaller than half of the list l it attempts to remove either the subset or its complement.
however evidence shows that keeping a small subset and removing its complement is not likely to succeed especially on structuredinputs like programs.
revisit queries attempting to remove the previously tried subset.
after removing a subset ddmin restarts the process from the first subset leading to repeated deletion attempts on earlier subsets.
although the removal of one subset may allow another subset to be removable such repetitions rarely succeed and thus offer limited improvement for the reduction .
other all other queries.
in addition to categorizing queries in ddmin into the above types we also calculate the success rate of each type aimingtable v the final size execution time and query number of ddmin probdd and cdd on bm xml.
the last row shows the overall average across all three benchmark suites.
final size execution time s query number benchmark original size ddmin probdd cdd ddmin probdd cdd ddmin probdd cdd xml 071d221 xml 071d221 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 1e9bc83 xml 2d4ec80 xml 327c8af xml 3398ac2 xml 3398ac2 xml 3398ac2 xml 3398ac2 xml 3398ac2 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 4c99b96 xml 8ede045 xml 8ede045 xml 8ede045 xml 8ede045 xml 8ede045 xml 8ede045 xml 8ede045 xml 8ede045 xml f053486 28bm xml mean all mean to reveal the bottlenecks of ddmin .
fig.
illustrates the distribution of queries for all types within ddmin as well as the query number for probdd across all three benchmark suites.
on all benchmark suites the number of successful queries inddmin and probdd is remarkably similar especially when contrasted with the substantial difference in the total number of queries.
specifically on bm c ddmin achieves successful queries closely matching the successful queries from probdd.
similarly on bm dbt and bm xml ddmin performs and2 317successful queries respectively both closely aligning with the and successful queries achieved by probdd.
besides ddmin always performs significantly more failed queries resulting in a larger total query number and thus a longer execution time as previously discussed in iv d. on all benchmark suites a large portion of ddmin s queriesis categorized as complement andrevisit however they both have a notably low success rate.
for instance on bm c out of a total of queries complement andrevisit account for .
and .
respectively.
within such queries in complement andrevisit merely .
and .
queries succeed i.e.
only a tiny portion of attempts successfully reduce elements.
these success rates are far less than those of queries within other .
as well as those of probdd .
.
on the other benchmark suites a similar phenomenon is observed.
queries within complement andrevisit categories constitute a large portion yet prove to be largely inefficient wasting a significant amount of time and resources.
on the contrary those in other achieve a much higher success rate on par with that of probdd and are responsible for most of the successful deletions.
therefore we believe that these two categories where queries are inefficient are the main bottlenecks behindddmin s low efficiency.
however these bottlenecks are absent in probdd as it does not consider complements of subsets and previously tried subsets for deletion.
g. minimality of probdd?
finding improving efficiency by avoiding ineffective attempts presents a trade off by not ensuring minimality while such limitation can be mitigated by iteratively running the reduction algorithm until a fixpoint is reached.
although probdd avoids revisit queries to enhance efficiency some reduction potentials may be missed as the deletion of a certain subset may enable a previously tried subset to become removable.
therefore a limitation of probdd lies in that it increases efficiency by sacrificing minimality.
to substantiate this limitation we examine how frequently probdd generates a list that is not minimal i.e.
can be further reduced by removing a single element.
for instance statistical analysis on bm creveals that among invocations of probdd of them fail to generate a minimal result accounting for .
.
for these failed invocations an average of .
elements tree nodes can be further removed via single element deletion.
however such limitation is not apparent across all benchmark suites as the results from probdd are not consistently larger than those from ddmin .
our further investigation reveals that these benchmarks are reduced on wrapper frameworks picireny and chisel.
both frameworks employ iterative loops to achieve a fixpoint effectively reducing some elements missed in the first iteration.
v. i mplications a counter based model building on the aforementioned demystification of probdd we discover that probability can be optimized away and subset size can be pre computed.
hence we propose counter based delta debugging cdd to reduce the complexity of both the theory and implementation of probdd and validate the correctness of our prior theoretical proofs.
subset size pre calculation.
based on lemma iii.
in iii b the size for each round can be pre calculated.
therefore as shown at line line in algorithm we utilize the current round rand the initial probability p0to determine the subset size s. the size of the selected subset decreases as the round counter increases.
this is intuitively reasonable since after a sufficient number of attempts on a large size have been made it becomes more advantageous to gradually reduce the subset size for future trials.
furthermore this trend aligns well with that of probdd in which probabilities of elements gradually increase resulting in a smaller subset size.
main workflow.
the simplified probdd is illustrated in algorithm from line to line .
before each round the cdd pre calculates the subset size on line and then partitions lusing this size on line .
then similar to ddmin it attempts to remove each subset on line line .
the subset size continuously decreases until it reaches meaning that each element will be individually removed once.algorithm cdd l input l a list of element to be reduced.
input l b the property to be preserved by l. input p0 the initial probability given by the user.
output the minimized list that still exhibits the property .
1r the round number initially .
2do compute subset size by round number s computesize r p0 partition l into subsets with selements.
if it does not divide evenly leave a smaller remainder as the final subset.
subsets partition l s foreach subset subsets do temp l subset remove subset if it is removable if temp is true then l temp update the rand move to next round.
r r 10while s 11return l 12function computesize r p input r the current round number.
input p0 the initial probability given by the user.
output the size of the subset to be used in the current round.
calculate the estimated probability of round r pr p0 .582r calculate corresponding subset size of round r sr arg maxs n s pr s return sr revisiting the running example.
returning to table iii under the same conditions cdd achieves the same results as probdd but without the need for probability calculations.
this is because both the probability and subset size scan be directly determined from the round number r. evaluation.
as shown in table iv and table v cdd outperforms ddmin w.r.t.
efficiency with .
less time and .
fewer queries.
meanwhile cdd performs comparably to probdd w.r.t.
final size execution time and query number with a p value of .
.
and .
respectively indicating insignificance between these two algorithms.
cdd is expected to perform on par with probdd since it is designed to provide further insight and simplify the intricate design of probdd rather than to surpass its capabilities.
furthermore its comparable performance to probdd further validates the nonnecessity of randomness and our assumption in lemma iii.
.
bottleneck and minimality.
revisiting the bottlenecks presented in fig.
cdd possesses a query number and success rate close to those of probdd indicating that cdd also overcomes the bottlenecks of ddmin .
additionally similar to probdd minimality is absent in cdd although iterations help mitigate this issue.finding cdd always achieves comparable performance to probdd which further supports our previous findings including the theoretical simplifications regarding size and probability analysis of randomness bottlenecks and 1minimality.
vi.
l imitations and threats to validity in this section we discuss the limitations of cdd and potential factors that may impair the validity of our experimental results.
a. limitations as discussed in iv g compared to ddmin neither probdd nor cdd guarantees minimality.
this limitation arises because after successfully removing a subset ddmin restarts the process from the first subset whereas probdd and cdd continue from the next subset skipping all the previously tried subsets.
therefore although probdd and cdd complete the reduction process more quickly they may miss certain reduction opportunities and produce larger results than ddmin .
however reduction and debloating tools generally invoke these reduction algorithms in iterative loops until a fix point is reached gradually refining the results and mitigating limitations as mentioned in iv g. table iv and table v further support this point by showing that with multiple iterations probdd and cdd achieve significantly higher efficiency compared to ddmin while still producing results that are comparable to ddmin w.r.t.
effectiveness.
b. threats to validity for internal validity the main threat comes from the potential impact from the assumption as discussed in iii a1.
specifically without assuming that the number of elements in lis always divisible by the current subset size we could not further refine the mathematical model of probdd to achieve a simpler representation.
however such assumption might impact the actual performance potentially negating the benefits of our simplification.
to this end we conduct extensive empirical experiments demonstrating that cdd the simplified algorithm derived from this assumption exhibits no significant difference from probdd.
for external validity the threat lies in the generalizability of our findings across application scenarios.
to mitigate this threat we perform experiments on 76benchmarks including c programs triggering real world compiler bugs xml inputs crashing xml processing tools and benchmarks from software debloating tasks.
these benchmarks have covered various use scenarios of minimization algorithms.
vii.
r elated work in this section we discuss related work of test input minimization around three aspects effectiveness efficiency and the utilization of domain knowledge.
effectiveness.
test input minimization is an np complete problem in which achieving the global minimum is usuallyinfeasible.
therefore existing approaches to improving effectiveness mainly aim to escape local minima by performing more exhaustive searches.
since enumerating all possible subsets is infeasible vulcan and c reduce enumerate all combinations of elements within a small sliding window and exhaustively attempt to delete each combination resulting in smaller final program sizes.
in contrast probdd and cdd do not exhibit clear actions targeted at breaking through local optima suggesting they cannot achieve better effectiveness than ddmin as aligned with our evaluation in iv.
efficiency.
if parallelism is not considered the core of boosting efficiency is the enhanced capability to avoid relatively inefficient queries.
for example hodovan and kiss proposed disregarding attempts to remove the complement of subsets the success rate of which is unacceptably low in some scenarios.
besides gharachorlu and sumner proposed one pass delta debugging opdd which continues with the subset next to the deleted one rather than starting over from the first subset.
this optimization also avoids some redundant queries in ddmin reducing runtime by .
as revealed by our analysis these two above mentioned optimizations are implicitly incorporated within probdd and cdd and thereby contributing to their higher efficiency than ddmin .
utilization of domain knowledge.
there is an inherent trade off between effectiveness and efficiency in test input minimization.
for the same algorithm achieving a better result i.e.
a smaller local optimum requires more queries to be spent on trial and error.
however employing domain knowledge can still improve the overall performance.
for instance j reduce is both more effective and efficient than hdd in reducing java programs as it escapes more local optima by program transformations while simultaneously avoiding more inefficient queries via semantic constraints leveraging the semantics of java.
our analysis on probdd indicates that the probabilities primarily function as counters and do not utilize or effectively learn the domain knowledge of an input.
besides the evaluation on cdd a simplified algorithm without utilizing probability demonstrates that prioritizing elements via such probabilities does not yield significant benefits thus validating our analysis.
viii.
c onclusion this paper conducts the first in depth analysis of probdd which is the state of the art variant of ddmin to further comprehend and demystify its superior performance.
with theoretical analysis of the probabilistic model in probdd we reveal that probabilities essentially serve as monotonically increasing counters and propose cdd for simplification.
evaluations on 76benchmarks from test input minimization and software debloating confirm that cdd performs on par with probdd substantiating our theoretical analysis.
furthermore our examination on query success rate and randomness uncovers that probdd s superiority stems from skipping inefficient queries.
finally we discuss trade offs in ddmin and probdd providing insights for future research and applications of test input minimization algorithms.