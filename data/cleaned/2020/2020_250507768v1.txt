arxiv .07768v1 may 2025enhancing code generation via bidirectional comment level mutual grounding yifeng di tianyi zhang purdue university west lafayette in usa di5 purdue.edu tianyi purdue.edu abstract large language models llms have demonstrated unprecedented capability in code generation.
however llmgenerated code is still plagued with a wide range of functional errors especially for complex programming tasks that llms have not seen before.
recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by llms diminishing their productivity and trust in llmbased code generation.
inspired by the mutual grounding theory in communication we propose an interactive approach that leverages code comments as a medium for developers and llms to establish a shared understanding.
our approach facilitates iterative grounding by interleaving code generation inline comment generation and contextualized user feedback through editable comments to align generated code with developer intent.
we evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state of the art llms e.g.
.
pass improvement for codedavinci on humaneval.
furthermore we conducted a user study with participants in comparison to two baselines interacting with github copilot and interacting with a multi step code generation paradigm called multi turn program synthesis.
participants completed the given programming tasks .
faster and with .
improvement in task success rate when using our approach.
both results show that interactively refining code comments enables the collaborative establishment of mutual grounding leading to more accurate code generation and higher developer confidence.
index terms llm code generation code refinement i. i ntroduction the quest for automated code generation dating back to the 1960s has evolved significantly.
this field has transitioned from early deductive program synthesis methods to the recent advent of large language models llms .
despite the significant progress llms often fail to align with developer intent due to factors such as reliance on spurious features lack of user context and misunderstanding of complex specifications .
recent efforts to address these limitations include model fine tuning new prompting strategies and iterative refinement paradigms .
however the improvement brought by these methods is still limited.
for example self debug only achieved a .
pass 1increase for codex on mbpp when test execution feedback is not available.
recent studies highlight the critical role of bi directional communication between developers and llms in programming tasks.
while developers can edit prompts llms often treat such edits as new prompts hindering their ability to understand and incorporate developers refinement pingcode refinementcomment generationhuman review code snippetcode solution fig.
.
generating a code solution in our pipeline.
intent.
conversational models like chatgpt offer multi turn dialogues for feedback.
however effectively encoding historical utterances and contextualizing feedback within conversations remains a complex and challenging task .
in this work we propose a new interactive approach called programming with interactive grounding pi ng .
figure illustrates this approach.
pi ng employs inline comments as a medium for bi directional communication between a developer and the model.
this approach is inspired by the grounding theory in communication which underscores the importance of mutual understanding in collaborative interactions.
unlike prior work that relies on coarse grained code explanations for refinement pi ng s use of inline comments offers a more fine grained approach.
these comments directly address individual statements of the code making it easier to target specific code segments for precise feedback and refinement.
given a code snippet generated by an llm pi ng uses a code comment generation model to create inline comments that clarify each statement s behavior.
these comments provide developers with an immediate understandable code description helping them quickly spot potential errors.
developers can edit the comments to specify the correct behavior for the erroneous statement.
a code refinement model then regenerates the statement and subsequent segment identified by the feedback rather than the entire code snippet.
inline comments are essentially natural language descriptions which are suitable for developers at all levels.
this approach allows for more precise error identification compared to prompt editing or conversational models which often require the model to infer the location of errors throughout the entire code snippet.
comment editing also directly indicateserror locations leading to more efficient model based code refinement.
this simplifies and improves the process of code refinement compared to previous approaches that require regenerating the entire code snippet.
we evaluate pi ng through simulated and real user studies on multiple code generation models.
in our simulated user studies we benchmark pi ng against eight state of the art code generation and refinement techniques demonstrating that pi ng significantly outperforms these baselines.
on humaneval pass 1rates increased by .
for code davinci .
for incoder and .
for codegen.
mbpp benchmark also saw notable gains.
in a user study with real programmers pi ng outperformed github copilot and multi turn program synthesis in task success rate by .
and .
respectively while improving the task completion speed by .
and .
respectively.
besides participants reported increased confidence and satisfaction demonstrating pi ng s effectiveness in enhancing code quality and user experience.
in summary our main contributions include we propose pi ng a new interactive code generation paradigm that enhances developer model grounding via comment refinement.
we implement pi ng as a vscode extension and have open sourced our code and data.
we comprehensively evaluate pi ng with various models benchmarks and user studies showing significant improvements in code accuracy and developer productivity over state of the art methods.
ii.
b ackground effective human interaction requires establishing common ground mutual knowledge beliefs and assumptions .
grounding in communication involves a collaborative process where speakers design utterances for listener comprehension and listeners provide feedback to demonstrate understanding .
clark and schaefer s contribution model outlines this as a presentation by the speaker and acceptance by the listener supported by mechanisms including repetition reformulation and elaboration .
the importance of common ground is underscored when communication falters due to participants differing backgrounds .
recent studies in code generation reveal challenges due to poor bi directional communication between models and developers .
models often misinterpret developer intent while developers struggle to understand the generated code .
this communication gap stemming from the lack of common ground leads to code that does not align with developers expectations and hinders effective feedback .
thus fostering shared understanding between models and developers is a key to enhancing code generation accuracy.
in nlp there have been some recent investigations on applying grounding theory to text generation tasks .
zhang et al.
proposed a retrieval augmented generation that retrieves related examples to ground the generative context in the relevant information.
chandu et al.
analyzed coordination and constraints in nlp tasks proposing adaptations for grounding.
however grounding in code generation is still underexplored.
our work aims to fill this gap with a grounding based pipeline for code generation and refinement enhancing developer model communication.
iii.
a pproach we formally define the task as follows given an initial contextcthat comprises a natural language problem description and an initial code snippet xproduced by a code generation model the task is to generate a refined code solution xthat fixes the errors in xand aligns the code more closely with the developer s intent as articulated in the problem description.
we introduce programming with interactive grounding ping which leverages inline code comments for bidirectional communication.
pi ng includes three steps comment generation human review and code refinement.
we elaborate on each step in the following sections.
a. comment generation ping first parsed the initial code snippet xinto an abstract syntax tree ast to segment the code snippet into individual statements.
we opted for statement level comment generation due to its fine grained nature which allows for a more accurate and detailed explanation of code behavior.
this approach contrasts the previous approach that leverages coarser grained code explanations or summaries by providing targeted insights into specific statements of code.
we do not consider import and definition statements since they are trivial to explain.
for compound statements such as if for while try and with we adopt a tailored approach for code segmentation.
for such nodes with a conditional statement the condition expression is first added to the statement list followed by its body statement nodes.
the statements in the else clause if present are handled afterward.
for try nodes the process directly navigates through its body nodes without additional processing.
for with statements the part of the statement excluding its suite is first added after which is its suite part.
if the generated code snippet xis not compilable which prevents its transformation to ast we resort to a simple strategy.
in such cases xis directly segmented into individual lines for subsequent comment generation.
given individual statements a specialized comment generation model mcgenerates an inline comment cfor each of them.
figure 1shows a code example with generated comments.
when processing the for loop pi ng first generates an explanation for the loop condition.
then it proceeds to clarify the operations within the loop body.
to implement the comment generation model we use codebert and fine tune it with our finetuning dataset.
codebert is a transformer based encoder only model.
it is pre trained on a diverse corpus of programming languages and natural language text.
to develop the fine tuning dataset we initialize the variable sum of digits to 0sum of digits convert nto a stringdigits str n iterate through each digit in digits for digit in digits update sum of digits by addingthe integer value of digit sum of digits int digit convert the sum of digits to a binary removethefirsttwocharactersandreturnreturn bin sum of digits .code snippet with comment x convert ntoa string convert ntoa binary andremoveitsprefix 0b digits str n .refine the comment of the incorrect line of codedef solve n given a positive integer n return the total sum of its binarydigits in binary.
example for n the sum of digits will be the output should be .
for n the sum of digits will be the output should be .
for n the sum of digits will be the output should be .
variables n integer constraints n .
output a string of binary number context c sum of digits digits str n for digit in digits sum of digits int digit return bin sum of digits code snippetx initialize the variable sum of digits to 0sum of digits convert nto a binaryandremoveitsprefix 0b digits str n digits str bin n .removeprefix 0b iterate through each digit in binary n for digit in digits update sum of digits by addingthe integer value of digit sum of digits int digit convert the sum of digits to a binary removethefirsttwocharactersandreturnreturn bin sum of digits .regenerate basedonthe refined comment fig.
.
refining a code snippet with our approach via comment editing.
adopted the approach outlined in the codebert paper and applied the preprocessing steps from codexglue to the codesearchnet dataset.
the postprocessing process first cleans the data by removing examples where the code cannot be parsed into an ast removing examples where the number of tokens in the document is fewer than or greater than removing examples that contain special tokens e.g.
img ... or and removing examples that are not written in english.
then it extracts comments and their corresponding code to produce a fine tuning dataset with code to comment pairs.
we use adam to finetune the parameters.
the learning rate was set to 5e with a batch size of .
we also set the maximum sequence length of input and inference as and respectively.
b. human review in the human review step the code snippet with inline comments is presented to the developer.
when users notice an error by reading the comments they can directly modify the corresponding comment to describe the desired behavior.
for example in figure the developer notices the statement in line is wrong based on the comment of that statement.
specifically instead of converting the input nto a string it should convert it to a binary first.
fixing this bug requires changing this line of code to call three apis converting the number to binary via bin transforming this binary to a string via str and finally removing the string s prefix via str.removeprefix .
the developer may not be familiar with the first and third apis as they are less commonly used.
to manually fix it the developer would typically need to search online and learn how to use these specific apis first which causes extra time and effort.
using pi ng the developer can modify the comment instead as shown in figure .
then pi ng will regenerate the code based on the refined comment which is more convenient for the developer.c.
code refinement the original context c the generated code up to the refined comment x and the refined comments are concatenated to form a new updated context c. a code refinement model m then processes cto regenerate code starting from the edited comment.
this regeneration aims to address the identified error while maintaining the integrity of the correct parts.
to develop the code refinement model we fine tune the .7b version of deepseek coder .
deepseek coder is a recently released code generation model with superior coding performance while being small enough to be fine tuned on our gpus.
instead of using the original code generation model we decided to fine tune it since the original one is designed to generate code based on general high level task descriptions.
however in our code refinement step the task is to generate a dedicated program statement based on a specific detailed inline comment.
therefore fine tuning is necessary.
to construct a high quality dataset for code refinement we utilize the stack dataset which includes .
gb of python code files.
we first follow the filtering methods of codex and remove files with an average line length greater than characters a maximum line length above characters less than of the characters being alphanumeric keywords in the first few lines of the file indicating that the file was likely automatically generated then we select code snippets that are sufficiently documented through comments.
for each code file we compute the comments to code ratio by counting the lines of comments and the lines of code.
finally we set minimum and maximum thresholds for this ratio selecting snippets within this range.
after testing various thresholds we established a minratio of .
and a maxratio of .
that optimally balances the number of code snippets against the richness of their comments.
we fine tune the model with the next token prediction objective which is widely used in decoder only models such as gpt .
we use the adam optimizer with a learning ratefig.
.
user interface of pi ng of2e 5for updating the model weights.
we use the crossentropy loss to optimize the model parameters.
this loss compares the model s predicted probability distribution for the next token in a code or comment sequence against the groundtruth token.
mathematically it is defined as l nx i 1yilog yi here ndenotes the dictionary size yiis the ground truth token and yiis the model s predicted token.
we compared the performance of our fine tuned model with the original model.
please refer to section iv for details.
d. implementation details we have implemented pi ng as an extension for visual studio code vscode .
figure shows the user interface of this extension.
the development of the pi ng extension for vscode involved writing lines of code.
the default models used in pi ng are deepseek coder for code generation codebert for comment generation and the fine tuned deepseek coder model for code refinement.
iv.
s imulated userstudy in the evaluation we strive to investigate to what extent ping improves the accuracy of different kinds of code generation models in a broad range of programming tasks under different settings.
since pi ng requires user feedback to guide the code refinement process conducting such experiments at scale requires recruiting a large number of developers which is costly and time prohibitive to achieve in academia.
therefore as a tradeoff the first author acted as a simulated user and interacted with pi ng in the experiments in this section.
in other words the first author manually inspectedthe code generated by pi ng in each setting and edited the comments to provide feedback for code refinement detailed in section iv b .
with the simulated user we investigate the following research questions rq1 how effectively can our interactive approach improve the code generation accuracy of different llms?
rq2 how does our approach compare with other code generation approaches?
rq3 how sensitive is our approach to different code comment generation models?
rq4 to what extent can fine tuning improve the code refinement accuracy?
in the end we collected a very large interaction dataset including code snippets generated by four different llms for programming tasks their inline comments generated by different comment generation models comments edited by the simulated user and code snippets generated by different code refinement models based on the edited comments.
we have released this dataset to support the reproducibility of our experiments and also facilitate the development of new code refinement models for the research community.
a. benchmarks we used two popular code generation benchmarks in this evaluation.
humaneval includes hand written python programming tasks.
mbpp includes crowd sourced python programming tasks.
since some tasks of mbpp have ambiguous task descriptions the authors created a sanitized version with tasks.
for our evaluation we randomly sampled tasks from this sanitized version about one third as analyzing all tasks manually takes a lot of effort.
a recent study found that the original test cases from humaneval do not sufficiently cover corner cases and thus cannot reliably assess the correctness of llm generated code .
the authors later released humaneval andmbpp which extend humaneval and mbpp with additional test cases.
in our evaluation we also measured model performance on humaneval and mbpp .
b. user feedback collection for the total of programming tasks from humaneval and mbpp we experimented with four llms to generate the initial code snippets.
we selected these models since they are well known code generation models with different performance levels.
we explain each of them below.
codegen leverages multi task learning on textual and programming language tasks codegen can effectively generate syntactically valid code while capturing natural language meaning.
for our evaluation we used its 16b version.
incoder utilizes a retrieve and edit approach in code generation.
given a text description it first retrieves relevant code snippets and then edits them to match the input.
its hybrid pointer generator network allows both copying and generating tokens.
for our evaluation we used its 6b version.
code davinci is openai s gpt based code generation model.
it generalizes well across programming languageswithout task specific training.
as a close sourced model its model size is undisclosed.
we used the api provided by openai to access this model and test it in our experiments.
deepseek coder is a new series of code language models that show superior coding performance.
these models are pre trained with a large project level code corpus with both the next token prediction and the fill in the middle objectives.
for our evaluation we used its .7b version.
for each task we analyzed the top one program generated by each model.
this resulted in code snippets.
of them fail to pass the test cases.
table i shows the distribution of these incorrect solutions across these models.
two annotators manually inspected these incorrect code solutions and edited the comments to refine the code.
one of the annotators is the first author a graduate student with over nine years of programming experience including three years of industry experience.
the other annotator who is not the coauthor of this paper is a graduate student invited externally with five years of programming experience.
we describe the detailed annotation procedure below.
the first step is to identify the buggy statements in each code snippet by examining the comments.
the two annotators first had a minute session to get familiar with the annotation task and go over code snippets together to practice.
subsequently they split the remaining code snippets into halves and each independently inspected code snippets.
during the inspection and fault localization process they discussed in case of uncertainty.
after finishing their first round of inspection they exchanged to check each other s results.
the initial agreement level between the two annotators in this step is .
in terms of cohen s kappa indicating a perfect agreement level .
this agreement level makes sense since in most cases it is obvious which line of code is incorrect.
the annotators then discussed and resolved all disagreements.
the second step is to edit the comment to reflect the expected behavior of a buggy statement for pi ng to refine the code.
like the previous step they first practiced together on snippets and then independently edited comments in half of the remaining snippets.
upon completion they inspected the edited comments with each other and checked whether they agreed on the edited comments.
the initial cohen s kappa is .
indicating a substantial agreement level .
compared with the previous step the agreement level is a bit lower since in some cases the annotators disagreed on the clarity and specificity of edited comments.
for example one annotator found another annotator s comment edit ambiguous too trivial or even confusing.
the annotators discussed each comment edit they initially disagreed on and came up with a final edit that both of them found appropriate.
they then ran pi ng to refine the code with the edited comment for each code snippet.
if the refined code still failed the test cases the two annotators repeated the previous steps to provide feedback to the refined code.
due to the enormous manual effort in this process the annotators only provided up to three rounds of feedback to the incorrect solutions from the humaneval dataset.table i comparison of code generation models evaluated model sizepass incorrect solution humaneval mbpp incoder 6b .
.
codegen mono 16b .
.
code davinci undisclosed .
.
deepseek coder .7b .
.
after this annotation process we logged all the generated code in each iteration and code comments before and after editing to form the interaction dataset described at the beginning of this section.
we reported the performance gains of each round of feedback in table iv.
c. evaluation metrics following the codex paper we measured each model s performance using pass k. we set kto and only considered the top program generated by each model in our experiments.
d. comparison baselines since rq1 only aims to measure the performance gain achieved on different llms we compared the pass 1of each model with and without the augmentation of pi ng.
we also compared the pass 1of each model after each feedback iteration and reported the improvement trend over iterations.
rq2 compares pi ng with other code generation and refinement techniques.
thus we selected eight state of the art methods as the baselines.
to ensure comparison consistency we use gpt .
with the default temperature .
and top p .
as the base model for pi ng and all baselines in our experiments.
for iterative methods such as self edit and self debugging we set the iteration upper bound to three since the performance of these methods often saturates after two or three iterations based on the experiments in their papers.
we describe each method below.
react .
react prompts an llm to generate both a reasoning trace and an action plan in an interleaved manner.
this allows the model to perform dynamic reasoning to adjust the action plan.
tot .
tot structures potential reasoning paths in a tree like manner which goes beyond the linear reasoning of traditional chain of thought cot prompting to enable the exploration of multiple reasoning pathways.
rap .
rap enhances llm s problem solving by using them as both reasoning agents and world models to generate actionable plans and conduct complex reasoning which predict environmental states and simulate action outcomes for more effective problem solving.
self edit .
self edit improves code generation accuracy by executing the generated code analyzing results and guiding the fault aware code editor to correct errors in a generate and edit cycle.
self planning .
self planning employs a two phase planning and implementation approach in llms totable ii pass on the human eval and human eval datasets humaneval humaneval original pi ng pi ng w o finetuning original pi ng pi ng w o finetuning incoder 6b .
.
.
.
.
.
.
.
.
.
codegen mono 16b .
.
.
.
.
.
.
.
.
.
code davinci .
.
.
.
.
.
.
.
.
.
deepseek coder .7b .
.
.
.
.
.
.
.
.
.
table iii pass on the mbpp and mbpp datasets mbpp mbpp original pi ng pi ng w o finetuning original pi ng pi ng w o finetuning incoder 6b .
.
.
.
.
.
.
.
.
.
codegen mono 16b .
.
.
.
.
.
.
.
.
.
code davinci .
.
.
.
.
.
.
.
.
.
deepseek coder .7b .
.
.
.
.
.
.
.
.
.
table iv pass 1rates across multiple iterations on human eval humaneval original iteration iterations iterations incoder 6b .
.
.
.
.
.
.
codegen mono 16b .
.
.
.
.
.
.
code davinci .
.
.
.
.
.
.
deepseek coder .7b .
.
.
.
.
.
.
enhance llms understanding and handling of complex code generation tasks.
self debugging .
self debugging empowers llms to perform rubber duck debugging on their own generated code via few shot demonstrations natural language code explanation and execution analysis.
scot .
scot improves traditional chain of thought cot prompting by incorporating the program structures to obtain structured cots which leads to more organized and efficient code generation.
codechain .
codechain prompts llms to generate modular code and refine it via self revisions.
it boosts accuracy by extracting clustering and reusing code submodules emulating expert programming practices.
for rq3 we evaluated how pi ng performs with a different comment generation model seq2seq .
prior research has validated seq2seq s effectiveness in generating inline comments .
we compared the pass 1rates when using the proposed codebert model and the seq2seq model.
v. e xperiment results a. rq1 effectiveness on different code generation models table ii shows the pass 1results after a single refinement iteration of pi ng across different llms on both humaneval and its more rigorous version humaneval .
similarly table iii shows the pass 1results after a single refinement iteration of pi ng on mbpp and its more rigorous version mbpp .results shown in table ii reveal significant improvements in code generation accuracy through just one iteration of refinement.
for instance the code davinci model shows a remarkable improvement on the humaneval dataset with the pass 1rate improved from .
to .
.
this enhancement highlights the significant influence of focused iterative feedback in correcting model misconceptions.
similar patterns of improvement were discernible across other models including incoder and codegen.
notably incoder s performance on the humaneval dataset ascended from .
to .
while codegen s accuracy experienced a boost from .
to .
on the mbpp benchmark.
these improvements again confirm the effectiveness of incorporating human insights into the code generation process.
the overarching findings from our investigation affirm the critical role of human feedback in guiding llms toward synthesizing more functionally accurate programs.
the consistent improvements across different models further validate the hypothesis that collaborative interaction is key to effectively addressing the challenges of code generation.
this collaborative paradigm facilitates a model s ability to dynamically adjust its outputs based on constructive feedback thereby incrementally moving closer to generating error free functional code.
we also conducted experiments to explore how extending the interactive refinement process beyond a single iteration impacts the improvements in code generation.
table iv demonstrates a consistent pattern of improvement across all evaluated models with each additional iteration yielding positive gains albeit at a diminishing rate.
for instance the incoder model shows a continuous gain from a baseline of .
to .
after three iterations marking a total improvement of .
.
similarly the codegen model s performance ascends progressively culminating in a .
pass 1rate while the code davinci model reaches a peak of .
.
in summary our analysis of both single and multiple iterations of interactive refinement reveals a clear trajectory of improvement in code generation accuracy.
the gains weretable v comparison of pingwith other code generation approaches on human eval and mbpp benchmarks humaneval mbpp pass time sec pass time sec react .
.
.
.
tot .
.
.
.
rap .
.
.
.
self edit .
.
.
.
self planning .
.
.
.
self debugging .
.
.
.
scot .
.
.
.
codechain .
.
.
.
ping .
.
.
.
consistent across diverse model architectures showing collaboration is vital for complex code generation.
the findings highlight the efficacy of human feedback in guiding llms toward higher accuracy levels affirming the value of interactive approaches in code generation.
finding ping significantly improves the code generation capability of four different llms on multiple benchmarks demonstrating the effectiveness of commentlevel code refinement.
b. rq2 comparison to other prompting approaches table v shows the pass 1results of different code generation and refinement methods on the humaneval benchmark.
note that the results on pi ng in this table are only based on one iteration of human feedback.
the results highlight the performance of our approach pi ng compared to other leading llm based approaches in code generation.
table v demonstrates that pi ng not only performs competitively but also surpasses other approaches on both humaneval and mbpp benchmarks.
it proves to be a highly effective approach for addressing the complexities of code generation and refinement with pass 1of .
and .
respectively.
on the humaneval benchmark pi ng s leading score of .
marks a notable advancement in code generation exceeding self planning the closest competitor.
this performance illustrates pi ng s effectiveness in producing correct code from natural language task descriptions.
similarly on the mbpp benchmark pi ng outperforms rap demonstrating superior refinement capabilities by integrating human feedback.
this success is largely due to pi ng s innovative use of inline comments to align generated code with user intent for more precise code refinement.
finding ping outperformed state of the art code generation and refinement methods demonstrating its effectiveness in leveraging human feedback for more accurate and contextualized code refinement.
c. rq3 sensitivity to code comment generation models table vi compares the pass 1rates of pi ng on different llms when codebert vs. seq2seq as the code comment generation model.
the results reveal an advantage in favor oftable vi pass 1with different comment generation models on the human eval dataset original seq2seq codebert incoder 6b .
.
.
.
.
codegen mono 16b .
.
.
.
.
code davinci .
.
.
.
.
using codebert over seq2seq across all llms.
however the impact of using different code comment generation models is not significant for all three llms around to differences in pass .
this implies that while using a better comment generation model can contribute to the refinement process the advantage of pi ng is more pronounced by leveraging human feedback to refine code comments.
even with the suboptimal seq2seq model the pass 1improvements over the original llms are around to .
in addition to measuring the impact of the code comment model on pass we further measured the accuracy of the codebert model.
due to the lack of ground truth comments the same two annotators from section iv b sampled codecomment pairs from the simulation dataset and categorized the comments into three accuracy levels fully accurate largely accurate but with missing information and contain wrong information .
this sample size is statistically significant with a confidence level and a margin of error of .
the annotators followed a similar data annotation process as in section iv b to first have a hour session to go over code comment pairs together.
then they independently annotated half of the remaining pairs and exchanged their annotations for validation.
the initial agreement between the two annotators is .
in terms of cohen s kappa indicating a substantial agreement level .
then they discussed the disagreements to achieve a consensus.
among code comment pairs the majority of comments generated by pi ng are fully correct.
of the comments are largely correct but with missing information while contain wrong information.
given that pi ng only regenerates the code for user revised comments missing or wrong information in other unchanged comments has little impact on the regenerated code.
yet we acknowledge that they may confuse or distract developers since developers may spend extra time scrutinizing the code and comments to figure out whether the missing information is because of incorrect code or simply a comment generation error.
finding ping s performance is not significantly impacted by the choice of comment generation model illustrating its robustness and the primary value of iterative human feedback in enhancing code generation accuracy.
d. rq4 the impact of finetuning rq4 investigates how fine tuning the code refinement model affects the code generation accuracy of pi ng.
column ping and column ping w o finetuning in table ii and iii comparethe results of pi ng with a fine tuned code refinement model against its performance without fine tuning.
both the incoder and codegen models show notable improvements in pass 1rates on both datasets with fine tuning.
incoder s performance increased by .
on humaneval and .
on mbpp while codegen saw a .
gain on humaneval.
these results highlight the effectiveness of finetuning in enhancing code refinement accuracy and understanding of user intent.
finding fine tuning code generation models enhances code refinement accuracy with our pi ng approach yielding marked improvements over baselines and fine tuning offering additional performance boosts.
vi.
u serstudy to evaluate the real world utility and usability of our grounding based approach we utilize our integrated extension for visual studio code to conduct a within subjects user study with participants including both college students and professional programmers.
this study aims to investigate the following research question rq5 how useful is our interactive approach to real programmers in practice?
a. participants we recruited participants males females from a diverse background comprising graduate students and professional developers.
they were recruited by emailing student mailing lists at an r1 university and reaching out to experienced software developers through our personal network.
had years of python experience early intermediate had years of experience late intermediate and participants had over years of python experience expert .
this choice allows us to assess how well our grounding based approach performs across a spectrum of python expertise.
b. task our task selection was inspired by the programming tasks used by xu et al.
and vaithilingam et al.
.
we first categorized the tranx developer study tasks and tasks from the ds benchmark into different common programming task types.
we then employed stratified random sampling from the two datasets.
this process resulted in a pool of code generation tasks with different types including file i o os web scraping web server client data analysis and data visualization.
the ground truth code for those tasks ranges from loc to loc.
c. comparison baselines in addition to pi ng we used github copilot and multi turn program synthesis multi turn for short from henceforth as comparison baselines.
github copilot powered by codex is an ai assistant that suggests code completions but lacks explicit repair feedback loops.
multiturn is a paradigm that decomposes code generation into steps where the model generates subprograms in response to natural language instructions.
here users can only accept the generated code and provide further instructions for subsequent steps.
the individually generated subprograms are concatenated into a single complete program as the final generated program.
github copilot offers continuous code generation without direct feedback mechanisms while multi turn enables task decomposition and multi turn interaction without any chances for refinement.
we used the vscode extension of github copilot for our studies.
for multi turn since it originally lacked a vscode extension we developed one with a similar user interface to pi ng s to ensure a fair comparison.
d. procedure our study employed a within subjects design to directly compare pi ng with two comparison baselines.
the whole study lasted about minutes.
participants first received an overview of the procedure completed a consent form and filled out a pre study questionnaire that collected their background information.
then each study was divided into three sessions with each focusing on a specific code generation tool.
during these sessions participants were asked to use the designated tool to tackle the assigned programming task within minutes.
to minimize learning effects the assignment orders of both tasks and tools are counterbalanced across participants to ensure that each task tool pair has the same number of trials.
in each study a participant was required to use all three different tools on three different tasks with the ordering of the three task tool pairs randomized.
each session began with a tutorial video about the particular tool in focus.
a subsequent minute practice period allowed participants to get familiar with the tool before delving into the actual tasks.
for each task participants were asked to study its task description and use the tool to generate the code first.
participants could review the code and then use the tool s interactive features to refine it until they deemed the code to be correct.
participants are allowed to adopt any approaches to verify the correctness of their current programs including writing unit tests.
if participants found a task too challenging they could choose to skip it.
after each session participants filled out a post task questionnaire to assess confidence in their final code perceived success rates and five different cognitive load questions from nasa tlx .
after wrapping up all sessions participants completed a final survey to compare their experiences of using these three tools.
we record all the screen activities for playback to aid in discussing observed behaviors.
all the systems logged interaction events including code editing and comment editing to analyze participants behavior patterns.
e. user performance results as shown in figure of participants successfully solved the assigned programming task using pi ng within the given time.
in comparison participants solved the given task with github copilot and with multi turn.
compared toping copilot multi turnparticipant count10 ping copilot multi turnaverage of iterations4.
.
.7fig.
.
the number of participants whose final code was functionally correct and the average number of iterations these participants took.
table vii the average task completion time inminutes task type ping github copilot multi turn file i o .
.
.
os .
.
.
web scraping .
.
web server client .
data analysis .
.
data visualization .
.
average .
.
github copilot and multi turn pi ng improved the success rate by .
and .
respectively.
it showcases the effectiveness of solving programming tasks by using inline comments to guide code generation.
table vii compares task completion times across different tools.
on average participants finished their tasks .
and .
faster with pi ng when compared to github copilot and multi turn respectively.
we further analyzed the average number of iterations participants tried to complete the given programming task with the assigned tool.
figure shows these results.
on average participants using pi ng required fewer iterations .
compared to github copilot .
and multi turn .
to complete a programming task.
this highlights how leveraging comments as a communication vehicle between users and llms improves the efficiency of identifying and resolving errors in llm generated code.
although our study did not prohibit participants from writing unit tests we observed that most participants chose not to write tests during the task.
instead they relied on the inline comments generated by pi ng to inspect and understand code functionality quickly and effectively.
at the end of each task some participants opted to write one or two unit tests to double check the correctness of their final code.
this reflects a practical balance between immediate comment driven insights and traditional unit testing for comprehensive validation.
f .
user confidence and cognitive overhead result in the post study survey participants self reported their confidence in solving the given programming task on a point scale very low confidence very high confidence .
notably of participants agreed or strongly agreed that ping helped generate code that aligned with their intent compared to participants for github copilot and just for multi turn.
furthermore pi ng significantly enhances strongly disagree neutral strongly agree participant count1 median .
median .
median .0i felt confident about the code generated using the assigned tool.
ping copilot multi turn fig.
.
the distribution of participants confidence in the final code generated using the assigned tools.
mental demand temporal demand performance effort frustration1234567ping copilot multi turn fig.
.
the distribution of participants cognitive load when solving the given programming tasks with the assigned tools.
developers confidence in auto generated patches.
as shown in figure participants agreed or strongly agreed that they felt confident about the generated code when using pi ng.
in contrast only and participant felt so when using github copilot and multi turn respectively.
this can be largely attributed to pi ng s interactive refinement process which empowers users to iteratively refine the generated code through inline comments offering a more transparent and controlled development experience.
this stark contrast in user confidence levels underscores the value of pi ng s approach emphasizing the importance of a feedback loop in fostering trust and satisfaction with ai generated code.
as shown in figure solving programming tasks with ping generally leads to a better user experience across all five cognitive load metrics compared to using multi turn.
three of the five metrics show statistically significant differences between using pi ng and multi turn based on the wilcoxon signed rank test performance p .
effort p .
and frustration p .
.
this shows the benefits of using comments as the communication vehicle between users and llms.
compared to copilot pi ng slightly reduces the programming workload in terms of mental demand temporal demand performance and frustration .
however these differences are not statistically significant.
additionally the user study shows our interactive approach assists real world programming in multiple ways.
for simple tasks of the participants directly used the code with minor edits.
for complex tasks while extensive changes were required over agreed the generated code gave usefultable viii the success rates across three difficulty levels ping github copilot multi turn easy medium hard structural scaffolds to build on.
of them reported that the inline comments enabled quickly identifying and fixing erroneous portions rather than debugging from scratch.
on average from iterations participants felt the code matched specifications sufficiently to serve as a quality starting point.
g. impact of task difficulty level we categorized the programming tasks into three levels easy medium and hard.
each level contains two tasks which are classified based on the lines of code loc in the ground truth code for each task.
tasks file i o loc and data analysis loc are categorized as easy tasks.
tasks os loc and web scraping loc are categorized as medium.
tasks web server client loc and data visualization loc are categorized as hard.
table viii shows the success rates for each difficulty level of programming tasks.
we observe that participants using pi ng outperform those using the other two tools in medium and hard tasks.
in easy tasks pi ng and github copilot demonstrate equivalent success rates both outperforming multi turn.
table ix shows the average task completion time for each difficulty level.
pi ng consistently saves time compared with github copilot and multi turn across all difficulty levels.
h. impact of user s programming expertise the impact of programming expertise on solving programming tasks with code generation tools is noteworthy.
table x shows the success rates of participants with different levels of programming expertise and table xi shows how participants years of python experience influenced their task completion time.
participants using pi ng perform better than those using the other two tools in hard tasks in terms of both success rates and average completion time.
for easy and medium tasks ping and github copilot show similar success rates both significantly outperforming multi turn.
pi ng also saves much time over the other two tools in medium tasks.
surprisingly we observed that programmers with years of python experience completed tasks faster using github copilot than with pi ng.
this result suggests two potential reasons.
one is that our tool might not be as beneficial for novices who may struggle with reading code comments and understanding complex logic in natural language.
the other one can be the small sample size of only two participants with years of python experience which could make this result a coincidence.
overall compared to programmers with less programming experience those with longer programming experience solved the tasks faster across the three code generation tools which is consistent with our intuition.table ix the average task completion time for each difficulty level ping github copilot multi turn easy .
.
.
medium .
.
.
hard .
.
table x the success rates of participants with different levels of programming expertise ping github copilot multi turn years years over years vii.
l imitation and future work the results of our experiments demonstrate the potential for grounding based interaction to significantly improve the accuracy and reliability of neural code generation models.
our approach which leverages user feedback through inline comment editing fosters a collaborative process of iterative alignment between the model s output and the user s intent.
it underscores the synergy of human insights and model capabilities and thus makes code generation more reliable and adaptable through mutual grounding.
an interesting area for further analysis is studying the patterns in user edits that prove most effective for code refinement.
certain types of edits to comments such as specifying additional conditions correcting logical errors or clarifying algorithm steps may be particularly influential in guiding the model towards more accurate code refinement.
identifying and categorizing these high leverage edits could inform techniques for eliciting more targeted feedback from users.
the comment generation and editing interface could also be enhanced to further optimize the grounding process.
for example highlighting model uncertainty and providing editing guidance could help users identify high impact refinements more easily.
optimizing the cycle time between edits and regeneration could also affect overall productivity.
when applying pi ng to complex codebases generating inline comments for each statement may make complex code look more overwhelming.
this could hinder code readability counter to clean code principles that emphasize simplicity and minimalism in annotations .
however compared to simple code complex code also benefits more from having detailed comments to facilitate program comprehension.
we propose a couple of solutions to mitigate the negative effect of generating statement level comments.
first we can allow users to hide all comments by default and only display the comments for the statements they do not understand.
second we can develop a more advanced method that groups closely related statements to a block and only generates one comment for that block to reduce the number of comments.table xi the average task completion time in minutes of participants with different levels of programming expertise ping github copilot multi turn years .
.
years .
.
.
over years .
.
.
in addition our user study did not differentiate between participants who were previously familiar with the assigned programming tasks and those encountering them for the first time.
given that task familiarity could influence cognitive load and problem solving strategies as noted by barke et al.
future work should incorporate this variable.
viii.
t hreats to validity a threat to internal validity is that our user feedback collected in the simulated user study was constructed by two experienced developers.
the patterns in refinements and resulting accuracy improvements on this synthetic dataset may not fully reflect real world usefulness.
our user study provides some mitigation by demonstrating productivity improvements with real users.
however the study was limited in scale and duration.
more rigorous in situ analysis is required to ascertain long term productivity over continued tool usage.
additionally in our user study we did not schedule breaks between tasks for participants.
executing tasks continuously without breaks may lead to fatigue among participants potentially affecting their performance and the internal validity of our findings.
future studies should consider incorporating adequate breaks or spreading tasks across multiple sessions.
in terms of external validity we only experimented with python code generation which may not generalize to other languages.
our set of models was also not exhaustive so the benefits of grounding based interaction for other model architectures are not fully characterized.
the approach may be more or less effective for very small or very large models.
ix.
r elated work in recent years large language models have demonstrated unprecedented performance on code generation tasks .
while these models share similar transformer architectures they vary in training objectives training datasets and model sizes.
for instance codex is pre trained on general text corpora like other gpt models and then fine tuned on million public software repositories hosted on github.
starcoder is pre trained on a multilingual code corpus called stack and then fine tuned on a python only code corpus.
furthermore compared with codex which only uses next token prediction as the training objective starcoder also utilizes an additional training objective called fill in the middle .
more recently magicoder demonstrates that using llms to generate code instructions based on open source code and then using them to fine tune llms can significantly improve their code generation capability.among the various approaches of improving llms for code generation the most related to us are the prompting methods for code generation and refinement .
for instance self debugging prompts llms to perform iterative debugging on the generated code based on test execution results and code explanations.
however unlike our approach they generated high level code explanations rather than inline code comments at the statement level.
our evaluation shows that inline comments can serve as a more effective method for grounding developer intent for code refinement even when unit tests are not available.
in addition to test cases and code explanations nijkamp et al.
proposed a multi step code generation paradigm where developers can express their intent step by step in a multi turn dialogue with an llm.
some approaches aim to automate this by using the same llm for task decomposition or planning .
in these prompting paradigms a developer or an agent needs to proactively decompose a task into smaller tasks to more precisely guide an llm for code generation.
by contrast our work focuses on bi directional communication where an agent explains the code to a developer a developer pinpoints which part of the code is wrong and the agent refines the erroneous part of the code accordingly.
overall our work differs from existing techniques by utilizing inline comments as a fine grained grounding mechanism for code refinement.
furthermore we have also developed specialized models for code comment generation and code refinement rather than relying on the latent capabilities of llms for these specialized tasks.
our evaluation shows that developing such specialized models via fine tuning is necessary given the distribution shift between the pre training dataset and the targeted tasks.
x. c onclusion in this work we introduced an interactive pipeline to facilitate the grounding of code generation models to user intent through inline comment editing which helps direct the code regeneration process to better match user expectations.
experiments with multiple code generation models on two popular benchmarks reveal notable improvements in code generation accuracy when performing grounding based code refinement.
moreover a user study shows productivity and usability benefits when compared to alternative code generation paradigms.
our approach enhances code generation to be more aligned with user intent making the process more manageable.
it highlights the synergy between human insights and models for improving code generation suggesting future enhancements in human ai collaboration research.
xi.
a cknowledgements we thank the anonymous reviewers for their insightful feedback as well as for the considerable time and effort they spent reviewing our work.
we also thank the participants of the user studies for their valuable contributions and comments.
this work was supported in part by nsf grants ite and ccf .