detecting tensorflow program bugs in real world industrial environment chen liu jie lu guangwei li ting y uan lian li feng tan jun yang liang y ou jingling xue skl computer architecture ict cas beijing china university of chinese academy of sciences china alibaba group university of new south wales new south wales australia liuchen17z lujie liguangwei yuanting lianli ict.ac.cn tanfeng.tf muzhuo.yj youliang.yl alibaba.com jingling cse.unsw.edu.au abstract deep learning has been widely adopted in industry and has achieved great success in a wide range of application areas.
bugs in deep learning programs can cause catastrophicfailures in addition to a serious waste of resources and time.
this paper aims at detecting industrial tensorflow program bugs.
we report an extensive empirical study on failedtensorflow jobs showing that existing static tools can effectivelydetect .
of the top three types of python bugs in industrialtensorflow programs.
in addition we propose for the first time a constraint based approach for detecting tensorflow shape related errors one of the most common tensorflow specificbugs together with an associated tool s hape tracer .
our evaluation on a set of industrial tensorflow programs showsthat s hape tracer is efficient and effective it analyzes each program in at most seconds and detects effectively out of60 industrial tensorflow program bugs with no false positives.
s hape tracer has been deployed in the platform x platform and will be released soon.
index t erms tensorflow bugs constraint solving i. i ntroduction deep learning has been widely adopted in industry.
assisted by open source frameworks developers can efficiently design new deep learning models for applications in a widerange of areas such as image recognition natural lan guage processing and self driving cars.
to enable developersto test and train their models effectively enterprises have builtdedicated platforms such as google cloud ai microsoftazure machine learning and amazon sagemaker .those platforms are equipped with rich computational re sources including gpus and ai accelerators running tens ofthousands of deep learning jobs every day.
like other software applications deep learning programs are often plagued by bugs.
in a real world industrial environ ment these bugs often lead to job failures wasting seriouslyresources and time.
there are a number of studies targeting deep learning program errors.
specifically zhang etal.
conducted an extensive empirical study on programfailures of deep learning jobs for microsoft s philly platform.
sifis et al.
developed a new static analysis to detect shape corresponding authors.errors in tensorflow programs which can effectively detect11 of shape related tensorflow bugs studied in .
in this paper we aim at detecting tensorflow the dominant open source deep learning framework program bugs in areal world industrial environment.
we perform an extensiveempirical study on failed tensorflow jobs submittedto the pla tform x platform by teams in alibaba group .
compared to our study focuses on job failures dueto tensorflow program bugs and targets a different industrialplatform.
our findings and actions are finding most bugs .
are common python bugs with argument mismatches undefined variables andmissing attributes as the top three types of bugs.
action we deployed four existing representative static tools mypy pylint pyflakes andpytype to detect python bugs in tensorflowprograms.
our results show that these four tools togetherdetect .
of the top three types of python bugs.
finding checkpoint errors .
and shape errors .
are the two most common types of tensorflow specific bugs.
as for the former category triggered byfailing to load a checkpoint file we are not aware ofany existing bug detection techniques reported in theliterature.
as for the latter category a static analysisapproach exists and will be evaluated in this paper.
action we have developed s hape tracer a new tool for detecting also shape related errors in real world tensorflow applications.
in contrast to the static analysis ap proach p ythia described in we adopt a constraintbased approach for the first time.
s hape tracer traverses program paths and builds a shape flow graph an abstract dataflow computation graph for each path.a constraint solver is then employed to solve shape related constraints introduced by shape operators foreach shape flow graph.
finally a bug is reported if theconstraint solver cannot find a feasible solution and asuggestion is offered as a warning if the user input is 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee constrained.
unlike p ythia based on static analysis s hape tracer based on constraint solving enables detecting subtle shape related errors when the rank number of dimensions or dimensions dimension sizes of a shape are completely unknown.
p ythia formulates the problem of detecting shape related errors as one of inferring theshapes of tensors from a set of datalog rules and itsanalysis cannot progress unless an unknown shape rankor dimension can be deduced to be a concrete value or a finite set of concrete values .
however in real world applications many unknown shapes cannot be con cretized this way as illustrated by the program given infigure for which we are required to solve the constraint batch size batch size batch size where batch size is provided as user input.
in particular the shapes of tensors that are statically unknown may be pro vided as user input by reading from the commandline orfiles or by calling unmodeled library functions making p ythia often ineffective as evaluated in section vi.
an immediate question arises does s hape tracer suffer from path explosion?
the answer is no.
tensorflowprograms have simple control flow structures makingsuch an approach practical and efficient.
we have ap plied s hape tracer to a set of real world buggy tensorflow applications.
our experimental results showthat s hape tracer is efficient by analyzing a program in at most seconds and effective by reporting outof bugs in detecting real world tensorflow bugs.
s hape tracer together with four other tools mypy pylint pyflakes and pytype have been pack aged as a new tool publicly released soon for detectingtensorflow program bugs and deployed to platform users.developers are recommended to run this packaged tool againsttheir applications before submitting a job to the platform.
in summary this paper makes the following contributions we report an extensive empirical study on in dustrial tensorflow job failures.
our findings show thatmost failure triggering bugs .
are python bugs and four existing representative static bug detection toolscan detect .
of the top three types of python bugs.
we propose the first constraint based approach for de tecting shape related errors one of the most commontensorflow specific bugs.
our approach explores pro gram paths systematically and can detect subtle errorswhen the rank or dimensions of a shape are unknown bysolving the shape related constraints for each path.
we have implemented our constraint based approach asa tool s hape tracer and applied it to a set of realworld buggy tensorflow applications.
s hape tracer is highly efficient and effective by analyzing each ap plication in at most seconds and detecting outof shape related errors with no false positives.
wehave also compared s hape tracer with p ythia to demonstrate the effectiveness of our new approach.the rest of this paper is organized as follows.
section ii gives an overview of tensorflow programs and the pla tform x platform.
in section iii we report an empirical study with real world tensorflow job failures and mo tivate this work.
section iv discusses how existing static toolsdetect python bugs.
section v introduces s hape tracer .i n section vi we evaluate s hape tracer using both opensource and real world tensorflow programs.
section viireviews the related work and section viii concludes the paper.
ii.
b ackground a. tensorflow programs the google born tensorflow library is the dominant open source deep learning framework.
it adopts the dataflow programming model which represents all the computations asdataflow graphs.
in a dataflow graph its nodes are computationunits i.e.
operators and its edges propagate tensors typed multi dimensional arrays from their source nodes to their sinknodes.
a dataflow graph is executed on the data provided withthe input data flowing along its edges which are processed byeach node before and the output results finally produced.
a tensorflow program commonly written in python consists of two phases construction and execution.
figure 1gives a simple example abstracted from a real world industrialapplication.
in the construction phase lines a com putation graph is configured each operator e.g.
tf.matmal at line generates some nodes and edges connecting databetween nodes.
in the execution phase lines a sessionobject is created to instantiate the graph which is executedmultiple times sess.run in line with data being fed into the placeholders e.g.
in xand inyat lines and respectively .
b. the pla tform xplatform the pla tform x platform is built by alibaba group and deployed in its commercial cloud.
pla tform x provides support for a variety of deep learning frameworks including tensorflow pytorch and mxnet .
as for otherplatforms users can submit their deep learning jobs via thecommandline or a web interface by specifying resourcessuch as cpu gpu times required and checking the status ofsubmitted jobs.
most platform users are production teams within this particular company.
everyday tens of thousands of deep learningjobs run on the platform.
there are a substantial number ofjob failures i.e.
aborted jobs.
the platform will tag eachfailed job and record its log messages for further investigation.these job failures not only lead to an expensive waste ofresources but also can take an enormous amount of humanefforts to debug.
iii.
a nempirical study our objective is to develop an effective failure prevention technique.
to this end we take failed tensorflow jobs onthe pla tform x platform as our study subjects.
there are a total number of failed jobs sampled in one month.
construction .
def fully connect input op name nin n out .
fc w tf.get variable name .
return tf.matmul input op fcw .
def predict input x class num .
mp tf.nn.conv2d input x tf.get variable mpc strides padding same .
reshaped tf.reshape mp .
fc fully connect reshaped fc1 .
logit fully connect fc fc2 class num .
return logit10.
in x tf.placeholder tf.float32 shape .
in y tf.placeholder tf.float32 shape .
y predict in x .
cross entropy tf.reduce mean tf.nn.softmax cross entropy with logits labels in y logits y .
train step tf.train.adamoptimizer 1e .minimize cross entropy execution15.
train img train lab read image batch size ... .
with tf.session as sess .
for i in range .
sess.run train step feed dict in x train img i n y train lab fig.
.
a sample tensorflow program abstracted a real world industrial application .
table i the top five most common types of job failures .
error type example error message patterns checkpoint errorassign requires shapes of both tensors to match.
lhs shape rhs shape key not found in checkpoint module attribute missing has no attribute no module named arguments mismatch takes exactly arguments given got an unexpected keyword argument undefined v ariablename is not defined local variable referenced before assignment shape errorshape must be rank b u ti sr a n k for op with input shapes cannot feed value of shape for tensor which has shape dimensions must be equal but are and for op with input shapes all failed jobs are submitted by different production teams in alibaba group.
for each failed job we contacted itscorresponding production team to collect related informationincluding source code execution logs and job scripts.
weare not able to obtain the input data to a job since they areregarded as being highly confidential.
thus it is difficult toreproduce a failure by rerunning the failed application.
figure shows the size distribution of studied applications.
tensorflow programs are small with lines of uncom mented code on average.
the largest program that we studiedhas lines of uncommented code.
note that the third partylibraries packaged in an application are not considered.
a. failure classification it is time consuming to manually analyze the applications one by one.
hence we apply log analysis to group failed applications throwing the same error message patterntogether.
figure gives an example.
the application throwsan error message at line which is parsed into a regularexpression input to reshape is a tensor with values but the requested shape has .
all the applications throwing fig.
.
distribution of program sizes in lines of uncommented code.
the same error message pattern are then grouped together.
inthe end there are failed groups.
we have sampled 630applications using the standard sample size calculator with aconfidence level of and confirmed that most applicationsin the same group fail due to the same root cause.
we have randomly selected two applications in each group and manually investigated their root causes to failure.
finally we obtain a total of common root causes.
table i highlightsthe top five with some error message patterns highlighted.
.
traceback most recent call last other stack traces .
tensorflow...errors impl.invalidargumenterror input to reshape is a tensor with values but the requested shape has fig.
.
the exception trace of a failed job.
fig.
.
bug type distribution in tensorflow programs.
tensorflow specific bugs are depicted in dark bars and python bugs in gray bars.
b. threats to v alidity first root cause analysis and failure classification involve manual inspection on application code which may be subjective.
to mitigate this threat each failed application wasexamined by two authors separately and the results were cross validated.
decisions were made only if both authors reached anagreement.
for some applications we also communicated withthe original developers to confirm our decisions.
second ourstudy subjects are all from the pla tform x platform.
hence some findings may not be applicable to other platforms.
tomitigate this threat we focus on failures caused by programbugs instead of platform specific issues e.g.
failures relatedto an execution environment .
the pla tform x platform is a widely used platform and the studied applications covera variety of areas including image and speech recognition natural language processing and recommendation systems.
c. findings we focus our study on bug related failures.
out of job failures failures are environment related throwing error messages such as remote file not found .i n addition jobs failed due to corrupted input data.
thosejob failures will not be further discussed.
the remaining10 failure triggering bugs are classified into two cate gories python bugs and tensorflow specific bugs.
figure 4divides these bugs into different types of bugs inpercentage terms with the python bugs shown in gray barsand tensorflow specific bugs in dark bars.
finding .
bugs are python bugs which also commonly exist in general purpose python applications.
python bugs let us examine some python bugs classified in figure .
the most common type of python bugs ismodule attribute missing referencing a non existent python class field or function accounting for .
of all bugs.there are also other common bug types such as argument mismatch invoking a function with an inconsistent number of actual arguments and undefined v ariables referencing a variable before its definition accounting for .
and12.
of all bugs respectively.
several python bug types are directly related to the dynamic features of python e.g.
type mismatch operating on objects of incompatible types illegal argument arguments not satisfying function specifications and not iterable callable iterating over objects of non collection types .
the otherpython bug types are common run time errors such as key not found accessing maps with non existent keys and divide by zero dividing a value by .
tensorflow specific bugs checkpoint error the most common bug type accounts for .
of all bugs.
platformusers frequently use the checkpointing mechanism to store atrained model to the cloud or to load an already trained modelfrom the cloud for inference or further training.
a checkpointbug arises when either the model file is missing or the loadedmodel is inconsistent with the required network structure.
theformer is related to a particular execution environment andhow to deal with the latter is worth a separate investigation.
shape error .
arises when invoking tensorflow operators with arguments of incompatible shapes incom patible ranks or dimensions .
it is difficult for developersto understand the tricky semantics of thousands of tensor flow apis leading to frequent shape error bugs in practice.
for example many tensorflow operators e.g.
softmax cross entrophy with logits at line in figure support the numpy broadcasting semantics which broadcasts a small array across a relatively large array by copying leadingdimensions of the higher rank argument and padding anydimension of size to the size of the matching dimensionfrom the other argument often leading to surprising results.
it can be difficult to debug shape error bugs.
as illustrated in figure an exception is thrown at line when invokingthe tf.reshape operator.
the tf.reshape operator changes the shapes of tensors as long as their sizes number of elements stay the same.
hence it fails to convert a tensor of 583elements to a specified shape of elements and throwsan exception.
however the tf.reshape operator is frequently used.
in the example there are tf.reshape operators and it is time consuming for developers to examine each operator.
finding shape error is one of the most common tensorflow specific bugs .
of total bugs and suchbugs can be detected effectively as demonstrated in .
the other types of tensorflow specific bugs include out of memory gpu out of memory commonly fixed by reducing the sizes of tensors loss nan invalid loss values gpu sync failed memory issues in gpu and graph not complete invalid dataflow graphs .
58table ii python bugs reported by four existing sta tic tools .
bugtype mypy pylint pyflakes pytype total mod att missing .
.
.
.
.
arg mismatch .
.
.
.
.
undef v ar .
.
.
.
.
total .
.
.
.
.
iv .
d etecting python bugs there are a number of static tools for finding bugs in python programs such as mypy pylint pyflakes and pytype .
we have investigated their effectiveness indetecting python bugs in industrial tensorflow programs.
table ii gives the results for the top three python bug types.
overall these four tools together have detected .
of allthe bugs of these three types.
since these bugs are simplesemantic errors the false positive rates of these four tools arelow.
among the four tools pyflakes is the best performer reaching .
.
however all the four tools perform poorlyonarguments mismatch bugs with pylint attaining only .
even as the best performer for bug type .
v. d etecting shape error bugs in tensorflow programs tensors are the basic data units.
a tensor is a multi dimension array and its shape refers to the number of dimensions rank and dimensions sizes.
shape error bugs are the errors incurred when the shape of a tensor does not match the specification of an operator.
in a static analysis p ythia is introduced for detecting shape error bugs by modeling tensor operators in datalog so that the shape of a tensor can often be de duced to be a concrete shape or a set of concrete shapes .
p ythia can detect out of the shape related bugs studied in .
however from our study on industrial tensorflowapplications there are still many cases where the rank ordimension sizes of a tensor are completely unknown.
for ex ample p ythia failed to report any error in the real world applications under testing due to unresolved unknown shapevalues.
therefore in this paper we introduce a new constraint based approach s hape tracer for detecting shape error bugs and we will compare it with p ythia in our evaluation.
in this section we first use three examples to motivate ourapproach and then describe it in detail.
a. motivating examples example figure depicts the computation graph for the program in figure where each edge is annotated with the shape information of its propagated tensor.
the shape ofa tensor can be input dependent a placeholder tensor can set some dimensions or the whole shape to none and its shape will be instantiated by feeding data to the placeholder usingthefeed dict operator e.g.
line when executing the graph.
the computation graph is executed by invoking session.run tensor .eval o roperation.run .
in figure at line the graph is executed to obtain the results from theoperator train step line .
the input data train image and train lab line are fed to the placeholders inx line and iny line respectively.
note that the first dimension of input data is configured by an input argument batch size as highlighted by the box in line .
as a result the shapesof tensors in xand inyare and respectively.
the tensor inxis passed as an actual parameter to the function predict at line and processed by conv2d line a core operator for convolution.
the conv2d operator is often used to extract intermediate features in complex neural net works.
it takes a dimensional input tensor a dimensional filter tensor and a strides vector with elements as input.
with the same padding strategy cond2d x f s same will produce a tensor of shape x s1 x s2 f .
hereafter we use the notation x to represent the ith dimension of tensor x s shape and the notation sito represent the ith element of vector s. in our example figures and the conv2d operator produces the tensor mp of shape .
the reshape operator at line changes the shape of the incoming tensor mp to a specified shape i.e.
a 2dimension array.
here the special dimension size 1denotes that the size of the corresponding dimension needs to becomputed dynamically.
a tensor can be reshaped correctly ifits size total number of items in the tensor is the same asthe size of the specified shape.
at line after reshaping wehave a new tensor reshaped of shape batch size .
at line the function fully connect is invoked with the tensor reshaped as its actual parameter.
thus the operators get variable line and matmul line are included in the computation graph.
the matmul operator multiply reshaped with fcw resulting in a new tensor fcof shape .
next the tensor fcis processed by the same function again at line .
finally logit is produced and returned asy.
the operator softmax cross entrophy with logits line produces normalized probabilities from input tensors iny and y .
it supports the broadcasting rule sizes of matching dimensions mustbe identical or one of them is in which case the resultingtensor adopts the other size in its corresponding shape dimen sion .
hence the operator can succeed only if the sizes of bothtensors first shape dimensions are the same or one of them is1 i.e.
batch size batch size batch size .a st h e user input batch size is configured to be the application failed with a runtime exception.
examples and let us look at another two examples given in figures and respectively.
in figure thetensor behavior input comes from user input and its shape i.e.
its rank and dimension sizes is completely unknown.at line tensor tmp user profile cnn is reshaped to a 4dimensional tensor user profile cnn which is then multiplied with behavior input via the operator matmul suggesting that the rank of behavior input is also since the operator will fail otherwise.
at line tensor attention weights is reshaped to a dimensional tensor tmp attention weights which is also multiplied with behavior input.
since the input tensor 59fig.
.
the computation graph for the program given in figure where the nodes represent the operators in the program and the tensors black dots flow along the graph edges annotated with the the shape information of their propagated tensors .
the bug triggered is highlighted.
.
user profile cnn tf.reshape tmp user profile cnn shape noutput behavior .
attention layer input tf.matmul behavior input user profile cnn ...... .
tmp attention weights tf.reshape attention weights shape .
behavior output tf.matmul tmp attention weights behavior input fig.
.
code from an industrial application where the shape of behavior input is completely unknown.
the bug triggering lines are highlighted in red.
behavior input cannot satisfy both constraints the application will always fail on one of the matmul operators at line or line .
in figure the tensor labels is a dimensional array and the tensor pred is a dimensional array.
all their dimension sizes are unknown.
the bug is triggered at line when the condition loss type mae holds since the operator absolute difference expects input tensors with the same shape i.e.
same rank and dimension sizes .
however the bug will not be triggered if the other branch is taken when input parameter loss type is logloss .
the operator sparse softmax cross entropy with logits allows the rank of the input argument pred to be one less than that of labels.
hence it will not trigger a bug.
b. methodology in the above three examples there are tensors with completely unknown shapes example or partially unknown shapes examples and .
it is difficult to write datalogrules as in p ythia and deduce those unknown shapes to a finite set of concrete shapes.
the tensors with unknownor partially unknown shapes can be frequently found in real world applications.
they can come from commandline input files or unsupported library functions.
note that it is difficultfor developers to manually annotate a tensor from files withunknown shape information.
in addition the tensorflow li brary provides thousands of apis and enterprises often offertheir own in house libraries.
it will be a daunting task if notimpossible to support all libraries apis in practice.
therefore we are motivated to develop s hape tracer a new tool founded on a constraint based approach.
werepresent the shape of a tensor symbolically by introducingsymbolic values for unknown ranks or unknown dimensionsizes.
constraints can be introduced from tensor operators scalar variables and conditional branches.
finally a constraintsolver is applied to check the satisfiability of these constraints.
for instance for example figure the value of input variable batch size is symbolic denoted as x. the computation graph will generate a constraint x x x together with other constraints.
the solution is x which can be provided to users as a warning.
in example figure the rank of tensor behavior input is symbolic denoted as x. the two matmul operators at line and line will introduce their respective constraints x andx .
as both cannot be satisfied together an error is found.
c.shape tracer we have implemented s hape tracer in w ala and used ariadane as its front end to parse python programs into w ala ir.
figure sketches the architecture of s hape tracer .
its three main components are summarized below.
first builder traverses program paths and builds a shapeflow graph an abstracted computation graph for each path.
next solver formulates a shape flow graph into a list of constraints which is then solved by z3 a state of the art constraint solver.
finally an error warning is issued if the constraints arenot satisfiable if the user input is constrained .
to reportprecisely the line number where a bug warning occurs reporter searches for the first operator introducing unsatisfiable constraints and reporting it to the user.
next we describe these three components in detail.
builder builder constructs a shape flow graph for each program path.
this may sound inefficient initially.
however .
if loss type mae .
loss tf.reduce mean tf.losses.absolute difference labels pred .
elif loss type logloss .
loss tf.reduce mean tf.nn.sparse softmax cross entropy with logits logits pred labels labels .
optimizer tf.train.gradientdescentoptimizer .
.minimize loss ... fig.
.
code from an industrial application where pred is a dimensional array with unknown dimension sizes and labels is a dimensional array with an unknown dimension size.
the bug triggering line is highlighted in red.
fig.
.
the high level architecture of s hape tracer .
the control flow structures of tensorflow programs are usually simple.
in general the number of program paths is rarelyreaching .
in an extreme case where the neural networkis constructed in a loop figure the largest number ofprogram paths observed is only.
a basic algorithm the shape flow graph of a program is its abstracted computation graph annotated with shape infor mation.
to build a shape flow graph we slice backwards froman invocation to session.run i.e.
from an output tensor.
since tensorflow programs commonly propagate values directlythrough assignments or parameter passing we slice alongthe use def chains of w ala s ssa single static assignment representation.
during the backward slicing function calls areinlined when a function call is met the return values ofthe callee function are added to the graph as new nodes and we continue slicing backwards from the newly addedreturn values.
in the end all operators i.e.
tensorflow apiinvocations tensors and scalars e.g.
actual parameters ofoperators that the output tensor is transitively dependent on are included in the graph.
let us explain our basic algorithm using the example in figure .
we slice from sess.run at line i.e.
the output tensor train step.
since train step is returned from the operatorminimize the operator and its operand i.e.
cross entropy are added to the graph.
similarly cross entropy is produced by the operator softmax cross entropy with logits line .
hence the operator and its operands in yand y are included.
from y we inline the function call to predict line and continue slicing from its return value logit line .
next the function def fully connect is inlined twice at lines and in that order.
the final shape flow graph is given in figure .
b graph duplication shape flow graphs are duplicated atphi nodes control flow confluence points in ssa .
when we encounter a phi node with nincoming values the graph is duplicated ntimes and each graph picks a distinct incoming value to continue slicing.
figure gives the shape flow graphsof example figure .
in ssa there exists a phi node atthe confluence point of different branches of the ifstatements lines .
thus we have two shape flow graphs one foreach branch.
loops although rarely seen in the graph construction phase are processed by unrolling a loop twice.
in our study there isonly one application building neural networks in a loop.
c shape information collection constants and scalar variables propagated directly along use def chains are recordedstraight forwardly.
we try to infer as much concrete informa tion as possible by applying constant propagation and comput ing concrete shape information according to the documentedsemantics of tensorflow apis.
we also consider the followingtwo special cases.
first the shape of a tensor can be setusing the tf.setshape function as shown in fig.
lines and .
hence for each tensor we check its uses for atf.setshape call to the object and update the shape of the tensor accordingly.
second in most cases values are directlypropagated.
however when initializing a tensor with a givenshape values are passed into the constructor of the shape asparameters and stored in its corresponding fields.
in general apointer analysis is required to compute field relateddependences.
however such fields of a shape object are onlystored once in its constructor during initialization .
therefore when encounter a field load we simply search for a uniquestore to the corresponding field.
solver solver formulates a shape flow graph as a list of constraints which are then solved by z3 .
we collect theconstraints from tensor operators and scalar instructions in theshape flow graph.
however we do not consider branch condi tions since shape related values rarely have data dependenceson conditionals in real world tensorflow applications.
figure defines the symbolic representation of shapes and values.
specifically t denotes the size of t s last shape dimension.
this variable is particularly useful when t s rank is unknown i.e.
tis symbolic.
by default we assume that all variables are symbolic unless otherwise specified.
if tis a constant value c we introduce a variable for each dimension size of t by applying the following function to concretize t concretize t c c productdisplay i 0t t t t t c this function sets t s rank to c sets t tot s last dimension size t t and concretizes t s size to the product of its all dimension sizes t producttextc i 0t .
constraints are introduced for operators according to their documented semantics.
for instance the c reshape a b op61fig.
.
the two shape flow graphs one for each path of the program given in figure .
the oval nodes are operators and square nodes are scalars.
the edges are annotated with shape information of their associated tensors blackdots .
.x tf.placeholder tf.float32 .
x.set shape .y tf.identity x .
y.set shape .x np.random.normal .
.
y sess.run y feed dict x x fig.
.
the code example ut from .
t t t ... t s dimension sizes t t s total size number of elements t t s rank number of dimensions v0 v1 ... v v v s element values x x s value fig.
.
symbolic representation of tensor t s shape vector v s values and scalar x s value.
vis a dimensional shape and its size v is a constant.
erator reshapes tensor ato tensor cof the same size with the shape specified by vector b. hence we have logicalanddisplay i b c bi c b concretize c b c a here the constraint c a states that tensor cand a have the same size as required by reshape and the remaining constraints specify the shape of caccording to vector b c s rank is defined by b s size c b and c s dimensions are defined by b s elements logicalandtext i b c b i .
note that the size of b i.e.
b is a constant.
hence tensor c is concretized concretize c b .
except for one element value e.g.
all the other element values are constant.
the same list of constraints is applicable to the case when tensora s rank is constant i.e.
when ais already concretized.
let us examine the operator softmax cross entropywith logits abbreviated as c logits a b for supporting numpy broadcasting .
before we dive into the details ofthe tricky broadcasting semantics we first introduce anotherhelper function broadcast a b c i j .
this function represents the constraints on the ith dimension of the higher ranked input tensor a the ith dimension of output tensor c and thematching jth dimension of the other input tensor b where j i broadcast a b c i j a b c a a c b b c a broadcast a b c i holds if one of the following three cases holds a matches with b producing the same size for c sith dimension a is in which case c sith dimension takes the size from b sjth dimension and b is in which case c sith dimension size takes that from a s. the three cases reproduce the semantics of broadcasting onepair of matching dimensions a and b .
the list of constraints for c logits a b is given by a b broadcast a b c broadcast a b c a b broadcast a b c a c a b broadcast a b c b c the above constraints are applied when aorbis symbolic.
in this case we only introduce the constraints on the first andlast dimension sizes of the input and output tensors.
in thecase of a b the constraints are applied to the first and last dimensions of all three tensors a b and c broadcast a b c broadcast a b c .
in the other two cases the constraints are applied to the last dimension ofthe three tensors and the output tensor ctakes the size from the higher ranked tensor e.g.
a c when a b .
when both aand bare constants i.e.
aand bare concretized ccan be concretized as follows a b concretize c a logicalanddisplay i abroadcast a b c i i a b concretize c a logicalanddisplay i a bc a logicalanddisplay i bbroadcast a b c i a b i a b concretize c b ... in the case of a b the output tensor cis concretized and each of its dimensions is defined by thebroadcast rule.
otherwise cis concretized with the higher rank e.g concretize c a when a b higher di62table iii number of reported errors w arnings in industrial and open source from applica tions .
tp is the number of reported true bugs fp is the number of reported false positives and fn is the number of false nega tives .
toolindustrial open source tp fp fn tp fp fn shape tracer pythia enhanced p ythia mensions are copied directly from the higher ranked tensor logicalandtext i a bc a and the matching dimensions are broadcasted logicalandtext i bbroadcast a b c i a b i .
similarly appropriate constraints are introduced for the other operators such as conv2d and matmul.
to date s hape tracer provides support for common operators with an average of .
lines of code for each operator.
for tensorsreturned from unsupported library functions their ranks aresymbolically represented.
in the end the constraints of ashape flow graph are fed into z3 .
reporter if z3 fails to solve a given set of constraints if a user input is constrained by a constant value an error warn ing is issued.
while errors can always trigger a bug warningssuggest expected user inputs.
to report the bug locationprecisely reporter searches for an operator introducing the unsatisfiable constraints found.
conceptually this is realizedby removing each operator one by one more precisely byremoving the constraints introduced by each operator in thereverse order of when it is added to the underlying dataflowgraph until the constraints become satisfiable.
in practice wehave accelerated this process using binary search.
vi.
e v alua tion our evaluation addresses the following research questions how effective is s hape tracer in detecting shape error bugs in tensorflow programs?
how does s hape tracer compare to a state of the art static analysis tool p ythia ?
how efficient is s hape tracer ?
all experiments were conducted on a laptop equipped withi5 cpu and 32gb ram.
a. rq1 effectiveness we evaluate s hape tracer using a set of buggy industrial tensorflow programs randomly picked from our study and the open source programs studied in .
table iii summarizes the results.
overall s hape tracer has successfully detected out of bugs in industrialprograms and out of bugs in open source applications.there are errors from the industrial programs and 9from the open source programs and warnings all fromthe industrial programs .
a bug can definitely be triggered forany of the reported errors.
the warnings are subject touser input in which case s hape tracer warns on expected input values.
the applications that exhibit these warnings canonly be correct if the corresponding shape related constraints1.
deep out none .
for idx info in enumerate self.deep info .
if idx .
deep out deep features .
else .
deep out res out .
deep out tf.matmul deep out info .
deep out tf.nn.leaky relu deep out .
if idx .
res out tf.concat .
else .
res out deep out .
if len deep out list idx .
deep out list.append deep out .
else .
deep out list deep out ...... .
loss tf.reduce mean tf.nn.sigmoid cross entropy with logits logits pred labels labels l2 fig.
.
code snippet of a false negative example the neural network is built in a loop.
each loop iteration builds one layer of the network lines .
thenext loop iteration uses the previous two layers as input to build a new layer line .
the bug is triggered at line as highlighted in red.
table iv f alse nega tives in the industrial program bugs explained .
reasons for producing false negatives too many shape related values from user input constructing neural networks in a loop.
are met.
according to our experience the odds for such a warning to be an error is much higher than that for theshaped related constraints to be always satisfiable.
thus these16 warnings can be considered as errors detected.
we willcompare s hape tracer and p ythia later.
false negatives shape tracer failed to detect industrial program bugs with the two reasons given in table iv.
bugs were not reported because the corresponding programsdefine most of their tensor shapes in configuration files result ing in many unknown shapes.
since s hape tracer failed to deduce a constant constrained input no error or warningwas given.
the other false negative occurs in the programillustrated in figure where a neural network is constructedin a loop.
shapetracer processes a loop by unrolling it twice which is not sufficient for detecting this particular bug.
there are also open source program bugs missed by shapetracer.
we will discuss these bugs in section vi b2.
false positives s hape tracer did not report any false positives for the programs in table iii.
to evaluate its precisionfurther we have further tested s hape tracer using a set of randomly selected correct programs from the pla tform x platform.
again no false positive was reported.
error and warning examples figure gives a buggy example with the bug triggered at line .
the operator absolute difference requires the incoming arguments of the same shapes producing constraint layer var .
furthermore shape tracer can infer that layer8 is of shape line .
def input fn .
other complex operation .
var .shape .
var tf.reshape var .
return var var .
def model fn var var reuse .
other complex operation .
layer7 and w7 comes from other operation .
layer8 tf.matmul layer7 w7 b7 now layer8 is .
tmp1 tf.losses.absolute difference layer8 var .
loss tf.reduce sum tmp1 .
return loss .var var input fn .loss model fn var var false .c sess.run fig.
.
code snippet with a bug abstracted from an industrial program.
sensitive variable names have been replaced with var 0and var .
.
def fun batch size .
other complex operation .
var cloud platform api0 batch size .
return var .var fun batch size batch size is from user input .x tf.reshape var warning reported by shapetracer filename.py line3 value filename.py line6 value fig.
.
code snippet of an example which contains a warning abstracted from an industrial program.
sensitive variable names have been replaced withvar 0and var .
the warning message is given in the box.
and var 0has a shape of line .
inference details are omitted here due to space limitation.
as a result we derive the constraints layer var layer var which are unsatisfiable.
hence an error is reported.
it is worthnoting that s hape tracer generates constraints for this program.
however reporter is able to examine every operator and precisely points out the bug location.
figure gives an example triggering a warning together with the warning message reported by s hape tracer .
the cloud platform api0 at line produces tensor var 1of shape with batch size being unknown which is propagated to var line .
the operator reshape at line reshapes var 0to the specified shape and expects the size of var 0to match with the size of the specified shape i.e.
batch size x where xstands for the symbolic value of the vector.
given such constraints s hape tracer will issue a warning as highlighted.
note that the warningand error messages are helpful to developers during both codereview or post mortem debugging.
b. rq2 comparing with p ythia table iii also compares s hape tracer with p ythia provided by its artifact on detecting shape error bugs in industrial and open source programs.
p ythia is a state ofthe art tool for detecting tensorflow shape related bugs.table v comparing shape tracer and pythia on open source programs with check beware denoting a correctly reported error w arning .
ut ut ut ut ut shape tracer check check pythia beware beware beware beware industrial programs pythia neither detects any industrial program bugs nor reports any false positives.
to figure out why we have carefully examined the logs and messagesprinted by p ythia and summarized the reasons below pythia failed on industrial programs due to implementation bugs.
it throws runtime exceptions inpython fac t gen when generating python facts.
we have reported this issue to the p ythia developers for further investigation.
pythia failed on the other industrial programs due to unknown shape values.
although facts were successfullygenerated the unknown shape values prevented its analy sis to progress further.
unknown shape values come fromuser inputs or unsupported operators.
note that althoughthere still exist a considerable number of unknown valuesin s hape tracer our constraint based approach still enables our analysis to detect many bugs as demonstratedin our motivating examples section v a .
pythia can successfully analyze a partially known shape with a none or special dimension .
however it cannot analyze both completely unknown shapes e.g.
an unknown rank and completely unknown dimensionswhen they cannot be concretized.
as a result it fails todetect any error in the set of real industrial programs.we further investigated on how to extend p ythia to deduce as many unknown shapes as possible.
an extraunknown tag is introduced for any unknown shape and new datalog rules are introduced to deduce the rankand dimension values of tagged unknown shapes.
for example for a matmul operator its two parameter shapes must have the identical rank.
thus we can infer therank of a completely unknown parameter shape from theother parameter.
we have extended p ythia with a set of datalog rules loc .
this extension enables pythia to detect shape related errors.
however it still fails to report the other errors due to unknown shapevalues or unfixed crashes.
for example three shape related errors that are missed by p ythia are related to complex constraints like batch size batch size illustrated in figure .
open source programs pythia performs much better on open source programs and we have successfully repro duced their results as in their paper using their givenartifact.
table v highlights the differences of the two tools onopen source programs.
p ythia reported warnings for the bugs ut ut and ut missed by s hape tracer .
pythia exploits heuristics to report these warnings e.g.
suspicious broadcasting .
we did not implement such heuris tics in s hape tracer because they can lead to many false 64fig.
.
analysis times of s hape tracer on industrial programs.
fig.
.
size distribution of shape flow graphs for industrial programs.
positives and developers often ignore such warnings.
in ut operator argmax y axis will result in a shape of with all values being zero.
in ut y y produces a tensor of shape due to broadcasting.
p ythia warns on such suspicious operations since the results look surprising.
however these programs still satisfy the shape relatedrules.
we attempted to incorporate similar heuristics but werediscouraged to do so by our industry sponsor developing the pla tform x platform because of spurious reports.
ut uses list slice in python to construct the shape of all initial tensors which is not yet supported by s hape tracer .
c. rq3 efficiency figure summarizes the analysis times of s hape tracer on the industrial programs.
s hape tracer is fast as it finishes all its analyses in at most seconds for each programon a standard laptop with 32gb ram.
note that the analysistimes include the times in exploring programs paths collectingand solving constraints and searching for bug locations.
figure shows the size distribution of shape flow graphs.
the sizes of shape flow graphs range from to nodes with an averages of .
nodes.
the average number ofconstraints for each shape flow graph is .
which looksseemingly large.
however as most of constraints are constantequality constraints z3 can solve them very efficiently.
vii.
r ela ted work empirical studies.
zhang et al.
investigated tensorflow program bugs from stack overflow and github.
follow ing islame et al.
performed a more comprehensivestudy on deep learning program bugs including bugsfrom applications using five different deep learning libraries caffe keras tensorflow theano and torch .
the authors in conducted an extensive empiricalstudy on job failures on microsoft s philly platform.
guo surveyed bugs in deep learning development anddeployment.
in this paper we also perform an empirical studyfocusing on industrial tensorflow job failures on anew platform motivating us to develop s hape tracer a new static tool for detecting tensorflow shape errors.
static bug detection.
python is the most popular language in developing deep learning applications .
python bugs in deep learning programs can be detected quite effec tively with existing static tools as confirmed in this paper.
a number of research efforts focus on shape related bugs.
ariadne is the first static shape analysis tool developedfor tensorflow.
however due to implementation issues e.g.
failing to analyze shapes inter procedurally it cannot effec tively detect errors in practice .
p ythia is shown to be able to detect out of open source program bugs usinga datalog based static analysis.
in this paper we introduce anew constraint based approach and a tool s hape tracer t o detect effectively industrial tensorflow program bugs.
testing.
there is a large body of research aiming at testing the robustness of deep learning models.
how to apply a constraint based approach to improve testing effectiveness isan interesting topic worth further investigation.
viii.
c onclusion this paper aims at detecting industrial tensorflow program bugs.
we have conducted an extensive empirical studyon failed industrial tensorflow jobs.
based on ourfindings we have applied four existing representative statictools to detect .
of the top three common pythonbugs in tensorflow programs.
to detect tensorflow specificbugs we have introduced the first constraint based approachfor detecting tensorflow shape related errors and developedan associated static tool s hape tracer .
we have applied shape tracer to a set of industrial tensorflow programs showing that s hape tracer is both efficient by analyzing a program in at most seconds and effective by detecting 40out of industrial tensorflow program bugs with no falsepositives .
s hape tracer is now deployed in the pla tform x platform and will be publicly available soon.