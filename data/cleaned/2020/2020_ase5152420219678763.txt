pyexplainer explaining the predictions of just in time defect models chanathip pornprasit chakkrit tantithamthavorn jirayus jiarpakdee michael fu patanamon thongtanunam monash university australia.
the university of melbourne australia.
abstract just in time jit defect prediction i.e.
an ai ml model to predict defect introducing commits is proposed to help developers prioritize their limited software quality assurance sqa resources on the most risky commits.
however theexplainability of jit defect models remains largely unexplored i.e.
practitioners still do not know why a commit is predicted asdefect introducing .
recently lime has been used to generateexplanations for any ai ml models.
however the randomperturbation approach used by lime to generate syntheticneighbors is still suboptimal i.e.
generating synthetic neighborsthat may not be similar to an instance to be explained producinglow accuracy of the local models leading to inaccurate explana tions for just in time defect models.
in this paper we proposepyexplainer i.e.
a local rule based model agnostic techniquefor generating explanations i.e.
why a commit is predicted asdefective of jit defect models.
through a case study of twoopen source software projects we find that our pyexplainerproduces synthetic neighbors that are more similarto an instance to be explained more accurate localmodels and explanations that are more uniqueand more consistent with the actual characteristics ofdefect introducing commits in the future than lime a state of the art model agnostic technique .
this could help practitionersfocus on the most important aspects of the commits to mitigatethe risk of being defect introducing.
thus the contributionsof this paper build an important step towards explainableai for software engineering making software analytics moreexplainable and actionable.
finally we publish our pyexplaineras a python package to support practitioners and researchers index terms explainable ai just in time defect prediction explainable software analytics i. i ntroduction modern software development projects tend to release software products in rapid cycles.
to ensure the quality of all newly arrived commits developers need to conduct codereview and provide feedback prior to merging them into therelease branch.
however such code review activities are stilltime consuming and expensive.
thus performing exhaustivecode review activities for all commits is infeasible due to thelimited software quality assurance sqa resources.
just in time jit defect prediction an ai ml model to predict defect introducing commits hasbeen proposed to help developers efficiently prioritize theirlimited sqa resources on the most risky commits.
in addition jit defect prediction is also used to provide insights about theimportant characteristics of defect introducing commits.
such the corresponding author.
email chakkrit monash.eduinsights can help qa teams and managers to develop proactivesoftware quality improvement plans to prevent pitfalls in thepast that lead to software defects in the future .
however the predictions of existing jit defect prediction approaches are still not explainable hindering the adoption ofjit defect models in practice .
recent researchshows that practitioners still asked many why questions e.g.
why a commit is predicted as defective sincecurrent jit defect prediction approaches are treated as black box which only provide the predictions not the explanations.such a lack of explainability of jit defect prediction ap proaches could lead to suboptimal software quality assurancepractices and suboptimal operational decision makings.
recently lime a state of the art model agnostic technique has been adopted in software engineering research e.g.
line level just in time defect prediction and explain able file level defect prediction .
lime is a techniquethat explains a prediction of ai ml models i.e.
what are thefeatures that influence a given prediction .
generally speaking given an instance to be explained e.g.
a commit limeproduces an explanation from a local model f prime that is trained using the randomly generated synthetic instances around theinstance to be explained x prime i.e.
synthetic neighbors and the predictions y prime obtained from the global black box model.
this allows the local model to mimic the behavior of theunderlying global black box models.
the quality of explanation produced by lime heavily relies on the neighborhood generation process .
ideally theneighborhood generation process should generate syntheticneighbors that are closely similar to the instance to be ex plained so that the local model can accurately approximatethe prediction of the global models.
in lime the randomperturbation approach is used to generate synthetic neighbors.however such a simple random perturbation approach maynot be suitable for sparse and high dimensional data likejit datasets .
it is possible that the random perturbationapproach will generate synthetic neighbors that may not besimilar to an instance to be explained which will lead thelocal model to inaccurately approximate the predictions of theglobal model.
thus these local models may not be effectivein generating explanations e.g.
generic .
in this paper we propose pyexplainer a local rule based model agnostic technique for explaining the predictions of jitdefect prediction models.
to produce a more accurate explana tion for the prediction of jit defect models our pyexplainer 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee generates synthetic neighbors based on the actual characteristics of defect introducing commits in the jit dataset usingthe crossover and mutation operations.
instead of generatingan explanation with a single rule feature with an importancescore like lime e.g.
the importance score of churn is .
our pyexplainer generates an explanation that accountsfor interactions between rule features e.g.
churn reviewers defect .
to evaluate our pyexplainer we compare with lime in three dimensions the similarity between synthetic neighbors and an instance to be explained the accuracy of the local models and the effectiveness in generating explanations.
through a case study of commits thatspan across two large scale software systems i.e.
openstackand qt we address the following three research questions rq1 does our pyexplainer produce better synthetic neighbors than lime for jit defect models?
the synthetic neighours produced by our pyexplainerare more similar to an instance to be ex plained than lime indicating that our pyexplainerproduces synthetic neighbors that are more closelysimilar to an instance to be explained than lime.
rq2 does our pyexplainer produce higher accuracy of local models than lime for jit defect models?when explaining the rf and lr jit defect models pyexplainer produces local models that are more accurate auc than the local models producedby lime indicating that the pyexplainer produceslocal models that have a higher ability to discriminatethe characteristics between defect and clean classes.
rq3 is our pyexplainer more effective in generating explanations than lime for jit defect models?the explanations generated by our pyexplainer are69 more unique i.e.
more specific to an in stance to be explained than lime.
on the otherhand the explanations generated by our pyexplainerare more consistent with the actual defect introducing commits in the testing data than lime.
thus the explanations generated by pyexplainer could help practitioners to focus on the most important aspects that areassociated with the risk of being defect introducing for a givenprediction instead of focusing on the less important aspects.
contributions.
the contributions of this paper are as follows we propose pyexplainer a local rule based modelagnostic technique for explaining the predictions of jitdefect models.
our results show that pyexplainer produces syntheticneighbours that are more similar to an instance to beexplained more accurate local models and ex planations that are more unique and more consistent withthe actual characteristics of defect introducing commits inthe future than lime.
finally we developed a proof of concept of visual ex planations and what if visualizations and published our pyexplainer as a python package.ii.
r elated work r esearch questions prior studies pointed out that practitioners often do not understand the reasons behind the predictions of softwareanalytics .
recent work also raises concerns thata lack of explainability of software analytics often hinder theadoption of software analytics in practice .
importantly jiarpakdee et al.
found that of recent defect prediction studies often focus on improving theaccuracy while as few as of recent defect prediction stud ies focus on making file level defect prediction models moreexplainable.
however jiarpakdee et al.
found that practitioners perceived that providing the explanations of defectprediction models are as important and useful as improving theaccuracy of defect prediction models.
yet the explainabilityof jit defect models remains largely unexplored.
the explanability of software analytics can be achieved at the global and the local levels.
the global explanation can be produced using modelspecific interpretation techniques that are built in the ai mlmodels e.g.
anov a for regression analysis variable impor tance analysis for random forest .
this explanation helps re searchers and software practitioners understand what importantfeatures that influence the predictions of the models .
however this global explanation is not specific to theprediction of each instance e.g.
a commit in the testing orunseen data since the global explanation is derived from thetraining dataset .
hence the global explanation may notbe accurate for a particular prediction.
on the other hand the local explanation is produced for a particular prediction of an instance in the testing or unseendataset which allow practitioners better understand the reasonsbehind the predictions of the ai ml models .
lime is a state of the art model agnostic technique which has beenwidely adopted to address various software engineering prob lems and other domains citations .
for example recentwork employed lime for line level defect predictions i.e.
identifying defective lines that contain the risky codetokens explained by lime .
jiarpakdee et al.
found that lime is effective in explaining the predictions of file level defect prediction models i.e.
why a file is predicted asdefective .
however lime has the following limitations.
first the lime s neighborhood generation process is still suboptimal.
the quality of an explanation for a prediction heavily relies on the quality of the generated syntheticneighbors around the instance to be explained .
thus if the neighbor generation process is suboptimal the localmodel may fail to provide accurate insights about the logicalreasoning of the global model.
jia et al.
found that the size of the neighbourhood has a large impact on thequality of the explanation.
krishnan et al.
found that when a model is learned from sparse and high dimensionaldata e.g.
just in time defect dataset the model is oftenunderfitting failing to capture the phenomenon of the databeing trained.
thus the neighbor generation process shouldideally generate synthetic neighbors that are similar to the 408instance to be explained.
therefore we investigate whether our pyexplainer produce better synthetic neighbors i.e.
thesynthetic neighbors that are more similar to an instance to beexplained than lime for jit defect models.
we formulatethe following rq rq1 does our pyexplainer produce better syntheticneighbors than lime for jit defect models?
second the approximation of the lime local models to the predictions of the global model is still suboptimal.
one of the key principles of model agnostic techniques is to buildthe best local model to mimic the behavior of the predictionsof the global models.
accuracy is often used to measure theextent to which how well the local model can approximate thepredictions of the global model .
thus a high accuracy oflocal models is desirable in order to derive the highest qualityexplanation i.e.
the local models can accurately approximatethe global model predictions for a subset of the data e.g.
localsurrogate models .
since lime uses the random perturbationmethod to generate synthetic neighbors the approximation ofthe lime local models to the predictions of the global modelmay be suboptimal leading to the inaccurate local modelsproduced by lime.
thus we formulate the following rq rq2 does our pyexplainer produce higher accuracy oflocal models than lime for jit defect models?
third the explanations generated by lime are still not specific to an instance to be explained.
another key principle of the model agnostic techniques is to build a uniqueexplanation that is specific to the prediction of an instanceto be explained.
thus explanations should be unique andhighlight the key characteristics i.e.
features of a committhat leads a global model to predict as defect introducing.however lime uses a quantile discretization i.e.
st 2nd 3rdquantiles for generating rule features.
the three bins used by lime may not be enough to capture the highly complexand highly skewed jit defect datasets.
thus the rule featuresused by lime may produce generic explanations that maynot be specific to the instance to be explained which may notreflect the actual characteristics of defect introducing commits.therefore we formulate the following rq rq3 is our pyexplainer more effective in generatingexplanations than lime for jit defect models?
iii.
o urpyexplainer al ocal rule based model agnostic technique in this section we present our pyexplainer a local rulebased model agnostic approach for explaining the predictionsof jit defect models.
overview.
figure illustrates an overview of the pyexplainer approach which consists of four main steps.
first given an instance to be explained i.e.
a commit and aglobal model our pyexplainer will generate synthetic neigh bors around the instance to be explained using the crossoverand mutation techniques .
second our pyexplainer willobtains the predictions of the synthetic neighbors from theglobal model.
third our pyexplainer builds a local rule basedregression model in order to learn the associations between thecharacteristics of the synthetic instances and the predictionsfrom the global model.
in the fourth step our pyexplainergenerates an explanation from the local model for the instanceto be explained.
we now describe each of the four steps below.
step generate synthetic neighbors around the instance to be explained.
our pyexplainer will first generate synthetic neighbors x prime around the instance to be explained using the crossover and mutation techniques .
to do so pyexplainer will find an initial set of actual neighbors i.e.
the actual instances around the instance to be explained in thetraining dataset.
to identify the actual neighbors pyexplainerapplies the exponential kernel function see eq.
to calculatethe similarity score between each instance in the trainingdataset i k and the instance to be explained i e .
k ik ie e x p dist i k ie 2w2 wheredist i k ie is the euclidean distance between instances ikandie andwis the kernel width as the multiplication of .
and the number of features of an instance w .
features as suggested by ribeiro et al.
.
based on the initial set of actual neighbor pyexplainer generates synthetic neighbors using crossover and mutationtechniques to expand the initial set.
the calculation of thecrossover i crossover and mutation i mutation techniques can be derived as follows icrossover i1 i2 i1 imutation i1 i2 i3 wherei1 i2 andi3are the randomly selected neighbourhood instances is a randomly generated number between 0and and is a randomly generated number between .5and1.
step obtain the predictions of the synthetic instances using the global model.
in step only the features of a synthetic neighbor x prime are generated.
hence pyexplainer uses the global model to obtain the predictions i.e.
whether it is defective or clean given features of a syntheticinstance .
this allows pyexplainer to learn the behaviour ofthe underlying global model.
step build a local rule based regression model using the rulefit technique.
to build a local model f prime pyexplainer uses a rule based logistic regression technique called rulefit .
rulefit is a classifier that combines treeensembles and linear models which allows us to interpret themodel like a traditional regression model while understandingthe logical reasons learnt from the rule features.
broadly speaking rulefit will first generate rule features x prime r e.g.
churn reviewers based on ensemble decision trees e.g.
gradient boosting trees .
then rulefit uses a regression model i.e.
logistic regression forbinary outcomes or linear regression for continuous outcomes to model the association between the predicted outcomes y prime 409a local model a global blackbox model an explanation an instance to be explained step generate synthetic instances step obtain predictions from the global model y synthetic neighbors around the instance to be explainedy f t t f fx neighborhoodsynthetic neighborsan instance to be explained actual neighbors step extract an explanation step build a rule based regression model la dev score coef.
synthetic neighbors fig.
an overview of the pyexplainer approach.
given an instance to be explained pyexplainer produces four maincomponent i.e.
synthetic neighbors a local model and an explanation.
each pyexplainer s explanation producesthree pieces of information i.e.
a rule based explanation an importance score and the direction of relationshipof either supporting or contradicting the prediction.
and the rule features x prime r together with the original features x .
then the degree of importance and the coefficients of rule features and the original features can be analyzed from this regression model.
the use of rulefit in our pyexplainer will address the limitation of lime which does not account for interactionsbetween features i.e.
the combination of rule features .
al though existing rule based model agnostic techniques e.g.
sqaplanner anchors and lore have beenproposed these techniques employed association rule miningtechniques e.g.
apriori fp growth which do not provide thedegree of importance of the rules and the coefficients.
withoutthe degree of importance of the rules and the coefficientsprovided by such techniques we cannot quantify how strongthe association between the rules and the predicted outcomeand what the direction of the relationship is.
step extract an explanation from the local rulebased model.
to generate an explanation our pyexplainer analyzes the local model which is built using the rulefit tech nique in step .
in the local model there are three key piecesinformation rule features importance scores and coefficients.
the importance score indicates the strength ofthe association between the rule feature and the predictedoutcome.
the coefficient can be used to indicate the directionof the relationship.
for example a positive coefficient indicatesthat a rule feature has a contribution towards the prediction ofthe true class i.e.
defect .
based on the three key pieces of information in the localmodel our pyexplainer generates an explanation by identify ing the rule feature that has the highest importance score hasa positive coefficient and satisfies the actual feature values ofthe instance to be explained.
for example suppose that a rulefeature churn reviewers has the highest importance score and has a positive coefficient our pyexplainer willgenerate the following rule based explanation churn reviewers defect which means that a commit is predicted as defective since churn is greater than and thenumber of reviewers is less than .
iv .
e xperimental design in this section we present the studied datasets and explain the details of our experimental design.
studied jit datasets.
we select just in time defect datasets from two large scale open source software projects i.e.
openstack and qt as provided by mcintosh and kamei .openstack is an open source software for cloud infrastruc ture service.
qt is a cross platform application developmentframework.
we choose openstack and qt datasets for ourstudy since both datasets are often used as a benchmark indefect prediction studies and are manuallyverified for the validity of the szz algorithm to reducethe number of false positives and false negatives .
tablei provides an overview of the studied datasets.
commit features.
for each dataset there are commit level features that span across dimensions i.e.
size e.g.
lines added lines deleted diffusion e.g.
modified files history developers experience and code review activities.
experiment design.
figure presents an overview of our experimental design which is composed of four main steps.
step split data into training and testing datasets.
to ensure that the evaluation of our just in time defect prediction reflects a real world scenario we first sort the date of the com mits to preserve the order of the commits in a chronologicalorder .
then we use a time wise hold out validationtechnique as used by mcintosh and kamei to split thedataset into training and testing datasets.
the useof the time wise hold out validation technique ensures that thecommits that appear later will not be used in model training.similarly the commits that appear earlier will not be used inmodel evaluation.
step build jit defect models.
for each training dataset we first mitigate collinearity using autospearman andhandle class imbalance using smote prior to build jit defectmodels.
below we describe each step in detail.
step mitigate collinearity using autospearman.
to ensure that the interpretation of our jit defect models is highlyaccurate we mitigate collinearity and multi collinearity assuggested by prior studies .
we use autospearman an automated feature selection approach to automatically se lect one feature from each group of highly correlated featuresthat shares the least correlation with the other features that arenot in the group .
as suggested by kraemer et al.
we use a threshold of .
to indicate strong correlation betweenfeatures.
as suggested by fox we use a vif threshold 410table i an overview of the studied jit defect datasets provided by mcintosh and kamei .
training data testing data project start date end date commits defective commits start date end date commits defective commits openstack qt rq2 local models jit defect dataset training data pyexplainer lime synthetic neighbors global jit defect modelsexplanations synthetic neighborrq1 step build jit defect modelsbuild local modelsgenerate explanationsrq3 step evaluate pyexplainer and lime model agnostic techniques to answer research questions buildgenerate generate generate step split train test data an instance to be explained in testing data step apply pyexplainer and lime model agnostic techniques to generate explanations fig.
an overview of the experimental design.
value of to indicate multicollinearity.
we use the implementation of autospearman as provided by the pyexplainer python package.
after using autospearman we finally select7 features that are not highly correlated with each other.
step handle class imbalance using smote.
to ensure that the predictions of our jit defect models arehighly accurate we apply a class rebalancing technique assuggested by prior work .
since the defective ratio ofour studied jit defect datasets are highly imbalanced i.e.
we apply smote to handle class imbalance only on the training dataset.
we choose the smote technique as suggested by prior work who found that thesmote technique outperforms other class rebalancing tech niques.
smote performs the following steps.
first smotecalculates the k nearest neighbors of a set of minority class.
then smote randomly chooses the neighbors and generatessynthetic instances around such neighbors.
finally smotecombines the synthetic instances with the undersampling ofthe majority class to produce the final set of balanced in stances.
we use the implementation of smote as providedbyimbalanced learn python library .
we use the default setting k of smote since our experiment with variousksettings has shown that varying the ksettings has little impact on the performance of our jit defect models.
step evaluate global jit defect models.
we build global jit defect models using the training data of eachproject.
we select the two classification techniques that arecommonly used in prior studies since they found thatrandom forests rf and logistic regressions lr oftenoutperform other classification techniques.
then we evaluatethe global jit defect models on the testing dataset using2 effort aware measures i.e.
recall effort and p opt and traditional performance measures i.e.
area underthe roc curve auc and f1 with a cutoff threshold of0.
.
we select classifiers using recall effort to ensurethat they are practical when they are deployed in practices .
recall effort measures the percentage of correctlypredicted defect introducing commits that can be found wheninspecting the top loc of the most risky commits.
our global jit defect models trained using both rf and lr techniques achieve similar performance for bothopenstack and qt projects.
table ii presents the accuracy of the jit defectt models.
for openstack our rf classifierachieves a recall effort of .
for rf and .
for lr indicating that our jit defect models can correctly predicted54 of defect introducing commits when spending only20 code inspection effort i.e.
loc .
similarly for qt ourrf classifier achieves a recall effort of .
for rfand .
for lr indicating that our jit defect models cancorrectly predicted of defect introducing commitswhen inspecting only code inspection effort i.e.
loc .
step apply our pyexplainer and the lime modelagnostic techniques to generate explanations.
for each prediction of jit defect models we apply our pyexplainerand lime to generate an explanation of each prediction.we choose lime as a baseline comparison since lime hasbeen widely used in se research similar topyexplainer lime produces three main components syn thetic neighbors local models and explanations.
limeperforms the following four steps to produce an explanation.
first lime randomly generates synthetic neighbors surrounding an instance to be explained using a random perturba tion method with an exponential kernel function of euclideandistance.
second lime obtains the predictions of the syn thetic neighbors from the global jit defect models.
third lime builds a local sparse linear regression model k lasso using the randomly generated instances and their predictionsfrom the global jit defect models.
forth lime generatesan explanation using the coefficients of the local k lassomodel with three key pieces of information a decision 411table ii the accuracy of jit defect models that are trained using random forest rf and logistic regression lr .
openstack classification recall effort popt auc f1 random forest .
.
.
.
logistic regression .
.
.
.
qt classification recall effort popt auc f1 random forest .
.
.
.
logistic regression .
.
.
.
rule feature the importance score and the directionof relationship of either supporting or contradicting theprediction towards a true class i.e.
defect .
step evaluate the pyexplainer and lime modelagnostic techniques.
both pyexplainer and lime use different techniques to generate synthetic neighbors i.e.
crossoverand mutation vs. random perturbation and the local models i.e.
rulefit vs. k lasso .
thus they may produce differentexplanations.
therefore we aim to investigate which model agnostic technique is the best to generate an explanation of theprediction obtained from jit defect models.
to evaluate py explainer and lime we focus on the three main componentsalong two dimensions.
in the first dimension we focus on thecommon internal mechanism of the model agnostic techniquesi.e.
the synthetic neighbour and the accuracy of their localmodels since these two components are used to generatean explanation for a prediction.
in the second dimension we focus on the explanations generated by pyexplainer andlime.
we describe the analysis approach for each researchquestion in section v. v. e xperimental results in this section we present the approach and results with respect to our three research questions.
rq1 does our pyexplainer produce better synthetic neighbors than lime for jit defect models?
approach.
to address rq1 we analyze the distance between an instance to be explained and the synthetic instances around the neighbourhood using the euclidean distance measure.
theeuclidean distance measure is the calculation of distancebetween two feature vectors in an n dimensional feature space i.e.
d i i2 radicalbig summationtextn j i1j i2j whered i1 i2 is a euclidean distance of two instances i1andi2 .
the smaller the distance the higher similarity of the both vectors instances .
thus the lower distances between an instance tobe explained and the synthetic instances indicate that suchgenerated synthetic instances yield high similarity with theinstance to be explained.
for each instance to be explained in the testing dataset we calculate the euclidean distance between the instance to beexplained and their synthetic instances.
since the data is notnormally distributed we compute the median value insteadof the average of the euclidean distance to approximate the openstack qt rf lr rf lr010002000300040005000pyexplainer lime fig.
rq1 the euclidean distance of neighborhood instances and instances to be explained obtained from model agnostic techniques i.e.
pyexplainer and lime .
average similarity of the instances around the neighbourhood.
then we compare the distributions of the median euclideandistance of the synthetic neighbors produced by both pyex plainer and lime.
finally we apply two statistical test i.e.
wilcoxon singedrank test and cliff s effect size to identify whether differences of the euclidean distance produced by pyexplainerand lime are statistically significant.
the wilcoxon signed rank test is a non parametric test that measures the differenceof distribution between two population i.e.
the euclideandistance of pyexplainer and lime .
cliff s is a nonparametric effect size test that measures the magnitude ofthe differences of the given two distributions.
we use thecliff s interpretation of romano et al.
as follows i.e.
negligible for .
small for .
medium for .
and large for .
finally we compute the relative percentage difference using the following equation diff pyexplainer lime lime .
results.
the synthetic neighours produced by our py explainer is more similar to an instance tobe explained than lime for both rf and lr jitdefect models.
figure shows that our pyexplainer achieves and lower euclidean distance for bothrf and lr jit defect models for both openstack and qt.for openstack we find that our pyexplainer achieves amedian euclidean distance of while lime achieves amedian euclidean distance .
for qt we find that ourpyexplainer achieves a median euclidean distance of while lime achieves a median euclidean distance .
thewilcoxon signed ranked test confirms that the distributionsof the euclidean distance of our pyexplainer is statisticallysignificantly smaller than lime p value .
with a large cliff s effect size for both classifiers and both projects.
this finding indicates that our pyexplainer produces synthetic neighbors that are more closely similar to an instanceto be explained than lime.
the less similarity of syntheticneighbors generated by lime has to do with the use of randomperturbation approach.
the random perturbation approach per412 openstack qtauc f1 rf lr rf lr0.
.
.
.
.
.
.
.
.
.00pyexplainer lime fig.
rq2 the accuracy of the local models produced by our pyexplainer and lime in terms of auc and f1.
turbs an instance to be explained by adding a value randomly drawn from a normal distribution.
however such a simplerandom perturbation approach is not suitable for sparse andhigh dimensional data like jit datasets .
in particular the random perturbation approach does not account for thecharacteristics of the actual jit datasets.
thus we found thatthe random perturbation approach often generates syntheticneighbors that are less similar to an instance to be explainedthan our pyexplainer.
on the other hand our pyexplainergenerates synthetic neighbors based on the characteristicsof jit dataset using the crossover and mutation operations producing a more accurate explanation for the predictions ofjit defect models.
rq2 does our pyexplainer produce higher accuracy of local models than lime for jit defect models?
approach.
to address rq2 we analyze the accuracy of the local models generated by pyexplainer and lime.
the accuracy of local models indicates how well local modelscan approximate or mimic the logic of the global models.to do so we first obtain the predicted class i.e.
cleanand defect of the synthetic instances from the global jitdefect models.
then we obtain the probability of clean anddefect class of synthetic instances from the local models.then we evaluate the accuracy of the predictions between thelocal models and the global jit model using two traditionalperformance measures i.e.
area under the roc curve auc and f1.
similar to rq1 we apply the wilcoxon signed ranktest and the cliff s effect size test to evaluate whether the accuracy of local models of pyexplainer are statisticallysignificantly higher than lime.
results.
pyexplainer produces local models that are fig.
rq2 the probability y axis of synthetic instances predicted by the local models of pyexplainer and lime when comparing to the actual class of that instances i.e.
the legendof defect and clean from the rf and lr global jit defectmodels.
more accurate than the lime s local models.
figure shows that the local models produced by our pyexplainer achieve a median auc of .
when explaining the rf andlr jit defect models for both openstack and qt.
on the otherhand the local models produced by lime achieves a medianauc of .
for openstack and .
for qt when explainingthe rf jit defect models while achieving a median aucof .
for openstack and .
for qt when explaining thelr jit defect models.
this indicates that our pyexplainerproduces local models that are and moreaccurate auc than lime for both rf and lr jit defectmodels.
finally we observe a similar conclusion when usingf measure i.e.
pyexplainer produces local models that are242.
.
and .
.
more accurate f1 thanlime for both rf and lr jit defect models.
the wilcoxonsigned ranked test confirms that the accuracy of local mod els produced by our pyexplainer is statistically significantlyhigher than the accuracy of local models produced by lime p value .
with a large cliff s effect size.
the more accurate local models i.e.
high auc produced by our pyexplainer have to do with the quality of syntheticneighbors generated by our pyexplainer.
first our pyex plainer uses crossover and mutation techniques to generatesynthetic neighbors.
thus the synthetic neighbors are moreclosely similar to an instance to be explained and more similarto the actual characteristics of defect introducing commits andclean commits from the training data.
therefore we performa deeper investigation to better understand the distributionof the probability of synthetic neighbours generated by the 413instance to be explained generated instances global modeltraining data neighborhood lime produces rq1 less similar synthetic neighbors and rq2 a less accurate local model.pyexplainer produces rq1 more similar synthetic neighbors and rq2 a more accurate local model.
fig.
the characteristics of synthetic neighbors generated by pyexplainer and lime.
pyexplainer and lime local models.
figure shows that the median probability of defect class .
.
and clean class .
.
generated by the pyexplainer local models differsby .
.
.
on the other hand the median probability ofdefect .
.
and clean .
.
classes generated bythe lime local models differs by .
.
.
this findingsindicates that the pyexplainer local models have a higherability to discriminate the characteristics between defectand clean classes producing higher auc than the limelocal models.
finally we illustrate the characteristics of synthetic neighbors generated by pyexplainer and lime in figure .
inrq1 the smaller euclidean distance by pyexplainer indicatesthat pyexplainer produces synthetic neighbors that are moresimilar to an instance to be explained and the actualcharacteristics of the jit defect datasets due to the use ofcrossover and mutation operations on training data.
in rq2 the higher auc and f1 by pyexplainer indicates that ourpyexplainer produces better synthetic neighbors leading tomore accurate local models than lime.
therefore the expla nation generated by pyexplainer is more closely similar to theexplanation of an instance to be explained than the explanationgenerated by lime.
rq3 is our pyexplainer more effective in generating explanations than lime for jit defect models?
approach.
to address rq3 we analyze the explanations produced by pyexplainer and lime using the two measures.
unique measures the percentage of unique explanations generated by each technique.
the higher percentage of unique explanations indicates that a model agnostic tech nique can effectively generate a more specific i.e.
lessduplicate explanation to the instance to be explained.
consistency measures the percentage of the defectintroducing commits in the testing data that have char acteristics satisfying the rule feature in the generatedexplanation.
the higher percentage of the consistencyindicates that a model agnostic technique can effectivelygenerate an explanation that is consistent with the actualcharacteristics of defect introducing commits.
openstack qt rf lr rf lr0255075100pyexplainer lime fig.
rq3 the percentage of the defect introducing commits in the testing data that are consistent with the generatedexplanation.
results.
the explanations generated by our pyexplainer are more unique i.e.
more specific to aninstance to be explained than lime.
we find that pyexplainer can produce unique explanations for all ofthe instances to be explained for both studied datasets.
onthe other hand lime can produce as few as uniqueexplanations for openstack and unique explanationsfor qt.
in other words for openstack we find that as muchas of defect introducing commits have the sameexplanation despite having different characteristics of thefeature values.
similarly for qt we find that as much as of commits have the same explanation despite havingdifferent characteristics of the testing instances.
thus the lessduplicate explanations generated by pyexplainer indicates thatpyexplainer can generate explanations that are more specificto an instance to be explained rather than lime.
the explanations generated by our pyexplainer are more consistent with the actual defect introducing commits in the testing data than lime.figure shows that pyexplainer achieves a median con sistency of for openstack and for qt.on the other hand we find that lime achieves a medianconsistency of for openstack and for qt.
theexperiment result indicates that the explanations generated byour pyexplainer are and more consistentwith the actual defect introducing commits in the testing datathan lime for openstack and qt respectively.
the wilcoxonsigned ranked test confirms that the percentage consistencyvalue of pyexplainer is statistically significantly higher thanlime p value .
with a large cliff s effect size for both jit defect models and both studied datasets.
vi.
d iscussion in this section we first discuss the usage scenario of how pyexplainer can be used in practice.
then we present ananalysis of the what if simulation when the explanations were 414considered i.e.
what if we change this would it reverse the predictions of the jit defect models?
.
finally we describe the implementation details of the pyexplainer python package.
a. a usage scenario let s consider bob as a developer in a large scale software project that adopts modern code review practices.
bob has his main responsibility to inspect commits that are submittedby other developers to ensure the quality of commits prior tointegration into the release branch.
suppose that on averagebob spends one hour to review one commit.
hence withhis average working hours per day he can review only8 commits per day.
given a huge number of newly arrivedcommits everyday e.g.
commits per day bob does notknow which commits should be reviewed first.
with the useof jit model the list of commits can be prioritized basedon the likelihood of being defect introducing provided by thejit defect model so that bob can efficiently spend his limitedtime on the most risky commits.
however bob still may notbe convinced by the predictions of jit defect models sincehedoes not understand why a commit is predicted as defectintroducing.
thus bob may not trust the jit defect models andmay decide to ignore the predictions resulting in suboptimalsqa resource allocation and prioritization.
with pyexplainer bob now better understands why a commit is predicted as defect introducing since pyexplainerprovides an explanation i.e.
which feature is the most impor tant for a given prediction .
for example pyexplainer providesan explanation e.g.
churn defect that a commit is predicted as defect introducing because the churn size isgreater than .
this kind of explanation could help bob tofocus on the most important aspects that are associated withthe risk of being defect introducing i.e.
considering reducingthe churn size of the commit instead of focusing on the lessimportant aspects e.g.
inviting more reviewers .
however it remains challenging for bob to consider which value ofa feature that should be changed to mitigate the risk.
inparticular given an explanation e.g.
churn defect bob still does not know how small a churn value should bethat could reverse the prediction of jit models from defectto clean.
thus an interactive what if visualization tool is needed to help bob making better decisions of how much thechurn value that should be changed.
b. what if analysis we conducted a what if simulation based on a hypothetical scenario if the explanations of our pyexplainer were considered.
in particular we investigated what if we change the value of a feature guided by the explanation would it reverse the pre diction of the jit defect model?.
for example an explanation churn defect generated by pyexplainer means that a commit is predicted as defect introducing since churnis greater than .
thus what if churn was less than would the prediction be reversed from defect to clean.
to conduct this what if simulation we first generate a simulated instance.
the simulated instance is an instance where the84 openstack qt rf lr rf lr0255075100 a the percentage of the simulated instances that can reversethe prediction from defect toclean.
openstack qt rf lr rf lr0255075100 b the difference between the probability of the original in stance and the probability of thesimulated instance.
fig.
the results of the what if analysis.
actual value of a feature guided by the rule based explanation was changed in the opposite direction of the comparison op erator of the explanation i.e.
decrease for increase for by one sd a standard deviation of that feature in the trainingdata from the rule threshold.
according to the above example the simulated instance is the modified original instance wherethe actual value of a feature e.g.
churn guided bythe rule based explanation churn defect was changed in the opposite direction of the comparison operatorof the explanation i.e.
decrease for by one sd e.g.
from the rule threshold .
thus the churn value of thesimulated instance is i.e.
.
then we input thissimulated instance to the global jit defect model and analyzewhether the simulated instance could reverse the predictionsof the global jit defect models.
we perform this what if simulation for all the explanations generated by our pyexplainer for all of the commits in the test ing dataset that are correctly predicted as defect introducing bythe jit defect models.
then we measure reversed i.e.
the percentage of the simulated instances that can reverse theprediction from defect to clean and prob diff i.e.
the difference between the probability of the original instanceand the probability of the simulated instance.
figure 8a shows that when considering the explanations guided by our pyexplainer rf and lr of the simulated instances that can reverse the predictionfrom defect to clean of the global jit defect models.furthermore figure 8b also shows that after considering theexplanations guided by our pyexplainer the probability of thesimulated instance is decreased by for the rf modelsand for the lr models when comparing to theprobability of the original instance.
this simulation highlightsthe importance of our pyexplainer for helping practitioners tofocus on the most important aspects that are associated withthe risk of being defect introducing for a given commit insteadof focusing on the less important aspects.
nevertheless the onesd used in this what if simulations is just an example theactual changed value should be subject to the domain experts.
415why this commit is predicted as defect introducing?
the value of linesadded is more than actual the value of reviewrevisions is less than actual the value of reviewers is less than actual the value of linesadded is more than the value of reviewrevisions is less than the value of reviewers is less than 3risk score a the visual explanation of the original instance predicted as defect with a risk score of .
why this commit is predicted as defect introducing?
the value of linesadded is more than actual the value of reviewrevisions is less than actual the value of reviewers is less than actual the value of linesadded is more than the value of reviewrevisions is less than the value of reviewers is less than 2risk score b the visual explanation of the simulated instance when changing the feature values predicted as clean with a risk score of .
fig.
the proof of concept visualization of our pyexplainer consists of the risk score i.e.
the probability of an instance to be explained by the global jit model the visual explanation in the black border and the interactive what ifvisualization for our pyexplainer.
c. the pyexplainer python package to ease the adoption of our pyexplainer by practitioners and to facilitate the replication of future research we developedthe pyexplainer python package.
in our pyexplainer pythonpackage we also developed a proof of concept of the visualexplanation and the interactive what if visualization.
the visual explanation is developed to present the rulebased explanation in a form of a bullet plot visualizationwith textual explanations.
figure 9a shows an example of thevisual explanation of an openstack commit a9a59cc .
ourvisual explanation is designed to provide the following keyinformation textual descriptions that explain why a commitis predicted as defect introducing the actual feature valuesof the commit i.e.
the vertical black bars and therange of feature values associated with the risk score i.e.
the predicted probability .
the green shades indicate the non risky range values of a feature while the red shades indicatethe risky range values of a feature.
an interactive what if visualization is developed to help practitioners interactively change the value of a feature whileimmediately generating the new estimated risk score i.e.
the probability obtained from the jit defect model .
thisvisualization will allow practitioners to explore different valuesof a feature prior to making a decision.
figure 9b shows an example of an interactive what if visualization for an openstack commit a9a59cc .
through thevisualization the user can change the value of the feature e.g.
changing the value of lines added from to the value ofreview revisions from to and the value of reviewers from to .
then the visualization will responsively updatethe predicted probability generated by the jit defect model e.g.
from to .
d. implications to practitioners and researchers the contributions of this paper build an important step towards a new research area of explainable ai for se by making the predictions of just in time defect models moreexplainable and actionable.
a lack of explainability and ac tionability of software analytics has been raised by bothpractitioners and researchers .for example rajapaksha et al.
proposed an approach to generate actionable suggestions i.e.
counterfactual explana tions for file level defect prediction.
peng and menzies also proposed a timelime approach an extension of limemodel agnostic technique to generate actionable suggestions i.e.
defect reduction plans .
however the approaches ofboth rajapaksha et al.
and peng and menzies are designed for release based defect prediction which requiremultiple releases for training and evaluation.
thus they arenot applicable to jit defect prediction models.
on the otherhand our results show that our pyexplainer is more effectivein generating explanations than lime for the predictions ofjit defect models while providing an interactive what ifvisualization so practitioners can make better data informeddecisions.
similar effort to other state of the art model agnos tic techniques e.g.
lime we make our pyexplainerpython package publicly available to ease the adoption bypractitioners and researchers.
e. threats to v alidity threats to construct validity relates to the hyperparameter settings of randomforest smote and lime techniques when conducting our experiment .
to ensure thereprehensibility the used parameter setting of such techniquesare reported in the replication package in our github reposi tory the replication package branch .
threats to internal validity relates to the randomization of our pyexplainer i.e.
the neighbour generation process .
tomitigate any conclusion instability threat we chose to generate2 neighbours.
after we repeated the experiment five times the conclusion of our paper remains the same.
nevertheless future work can explore what would be the minimum syntheticneighbours that can produce stable local explanations i.e.
thesame local explanations when they are regenerated .
threats to the external validity relates to the generalizability of our pyexplainer approach.
however our experiment onlyfocused on the just in time defect prediction problem with thelimited number of the studied classification techniques and thelimited number of studied projects.
thus other classificationtechniques and other projects should be explore in future work.
416vii.
c onclusion prior studies proposed just in time jit defect prediction yet its explanability remains largely unexplored i.e.
practitioners still do not know why a commit is predicted as defect introducing .
in this paper we propose pyexplainer a novellocal rule based model agnostic technique for explaining thepredictions of jit defect models.
through a case study of twoopen source software projects we find that our pyexplainerproduces synthetic neighbours that are moresimilar to an instance to be explained moreaccurate local models and explanations that are more unique and more consistent with the actualcharacteristics of defect introducing commits in the future thanlime a state of the art model agnostic technique .
pyexplainer is designed for explaining the predictions of any classification problems.
future work should explore if ourpyexplainer can be used to effectively explain the predictionsof other classification problems e.g.
vulnerability prediction code smell detection in software engineering.
publishing the pyexplainer python package.
to ease the adoption of our pyexplainer by practitioners and to facilitatethe replication of future research the pyexplainer packageis available in both conda andpip package installer for python .
our pyexplainer python package also has a codecoverage of measured by codecov with an a qualitygraded by lgtm.
g yy viii.
a t utorial of the pyexplainer package .
below we present a tutorial of how to use the pyexplainer python package step by step using code block .
step the pyexplainer package is installed using pip python package management system .
step the pyexplainer package is imported.
step the data for demonstration is obtained from pyexplainer package.
the data is composed of x train y train indep dep blackbox model x explain y explain.
the x train variables are used to generate neighborhood instances.
the indep anddep variables specify the feature names and label respectively.
theblackbox model is the global jit defect models from scikit learn module.
the x explain represents an instance to be explained while the y explain is the label of the instance to be explained.
step a pyexplainer object is created and an explanation is obtained from the explain function.
step the visual explanation and the what if visualization are generated by the visualise function.
ix.
a cknowledgement chakkrit tantithamthavorn was supported by the australian research council s discovery early career researcheraward decra funding scheme de200100941 .
patanamonthongtanunam was supported by the australian researchcouncil s discovery early career researcher award de cra funding scheme de210101091 .
step install the pyexplainer package !pip install pyexplainer step import necessary libraries 4from pyexplainer.pyexplainer pyexplainer import pyexplainer as pyexp 5from pyexplainer import pyexplainer pyexplainer step get the preprocessed data and global model to be tuned in to the pyexplainer object 7dflt pyexplainer pyexplainer.get dflt step create a pyexplainer object using the preprocessed data and model and generate rules by utilising the built in local model in pyexplainer 9exp pyexp x train dflt y train dflt indep dflt dep dflt blackbox model dflt 14rules exp.explain x explain dflt y explain dflt step visualise the rules generated by the local model and the prediction generated by the global model 17exp.visualise rules code block an example tutorial of the pyexplainer pythonpackage.