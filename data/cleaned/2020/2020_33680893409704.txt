do the machinelearning modelsona crowdsourcedplatform exhibit bias?an empirical study onmodelfairness sumon biswas dept.
of computerscience iowastateuniversity ames ia usa sumon iastate.eduhridesh rajan dept.
of computerscience iowastateuniversity ames ia usa hridesh iastate.edu abstract machine learning models are increasingly being used in important decision making software such as approving bank loans recommending criminal sentencing hiring employees and so on.
it is important to ensure the fairness of these models so that no discriminationismadebasedon protectedattribute e.g.
race sex age whiledecisionmaking.algorithmshavebeendevelopedtomeasure unfairness andmitigate them to a certain extent.in this paper we have focused on the empirical evaluation of fairness and mitigations on real world machine learning models.
we have created a benchmark of top rated models from kaggle used for different tasks and then using a comprehensive set of fairness metrics evaluatedtheirfairness.then wehaveapplied7mitigationtechniques on these models and analyzed the fairness mitigation results and impactsonperformance.wehavefoundthatsomemodeloptimization techniques result in inducing unfairness in the models.
on the otherhand althoughtherearesomefairnesscontrolmechanisms inmachinelearninglibraries theyarenotdocumented.themitigation algorithm also exhibit common patterns suchas mitigation inthepost processingisoftencostly intermsofperformance and mitigation in the pre processing stage is preferred in most cases.
we have also presented different trade off choices of fairness mitigationdecisions.
ourstudysuggestsfuture researchdirectionsto reduce the gap between theoretical fairness aware algorithms and the software engineeringmethodsto leverage themin practice.
ccs concepts softwareanditsengineering softwarecreationandmanagement computingmethodologies machine learning.
keywords fairness machine learning models acmreference format sumon biswas and hridesh rajan.
.
do the machine learning models ona crowdsourced platformexhibitbias?anempirical study onmodel fairness.in proceedingsofthe28thacmjointeuropeansoftwareengineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse november8 13 virtualevent usa.
acm newyork ny usa 12pages.
esec fse november 8 13 virtual event usa copyright held by the owner author s .
acm isbn .
introduction sincemachinelearning ml modelsareincreasinglybeingusedin making important decisions that affect human lives it is important to ensure that the prediction is not biased toward any protected attribute such as race sex age marital status etc.
ml fairness has been studied for about past years and several fairness metricsandmitigationtechniques have been proposed.
many testing strategies have been developed to detect unfairness in software systems.
recently a few toolshavebeenproposed toenhancefairnessofml classifiers.
however we are not aware howmuch fairnessissues existinmlmodelsfrompractice.dothemodelsexhibitbias?ifyes whatarethedifferentbiastypesandwhatarethemodelconstructs related to the bias?
also is there a pattern of fairness measures when different mitigation algorithms are applied?
in this paper we have conducted an empirical study on ml models to understand these characteristics.
harrison etal.studiedhowmlmodelfairnessisperceivedby502 mechanical turk workers .recently holstein et al.conducted anempiricalstudyonmlfairnessbysurveyingandinterviewing industrypractitioners .
theyoutlined thechallengesfaced by the developers and the support they need to build fair ml systems.
they also discussed that it is important to understand the fairness ofexistingmlmodelsandimprovesoftwareengineeringtoachieve fairness.inthispaper wehaveanalyzedthefairnessof40mlmodels collected from a crowd sourced platform kaggle and answered the following researchquestions.
rq1 unfairness what are the unfairness measures ofthe ml models in the wild and which of them are more or less prone to bias?
rq2 bias mitigation what are the root causes of the bias in ml models and what kind of techniques can successfully mitigate thosebias?
rq3 impact what are the impacts of applying different bias mitigating techniques onml models?
first we have created a benchmark of ml models collected fromkaggle.wehavemanuallyverifiedthemodelsandselected appropriate ones for the analysis.
second we have designed an experimental setup to measure achieve and report fairness of the ml models.
then we have analyzed the result to answer the research questions.
the key findings are model optimization goals are configured towards overall performance improvement causing unfairness.afewmodelconstructsaredirectlyrelatedtofairnessof themodel.however mllibrariesdonotexplicitlymentionfairness in documentation.
models with effective pre processing mitigation algorithm are more reliable and pre processing mitigations always retain performance.
we have also reported different patterns of 642this work is licensed under a creative commons attribution international .
license.
esec fse november8 virtualevent usa sumonbiswas andhrideshrajan exhibiting bias and mitigating them.
finally we have reported the trade offconcernsevident for thosemodels.
the paper is organized asfollows 2describesthebackground and necessary terminology used in this paper.
in we have described the methodology of creating the benchmark and setting upexperiment anddiscussedthefairnessmetricsandmitigation techniques.
4describes the fairness comparison of the models 5describes the mitigation techniques and 9describes the impacts of mitigation.
we have discussed the threats to validity in describedthe relatedwork in andconcludedin .
background the basic idea of ml fairness is that the model should not discriminatebetweendifferentindividualsorgroupsfromtheprotected attribute class .protected attribute e.g.
race sex age religion is an input feature which should not affect the decision making of the models solely.
chen et al.listed protected attributes for fairness analysis .
one trivial idea is to remove the protectedattributefromthedatasetandusethatastrainingdata.
pedreshiet al.showed that due to the redundant encoding of trainingdata itispossiblethatprotectedattributeispropagatedtoother correlatedattributes .
therefore we needfairnessawarealgorithmstoavoidbiasinmlmodels.inthispaper wehaveconsidered both group fairness and individual fairness.
group fairness measureswhetherthemodelpredictiondiscriminatesbetweendifferent groups in theprotected attributeclass e.g.
sex male female .
individualfairness measureswhethersimilarpredictionismadefor similarindividualsthoseareonlydifferentinprotectedattribute .
based on different definitions of fairness many group and individualfairnessmetricshavebeenproposed.additionally many fairness mitigation techniques have been developed to remove unfairnessorbiasfromthemodelprediction.thefairnessmetricsand mitigationtechniques have been describedinthe nextsection.
methodology inthissection first wehavedescribedthemethodologytocreate thebenchmarkofmlmodelsforfairnessanalysis.thenwehave described our experiment design and setup.
finally we have discussedthefairnessmetricsweevaluatedandmitigationalgorithms we appliedoneachmodel.
.
benchmark collection we have collected ml models from kaggle kernels .
kaggle is one of the most popular data science ds platform owned by google.
data scientists researchers and developers can host or takepartindscompetition sharedataset task andsolution.many kaggle solutions resulted in impactful ml algorithms and research suchasneuralnetworksusedbygeoffreyhintonandgeorgedahl improvingthesearchforthehiggsbosonatcern stateof the art hiv research etc.
there are competitions and 622datasetsinkaggletodate.theuserscansubmitsolutions forthecompetitionsanddataset specifictasks.tocreateabenchmark to analyze the fairness ofml models we have collected kernelsfromthekaggle.eachkernelprovidessolution codeand description for a specific data science task.
in this study we have analyzed ml models that operate on datasets utilized by prior german credit adult census bank marketing popular fairness datasets from literature protected attribute in dataset competitionkernel contains predictive model more than upvotes more than accuracy best model from a kernel total376 kaggle competition kaggle kernels prediction is favorable to a group individualkaggle kernels home credit titanic ml filtering criteria dataset kernels dataset kernelsselect top voted models for each dataset datasets models figure benchmarkmodelcollection process studies on fairness and datasets with protected attribute e.g.
sex race .
with this goal we have collected the ml models with different filtering criteria for each category.
the overall process of collecting the benchmarkhas been depictedinfigure .
toidentifythedatasetsusedinpriorfairnessstudies werefer totheworkonfairnesstestingbygalhotra etal.
wheretwo datasets german credit and adult census have been used.
udeshi et al.experimented on models for the adult census dataset .
aggarwal et al.used six datasets german credit adult census bank marketing us executions fraud detection and raw car rentals .amongthesedatasets germancredit adultcensus and bank marketing dataset are available on kaggle.
from the solutionsforthesedatasets wehavecollected440kernels 65for german credit for adult census and for bank marketing .
furthermore wehavefilteredthekernelsbasedonthreecriteria to select the top rated ones contain predictive models some kernelsonlycontainexploratorydataanalysis atleast5upvotes and accuracy .
often a kernel contains multiple models andtriestofindthebestperformingone.inthesecases wehave selected the best performing model from every kernel.
thus we haveselectedthetop8modelsbasedonupvotesforeachofthe3 datasets andgot ml models.
chenet al.
listed protected attributes e.g.
age sex race etc.
forfairness analysis.we have found 7competitions inkaggle that contain any of these attributes.
from the selected ones we havefilteredoutthecompetitionsthatinvolvepredictiondecisions not being favorable to individuals or a specific group.
for example although this competition has customers ageandsexin the dataset the classification task is to recommend an appropriate product to the customers which we can not classify as fair or unfair.thus wehavegottwoappropriatecompetitionswithseveral kernels.
to select ml models from these competitions we have utilizedthesamefilteringcriteriausedbeforeandselected8models for each dataset based on the upvotes.
finally we have created a benchmark containing top rated kaggle models that operate on5datasets.thecharacteristicsofthedatasetsandtasksinthe benchmarkare shownintable .
.
experiment design aftercreatingthebenchmark wehaveexperimentedonthemodels evaluatedperformanceandfairnessmetrics andapplieddifferent bias mitigation techniques to observe the impacts.
our experiment designprocessisshowninfigure .theexperimentsonthebenchmarkhave been peer reviewedandpublishedas an artifact .
in our benchmark we have models from five dataset categories.
tobeabletocomparethefairnessofdifferentmodelsineachdataset category wehaveusedthesamedatapreprocessingstrategy.we 643dothe machinelearning modelsonacrowdsourcedplatform exhibit bias?
anempirical studyonmodel fairness esec fse november8 virtualevent usa table the datasetsused inthefairness experimentation.
f feature count.
pa protected attribute.
dataset size fpadescription german credit 00021age sexthisdatasetcontainspersonalinformationaboutindividualsandpredictscreditrisk good or bad credit .
the ageprotected attribute is categorized into young and old basedon .
adultcensus 12race sexthis dataset comprises of individual information from the u.s. census.
the target featureofthisdatasetistopredictwhetheranindividualearns 000ornotinayear.
bankmarketing 20agethis dataset contains the direct marketing campaigns data of a portuguese bank.
the goal isto predict whether aclientwillsubscribe for aterm depositornot.
home credit 240sexthis dataset contains data related to loan applications for individuals who do not get loan from the traditional banks.
the target feature is to predict whether an individual who can repaythe loan getthe applicationacceptedornot.
titanic ml 89110sexthis dataset contains data about the passengers of titanic.
the target feature is to predict whether the passenger survived the sinking of titanic or not.
the target of the test set is not published.so we have taken the training data andfurther split itintotrainandtest.
post processing pre processing5 dataset data kernelfor each dataset data encoding process missing valuesshuffle split data train test select protected attribute select un privileged class select favorable label s preprocess data preprocess data for fairness kernel models8 kernels evaluate model performance accuracy f1 score7 fairness metrics di spd eod aod err cnt tiapply bias mitigation algorithms in processingfor each datasetextract best modeltrain modeltesttrain conduct this experiment times on each model and take mean of results fairness report for each model figure experimentationto computeperformance fairness andmitigationimpacts ofmachinelearning models.
have processed the missing or invalid values transformed continuousfeaturestocategorical e.g.
age young age old and converted non numerical features to numerical e.g.
female male .wehavedonesomefurtherpreprocessingtothedataset to be used for fairness analysis specify the protected attributes privilegedandunprivilegedgroup andwhatarethefavorablelabel or outcome of the prediction.
for example in the home credit dataset sexis the protected attribute where maleis the privileged group femaleis the unprivileged group and the prediction label is creditriskofthepersoni.e.
good favorablelabel orbad.forall thedatasets wehaveusedshufflingandsametrain testsplitting before feeding the data to the models.
for each dataset category we have eight kaggle kernels.
the kernelscontainsolutioncodewritteninpythonforsolvingclassificationproblems.ingeneral thekernelsfollowthesestages data exploration preprocessing feature selection modeling training evaluation and prediction.
from the kernels we have manually extractedthecodeformodeling training andevaluation.forexample thiskernel loadsthegermancreditdataset performs exploratoryanalysisandselectsasubsetofthefeaturesfortraining preprocesses data and finally implements xgboost classifier for predictingthecreditriskofindividuals.wehavemanuallysliced thecodefor modeling training and evaluation.
often thekernels trymultiplemodels evaluateresults andfindthebestmodel.from a single kernel we have only sliced the best performing model found by the kernel.some kernels donot specify the best model.
in this case we have selected the model with the best accuracy.forexample thiskernel worksonadultcensusdatasetand implements four models logistic regression decision tree knearest neighbor and gradient boosting for predicting income of individuals.
we have selected the gradient boosting classifier modelsince itgives the bestaccuracy.
afterextractingthebestmodel wetrainthemodelandevaluate performance accuracy f1 score .
we have found that the model performance in our experiment is consistent with the prediction made in the kernel.
then we have evaluated different fairness metricsdescribedin .
.
.next wehaveapplied7differentbias mitigation algorithms separately and evaluated the performance and fairness metrics.
thus we collect the result of metrics performance metric fairness metric before applying any mitigation algorithmandafterapplyingeachmitigationalgorithm.foreach model we have done this experiment times and taken the mean oftheresultsassuggested by .
we have usedtheopen source pythonlibraryaif360 developedbyibmforfairnessmetrics andbiasmitigationalgorithms.allexperimentshavebeenexecuted on a mac os .
.
having .
ghz intel core i7 processor with gb ram andpython3.
.
.
.
measures wehavecomputedthealgorithmicfairnessofeachsubjectmodel in our benchmark.
let d x y z be a dataset where xis the training data yisthe binaryclassification label y if the label isfavorable otherwise y zistheprotectedattribute z forprivilegedgroup otherwise z and yisthepredictionlabel 644esec fse november8 virtualevent usa sumonbiswas andhrideshrajan 1forfavorabledecisionand0forunfavorabledecision .ifthereare multiplegroupsforprotectedattributes wehaveemployedabinary groupingstrategy e.g.
raceattributeinadultcensusdatasethas been changedto white non white .
.
.
accuracy measure.
before measuring the fairness of the model we have computedthe performance in terms ofaccuracy andf1 score.
accuracy accuracy is given by the ratio of truly classified items andtotalnumber ofitems.
accuracy truepositive truenegative total f1score thismetricisgivenbytheharmonicmeanofprecision andrecall.
f1 precision recall precision recall .
.
fairnessmeasure.
manyquantitativefairnessmetricshave been proposed in the literature based on different definitions of fairness.
for example aif toolkit has apis for computing 71fairnessmetrics .inthispaper withoutbeingexhaustive a representative list of metrics have been selected to evaluate the fairness of ml models.
we have adopted the metrics recommendation of friedler et al.
and further added the individual fairness metrics.
metrics based on baserates disparateimpact di thismetricisgivenbytheratiobetweenthe probability of unprivileged group gets favorable prediction and the probability ofprivilegedgroup getsfavorable prediction .
di p p statistical parity difference spd this measure is similar to di but instead ofthe ratioofprobabilities difference iscalculated .
spd p p metrics based on group conditioned rates equal opportunity difference eod this is given by the truepositive rate tpr difference between unprivileged and privileged groups.
tpru p tprp p eod tpru tprp average odds difference aod this is given by the average of false positiverate fpr differenceandtrue positiveratedifference between unprivilegedandprivilegedgroups .
fpru p fprp p aod fpru fprp tpru tprp error rate difference erd error rate isgiven by the addition of false positive rate fpr andfalse negative rate fnr .
err fpr fnr erd erru errp metrics based on individual fairness consistency cnt thisindividualfairnessmetricmeasureshowsimilarthepredictionsarewhentheinstancesaresimilar .here n nei hbors isthe number of neighborsfor the knn algorithm.
cnt n n nei hborsn summationdisplay.
i yi summationdisplay.
j nn nei hbors xi yj theilindex ti thismetricisalsocalledtheentropyindexwhich measuresboththegroupandindividualfairness .theilindex isgiven bythe following equation where bi yi yi .
ti nn summationdisplay.
i 1bi lnbi .
bias mitigation techniques inthissection wehavediscussedthebiasmitigationtechniques that have been applied to the models.
these techniques can be broadlyclassifiedintopreprocessing in processing andpostprocessing approaches.
preprocessingalgorithms.
preprocessingalgorithmsdonotchange themodelandonlyworkonthedatasetbeforetrainingsothatmodels can produce fairer predictions.
reweighing inabiaseddataset differentweightsareassigned to reduce the effect of favoritism of a specific group.
if a class of inputhasbeenfavored thenalowerweightisassignedincomparison to the class not been favored.
disparate impact remover this algorithm is based on the conceptofthemetricdithatmeasuresthefractionofindividuals achieves positive outcomes from an unprivileged group in comparison to the privileged group.
to remove the bias this technique modifies the value of protected attribute to remove distinguishing factors.
in processingalgorithms.
in processingalgorithmsmodifythe ml modelto mitigate the biasinthe originalmodelprediction.
adversarial debiasing this approach modifies the ml model by introducing backward feedback negative gradient for predicting the protected attribute.
this is achieved by incorporating an adversarial model that learns the difference between protected and otherattributes that can be utilizedto mitigate the bias.
prejudiceremoverregularizer ifanmlmodelreliesonthedecisionbasedontheprotectedattribute wecallthatdirectprejudice.
in order to remove that one could simply remove the protected attribute or regulate the effect in the ml model.
this technique appliesthelatterapproach wherearegularizerisimplementedthat computes the effectofthe protectedattribute.
post processing algorithms.
this genre of techniques modifies the prediction result instead of the ml models orthe inputdata.
equalizedodds e thisapproachchangestheoutputlabels tooptimizetheeodmetric.inthisapproach alinearprogramis solvedto obtain the probabilitiesof modifyingprediction.
calibrated equalized odds to achieve fairness this technique also optimizes eod metric by using the calibrated prediction score producedbythe classifier.
reject option classification this technique favors the instancesinprivilegedgroupoverunprivilegedonesthatlieinthe decision boundary withhigh uncertainty.
645dothe machinelearning modelsonacrowdsourcedplatform exhibit bias?
anempirical studyonmodel fairness esec fse november8 virtualevent usa unfairness inml models inthissection wehaveexploredtheanswerofrq1byanalyzing different fairness measures exhibited by the ml models in our benchmark.dothemodelshavebiasintheirprediction?ifso which models are fairer andwhich are more biased?
what iscausing the models to be more prone to bias?
what kind of fairness metric is sensitive to different models?
to answer these questions we haveconductedexperimentonthemlmodelsandcomputedthe fairness metrics.
the result is presented in table .
the unfairness measuresforallthe40modelsaredepictedinfigure .tobeableto compareallthemetricsinthesamechart disparateimpact di and consistency cnt havebeenplottedinthelogscale.ifthevalueof a fairness metric is there is no bias in the model according to the corresponding metric.
if the measure is less than or greater than biasexists.thenegativebiasdenotesthatthepredictionisbiased towards privileged group and positive bias denotes that prediction isbiasedtowardsunprivilegedgroup.
wehavefoundthatallthemodelsexhibitunfairnessandmodelsspecifictoadatasetshowsimilarbiaspatterns.fromfigure we can see that all the models exhibit bias with respect to most ofthefairnessmetrics.foramodel metricvaluesvarysincethe metrics follow different definitions of fairness.
therefore we have compared bias of different models both cumulatively and using the specific metric individually.
to compare total bias across all the metrics we have taken the absolute value of the measures and computedthesumofbiasforeachmodel.infigure wecansee the total bias exhibited by the models.
although the bias exhibited bymodelsforeachdatasetfollowsimilarpattern certainmodels are fairer thanothers.
finding model optimization goals seek overall performance improvement whichiscausing unfairness.
model gc1 exhibits the lowest bias among german credit models.
gc1 is a random forest rft classifier model which is built by usingagridsearchoveragivenrangeofhyperparameters.after the grid search the bestfoundclassifier is 1randomforestclassifier bootstrap true ccp alpha .
class weight none criterion gini max depth max features max leaf nodes none max samples none min impurity decrease .
min impurity split none min samples leaf min samples split min weight fraction leaf .
n estimators n jobs none oob score false random state warm start false we have found that gc6 is also a random forest classifier built throughgridsearch.however gc6 isless fairin termsofcumulativebias figure andindividualmetrics figure excepterror ratedifference erd .wehaveinvestigatedthereasonofthefairnessdifferencesinthesetwomodelsbyrunningbothofthemby changing one hyperparameter at a time.
we have found that the fairness difference is caused by the scoring mechanism used by thetwo models.gc1uses scoring recall whereasgc6uses scoring precision as showninthe following code snippet.
model gc1 2param grid max depth n estimators max features 3gc1 randomforestclassifier random state 4grid search gridsearchcv gc1 param grid param grid cv scoring recall verbose model gc6 6params n estimators max depth random state n jobs 7gc6 randomforestclassifier 8grid search cv gridsearchcv gc6 params scoring precision further investigation shows in german credit dataset the data rows are personal information about individuals and task is to predicttheircreditrisk.thedataitemsarenotbalancedwhen sexofthe individualsisconcerned.thedatasetcontains69 datainstances ofmaleand femaleindividuals.
when the model is optimized towards recall gc1 rather than precision gc6 the total number of true positives decreasesandfalse negative increases.since thenumberofinstancesforprivilegedgroup male ismorethan the unprivileged group female decrease in the total number of true positives also increases the probability of unprivileged group to be classified as favorable.
therefore the fairness of gc1 is more than gc2 although the accuracy is less.
unlike other group fairness metrics error rate difference erd accounts for false positive and false negative rate difference between privileged and unprivilegedgroup.asdescribedbefore optimizingthemodelforrecall increasesthetotalnumberoffalse negatives.wehavefoundthat the percentage of malecategorized as favorable is less than the percentage of femalecategorized as favorable.
therefore an increase intheoverallfalse negative alsoincreased theerrorrate of unprivilegedgroup whichinturncausedgc1tobemorebiased thangc2 interms oferd.
from the above discussion we have observed that the model optimization hyperparameter only considers the overall rates of the performance.
however if we split the data instances based on protected attribute groups then we see the change of rates vary for different groups which induces bias.
the libraries for modelconstructionalsodonotprovideanyoptiontospecifymodel optimization goals specific to protected attributes and make fairer prediction.
here we have seen that gc1 has less bias than gc6 by compromising little accuracy.
do all the models achieve fairness by compromising withperformance?
we have found thatmodels can achievefairnessalongwithhighperformance.tocomparemodel performancewiththeamountofbias wehaveplottedtheaccuracy and f1 score of the models with the cumulative bias in figure .
we can see that gc6 is the most efficient model in terms of performance and has less bias than out of other models in german credit data.
ac6 has more accuracy and f1 score than any other modelsinadultcensus andexhibitslessbiasthanac1 ac2 ac4 ac5 and ac7.
therefore models can have better performance and fairnessat the same time.
finding libraries for model creation do not explicitly mentionfairnessconcernsinmodelconstructs.
from figure we can see that hc1 and hc2 show difference in mostofthefairnessmetrics whileoperatingonthesamedataset i.e.
home credit.
hc2 is fairer than hc1 with respect to all the metrics except di.
from table we can see that hc1 has positive bias whereashc2exhibitnegativebias.thisindicatesthathc1 is biased towards unprivileged group and hc2 is biased towards privileged group.
we have found that hc1 and hc2 both are using 646esec fse november8 virtualevent usa sumonbiswas andhrideshrajan .
.
.
.
.
.
gc1 gc2 gc3 gc4 gc5 gc6 gc7 gc8 ac1 ac2 ac3 ac4 ac5 ac6 ac7 ac8 bm1 bm2 bm3 bm4 bm5 bm6 bm7 bm8 hc1 hc2 hc3 hc4 hc5 hc6 hc7 hc8 tm1 tm2 tm3 tm4 tm5 tm6 tm7 tm8di spd eod aod erd cnt ti figure unfairness exhibited by theml models with respectto differentmetrics light gradient boost lgb model for prediction.
the code for buildingthe twomodels are model hc1 2hc1 lgb .
lgbmclassifier n estimators objective binary class weight balanced learning rate .
reg alpha .
reg lambda .
subsample .
n jobs random state 3hc1.
fit x train y train eval metric auc categorical feature cat indices verbose model hc2 5hc2 lgbmclassifier n estimators learning rate .
num leaves colsample bytree .
subsample .
max depth reg alpha .
reg lambda .
min split gain .
min child weight silent verbose 6hc2.
fit x train y train eval metric auc verbose we have executed both the models with varied hyperparameter combinations and found that class weight balanced is causing hc1 not to be biased towards privileged group.
by specifyingclass weight we can set more weight to the data items belongingtoan infrequentclass.higherclassweightimplies that thedataitemsaregettingmoreemphasisinprediction.whenthe class weight is set to balanced the model automatically accounts forclassimbalanceandadjusttheweightofdataitemsinversely proportionaltothefrequencyoftheclass .inthiscase hc1 mitigates the male female imbalance in its prediction.
therefore it does not exhibit bias towards the privileged group male .
on the other hand hc2 has less bias but it is biased towards privilegedgroup.althoughwewantmodelstobefairwithrespectto allgroupsandindividuals trade offmightbeneededandinsome cases biastowardunprivilegedmaybe adesirable trait.
wehaveobservedthat class weight hyperparameterinlgbmclassifierallowsdeveloperstocontrolgroupfairnessdirectly.however the library documentation of lgb classifier suggests that this parameterisusedforimprovingperformanceofthemodels .
though the library documentation mentions about probability calibration of classes to boost the prediction performance using this parameter however there is no suggestion regarding the effect on the biasintroduceddueto the wrongchoiceofthis parameter.
.
.
.
.
.
.
.
.
gc1 gc2 gc3 gc4 gc5 gc6 gc7 gc8 ac1 ac2 ac3 ac4 ac5 ac6 ac7 ac8 bm1 bm2 bm3 bm4 bm5 bm6 bm7 bm8 hc1 hc2 hc3 hc4 hc5 hc6 hc7 hc8 tm1 tm2 tm3 tm4 tm5 tm6 tm7 tm8 german credit adult census bank marketing home credit titanic mlaccuracy f1 score total bias figure cumulative bias andperformanceofthemodels fromthediscussions wecanconcludethatlibrarydevelopers still do not provide explicit ways to control fairness of the models.
although someparameters directlycontrol the fairness ofthe models libraries donot explicitly mentionthat.finding standardizingfeaturesbeforetrainingmodels canhelptoremovedisparitybetweengroupsintheprotected class.
fromfigure 3andfigure weobservethatexceptbm5 othermodels in bank marketing exhibit similar unfairness.
bm5 is a support vector classifier svc tuned using a grid search over given range of parameters.
in the modeling pipeline before training the best found svc the features are transformed using standardscalar .
below is the model construction code for bm5 with the best found hyperparameters 1tuned parameters gamma c 2svc gridsearchcv svc tuned parameters cv scorin g precision best found svc after grid search svc c break ties false cache size class weig ht none coef0 .
decision function shape ovr degree ga mma .
kernel rbf max iter probability true random state none shrinking true tol .
5model make pipeline standardscaler svc 6mdl model .
fit x train y train we have found that the usage of standardscalar in the model pipelineiscausingthemodelbm5tobefairer.especiallydiofbm5 is0.14whereas themeanofothersevenbmmodelsisveryhigh .
.standardscalar transformsthedatafeaturesindependently so that the mean value becomes and the standard deviation becomes1.essentially ifafeaturehasvarianceinordersofmagnitude thananotherfeature themodelmightlearnfromthedominating feature more which might not be desirable .
in this case bank marketing dataset has features among which has mean close to .
however ageis the protected attribute having a mean value .
older younger since the number of older is significantlymorethan younger.
therefore ageisthedominating feature in these bm models.
bm5 mitigates that effect by using standard scaling to all features.
therefore balancing the importanceofprotectedfeaturewithotherfeaturescanhelptoreduce bias in the models.
this example also shows the importance of understanding the underlying properties of protected features and theireffectiveness onprediction.
finding4 droppingafeaturefromthedatasetcanchange the modelfairnesseffectively.
both themodels ac5 and ac6 are using xgb classifier for prediction but ac6 is fairer than ac5.
among the metrics in terms of consistency cnt ac5 shows bias .
times more than ac6.
we have investigated the model construction and found that ac5 and ac6 differ in three constructs features used in the model number 647dothe machinelearning modelsonacrowdsourcedplatform exhibit bias?
anempirical studyonmodel fairness esec fse november8 virtualevent usa oftreesusedintherandomforest andlearningrateoftheclassifier.
wehaveobservedthatthenumberoftreesandlearningratedid not change the bias of the models.
in ac5 the model excluded one feature from the training data.
bank marketing dataset contains personalinformationaboutindividualsandpredictswhetherthe personhasanannualincomemorethan50kdollarsornot.inac5 the model developer dropped one feature that contains number of yearsofeducation sincethereisothercategoricalfeaturewhich represents education of the person e.g.
bachelors doctorate etc.
.
ac6 is using all the features in the dataset.
cnt measures the individual fairness of the modelsi.e.
how two similar individuals not necessarilyfromdifferentgroupsofprotectedattributeclass are classifiedtodifferentoutcomes.therefore droppingthenumber ofyearsofeducationiscausingthemodeltoclassifysimilarindividualstodifferentoutcome whichinturngeneratingindividual unfairness.
finding5 differentmetricsareneededtounderstandbias indifferentmodels.
from figure we can see thatthe models show different patterns ofbiasintermsofdifferentfairnessmetrics.forexample compared to any bank marketing models bm5 has disparity impact di less than half but the error rate difference erd more than twice.
if themodeldeveloperonlyaccountsfordi thenthemodelwould appearfairerthanwhatitactuallyis.asanotherexample gc6is fairer than of all the models in terms of total bias but if we only consider consistency cnt gc6 is fairer than only of all the models.
however previous studies show that achieving fairness with respect to all the metrics is difficult and for some pair of metrics mathematically impossible .
also the definition offairnesscanvary dependingontheapplicationcontextandthe stakeholders.therefore itisimportanttoreportoncomprehensive set of fairness measures and evaluate the trade off between the metricstobuildfairer.wehaveplottedthecorrelationbetweendifferentmetricsfromtwodatasetsinfigure .afewmetricpairshave a similar correlation in both the datasets such as spd eod spd aod .
this is understandable from the definitions of these metrics because they are calculated using same or correlated group conditioned rates true positives and false positives .
although there are many metric pairs which are positively or negatively correlated there is no pattern in correlation values between the two datasets.
forinstance cntandtiarehighlynegativelycorrelatedingerman credit models but positively correlated in titanic ml models.
therefore we need a comprehensive set of metrics to evaluate fairness.
finding6 except di eod and aod all the fairness measuresremainconsistentovermultipletrainingandprediction.
tomeasurethestabilityofthefairnessandperformancemetrics wehavecomputedthestandarddeviationofeachmetricover10 runssimilarto .ineachrun thedatasetisshuffledbeforethe train test split and model is trained on a new randomized training set.wehaveseenthatthemodelsarestablefortheperformance metrics and most of the fairness metrics.
in particular the average german credit titanic ml figure corelation between the metrics.
bottom diagonal isforgermancreditmodels topdiagonalisfortitanicml models.
of the standard deviations of accuracy f1 score di spd eod aod erd cnt and ti over all the models are .
.
.
.
.
.
.
.
.
respectively.exceptfordi eodand aod the average standard deviation is very low less than .
.
for these three metrics we have plotted the standard deviations in figure6.wecanseethatthetrendofstandarddeviationsissimilar tothemodelsofaspecificdataset.inourbenchmark thelargest dataset is home credit which has the lowest standard deviation andthesmallestdatasetistitanicml whichhasthemost.sincein largerdataset evenaftershufflingthetrainingdataremains more consistent the deviation is less.
on the other hand the titanic ml datasetis the smallest insize having data instances.the class distribution of data instances do not remain consistent when a randomtrainingsetischosen.therefore whiledealingwithsmaller datasets it is important to choose a training set that represents the originaldata andevaluate fairnessmultiple times.
model00.
.
.
.
gc1gc2gc3gc4gc5gc6gc7gc8 ac1ac2ac3ac4ac5ac6ac7ac8 bm1bm2bm3bm4bm5bm6bm7bm8hc1hc2hc3hc4hc5hc6hc7hc8 tm1tm2tm3tm4tm5tm6tm7tm8di eod aod figure standard deviation of the metrics di eod and aod over multiple experiments.
other metrics have very low standard deviation.
di has more standard deviation than other metrics.
di is computedusingtheratiooftwoprobabilities pu pp wherepuisthe probability of unprivileged group getting favorable label and pp is the probability of privileged group getting favorable label.
even theprobabilitydifferenceisverylow thevalueofdicanbevery high.therefore di fluctuatesmore frequently thanothermetrics.
648esec fse november8 virtualevent usa sumonbiswas andhrideshrajan table unfairness measuresofthemodels before andafter the mitigations beforemitigation aftermitigationmodelaccf1dispdeodaoderdcnt tiaccf1dispdeodaoderdcnt tirank gc1 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.058raod pce gc2 xgb .
.
.
.
.
.
.
.
.
.
.
.
.057aord pce gc3 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.100ar dpoce gc4 svc .
.
.
.
.
.
.
.
.
.
.
.
.057aord pec gc5 evc .
.
.
.
.
.
.
.
.
.
.
.
.058aord pec gc6 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.121rd apceo gc7 xgb .
.
.
.
.
.
.
.
.
.
.
.
.057adr pocegerman credit sex gc8 knn .
.
.
.
.
.
.
.
.
.
.
.057ar dpcoe ac1 lrg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.040orcdap e ac2 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.232aroc dpe ac3 gbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.120roac dpe ac4 cbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.082orac pde ac5 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.111roac pde ac6 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.078orac pde ac7 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.187aorcd peadultcensus race ac8 dct .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.121roac dpe bm1 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.074rocpd ea bm2 lgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.066ordc pae bm3 gbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.089078codr ape bm4 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.111rca opde bm5 svc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.136ercdo ap bm6 gbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.065ocrd pae bm7 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.085codra pebank marketing age bm8 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.073oracdp e hc1 lgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.030apecr od hc2 lgb .
.
.
.
.
.
.
.
.
.
.
.
.
.084pecroa d hc3 gnb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0oa decpr hc4 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0cedrp oa hc5 cbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.001acepr do hc6 cbc .
.
.
.
.
.
.
.
.
.
.
.
.
.056acper do hc7 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0ecpr doahome credit sex hc8 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.001cprd aeo tm1 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.189oaerdp c tm2 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.223oaerdp c tm3 ebg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.223oaerd pc tm4 lrg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.207oaeprd c tm5 gbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.205oaerd cp tm6 xgb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.190oaerd cp tm7 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.219oaerdp ctitanicml sex tm8 rft .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.172oaerd pc experiment has been conducted for multiple protected attributes.
rft randomforest xgb xgboost svc support vector classifier evc ensemblevoting classifier knn k nearest neighbors lrg logistic regression gbc gradient boosting classifier cbc cat boost classifier dct decision tree lgb light gradient boost gnb gaussian naive bayes ebg ensemblebagging.
mitigation techniquesapplied to the models areas follows.
resultisshown for the best mitigation.
rank of mitigation uses acronymbelow mitigations before havebeen able to mitigate bias rest havenot.
reweighing r diremover d adversarial debiasing a prejudiceremover p equalized odds e calibrated equalized odds c reject optionclassification o finding7 afairmodelwithrespecttooneprotectedattributeisnotnecessarilyfairwithrespecttoanotherprotected attribute.
to understand the behavior of the same models on different protectedattributes wehaveanalyzedthefairnessofgermancredit and adult censusmodels ontwo protected attributes.
in figure we have plotted the fairness measures of german credit models onsexandageand adult census models on sexandrace.
we have foundthatthemodelscanshowdifferentfairnesswhendifferent protectedattributeisconsidered.thetotalbiasexhibitedbygermancredit dataset are for sexattribute .82andfor ageattribute .
.foradultcensus thetotalbiasare for sexattribute15.15and forraceattribute .
.
however most of the models exhibit similar trendofdifferenceinthefairnesswhenconsideringtwodifferent attributes.
gc1 and gc6 show cumulative bias .
and .
when sexis considered.
surprisingly gc1 and gc6 shows cumulative bias .85and .
when ageis considered.
gc1 is much fairer model than gc6inthefirstcasebutinthesecondcase thefairnessisalmost similar.
we have discussed the behavior of these two models in finding1andexplainedhowgc1isfairerwhen sexistheprotected attribute.however thefairprediction does not persist for the age becausethereisnoimbalanceingermancreditwithrespectto age groups.
therefore gc1 and gc6 showsimilar fairness when ageis considered.
.
.
.
.
.
sex age sex age sex age sex age sex age sex age sex age sex age sex race sex race sex race sex race sex race sex race sex race sex racegc1gc2gc3gc4gc5gc6gc7gc8ac1ac2ac3ac4ac5ac6ac7ac8 dispdeodaoderdcntti figure7 fairnessofmlmodelswithrespecttodifferentprotected attributes 649dothe machinelearning modelsonacrowdsourcedplatform exhibit bias?
anempirical studyonmodel fairness esec fse november8 virtualevent usa mitigation inthissection wehaveinvestigatedthefairnessresultsofthemodels after applying bias mitigation techniques.
we have employed7 differentbiasmitigationalgorithmsseparatelyon40modelsand comparedthefairnessresultswiththeoriginalfairnessexhibitedby themodels.foreachmodel wehaveselectedthemostsuccessful mitigationalgorithmandplottedthefairnessvaluesaftermitigation in figure .
we have observed that similar to figure the fairness patterns are similar for the models in a dataset.
di spd andcntare the mostdifficult metrics to mitigate.
to understand the root causes of unfairness we have focused on the models which exhibit more or less bias and then investigatedtheeffectsofdifferentmitigationalgorithms.here among the mitigation algorithms the preprocessing techniques operate on the training data and retrain the original model to remove bias.
ontheotherhand post processingtechniquesdonotchangethe training data or original model but change the prediction made by the model.
the in processing techniques do not alter the dataset or predictionresultbutemploy completelynew modeling technique.
finding8 models with effective preprocessing mitigation techniqueispreferable thanothers.
we have found that reweighing algorithm has effectively debiased many models gc1 gc6 ac3 ac5 ac8 bm1 and bm4.
these models produce fairer results when the dataset is pre processed usingreweighing.inotherwords thesemodelsdonotpropagate bias themselves.
in other cases where pre processing techniques arenoteffective wehadtochangethemodeloraltertheprediction which implies that bias is induced or propagated by the models.
another advantage is that in these models after mitigations the modelshave retainedtheaccuracyand f1score.othermitigation techniques often hampered the performance of the model.
for a few other models gc3 gc8 ac1 ac2 ac4 ac6 bm2 bm5 bm8 reweighing has been the second most successful mitigation algorithm.
among these models in ac1 ac2 bm2 and bm5 the mostsuccessfulalgorithmtomitigatebiaslossaccuracyorf1score at least .
in all of these cases reweighing has retained both accuracyandf1 score.
finding9 models with more bias are debiased effectively bypost processingtechniques whereasoriginallyfairermodelsaredebiasedeffectivelybypreprocessingorin processing techniques.
from table we can see that out of models are debiased by one of the three post processing algorithms i.e.
equalized odds eo calibrated equalized odds ceo and reject option classifier roc .thesealgorithmshavebeenabletomitigatebias not necessarilythemostsuccessful in90 ofthemodels.especially rocand ceoarethedominantpost processingtechniques.roc takes the model prediction and gives the favorable outcome to the unprivilegedgroupandunfavorableoutcometoprivilegedgroup with a certain confidence around the decision boundary .
ceo takestheprobabilitydistribution score generatedby theclassifier andfindtheprobabilityofchangingoutcomelabelandmaximizeequalizedodds .eoalsochangestheoutcomelabelwithcertainprobabilityobtainedbysolvingalinearprogram .wehave foundthatthesepost processingmethodshavebeenabletomitigate bias more effectively when the original model produces more biased results.
from figure we can see that the most biased models are tm4 tm8 tm5 tm1 hc7 where the post processing hasbeenthemostsuccessfulalgorithms.onthecontrary incase ofthe5leastbiasedmodel gc1 gc8 bm5 gc6 gc3 ratherthan mitigating allthree post processing techniques increasedbias.
in table2 we have shown the rank of mitigation algorithms todebiaseachmodel.intable wehaveshownthemeanofthe ranksofeachmitigationalgorithms whererankofmostsuccessful algorithmis1andleastis7.wecanseethatformostbiasedmodels reject option classification and equalized odds have been more successful than all others.
for the least biased models both preprocessing algorithmsand adversarial debiasing have been effective andthe post processing algorithms have been ineffective.
table3 meanrankofeachbiasmitigationalgorithmfor10 least biased models lbm most biased models mbm andoverall.
stage algorithms lbmmbm all reweighing r .
.
.03preprocessingdisparateimpactremover d .
.
.
adversarial debiasing a .
3in processingprejudiceremover regularizer p .
.
.
equalized odds e .
.
.
calibrated equalized odds c .
.
.
post processing reject optionclassification o .
.
.
impact while mitigating bias there is a chance that the performance of the model is diminished.
the most successful algorithm in debiasinga modeldoes not always give good performance.
so often the developers have to trade off between fairness and performance.
in this section we have investigatedthe answerto rq3.
whatare the impactswhenthebiasmitigationalgorithmsareappliedtothemodels?wehaveanalyzedtheaccuracyandf1scoreofthemodelsafter applying the mitigation algorithms.
first for each model we have analyzedthe impactsof the most effectivemitigationalgorithmsin removing bias.
in figure we have plotted the change in accuracy f1score andtotalbiaswhenthemostsuccessfulmitigatingalgorithms are applied.
we can see that while mitigating bias many models are losing their performance.
from table pre processing algorithms especiallyreweighinghasbeenthemosteffectivein modelgc1 gc6 ac3 ac5 ac8 bm1 andbm3.fromfigure thesemodels alwaysretain their performance after mitigation.
finding10 whenmitigatingbiaseffectively in processing mitigationalgorithmsshowuncertainbehaviorintheirperformance.
amongin processingalgorithms adversarialdebiasinghasbeen themosteffectivein11models gc2 gc3 gc4 gc5 ac2 ac7 hc1 hc5 hc6 andprejudiceremoverhasbeenthemosteffective in1model hc2 .wehavefoundthatforgermancreditmodelsadversarialdebiasinghasbeeneffectivewithoutlosingperformance.
650esec fse november8 virtualevent usa sumonbiswas andhrideshrajan figure the fairness exhibited by the models after applying the bias mitigation techniques.
the color coding in table 2is used to denote themostsuccessfulmitigationalgorithm foreachmodel.
.
.
.
.
.
.
.
.
gc1gc2gc3gc4gc5gc6gc7gc8 ac1ac2ac3ac4ac5ac6ac7ac8 bm1bm2bm3bm4bm5bm6bm7bm8hc1hc2hc3hc4hc5hc6hc7hc8 tm1tm2tm3tm4tm5tm6tm7tm8accuracy f1 score bias figure change of performance and bias after applying bias mitigation technique negative value indicates reduction butinother cases ac1 ac7 hc1 andhc7 the accuracyhas decreasedatleast21.
.inhc2 prejudiceremoveralsolosesf1score whilemitigatingthebias.since in processingtechniquesemploy newmodelandignorethepredictionoftheoriginalmodel inallsituations datasetandtask itisnotgivingbetterperformance.inour evaluation adversarial debiasing is giving good performance with german credit dataset but not on adult census or home credit dataset.
therefore in processing techniques are not appropriate whenwecannotchangetheoriginalmodeling.also sincethese techniques are uncertain in retaining performance the developers shouldbecarefulabouttheaccuracyofpredictionaftertheintervention.
finding11 althoughpost processingalgorithmsarethe mostdominatingindebiasing theyarealwaysdiminishing the modelaccuracyandf1 score.
from table we can see that in out of models one of the post processingalgorithmsarebeingthemostsuccessful.butinall of the cases they are losing performance.
the average accuracy reduction in these models is .
and average f1 decrease is .
.
forexample inac1 themostbiasmitigatingalgorithmisreject optionclassificationbutthemodelisloosing26.
accuracyand f1 score.
in these cases developers should move to the next best mitigation algorithm.
in a few other cases such as hc8 the rejectoptionclassificationmitigatesbiaswithonly1.
lossinaccuracyand1.
lossinf1score.insuchsituations post processing techniques can be appliedto mitigate the bias.
finding trade offbetweenperformanceandfairness exists andpost processingalgorithmshavemostcompetitive replacement.since some most mitigating algorithms are having performance reduction foreachmodel wehavecomparedthemostsuccessful algorithm with the next best mitigation algorithm in figure .
we have found that for out of models the performance of the 2nd ranked algorithm is same or better than the 1st ranked algorithm.
among them in ac4 ac6 bm5 hc5 and hc8 the 2nd ranked algorithm has bias very close not more than .
to the 1st ranked one.
all of these except hc5 the 1st ranked bias mitigation algorithmisa post processingtechnique.we observe that competitive alternative mitigation technique is more common forpost processingmitigationalgorithms.therefore ifweincrease the tolerable range of bias then other mitigation techniques would be betteralternative interms of performance.
.
.
.
gc1gc2gc3gc4gc5gc6gc7gc8 ac1ac2ac3ac4ac5ac6ac7ac8 bm1bm2bm3bm4bm5bm6bm7bm8hc1hc2hc3hc4hc5hc6hc7hc8 tm1tm2tm3tm4tm5tm6tm7tm8accuracy f1 score bias figure change of performance and bias between the 1st and 2nd most successful mitigation algorithms negative value indicates reduction threats to validity benchmarkcreation.
toavoidexperimentingonlow qualitykernels wehave onlyconsideredthekernelswith morethan5votes.
inaddition wehaveexcludedthekernelswherethemodelaccuracyisverylow lessthan65 .finally wehaveselectedthetop voted ones from the list.
we have also verified that the collected kernelsarerunnable.toensurethemodelscollectedfromkaggle are appropriate for fairness study we have first selected the fairnessanalysisdatasetsfrompreviousworksandsearchedmodels for those datasets.
finally we have searched competitions that use dataset withprotectedattributes usedinthe literature.
fairnessandperformanceevaluation.
ourcollectedmodelsgive the same performance as mentioned in the corresponding kaggle kernels.forevaluatingfairnessandapplyingmitigationalgorithms we have used aif toolkit developed by ibm.
bellamy et al.presentedfairnessresults 4metrics fortwomodels logistic regressionandrandomforest onadultcensusdatasetwithprotected attribute race .
we havedone experimentwiththesame setupandvalidatedourresult .similarto foreachmetric 651dothe machinelearning modelsonacrowdsourcedplatform exhibit bias?
anempirical studyonmodel fairness esec fse november8 virtualevent usa we have evaluated times and taken the mean of the values.
the stability comparison ofthe results isshownin .
fairness comparison.
as different metrics are computed based ondifferentdefinitionsoffairness wehavecomparedbiasusing a specific metric or cumulatively.
finally in this paper we have focusedoncomparingfairnessofdifferentmodels.therefore for each dataset we followed the same method to pre process training andtestingdata.
related works seforfairnessinml.
thislineofworkistheclosesttoourwork.
fairtest proposes methodology to detect unwarranted featureassociationsandpotentialbiasesinadatasetusingmanually written tests.
themis generates random tests automatically to detectcausal fairness using black box decision makingprocess.
aequitas is a fully automated directed test generation module togeneratediscriminatoryinputsinmlmodels whichcanbeused tovalidateindividualfairness.fairml introducesanorthogonal transformationmethodologytoquantifytherelativedependence ofblack boxmodelstoitsinputfeatures withthegoalofassessing fairness.a morerecentwork proposes black boxfairnesstestingmethodtodetectindividualdiscriminationinmlmodels.they propose a test case generation algorithm based on symbolic execution and local explainability.
the above works have proposed noveltechniquestodetectandtestfairnessinmlsystems.however we have focused on empirical evaluation of fairness in ml models written by practitioners and reported our findings.
friedler et al.also worked on an empirical study but compared between fairness enhancing interventions and not models .
harrison etal.conductedsurveybasedempiricalstudytounderstandhow fairnessofdifferentmodelsisperceivedbyhumans .holstein et al.also conducted survey on industry developers to find the challenges for developing fairness aware tools and models .
however noempiricalstudyhasbeenconductedtomeasureand comparefairnessofmlmodelsinpractice andanalyzetheimpacts ofmitigationalgorithms onthe models.
fairness measure and algorithms.
the machine learning communityhasfocusedonnoveltechniquestoidentify measureand mitigate bias .
this body of work concentrate on the theoretical aspects of bias in ml classifiers.
differentfairnessmeasuresandmitigationalgorithmshavebeen discussed in .3and 3. .
in this work we have focused on the software engineeringaspectsofml models usedinpractice.
mlmodeltesting.
deepcheck proposeslightweightwhitebox symbolic analysis to validate deep neural networks dnn .
deepxplore proposes a white box framework to generate test input that can exploit the incorrect behavior of dnns.
deeptest usesdomain specificmetamorphicrelationstodetecterrorsin dnn based software.
these works have focused on the robustness propertyofmlsystems whereaswehavestudiedfairnessproperty that isfundamentallydifferentfrom robustness .
conclusion ml fairness has received much attention recently.
however ml libraries do not provide enough support to address the issue in practice.inthispaper wehaveempiricallyevaluatedthefairnessofmlmodels anddiscussed ourfindings ofsoftware engineering aspects.
first we have created a benchmark of ml models from different problem domains.
then we have used a comprehensive set of fairness metrics to measure fairness.
after that we have applied7mitigationtechniquesonthemodelsandcomputedthe fairness metric again.
we have also evaluated the performance impactofthemodelsaftermitigationtechniquesareapplied.we have found what kind of bias is more common and how they could be addressed.
our study also suggests that further se research and library enhancements are neededto make fairnessconcernsmore accessible to developers.